
# Simulating Sampling Distributions


Usually when we collect biological data, it's because we're trying to learn  about some underlying "population" of interest.  Population here could refer to an actual population (e.g. all males over 20 in the United States; brushtail possums in the state of Victoria, Australia), an abstract population (e.g. corn plants grown from Monsanto "round up ready" seed; yeast cells with genotypes identical to the reference strain S288c), outcomes of a stochastic process we can observe and measure (e.g. meiotic recombination in flies; hadrons detected at the LHC during a particle collision experiment), etc.

It is often impractical or impossible to measure all objects/individuals in a population of interest, so we take a **sample** from the population and make measurements on the variables of interest in that sample.  We do so with the hope that the various statistics we calculate on the variables of interest in that sample  will be useful estimates of those same statistics in the underlying population. 

However, we must always keep in mind that the statistics we calculate from our sample will almost never exactly match those of the underlying population.  That is when we collect a sample, and measure a statistic (e.g. mean) on variable X in the sample, there is a degree of *uncertainty* about how well our estimate matches the true value of that statistic for X in the underlying population.

_Statistical inference_ is about quantifying the uncertainty associated with statistics and using that information to test hypotheses and evaluate models.

Today we're going to review a fundamental concept in statistical inference, the notion of a _sampling distribution_ for a statistic of interest.  A sampling distribution is the probability distribution of a given statistic for samples of a given size. Traditionally sampling distributions were derived analytically. In this class session we'll see how to approximate sampling distributions for any a statistic using computer simulation.


## Libraries

```{r, warning=FALSE,message=FALSE}
library(tidyverse)
library(magrittr)
library(stringr)
```


## Data set: Simulated male heights

To illustrate the concept of sampling distributions, we'll use a simulated data set to represent the underlying population we're trying to estimate statistics for.  This will allow us to compare the various statistics we calculate and their sampling distributions to their "true" values.

Let's simulate a population consisting of 25,000 individuals with a single trait of interest -- height (measured in centimeters).  We will simulate this data set based on information about the distribution of the heights of adult males in the US  estimated in a study carried out from 2011-2014 by the US Department of Health and Human Services[^1].

[^1]: US Dept. of Health and Human Services; et al. (August 2016). "Anthropometric Reference Data for Children and Adults: United States, 2011â€“2014" (PDF). National Health Statistics Reports. 11. https://www.cdc.gov/nchs/data/series/sr_03/sr03_039.pdf

### Seeding the pseudo-random number generator

When carrying out simulations, we employ random number generators (e.g. to choose random samples). Most computers can not generate true random numbers -- instead  they use algorithms that approximate the generation of random numbers (pseudo-random number generators). One important difference between a true random number generator and a pseudo-random number generator is that we can regenerate a series of pseudo-random numbers if we know the "seed" value that initialized the algorithm. We can specifically set this seed value, so that we can guarantee that two different people evaluating this notebook get the same results, even though we're using (pseudo)random numbers in our simulation.

```{r}
# make our simulation repeatable by seeding RNG
set.seed(20180321)
```

### Generating the simulated population

Having seeded our RNG, we then generate our simulated population using the `rnorm()` function. `rnorm()` draws random values from a normal distribution with the given parameters of mean and standard deviation. We didn't have to use a normal distribution for our simulations, but many biological variables are approximately normally distributed, so this is a common assumption.

```{r}
# male mean height and sd in centimeters from USDHHS report
mean.ht <- 175.7
sd.ht <- 15.19

height.data <- data_frame(height = rnorm(25000, mean = mean.ht, sd = sd.ht))
```



### Properties of the simulated population

Let's take a moment to visualize the distribution of heights in our population of interest:

```{r}
ggplot(height.data, aes(x = height)) +
  geom_histogram(aes(y = ..density..), bins=50, alpha=0.5) + 
  geom_density() +
  labs(x = "Height (cm)", title = "Distribution of Heights in the Population of Interest")
```

When we generated this simulated population, I specified that the heights should be drawn from a normal distribution.  As one would expect, the histogram and density plot look fairly close to the classic bell-shaped curve of a normal distribution.

Let's turn some key summary statistics of the population. The values of these summary statistics are those we're trying to estimate from our sample(s).  We'll refer to these as the "true" values.

```{r}
true.values <- 
  height.data %>% 
  summarize(mean.height = mean(height), 
            sd.height = sd(height))

true.values
```

In mathematical notation our population is normally distributed with a mean height of `r mean(height.data$height)`cm and standard deviation of `r sd(height.data$height)`cm. Note that these values are close to, but not exactly the parametes we passed to `rnorm()`.


## Random sampling from the simulated population

Let's simulate the process of taking a single sample of 30 individuals from our population, using the `dplyr::sample_n()` function:

```{r}
sample.a <-
  height.data %>%
  sample_n(30)
```

Now we'll create a histogram of the height variable in our sample. For reference we'll also plot the histogram for the true population in the background (but remember, in the typical case you don't know what the true population looks like)

```{r}
sample.a %>%
  ggplot(aes(x = height)) + 
  geom_histogram(data=height.data, aes(x = height, y = ..density..), alpha=0.25, bins=50) + 
  geom_histogram(aes(y = ..density..), fill = 'steelblue', alpha=0.75, bins=9) + 
  geom_vline(xintercept = true.values$mean.height, linetype = "solid") + 
  geom_vline(xintercept = mean(sample.a$height), linetype = "dashed") + 
  labs(x = "Height (cm)", y = "Density", 
       title = "Distribution of heights in the underlying population (grey)\nand a single sample of size 30 (blue)")
```

The solid vertical line represent the true mean of the population, the dashed line represents the sample mean.  Comparing the two distributions we see that while our sample of 30 observations is relatively small,its location (center) and spread that are roughly similar to those of the underlying population.

Let's create a table giving the estimates of the mean and standard deviation in our sample:

```{r}
sample.a %>% 
  summarize(sample.mean = mean(height), 
            sample.sd = sd(height))
```

Based on our sample, we estimate that the mean height of males in our population of interest is `r mean(sample.a$height)`cm with a standard deviation of `r sd(sample.a$height)`cm. 

### Another random sample

Let's step back and think about our experiment. We took a random sample of 30 indiviuals from the population. The very nature of a "random sample" means we could just as well have gotten a different collection of individuals in our sample.  Let's take a second random sample of 25 individuals and see what the data looks like this time:

```{r}
sample.b <-
  height.data %>%
  sample_n(30)

ggplot(sample.b, aes(x = height)) + 
  geom_histogram(data=height.data, aes(x = height, y = ..density..), alpha=0.25, bins=50) + 
  geom_histogram(aes(y = ..density..), fill = 'steelblue', alpha=0.75, bins=9) +
  geom_vline(xintercept = true.values$mean.height, linetype = "solid") + 
  geom_vline(xintercept = mean(sample.b$height), linetype = "dashed")  +
  labs(x = "Height (cm)", y = "Density", 
       title = "Distribution of heights in the underlying population (grey)\nand a single sample of size 30 (blue)")  
```

```{r}
sample.b %>% 
  summarize(sample.mean = mean(height), 
            sample.sd = sd(height))
```

This time we estimated the mean height to be `r mean(sample.b$height)` cm and the standard deviation to be `r sd(sample.b$height)` cm.  


### Simulating the generation of many random samples

When we estimate population parameters, like the mean and standard deviation, based on a *sample*, our estimates will differ from the true population values by some amount. Any given random sample might provide better or worse estimates than another sample. 

We can't know how good our estimates of statistics like the mean and standard deviation are from any specific sample, but we we can study the behavior of such estimates _across many simulated samples_ and learn something about how well our estimates do on average, as well the spread of these estimates.


### A function to estimate statistics of interest in a random sample

First we're going to write a function called `rsample.stats` that to carries out the following steps:

* Given a data frame `x`
* Take a random sample of size `n`
* For the variable specified by the character string, `var.name`, calculate the mean and standard deviation of that variable in the random sample
* Return a table giving the sample size, sample mean, and sample standard deviation, represented as a data frame


```{r}
rsample.stats <- function(x, n, var.name) {
  sample_x <- sample_n(x, n)
  data_frame(sample.size = n, 
             sample.mean = mean(sample_x[[var.name]]), 
             sample.sd = sd(sample_x[[var.name]]))
}
```

Let's test `rsample.stats`:

```{r}
rsample.stats(height.data, 30, "height")
```




### Generating statistics for many random samples

Now we'll see how to combine `rsample.stats` with two additional functions to repeatedly run the `rsample.stats` function:


```{r}
df.samples.of.30 <-
  rerun(1000,  rsample.stats(height.data, 30, "height")) %>%
  bind_rows()
```

The function `rerun` is defined in the `purrr` library (automatically loaded with tidyverse).  `purrr:rerun()` re-runs an expression(s) multiple times. The first argument to `rerun()` is the number of times you want to re-run, and the following arguments are the expressions to be re-run.  Thus the second line of the code block above re-runs the `rsample.stats` function 1000 times using `height.data` as the input,  generating sample statistics for samples of size 30 each time it's run. `rerun` returns a list whose length is the specified number of runs.

The third line includes a call the `dplyr::bind_rows()`.  This simply takes the list that `rerun` returns and collapses the list into a single data frame.  `df.samples.of.30` is thus a data frame in which each row gives the sample size, sample mean, and sample standard deviation for a random sample of 30 individuals drawn from our underlying population (`height.data`).

```{r}
df.samples.of.30
```

## Simulated sampling distribution of the mean 

Let's review what we just did:

* We generated 1000 samples of size 30
* For each of the samples we calculated the mean and standard deviation of the height variable _in that sample_
* We combined each of those estimates of the mean and standard deviation into a data frame

The 1000 estimates of the mean we generated represents a new distribution -- what we will call a **sampling distribution of the mean for samples of size 30**.  Let's plot this sampling distribution:


```{r}
ggplot(df.samples.of.30, aes(x = sample.mean, y = ..density..)) +
  geom_histogram(bins=25, fill = 'firebrick', alpha=0.5) + 
  geom_vline(xintercept = true.values$mean.height, linetype = "dashed") + 
  labs(x = "Sample means", y  = "Density",
       title = "Distribution of mean heights for 1000 samples of size 30")

```

This particular sampling distribution of the mean is a probability distribution that we can use to estimate the probability that a sample mean falls within a given interval, assuming our sample is a random sample of size 30 drawn from our underlying population.

From our visualization, we see that the distribution of sample mean heights is approximately centered around the true mean height.  Most of the sample estiamtes of the mean height are within 5 cm of the true population mean height (175.6cm), but a small number of estimates of the sample mean as off by nearly 10cm. 

Let's make this more precise by calculating the mean and standard deviation of the sampling distribution of means (I included the min and max as well).

```{r}
df.samples.of.30 %>%
  summarize(mean.of.means = mean(sample.mean),
            sd.of.means = sd(sample.mean),
            min.of.means = min(sample.mean),
            max.of.means = max(sample.mean))

```


### Sampling distributions for different sample sizes

In the example above we simulated the sampling distribution of the mean for samples of size 30.  How would the sampling distribution change if we increased the sample size?  In the next code block we generate sampling distributions of the mean (and standard deviation) for samples of size 50, 100, 250, and 500.


```{r}
df.samples.of.50 <-
  rerun(1000,  height.data %>% rsample.stats(50, "height")) %>%
  bind_rows()

df.samples.of.100 <-
  rerun(1000,  height.data %>% rsample.stats(100, "height")) %>%
  bind_rows()

df.samples.of.250 <-
  rerun(1000,  height.data %>% rsample.stats(250, "height")) %>%
  bind_rows()

df.samples.of.500 <-
  rerun(1000,  height.data %>% rsample.stats(500, "height")) %>%
  bind_rows()
```

To make plotting and comparison easier we will combine each of the individual data frames, representing the different sampling distributions for samples of a given size, into a single data frame. 

```{r}
df.combined <- 
  bind_rows(df.samples.of.30,
            df.samples.of.50,
            df.samples.of.100,
            df.samples.of.250,
            df.samples.of.500) %>%
  # create a factor version of sample size to facilitate plotting
  mutate(sample.sz = as.factor(sample.size))
```

We then plot each of the individual sampling distributions, faceting on sample size.


```{r, fig.width = 15, fig.height = 3.5}
ggplot(df.combined, aes(x = sample.mean, y = ..density.., fill = sample.sz)) + 
  geom_histogram(bins=25, alpha=0.5) + 
  geom_vline(xintercept = true.values$mean.height, linetype = "dashed")  +
  facet_wrap(~ sample.sz, nrow = 1) + 
  scale_fill_brewer(palette="Set1") + # change color palette
  labs(x = "Sample means", y  = "Density",
       title = "Distribution of mean heights for samples of varying size")  
```

### Discussion of trends for sampling distributions of different sample sizes

The key trend we see when comparing the sampling distributions of the mean for samples of different size is that as the sample size gets larger, the spread of the sampling distribution of the mean becomes narrower around the true mean. This means that _as sample size increases, the uncertainty associated with our estimates of the mean decreases_.  

Let's create a table, grouped by sample size, to help quantify this pattern:

```{r}
sampling.distn.mean.table <-
  df.combined %>%
  group_by(sample.size) %>%
  summarize(mean.of.means = mean(sample.mean),
            sd.of.means = sd(sample.mean),
            min.of.means = min(sample.mean),
            max.of.means = max(sample.mean))

sampling.distn.mean.table
```


## Standard Error of the Mean

We see from the graph and table above that our estimates of the mean cluster more tightly about the true mean as our sample size increases. This is obvious when we compare the standard deviation of our mean estimates as a function of sample size.

The standard deviation of the sampling distribution of a statistic of interest is called the **Standard Error** of that statistic. Here, through simulation, we are approximating the **Standard Error of the Mean**.

When sample sizes are large (>30 observations), one can show mathematically that for normally distributed data the expected Standard Error of the Mean as a function of sample size is approximately:
$$
\mbox{Standard Error of Mean} \approx \frac{\sigma}{\sqrt{n}}
$$
where $\sigma$ is the population standard deviation (i.e. the "true" standard deviation), and $n$ is the sample size.

Let's compare that theoretical expectation to our simulated results:

```{r, fig.width=6}
se.mean.theory <- sapply(seq(10,500,10), 
                         function(n){ true.values$sd.height/sqrt(n) })

df.se.mean.theory <- data_frame(sample.size = seq(10,500,10),
                                std.error = se.mean.theory)

ggplot(sampling.distn.mean.table, aes(x = sample.size, y = sd.of.means)) +
   # plot standard errors of mean based on our simulations
  geom_point() +  
  # plot standard errors of the mean based on theory
  geom_line(aes(x = sample.size, y = std.error), data = df.se.mean.theory, color="red") +
  labs(x = "Sample size", y = "Std Error of Mean",
       title = "A comparison of theoretical (red line) and simulated (points) estimates of\nthe standard error of the mean for samples of different size")
```

We see that as sample sizes increase, the standard error of the mean decreases.  This means that as our samples get larger, our uncertainty in our sample estimate of the mean (our best guess for the population mean) gets smaller.

## Sampling Distribution of the Standard Deviation

Above we explored how the sampling distribution of the mean changes with sample size.  We can similarly explore the sampling distribution of any other statistic, such as the standard deviation, or the median, or the the range, etc.

Recall that when we drew random samples we calculated the standard deviation of each of those samples in addition to the mean.  This means we can immediately visualize the sampling distribution of the standard deviation as shown below:

```{r, fig.width = 15, fig.height = 3.5}
ggplot(df.combined, aes(x = sample.sd, y = ..density.., fill = sample.sz)) + 
  geom_histogram(bins=25, alpha=0.5) + 
  geom_vline(xintercept = true.values$sd.height, linetype = "dashed")  +
  facet_wrap(~ sample.sz, nrow = 1) + 
  scale_fill_brewer(palette="Set1") +
  labs(x = "Sample standard deviations", y  = "Density",
       title = "Sampling distribution of standard deviation of height for samples of varying size")  
```

The key trend we saw when examining the sampling distribution of the mean is also apparent for standard deviation -- bigger samples lead to tighter sampling distributions and hence less uncertainty in the sample estimates of the standard deviation.

As before, we summarize key statistics of the sampling distribution in a table:
```{r}
sampling.distn.sd.table <-
  df.combined %>%
  group_by(sample.size) %>%
  summarize(mean.of.sds = mean(sample.sd),
            sd.of.sds = sd(sample.sd),
            min.of.sds = min(sample.sd),
            max.of.sds = max(sample.sd))

sampling.distn.sd.table
```

For normally distributed data the expected Standard Error of the Standard Deviation (i.e. the standard deviation of standard deviations!) is approximately:

$$
\mbox{Standard Error of Standard Deviation} \approx \frac{\sigma}{\sqrt{2(n-1)}}
$$
where $\sigma$ is the population standard deviation, and $n$ is the sample size.

As before, let's visually compare the theoretical expectation to our simulated estimates.

```{r, fig.width=6}
se.sd.theory <- sapply(seq(10, 500, 10), 
                       function(n){ true.values$sd.height/sqrt(2*(n-1))})

df.se.sd.theory <- data_frame(sample.size = seq(10,500,10), 
                              std.error = se.sd.theory)

ggplot(sampling.distn.sd.table, aes(x = sample.size, y = sd.of.sds)) +
   # plot standard errors of mean based on our simulations
  geom_point() +  
  # plot standard errors of the mean based on theory
  geom_line(aes(x = sample.size, y = std.error), data = df.se.sd.theory, color="red") +
  labs(x = "Sample size", y = "Std Error of Standard Deviation",
       title = "A comparison of theoretical (red line) and simulated (points) estimates of\nthe standard error of the standard deviation for samples of different size")
```


## What happens to the sampling distribution of the mean and standard deviation when our sample size is small?


We would hope that, regardless of sample size, the sampling distributions of both the mean and standard deviation should be centered around the true population value, $\mu$ and $\sigma$ respectively.  That seemed to be the case for the modest to large sample sizes we've looked at so far (30 to 500 observations).  Does this also hold for small samples? Let's use simulation to explore how well this is expectation is met for small samples.

As we've done before, we simulate the sampling distribution of the mean and standard deviation for samples of varying size (we also calculate some other values which will be useful for our exposition below). 

```{r}
# we'll use the same mean and sd we've been using previously
mu = mean.ht
sigma = sd.ht

# list of sample sizes we'll generate
ssizes <- c(2, 3, 4, 5, 7, 10, 20, 30)

# number of simulations to carry out *for each sample size*
nsims <- 2500

df.combined.small <- data_frame(sample.size = double(), 
                          sample.mean = double(), 
                          sample.sd = double(),
                          estimated.SE = double(), 
                          sample.zscore = double())

for (i in ssizes) {
  # create empty vectors to hold simulation stats (for efficiency)
  s.means <- rep(NA, nsims)
  s.sds <- rep(NA, nsims)
  s.SEs <- rep(NA, nsims)
  s.zscores <- rep(NA, nsims)
  
  for (j in 1:nsims) {
    s <- rnorm(i, mean = mu, sd = sigma)  # draw random sample
    s.means[j] <- mean(s) # calculate mean of that sample
    s.sds[j] <- sd(s) # calculate sd of that sample
    SE <- sd(s)/sqrt(i)  # cacluate estimated SE of that sample
    s.SEs[j] <- SE
    s.zscores[j] <- (mean(s) - mu)/SE
  }
  df <- data.frame(sample.size = i, sample.mean = s.means, sample.sd = s.sds, 
                   estimated.SE = s.SEs, sample.zscore = s.zscores)
  
  df.combined.small <- bind_rows(df.combined.small, df)
}

df.combined.small %<>%
  mutate(sample.sz = as.factor(sample.size))
```


### For small samples, sample standard deviations systematically underestimate the population standard deviation


Let's examine how the well centered the sampling distributions of the mean and standard deviation are around their true values, as a function of sample size.


First a table summarizing this information:

```{r}
by.sample.size <-
  df.combined.small %>%
  group_by(sample.size) %>%
  summarize(mean.of.means = mean(sample.mean),
            mean.of.sds = mean(sample.sd))

by.sample.size
```

We see that the sampling distributions of means are well centered around the true mean, and there is no systematic bias one way or the other. By contrast the sampling distribution of standard deviations tends to underestimate the true standard deviation when the samples are small (less than 30 observations).

We can visualize this bias as shown here:
```{r}
ggplot(by.sample.size, aes(x = sample.size, y = mean.of.sds)) +
  geom_point(color = 'red') +
  geom_line(color = 'red') +
  geom_hline(yintercept = sigma, color = 'black', linetype='dashed') +
  labs(x = "Sample Size", y = "Mean of Sampling Distn of Std Dev")

```

The source of this bias is clear if we look at the sampling distribution of the standard deviation for samples of size 3, 5, and 30.

```{r, fig.width = 12}
filtered.df <- 
  df.combined.small %>%
  filter(sample.size %in% c(3, 5, 30))

ggplot(filtered.df, aes(x = sample.sd, y = ..density.., fill = sample.sz)) + 
  geom_histogram(bins=50, alpha=0.65) +
  facet_wrap(~sample.size, nrow = 1) +
  geom_vline(xintercept = sigma, linetype = 'dashed') +
  labs(x = "Std Deviations", y = "Density",
       title = "Sampling distributions of the standard deviation\nAs a function of sample size")
  
```

There's very clear indication that the the sampling distribution of standard deviations is not centered around the rule value for $n=3$ and for $n=5$, however with samples of size 30 the sampling distribution of the standard deviation appears fairly well centered around the true value of the underlying population.


### Underestimates of the standard deviation given small $n$ lead to understimates of the SE of the mean

When sample sizes are small, sample estimates of the standard deviation, $s_x$, tend to underestimate the true standard deviation, $\sigma$, then it follows that sample estimates of the standard error of the mean, $SE_\overline{x} = \frac{s_x}{\sqrt{n}}$, must tend to understimate the true standard error of the mean, $SE_\mu = \frac{\sigma}{\sqrt{n}}$.

## The t-distribution is the appropriate distribution for describing the sampling distribution of the mean when $n$ is small

The problem associated with estimating the standard error of the mean for small sample sizes was recognized in the early 20th century by William Gosset, an employee at the Guinness Brewing Company. He published a paper, under the pseudonym "Student", giving the appropriate distribution for describing the standard error of the mean as a function of the sample size $n$. Gosset's distribution is known as the "t-distribution" or "Student's t-distribution".

The t-distribution is specified by a single parameter, called degrees of freedom ($df$) where ${df} = n - 1$. As $df$ increases, the t-distribution becomes more and more like the normal such that when $n \geq 30$ it's nearly indistinguishable from the standard normal distribution.

In the figures below we compare the t-distribution and the standard normal distribution  for sample sizes  $n = {4, 8, 32}$.

```{r, fig.width = 6}
x <- seq(-6, 6, length.out = 200)
n <- c(5, 10, 30) # sample sizes

distns.df <- data_frame(sample.size = double(), z.or.t = double(),
                        norm.density = double(), t.density = double())
for (i in n) {
  norm.density <- dnorm(x, mean = 0, sd = 1)
  t.density <- dt(x, df = i - 1)
  df.temp <- data_frame(sample.size = i, z.or.t = x,
                        norm.density = norm.density, t.density  = t.density)
  distns.df <- bind_rows(distns.df, df.temp)
}

distns.df %<>% mutate(df = as.factor(sample.size - 1))
  
ggplot(distns.df, aes(x = z.or.t, y = t.density, color = df)) + 
  geom_line() +
  geom_line(aes(y = norm.density), color='black', linetype="dotted") + 
  labs(x = "z or t value", y = "Probablity density",
       title = "Standard normal distribution (black dotted line)\nversus t-distributions for different degrees of freedom")
  
```









