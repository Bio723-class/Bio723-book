[
["index.html", "Biology 723: Statistical Computing for Biologists Chapter 1 Introduction 1.1 How to use these lecture notes", " Biology 723: Statistical Computing for Biologists Paul M. Magwene 2018-04-11 Chapter 1 Introduction Bio 723 (formerly Bio 313) is a course I offer at Duke University. The focus of this course is statistical computing for the biological sciences with an emphasis on common multivariate statistical methods and techniques for exploratory data analysis. A major goal of the course is to help graduate students in the biological sciences develop practical insights into methods that they are likely to encounter in their own research, and the potential advantages and pitfalls that come with their use. In terms of mathematical perspectives, the course emphasize a geometric approach to understanding multivariate statistics. I try to help students develop an intuition for the geometry of vector spaces and discuss topics like correlation, regression, and principal components analysis in terms of angles between vectors, dot products, and projection. 1.1 How to use these lecture notes In this and future materials to be posted on the course website you’ll encounter blocks of R code. Your natural intuition will be to cut and paste commands or code blocks into the R interpretter to save yourself the typing. DO NOT DO THIS!! In each of the examples below, I provide example input, but I don’t show you the output. It’s your job to type in these examples at the R console, evaluate what you typed, and to look at and think critically about the output. You will make mistakes and generate errors! Part of learning any new skill is making mistakes, figuring out where you went wrong, and correcting those mistakes. In the process of fixing those errors, you’ll learn more about how R works, and how to avoid such errors, or correct bugs in your own code in the future. If you cut and paste the examples into the R interpretter the code will run, but you will learn less than if you input the code yourself and you’ll be less capable of apply the concepts in new situations. The R interpretter, like all programming languages, is very exacting. A mispelled variable or function name, a stray period, or an unbalanced parenthesis will raise an error and finding the sources of such errors can sometimes be tedious and frustrating. Persist! If you read your code critically, think about what your doing, and seek help when needed (teaching team, R help, Google, etc) you’ll eventually get better at debugging your code. But keep in mind that like most new skills, learning to write and debug your code efficiently takes time and experience. "],
["getting-started-with-r.html", "Chapter 2 Getting Started with R 2.1 What is R? 2.2 What is RStudio? 2.3 Entering commands in the console 2.4 Comments 2.5 Using R as a Calculator 2.6 Numeric data types in R 2.7 Variable assignment in R 2.8 Working with Vectors in R 2.9 The R Help System 2.10 Packages", " Chapter 2 Getting Started with R 2.1 What is R? R is a statistical computing environment and programming language. It is free, open source, and has a large and active community of developers and users. There are many different R packages (libraries) available for conducting out a wide variety of different analyses, for everything from genome sequence data to geospatial information. 2.2 What is RStudio? RStudio (http://www.rstudio.com/) is an open source integrated development environment (IDE) that provides a nicer graphical interface to R than does the default R GUI. The figure below illustrates the RStudio interface, in it’s default configuration. For the exercises below you’ll be primarily entering commands in the “console” window. We’ll review key parts of the RStudio interface in greater detail in class. Figure 2.1: RStudio window with the panes labeled 2.3 Entering commands in the console You can type commands directly in the console. When you hit Return (Enter) on your keyboard the text you typed is evaluated by the R interpreter. This means that the R program reads your commands, makes sure there are no syntax errors, and then carries out any commands that were specified. Try evaluating the following arithmetic commands in the console: 10 + 5 [1] 15 10 - 5 [1] 5 10 / 5 [1] 2 10 * 5 [1] 50 If you type an incomplete command and then hit Return on your keyboard, the console will show a continuation line marked by a + symbol. For example enter the incomplete statement (10 + 5 and then hit Enter. You should see something like this. &gt; (10 + 5 + The continuation line tells you that R is waiting for additional input before it evaluates what you typed. Either complete your command (e.g. type the closing parenthesis) and hit Return, or hit the “Esc” key to exit the continuation line without evaluating what you typed. 2.4 Comments When working in the R console, or writing R code, the pound symbol (#) indicates the start of a comment. Anything after the #, up to the end of the current line, is ignored by the R interpretter. # This line will be ignored 5 + 4 # the first part of this line, up to the #, will be evaluated [1] 9 Throughout this course I will often include short explanatory comments in my code examples. 2.5 Using R as a Calculator The simplest way to use R is as a fancy calculator. 10 + 2 # addition 10 - 2 # subtraction 10 * 2 # multiplication 10 / 2 # division 10 ^ 2 # exponentiation 10 ** 2 # alternate exponentiation pi * 2.5^2 # R knows about some constants such as Pi 10 %% 3 # modulus operator -- gives remainder after division 10 %/% 3 # integer division Be aware that certain operators have precedence over others. For example multiplication and division have higher precedence than addition and subtraction. Use parentheses to disambiguate potentially confusing statements. (10 + 2)/4-5 # was the output what you expected? [1] -2 (10 + 2)/(4-5) # compare the answer to the above [1] -12 Division by zero produces an object that represents infinite numbers. Infinite values can be either positive or negative 1/0 # Inf [1] Inf -1/0 # -Inf [1] -Inf Invalid calculations produce a objected called NaN which is short for “Not a Number”: 0/0 # invalid calculation [1] NaN 2.5.1 Common mathematical functions Many commonly used mathematical functions are built into R. Here are some examples: abs(-3) # absolute value [1] 3 abs(3) [1] 3 cos(pi/3) # cosine [1] 0.5 sin(pi/3) # sine [1] 0.8660254 log(10) # natural logarithm [1] 2.302585 log10(10) # log base 10 [1] 1 log2(10) # log base 2 [1] 3.321928 exp(1) # exponential function [1] 2.718282 sqrt(10) # square root [1] 3.162278 10 ^ 0.5 # same as square root [1] 3.162278 2.6 Numeric data types in R There are three standard types of numbers in R. You can use the function typeof() to get information about an objects type in R. “double” – this is the default numeric data type, and is used to represent both real numbers and whole numbers (unless you explicitly ask for integers, see below). “double” is short for “double precision floating point value”. All of the previous computations you’ve seen up until this point used data of type double. typeof(10.0) # real number [1] &quot;double&quot; typeof(10) # whole numbers default to doubles [1] &quot;double&quot; “integer” – when your numeric data involves only whole numbers, you can get slighly better performance using the integer data type. You must explicitly ask for numbers to be treated as integers. typeof(as.integer(10)) # now treated as an integer [1] &quot;integer&quot; “complex” – R has a built-in data type to represent complex numbers – numbers with a “real” and “imaginary” component. We won’t encounter the use of complex numbers in this course, but they do have many important uses in mathematics and engineering and also have some interesting applications in biology. typeof(1 + 0i) [1] &quot;complex&quot; sqrt(-1) # sqrt of -1, using doubles [1] NaN sqrt(-1 + 0i) # sqrt of -1, using complex numbers [1] 0+1i 2.7 Variable assignment in R An important programming concept in all programming languages is that of “variable assignment”. Variable assignment is the act of creating labels that point to particular data values in a computers memory, which allows us to apply operations to the labels rather than directly to specific. Variable assignment is an important mechanism of abstracting and generalizing computational operations. Variable assignment in R is accomplished with the assignment operator, which is designated as &lt;- (left arrow, constructed from a left angular brack and the minus sign). This is illustrated below: x &lt;- 10 # assign the variable name &#39;x&#39; the value 10 sin(x) # apply the sin function to the value x points to [1] -0.5440211 x &lt;- pi # x now points to a different value sin(x) # the same function call now produces a different result [1] 1.224647e-16 2.7.1 Valid variable names As described in the R documentation, “A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number. Names such as ‘.2way’ are not valid, and neither are the reserved words.” Here are some examples of valid and invalid variable names. Mentally evaluate these based on the definition above, and then evaluate these in the R interpetter to confirm your understanding : x &lt;- 10 x.prime &lt;- 10 x_prime &lt;- 10 my.long.variable.name &lt;- 10 another_long_variable_name &lt;- 10 _x &lt;- 10 .x &lt;- 10 2.x &lt;- 2 * x 2.8 Working with Vectors in R Vectors are the core data structure in R. Vectors store an ordered lists of items, all of the same type. Learning to compute effectively with vectors and one of the keys to efficient R programming. Vectors in R always have a type (accessed with the typeof() function) and a length (accessed with the length() function). The simplest way to create a vector at the interactive prompt is to use the c() function, which is short hand for combine' orconcatenate’. x &lt;- c(2,4,6,8) # create a vector, assignn it the variable name `x` length(x) [1] 4 typeof(x) [1] &quot;double&quot; You can also use c() to concatenate two or more vectors together. y &lt;- c(1,3,5,7,9) # create another vector, labeled y xy &lt;- c(x,y) # combine two vectors z &lt;- c(pi/4, pi/2, pi, 2*pi) xyz &lt;- c(x, y, z) # combine three vectors 2.8.1 Vector Arithmetic The basic R arithmetic operations work on vectors as well as on single numbers (in fact single numbers are vectors). x &lt;- c(2, 4, 6, 8, 10) x * 2 # multiply each element of x by 2 [1] 4 8 12 16 20 x - pi # subtract pi from each element of x [1] -1.1415927 0.8584073 2.8584073 4.8584073 6.8584073 y &lt;- c(0, 1, 3, 5, 9) x + y # add together each matching element of x and y [1] 2 5 9 13 19 x * y # multiply each matching element of x and y [1] 0 4 18 40 90 x/y # divide each matching element of x and y [1] Inf 4.000000 2.000000 1.600000 1.111111 Basic numerical functions operate element-wise on numerical vectors: sin(x) [1] 0.9092974 -0.7568025 -0.2794155 0.9893582 -0.5440211 cos(x * pi) [1] 1 1 1 1 1 log(x) [1] 0.6931472 1.3862944 1.7917595 2.0794415 2.3025851 2.8.2 Vector recycling When vectors are not of the same length R `recycles’ the elements of the shorter vector to make the lengths conform. x &lt;- c(2, 4, 6, 8, 10) length(x) [1] 5 z &lt;- c(1, 4, 7, 11) length(z) [1] 4 x + z [1] 3 8 13 19 11 In the example above z was treated as if it was the vector (1, 4, 7, 11, 1). 2.8.3 Simple statistical functions for numeric vectors Now that we’ve introduced vectors as the simplest data structure for holding collections of numerical values, we can introduce a few of the most common statistical functions that operate on such vectors. First let’s create a vector to hold our sample data of interest. Here I’ve taken a random sample of the lengths of the last names of students enrolled in Bio 723 during Spring 2018. len.name &lt;- c(7, 7, 6, 2, 9, 9, 7, 4, 10, 5) Some common statistics of interest include minimum, maximum, mean, median, variance, and standard deviation: min(len.name) [1] 2 max(len.name) [1] 10 mean(len.name) [1] 6.6 median(len.name) [1] 7 var(len.name) # variance [1] 6.044444 sd(len.name) # standard deviation [1] 2.458545 The summary() function applied to a vector of doubles produce a useful table of some of these key statistics: summary(len.name) Min. 1st Qu. Median Mean 3rd Qu. Max. 2.00 5.25 7.00 6.60 8.50 10.00 2.9 The R Help System R comes with fairly extensive documentation and a simple help system. You can access HTML versions of the R documentation under the Help tab in Rstudio. The HTML documentation also includes information on any packages you’ve installed. Take a few minutes to browse through the R HTML documentation. In addition to the HTML documentation there is also a search box where you can enter a term to search on (see red arrow in figure below). Figure 2.2: The RStudio Help tab 2.9.1 Getting help from the console In addition to getting help from the RStudio help tab, you can directly search for help from the console. The help system can be invoked using the help function or the ? operator. help(&quot;log&quot;) ?log If you are using RStudio, the help results will appear in the “Help” tab of the Files/Plots/Packages/Help/Viewer (lower right window by default). What if you don’t know the name of the function you want? You can use the help.search() function. help.search(&quot;log&quot;) In this case help.search(&quot;log&quot;) returns all the functions with the string log in them. For more on help.search type ?help.search. Other useful help related functions include apropos() and example(). apropos returns a list of all objects (including function names) in the current session that match the input string. apropos(&quot;log&quot;) [1] &quot;as.data.frame.logical&quot; &quot;as.logical&quot; [3] &quot;as.logical.factor&quot; &quot;dlogis&quot; [5] &quot;is.logical&quot; &quot;log&quot; [7] &quot;log10&quot; &quot;log1p&quot; [9] &quot;log2&quot; &quot;logb&quot; [11] &quot;Logic&quot; &quot;logical&quot; [13] &quot;logLik&quot; &quot;loglin&quot; [15] &quot;plogis&quot; &quot;qlogis&quot; [17] &quot;rlogis&quot; &quot;SSlogis&quot; example() provides examples of how a function is used. example(log) log&gt; log(exp(3)) [1] 3 log&gt; log10(1e7) # = 7 [1] 7 log&gt; x &lt;- 10^-(1+2*1:9) log&gt; cbind(x, log(1+x), log1p(x), exp(x)-1, expm1(x)) x [1,] 1e-03 9.995003e-04 9.995003e-04 1.000500e-03 1.000500e-03 [2,] 1e-05 9.999950e-06 9.999950e-06 1.000005e-05 1.000005e-05 [3,] 1e-07 1.000000e-07 1.000000e-07 1.000000e-07 1.000000e-07 [4,] 1e-09 1.000000e-09 1.000000e-09 1.000000e-09 1.000000e-09 [5,] 1e-11 1.000000e-11 1.000000e-11 1.000000e-11 1.000000e-11 [6,] 1e-13 9.992007e-14 1.000000e-13 9.992007e-14 1.000000e-13 [7,] 1e-15 1.110223e-15 1.000000e-15 1.110223e-15 1.000000e-15 [8,] 1e-17 0.000000e+00 1.000000e-17 0.000000e+00 1.000000e-17 [9,] 1e-19 0.000000e+00 1.000000e-19 0.000000e+00 1.000000e-19 2.10 Packages Packages are libraries of R functions and data that provide additional capabilities and tools beyond the standard library of functions included with R. Hundreds of people around the world have developed packages for R that provide functions and related data structures for conducting many different types of analyses. Throughout this course you’ll need to install a variety of packages. Here I show the basic procedure for installing new packages from the console as well as from the R Studio interface. 2.10.1 Installing packages from the console The built-in function install.packages provides a quick and conveniet way to install packages from the R console. 2.10.2 Install the tidyverse package To illustrate the use of install.package, we’ll install a collection of packages (a “meta-package”) called the tidyverse. Here’s how to install the tidyverse meta-package from the R console: install.packages(&quot;tidyverse&quot;, dependencies = TRUE) The first argument to install.packages gives the names of the package we want to install. The second argument, dependencies = TRUE, tells R to install any additional packages that tidyverse depends on. 2.10.3 Installing packages from the RStudio dialog You can also install packages using a graphical dialog provided by RStudio. To do so pick the Packages tab in RStudio, and then click the Install button. Figure 2.3: The Packages tab in RStudio In the packages entry box you can type the name of the package you wish to install. 2.10.4 Install the stringr package Let’s install another useful package called “stringr”. Type the package name in the “Packages” field, make sure the “Install dependencies” check box is checked, and then press the “Install” button. Figure 2.4: Package Install Dialog 2.10.5 Loading packages with the library() function Once a package is installed on your computer, the package can be loaded into your R session using the library function. To insure our previous install commands worked correctly, let’s load the packages we just installed. library(stringr) library(tidyverse) Since the tidyverse pacakge is a “meta-package” it provides some additional info about the sub-packages that got loaded. When you load tidyverse, you will also see a message about “Conflicts” as several of the functions provided in the dplyr package (a sub-package in tidyverse) conflict with names of functions provided by the “stats” package which usually gets automically loaded when you start R. The conflicting funcdtions are filter and lag. The conflicting functions in the stats package are lag and filter which are used in time series analysis. The dplyr functions are more generally useful. Furthermore, if you need these masked functions you can still access them by prefacing the function name with the name of the package (e.g. stats::filter). "],
["r-basics-data-types-and-data-structures.html", "Chapter 3 R Basics: Data types and data structures 3.1 Logical (Boolean) values 3.2 Character strings 3.3 Vectors 3.4 Lists 3.5 Data frames", " Chapter 3 R Basics: Data types and data structures In the previous chapter we began our exploration of R with some simple numerical examples. We discussed the three basic numeric data types in R (doubles, integers, complex numbers), illustrated simple mathematical functions (abs, log, cos, etc), introduced the concept of variable assignment, and made use of a simple data structure called a vector. In this chapter we’re going to: 1) introduced two additional data types (logical and character types); 2) expand our knowledge of R vectors; and 3) introduce two new data structures (lists and data frames). 3.1 Logical (Boolean) values The “logical” data type (know as “Booleans” in many other languages, after the mathematician George Bools) represent true or false values. These are written TRUE and FALSE in R. typeof(TRUE) [1] &quot;logical&quot; typeof(FALSE) [1] &quot;logical&quot; Logical values or returned by numeric comparison operations such as testing equality, less than, greater than, etc. This is illustrated below 10 &lt; 9 # less than [1] FALSE 10 &gt; 9 # greater than [1] TRUE 10 &lt;= (5 * 2) # less than or equal to [1] TRUE 10 &gt;= pi # greater than or equal to [1] TRUE 10 == 10 # equals [1] TRUE 10 != 10 # does not equal [1] FALSE 10 == (sqrt(10)^2) # Surprised by the result? See below. [1] FALSE 4 == (sqrt(4)^2) # Even more confused? [1] TRUE Be careful to distinguish between == (tests equality) and = (the alternative assignment operator equivalent to &lt;-). I encourage you to only use the = assignment operator in function calls, so as to help avoid confusion between testing for equality and assignment. How about the last two statement comparing two values to the square of their square roots? Mathematically we know that both \\((\\sqrt{10})^2 = 10\\) and \\((\\sqrt{4})^2 = 4\\) are true statements. Why does R tell us the first statement is false? What we’re running into here are the limits of computer precision. A computer can’t represent \\(\\sqrt 10\\) exactly, whereas \\(\\sqrt 4\\) can be exactly represented. Precision in numerical computing is a complex subject and a detailed discussion is beyond the scope of this course. Later in the course we’ll discuss some ways of implementing sanity checks to avoid situations like that illustrated above. 3.1.1 Comparison operators applied to numeric vectors Comparison operators can be applied to numeric vectors. As with other vector operations, the comparison operators are applied element by element, and the value returned is a vector of logical values. x &lt;- c(1, 2, 3) y &lt;- c(3, 2, 1) x &gt; y # compare each matching element [1] FALSE FALSE TRUE 3.1.2 Logical vectors As illustrated above, comparison operations applied to numeric vectors return logical vectors. You can create your own logical vectors with the c() function in the same way as we create numeric vectors: ww &lt;- c(TRUE, TRUE, TRUE, FALSE) zz &lt;- c(FALSE, TRUE) 3.1.3 Logical operators Logical values support Boolean operations, like logical negation (“not”), “and”, “or”, “xor”, etc. This is illustrated below: x &lt;- TRUE y &lt;- FALSE !x # logical negation [1] FALSE x &amp; y # AND: are x and y both TRUE? [1] FALSE x | y # OR: are either x or y TRUE? [1] TRUE xor(x,y) # XOR: is either x or y TRUE, but not both? [1] TRUE The logical operators above work element-wise on logical vectors: xx &lt;- c(TRUE, TRUE, TRUE, FALSE) yy &lt;- c(FALSE, TRUE) !xx # logical negation, element-wise [1] FALSE FALSE FALSE TRUE xx &amp; yy # element-wise AND, note recycling of shorter vector [1] FALSE TRUE FALSE FALSE The function isTRUE is sometimes useful, especially when writing functions. isTRUE applies to length-one logical vectors. x &lt;- 5 y &lt;- 10 z &lt;- x &gt; y isTRUE(z) [1] FALSE 3.2 Character strings Character strings (“character”) represent single textual characters or longer string. They are created by wrapping text either single our double quotes. typeof(&quot;a&quot;) [1] &quot;character&quot; typeof(&quot;abc&quot;) [1] &quot;character&quot; Character strings have a length, which can be found using the nchar function: first.name &lt;- &quot;jasmine&quot; nchar(first.name) [1] 7 There are a number of built-in functions for manipulating character strings. Here are some of the most common ones: last.name &lt;- &#39;smith&#39; paste(first.name, last.name) # join strings [1] &quot;jasmine smith&quot; substr(first.name, 1, 3) # get substrings [1] &quot;jas&quot; 3.2.1 The stringr package The stringr package provides a variety of useful functions for working with character strings. You should have already installed the stringr package (see the Installing Packages handout), but do so now if you forgot. All of the functions in the stringr package are prefixed with str_. Here are some examples: # You don&#39;t need to reload packages if you already loaded # them in your R session. This is just for completeness. library(stringr) darwin.quote &lt;- &quot;There is grandeur in this view of life, with its several powers, having been originally breathed into a few forms or into one...&quot; str_length(darwin.quote) # equivalent to nchar [1] 128 # how many times does the character &quot;s&quot; appear in the string? str_count(darwin.quote, &quot;s&quot;) [1] 6 # duplicate a string str_dup(&quot;hello&quot;, 3) [1] &quot;hellohellohello&quot; # other interesting functions str_to_title(&quot;on the origin of species&quot;) [1] &quot;On The Origin Of Species&quot; str_to_upper(&quot;loud and bombastic&quot;) [1] &quot;LOUD AND BOMBASTIC&quot; 3.3 Vectors We previously introduced the vector data structure in a previous hands-on document (see “Getting Started with R”). Here we’ll explore some additional operations on vectors. Just as a reminder, vectors store an ordered list of items all of the same type (e.g. numeric, logical, character). Vectors in R always have a length (accessed with the length() function) and a type (accessed with the typeof() function). 3.3.1 Indexing Vectors For a vector of length \\(n\\), we can access the elements by the indices \\(1 \\ldots n\\). We say that R vectors (and other data structures like lists) are `one-indexed’. Many other programming languages, such as Python, C, and Java, use zero-indexing where the elements of a data structure are accessed by the indices \\(0 \\ldots n-1\\). Indexing errors are a common source of bugs. Indexing a vector is done by specifying the index in square brackets as shown below: x &lt;- c(2, 4, 6, 8, 10) length(x) [1] 5 x[1] [1] 2 x[4] [1] 8 Negative indices are used to exclude particular elements. x[-1] returns all elements of x except the first. x[-1] [1] 4 6 8 10 You can get multiple elements of a vector by indexing by another vector. In the example below, x[c(3,5)] returns the third and fifth element of x`. x[c(3,5)] [1] 6 10 3.3.2 Combining Indexing and Comparison of Vectors A very powerful feature of R is the ability to combine the comparison operators with indexing. This facilitates data filtering and subsetting. Some examples: x &lt;- c(2, 4, 6, 8, 10) x[x &gt; 5] [1] 6 8 10 x[x &lt; 4 | x &gt; 6] [1] 2 8 10 In the first example we retrieved all the elements of x that are larger than 5 (read as “x where x is greater than 5”). In the second example we retrieved those elements of x that were smaller than four or greater than six. Combining indexing and comparison is a powerful concept which we’ll use repeatedly in this course. 3.3.3 Vector manipulation You can combine indexing with assignment to change the elements of a vectors: x &lt;- c(2, 4, 6, 8, 10) x[2] &lt;- -4 x [1] 2 -4 6 8 10 You can also use indexing vectors to change multiple values at once: x &lt;- c(2, 4, 6, 8, 10) x[c(1, 3, 5)] &lt;- 6 x [1] 6 4 6 8 6 Using logical vectors to manipulate the elements of a vector also works: x &lt;- c(2, 4, 6, 8, 10) x[x &gt; 5] = 5 # truncate all values to have max value 5 x [1] 2 4 5 5 5 3.3.4 Vectors from regular sequences There are a variety of functions for creating regular sequences in the form of vectors. 1:10 # create a vector with the integer values from 1 to 10 [1] 1 2 3 4 5 6 7 8 9 10 20:11 # a vector with the integer values from 20 to 11 [1] 20 19 18 17 16 15 14 13 12 11 seq(1, 10) # like 1:10 [1] 1 2 3 4 5 6 7 8 9 10 seq(1, 10, by = 2) # 1:10, in steps of 2 [1] 1 3 5 7 9 seq(2, 4, by = 0.25) # 2 to 4, in steps of 0.25 [1] 2.00 2.25 2.50 2.75 3.00 3.25 3.50 3.75 4.00 3.3.5 Additional functions for working with vectors The function unique() returns the unique items in a vector: x &lt;- c(5, 2, 1, 4, 6, 9, 8, 5, 7, 9) unique(x) [1] 5 2 1 4 6 9 8 7 rev() returns the items in reverse order (without changing the input vector): y &lt;- rev(x) y [1] 9 7 5 8 9 6 4 1 2 5 x # x is still in original order [1] 5 2 1 4 6 9 8 5 7 9 There are a number of useful functions related to sorting. Plain sort() returns a new vector with the items in sorted order: sort(x) # returns items of x sorted [1] 1 2 4 5 5 6 7 8 9 9 x # but x remains in its unsorted state [1] 5 2 1 4 6 9 8 5 7 9 The related function order() gives the indices which would rearrange the items into sorted order: order(x) [1] 3 2 4 1 8 5 9 7 6 10 order() can be useful when you want to sort one list by the values of another: students &lt;- c(&quot;fred&quot;, &quot;tabitha&quot;, &quot;beatriz&quot;, &quot;jose&quot;) class.ranking &lt;- c(4, 2, 1, 3) students[order(class.ranking)] # get the students sorted by their class.ranking [1] &quot;beatriz&quot; &quot;tabitha&quot; &quot;jose&quot; &quot;fred&quot; any() and all(), return single boolean values based on a specified comparison provided as an argument: y &lt;- c(2, 4, 5, 6, 8) any(y &gt; 5) # returns TRUE if any of the elements are TRUE [1] TRUE all(y &gt; 5) # returns TRUE if all of the elements are TRUE [1] FALSE which() returns the indices of the vector for which the input is true: which(y &gt; 5) [1] 4 5 3.4 Lists R lists are like vectors, but unlike a vector where all the elements are of the same type, the elements of a list can have arbitrary types (even other lists). Lists are a powerful data structure for organizing information, because there are few constraints on the shape or types of the data included in a list. Lists are easy to create: l &lt;- list(&#39;Bob&#39;, pi, 10) Note that lists can contain arbitrary data. Lists can even contain other lists: l &lt;- list(&#39;Bob&#39;, pi, 10, list(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;, &quot;qux&quot;)) Lists are displayed with a particular format, distinct from vectors: l [[1]] [1] &quot;Bob&quot; [[2]] [1] 3.141593 [[3]] [1] 10 [[4]] [[4]][[1]] [1] &quot;foo&quot; [[4]][[2]] [1] &quot;bar&quot; [[4]][[3]] [1] &quot;baz&quot; [[4]][[4]] [1] &quot;qux&quot; In the example above, the correspondence between the list and its display is obvious for the first three items. The fourth element may be a little confusing at first. Remember that the fourth item of l was another list. So what’s being shown in the output for the fourth item is the nested list. An alternative way to display a list is using the str() function (short for “structure”). str() provides a more compact representation that also tells us what type of data each element is: str(l) List of 4 $ : chr &quot;Bob&quot; $ : num 3.14 $ : num 10 $ :List of 4 ..$ : chr &quot;foo&quot; ..$ : chr &quot;bar&quot; ..$ : chr &quot;baz&quot; ..$ : chr &quot;qux&quot; 3.4.1 Length and type of lists Like vectors, lists have length: length(l) [1] 4 But the type of a list is simply “list”, not the type of the items within the list. This makes sense because lists are allowed to be heterogeneous (i.e. hold data of different types). typeof(l) [1] &quot;list&quot; 3.4.2 Indexing lists Lists have two indexing operators. Indexing a list with single brackets, like we did with vectors, returns a new list containing the element at index \\(i\\). Lists also support double bracket indexing (x[[i]]) which returns the bare element at index \\(i\\) (i.e. the element without the enclosing list). This is a subtle but important point so make sure you understand the difference between these two forms of indexing. 3.4.2.1 Single bracket list indexing First, let’s demonstrate single bracket indexing of the lists l we created above. l[1] # single brackets, returns list(&#39;Bob&#39;) [[1]] [1] &quot;Bob&quot; typeof(l[1]) # notice the list type [1] &quot;list&quot; When using single brackets, lists support indexing with ranges and numeric vectors: l[3:4] [[1]] [1] 10 [[2]] [[2]][[1]] [1] &quot;foo&quot; [[2]][[2]] [1] &quot;bar&quot; [[2]][[3]] [1] &quot;baz&quot; [[2]][[4]] [1] &quot;qux&quot; l[c(1, 3, 5)] [[1]] [1] &quot;Bob&quot; [[2]] [1] 10 [[3]] NULL 3.4.2.2 Double bracket list indexing If double bracket indexing is used, the object at the given index in a list is returned: l[[1]] # double brackets, return plain &#39;Bob&#39; [1] &quot;Bob&quot; typeof(l[[1]]) # notice the &#39;character&#39; type [1] &quot;character&quot; Double bracket indexing does not support multiple indices, but you can chain together double bracket operators to pull out the items of sublists. For example: # second item of the fourth item of the list l[[4]][[2]] [1] &quot;bar&quot; 3.4.3 Naming list elements The elements of a list can be given names when the list is created: p &lt;- list(first.name=&#39;Alice&#39;, last.name=&quot;Qux&quot;, age=27, years.in.school=10) You can retrieve the names associated with a list using the names function: names(p) [1] &quot;first.name&quot; &quot;last.name&quot; &quot;age&quot; &quot;years.in.school&quot; If a list has named elements, you can retrieve the corresponding elements by indexing with the quoted name in either single or double brackets. Consistent with previous usage, single brackets return a list with the corresponding named element, whereas double brackets return the bare element. For example, make sure you understand the difference in the output generated by these two indexing calls: p[&quot;first.name&quot;] $first.name [1] &quot;Alice&quot; p[[&quot;first.name&quot;]] [1] &quot;Alice&quot; 3.4.4 The $ operator Retrieving named elements of lists (and data frames as we’ll see), turns out to be a pretty common task (especially when doing interactive data analysis) so R has a special operator to make this more convenient. This is the $ operator, which is used as illustrated below: p$first.name # equivalent to p[[&quot;first.name&quot;]] [1] &quot;Alice&quot; p$age # equivalent to p[[&quot;age&quot;]] [1] 27 3.4.5 Changing and adding lists items Combining indexing and assignment allows you to change items in a list: suspect &lt;- list(first.name = &quot;unknown&quot;, last.name = &quot;unknown&quot;, aka = &quot;little&quot;) suspect$first.name &lt;- &quot;Bo&quot; suspect$last.name &lt;- &quot;Peep&quot; suspect[[3]] &lt;- &quot;LITTLE&quot; str(suspect) List of 3 $ first.name: chr &quot;Bo&quot; $ last.name : chr &quot;Peep&quot; $ aka : chr &quot;LITTLE&quot; By combining assignment with a new name or an index past the end of the list you can add items to a list: suspect$age &lt;- 17 # add a new item named age suspect[[5]] &lt;- &quot;shepardess&quot; # create an unnamed item at position 5 Be careful when adding an item using indexing, because if you skip an index an intervening NULL value is created: # there are only five items in the list, what happens if we # add a new item at position seven? suspect[[7]] &lt;- &quot;wanted for sheep stealing&quot; str(suspect) List of 7 $ first.name: chr &quot;Bo&quot; $ last.name : chr &quot;Peep&quot; $ aka : chr &quot;LITTLE&quot; $ age : num 17 $ : chr &quot;shepardess&quot; $ : NULL $ : chr &quot;wanted for sheep stealing&quot; 3.4.6 Combining lists The c (combine) function we introduced to create vectors can also be used to combine lists: list.a &lt;- list(&quot;little&quot;, &quot;bo&quot;, &quot;peep&quot;) list.b &lt;- list(&quot;has lost&quot;, &quot;her&quot;, &quot;sheep&quot;) list.c &lt;- c(list.a, list.b) list.c [[1]] [1] &quot;little&quot; [[2]] [1] &quot;bo&quot; [[3]] [1] &quot;peep&quot; [[4]] [1] &quot;has lost&quot; [[5]] [1] &quot;her&quot; [[6]] [1] &quot;sheep&quot; 3.4.7 Converting lists to vectors Sometimes it’s useful to convert a list to a vector. The unlist() function takes care of this for us. # a homogeneous list ex1 &lt;- list(2, 4, 6, 8) unlist(ex1) [1] 2 4 6 8 When you convert a list to a vector make sure you remember that vectors are homogeneous, so items within the new vector will be coerced to have the same type. # a heterogeneous list ex2 &lt;- list(2, 4, 6, c(&quot;bob&quot;, &quot;fred&quot;), list(1 + 0i, &#39;foo&#39;)) unlist(ex2) [1] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;bob&quot; &quot;fred&quot; &quot;1+0i&quot; &quot;foo&quot; Note that unlist() also unpacks nested vectors and lists as shown in the second example above. 3.5 Data frames Along with vectors and lists, data frames are one of the core data structures when working in R. A data frame is essentially a list which represents a data table, where each column in the table has the same number of rows and every item in the a column has to be of the same type. Unlike standard lists, the objects (columns) in a data frame must have names. We’ve seen data frames previously, for example when we loaded data sets using the read_csv function. 3.5.1 Creating a data frame While data frames will often be created by reading in a data set from a file, they can also be created directly in the console as illustrated below: age &lt;- c(30, 26, 21, 29, 25, 22, 28, 24, 23, 20) sex &lt;- rep(c(&quot;M&quot;,&quot;F&quot;), 5) wt.in.kg &lt;- c(88, 76, 67, 66, 56, 74, 71, 60, 52, 72) df &lt;- data.frame(age = age, sex = sex, wt = wt.in.kg) Here we created a data frame with three columns, each of length 10. 3.5.2 Type and class for data frames Data frames can be thought of as specialized lists, and in fact the type of a data frame is “list” as illustrated below: typeof(df) [1] &quot;list&quot; To distinguish a data frame from a generic list, we have to ask about it’s “class”. class(df) # the class of our data frame [1] &quot;data.frame&quot; class(l) # compare to the class of our generic list [1] &quot;list&quot; The term “class” comes from a style/approach to programming called “object oriented programming”. We won’t go into explicit detail about how object oriented programming works in this class, though we will exploit many of the features of objects that have a particular class. 3.5.3 Length and dimension for data frames Applying the length() function to a data frame returns the number of columns. This is consistent with the fact that data frames are specialized lists: length(df) [1] 3 To get the dimensions (number of rows and columns) of a data frame, we use the dim() function. dim() returns a vector, whose first value is the number of rows and whose second value is the number of columns: dim(df) [1] 10 3 We can get the number of rows and columns individually using the nrow() and ncol() functions: nrow(df) # number of rows [1] 10 ncol(df) # number of columsn [1] 3 3.5.4 Indexing and accessing data frames Data frames can be indexed by either column index, column name, row number, or a combination of row and column numbers. 3.5.4.1 Single bracket indexing of the columns of a data frame The single bracket operator with a single numeric index returns a data frame with the corresponding column. df[1] # get the first column (=age) of the data frame age 1 30 2 26 3 21 4 29 5 25 6 22 7 28 8 24 9 23 10 20 The single bracket operator with multiple numeric indices returns a data frame with the corresponding columns. df[1:2] # first two columns age sex 1 30 M 2 26 F 3 21 M 4 29 F 5 25 M 6 22 F 7 28 M 8 24 F 9 23 M 10 20 F df[c(1, 3)] # columns 1 (=age) and 3 (=wt) age wt 1 30 88 2 26 76 3 21 67 4 29 66 5 25 56 6 22 74 7 28 71 8 24 60 9 23 52 10 20 72 Column names can be substituted for indices when using the single bracket operator: df[&quot;age&quot;] age 1 30 2 26 3 21 4 29 5 25 6 22 7 28 8 24 9 23 10 20 df[c(&quot;age&quot;, &quot;wt&quot;)] age wt 1 30 88 2 26 76 3 21 67 4 29 66 5 25 56 6 22 74 7 28 71 8 24 60 9 23 52 10 20 72 3.5.4.2 Single bracket indexing of the rows of a data frame To get specific rows of a data frame, we use single bracket indexing with an additional comma following the index. For example to get the first row a data frame we would do: df[1,] # first row age sex wt 1 30 M 88 This syntax extends to multiple rows: df[1:2,] # first two rows age sex wt 1 30 M 88 2 26 F 76 df[c(1, 3, 5),] # rows 1, 3 and 5 age sex wt 1 30 M 88 3 21 M 67 5 25 M 56 3.5.4.3 Single bracket indexing of both the rows and columns of a data frame Single bracket indexing of data frames extends naturally to retrieve both rows and columns simultaneously: df[1, 2] # first row, second column [1] M Levels: F M df[1:3, 2:3] # first three rows, columns 2 and 3 sex wt 1 M 88 2 F 76 3 M 67 # you can even mix numerical indexing (rows) with named indexing of columns df[5:10, c(&quot;age&quot;, &quot;wt&quot;)] age wt 5 25 56 6 22 74 7 28 71 8 24 60 9 23 52 10 20 72 3.5.4.4 Double bracket and $ indexing of data frames Whereas single bracket indexing of a data frame always returns a new data frame, double bracket indexing and indexing using the $ operator, returns vectors. df[[&quot;age&quot;]] [1] 30 26 21 29 25 22 28 24 23 20 typeof(df[[&quot;age&quot;]]) [1] &quot;double&quot; df$wt [1] 88 76 67 66 56 74 71 60 52 72 typeof(df$wt) [1] &quot;double&quot; 3.5.5 Logical indexing of data frames Logical indexing using boolean values works on data frames in much the same way it works on vectors. Typically, logical indexing of a data frame is used to filter the rows of a data frame. For example, to get all the subject in our example data frame who are older than 25 we could do: # NOTE: the comma after 25 is important to insure we&#39;re indexing rows! df[df$age &gt; 25, ] age sex wt 1 30 M 88 2 26 F 76 4 29 F 66 7 28 M 71 Similarly, to get all the individuals whose weight is between 60 and 70 kgs we could do: df[(df$wt &gt;= 60 &amp; df$wt &lt;= 70),] age sex wt 3 21 M 67 4 29 F 66 8 24 F 60 3.5.6 Adding columns to a data frame Adding columns to a data frame is similar to adding items to a list. The easiest way to do so is using named indexing. For example, to add a new column to our data frame that gives the individuals ages in number of days, we could do: df[[&quot;age.in.days&quot;]] &lt;- df$age * 365 dim(df) [1] 10 4 "],
["r-markdown-and-r-notebooks.html", "Chapter 4 R Markdown and R Notebooks 4.1 R Notebooks 4.2 Creating an R Notebook 4.3 The default R Notebook template 4.4 Code and Non-code blocks 4.5 Running a code chunk 4.6 Running all code chunks above 4.7 “Knitting an” R Markdown to HTML 4.8 Sharing your reproducible R Notebook", " Chapter 4 R Markdown and R Notebooks RStudio comes with a useful set of tools, collectively called R Markdown, for generating “literate” statistical analyses. The idea behind literate statistical computing is that we should try to carry out our analyses in a manner that is transparent, self-explanatory, and reproducible. Literate statistical computing helps to ensure your research is reproducible because: The steps of your analyses are explicitly described, both as written text and the code and function calls used. Analyses can be more easily checked for correctness and reproduced from your literate code. Your literate code can serve as a template for future analyses, saving you time and the trouble of remembering all the gory details. As we’ll see, R Markdown will allow us to produce statistical documents that integrateprose, code, figures, and nicely formatted mathematics so that we can share and explain our analyses to others. Sometimes those “others” are advisors, supervisors, or collaborators; sometimes the other is you six months from now. For the purposes of this class, you will be asked to complete problem sets in the form of R Markdown documents. R Markdown documents are written in a light-weight markup language called Markdown. Markdown provides simple plain text “formatting” commands for specifying the structured elements of a document. Markdown was invented as a lightweight markup language for creating web pages and blogs, and has been adopted to a variety of different purposes. This hands-on provides a brief introduction to the capabilities of R Markdown. For more complete details, including lots of examples, see the R Markdown Website. 4.1 R Notebooks We’re going to create a type of R Markdown document called an “R Notebook”. The R Notebook Documentation describes R Notebooks as so: “An R Notebook is an R Markdown document with code chunks that can be executed independently and interactively, with output visible immediately beneath the input.” 4.2 Creating an R Notebook To create an R Notebook select File &gt; New File &gt; R Notebook from the files menu in RStudio. Figure 4.1: Using the File menu to create a new R Notebook. 4.3 The default R Notebook template The standard template that RStudio creates for you includes a header section like the following where you can specify document properties such as the title, author, and change the look and feel of the generated HTML document. --- title: &quot;R Notebook&quot; output: html_notebook --- The header is followed by several example sections that illustrate a few of the capabilities of R Markdown. Delete these and replace them with your own code as necessary. 4.4 Code and Non-code blocks R Markdown documents are divided into code blocks (also called “chunks”) and non-code blocks. Code blocks are sets of R commands that will be evalauted when the R Markdown document is run or “knitted” (see below). Non-code blocks include explanatory text, embedded images, etc. The default notebook template includes both code and non-code blocks. 4.4.1 Non-code blocks The first bit of text in the default notebook template is a non-code block that tells you how to use the notebook: This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. The text of non-code blocks can include lightweight markup information that can be used to format HTML or PDF output generated from the R Markdown document. Here are some examples: # Simple textual formatting This is a paragraph with plain text. Nothing fancy will happen here. This is a second paragraph with *italic*, **bold**, and `verbatim` text. # Lists ## Bullet points lists This is a list with bullet points: * Item a * Item b * Item c ## Numbered lists This is a numbered list: 1. Item 1 #. Item 2 #. Item 3 ## Mathematics R Markdown supports mathematical equations, formatted according to LaTeX conventions. Dollar signs ($) are used to offset mathematics like so: $x^2 + y^2 = z^2$. Notice from the example above that R Markdown supports LaTeX style formatting of mathematical equations. For example, $x^2 + y^2 = z^2$ appears as \\(x^2 + y^2 = z^2\\). 4.4.2 Code blocks Code blocks are delimited by matching sets of three backward ticks (```). Everything within a code block is interpretted as an R command and is evaluated by the R interpretter. Here’s the first code block in the default notebook template: ```{r} plot(cars) ``` 4.5 Running a code chunk You can run a single code block by clicking the small green “Run” button in the upper right hand corner of the code block as shown in the image below. Figure 4.2: Click the Run button to execute a code chunk. If you click this button the commands within this code block are executed, and any generated output is shown below the code block. Try running the first code block in the default template now. After the code chunk is executed you should see a plot embedded in your R Notebook as shown below: Figure 4.3: An R Notebook showing an embedded plot after executing a code chunk. 4.6 Running all code chunks above Next to the “Run” button in each code chunk is a button for “Run all chunks above” (see figure below). This is useful when the code chunk you’re working on depends on calculations in earlier code chunks, and you want to evaluated those earlier code chunks prior to running the focal code chunk. Figure 4.4: Use the ‘Run all chunks above’ button to evaluate all previous code chunks. 4.7 “Knitting an” R Markdown to HTML Save your R Notebook as first_rnotebook.Rmd (RStudio will automatically add the .Rmd extension so you don’t need to type it). You can generate an HTML version of your notebook by clicking the “Preview” menu on the Notebook taskbar and then choosing “Knit to HTML” (see image below). Figure 4.5: Use the ‘Knit to HTML’ menu to generate HTML output from your R Notebook When an RMarkdown document is “knit”, all of the code and non-code blocks are executed in a “clean” environment, in order from top to bottom. An output file is generated (HTML or one of the other available output types) that shows the results of executing the notebook. By default RStudio will pop-up a window showing you the HTML output you generated. Knitting a document is a good way to make sure your analysis is reproducible. If your code compiles correctly when the document is knit, and produces the expected output, there’s a good chance that someone else will be able to reproduce your analyses independently starting with your R Notebook document (after accounting for differences in file locations). 4.8 Sharing your reproducible R Notebook To share your R Notebook with someone else you just need to send them the source R Markdown file (i.e. the file with the .Rmd extension). Assuming they have access to the same source data, another user should be able to open the notebook file in RStudio and regenerate your analyses by evaluating the individual code chunks or knitting the document. In this course you will be submitting homework assignments in the form of R Notebook markdown files. "],
["introduction-to-ggplot2.html", "Chapter 5 Introduction to ggplot2 5.1 Loading ggplot2 5.2 Example data set: Anderson’s Iris Data 5.3 Template for single layer plots in ggplot2 5.4 An aside about function arguments 5.5 Strip plots 5.6 Histograms 5.7 Faceting to depict categorical information 5.8 Density plots 5.9 Violin or Beanplot 5.10 Boxplots 5.11 Building complex visualizations with layers 5.12 Useful combination plots 5.13 ggplot layers can be assigned to variables 5.14 Adding titles and tweaking axis labels 5.15 ggplot2 themes 5.16 Other aspects of ggplots can be assigned to variables 5.17 Bivariate plots 5.18 Bivariate density plots 5.19 Combining Scatter Plots and Density Plots with Categorical Information 5.20 Density plots with fill 5.21 2D bin and hex plots 5.22 The cowplot package", " Chapter 5 Introduction to ggplot2 Pretty much any statistical plot can be thought of as a mapping between data and one or more visual representations. For example, in a scatter plot we map two ordered sets of numbers (the variables of interest) to points in the Cartesian plane (x,y-coordinates). The representation of data as points in a plane can be thought of as a type of geometric mapping. In a histogram, we divide the range of a variable of interest into bins, count the number of observations in each bin, and represent those counts as bars. The process of counting the data in bins is a type of statistical transformation (summing in this case), while the representation of the counts as bars is another example of a geometric mapping. Both types of plots can be further embellished with additional information, such as coloring the points or bars based on a categorical variable of interest, changing the shape of points, etc. These are examples of aesthetic mappings. An additional operation that is frequently useful is faceting (also called conditioning), in which a series of subplots are created to show particular subsets of the data. The package ggplot2 is based on a formalized approach for building statistical graphics as a combination of geometric mappings, aesthetic mappings, statistical transformations, and faceting (conditioning). In ggplot2, complex figures are built up by combining layers – where each layer includes a geometric mapping, an aesthetic mapping, and a statistical transformation – along with any desired faceting information. Many of the key ideas behind ggplot2 (and its predecessor,“ggplot”) are based on a book called “The Grammar of Graphics” (Leland Wilkinson, 1985). The “grammar of graphics” is the “gg” in the ggplot2 name. 5.1 Loading ggplot2 ggplot2 is one of the packages included in the tidyverse meta-package we installed during the previous class session (see the previous lecture notes for instruction if you have not installed tidyverse). If we load the tidyverse package, ggplot2 is automatically loaded as well. library(tidyverse) However if we wanted to we could load only ggplot2 as follows: library(ggplot2) # not necessary if we already loaded tidyverse 5.2 Example data set: Anderson’s Iris Data To illustrate ggplot2 we’ll use a dataset called iris. This data set was made famous by the statistician and geneticist R. A. Fisher who used it to illustrate many of the fundamental statistical methods he developed (Recall that Fisher was one of the key contributors to the modern synthesis in biology, reconciling evolution and genetics in the early 20th century). The data set consists of four morphometric measurements for specimens from three different iris species (Iris setosa, I. versicolor, and I. virginica). Use the R help to read about the iris data set (?iris). We’ll be using this data set repeatedly in future weeks so familiarize yourself with it. The iris data is included in a standard R package (datasets) that is made available automatically when you start up R. As a consequence we don’t need to explicitly load the iris data from a file. Let’s take a few minutes to explore this iris data set before we start generating plots: names(iris) # get the variable names in the dataset [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; [5] &quot;Species&quot; dim(iris) # dimensions given as rows, columns [1] 150 5 head(iris) # can you figure out what the head function does? Sepal.Length Sepal.Width Petal.Length Petal.Width Species 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3.0 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5.0 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa tail(iris) # what about the tail function? Sepal.Length Sepal.Width Petal.Length Petal.Width Species 145 6.7 3.3 5.7 2.5 virginica 146 6.7 3.0 5.2 2.3 virginica 147 6.3 2.5 5.0 1.9 virginica 148 6.5 3.0 5.2 2.0 virginica 149 6.2 3.4 5.4 2.3 virginica 150 5.9 3.0 5.1 1.8 virginica 5.3 Template for single layer plots in ggplot2 A basic template for building a single layer plot using ggplot2 is shown below. When creating a plot, you need to replace the text in brackets (e.g. &lt;DATA&gt;) with appropriate objects, functions, or arguments: # NOTE: this is pseudo-code. It will not run! ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) The base function ggplot() is responsible for creating the coordinate system in which the plot will be display. To this coordinate system we add a geometric mapping (called a “geom” for short) that specifies how data gets mapped into the coordinate system (e.g. points, bars, etc). Included as an input to the geom function is the aesthetic mapping function that specifies which variables to use in the geometric mapping (e.g. which variables to treat as the x- and y-coordinates), colors, etc. For example, using this template we can create a scatter plot that show the relationship between the variables Sepal.Width and Petal.Width. To do so we subsitute iris for &lt;DATA&gt;, geom_point for &lt;GEOM_FUNCTION&gt;, and x = Sepal.Width and y = Petal.Width for &lt;MAPPINGS&gt;. ggplot(data = iris) + geom_point(mapping = aes(x = Sepal.Width, y = Petal.Width)) If we were to translate this code block to English, we might write it as “Using the iris data frame as the source of data, create a point plot using each observeration’s Sepal.Width variable for the x-coordinate and the Petal.Width variable for the y-coordinate.” 5.4 An aside about function arguments The inputs to a function are also known as “arguments”. In R, when you call a function you can specify the arguments by keyword (i.e. using names specified in the function definition) or by position (i.e. the order of the inputs). In our bar plot above, we’re using using keyword arguments. For example, in the line ggplot(data = iris), iris is treated as the “data” argument. Similarly, in the second line, aes(x = Sepal.Width, y = Petal.Width) is the “mapping” argument to geom_bar. Note that aes is itself a function (see ?aes) that takes arguments that can be specified positionally or with keywords. If we wanted to, we could instead use position arguments when calling a function, by passing inputs to the function corresponding to the order they are specified in the function definition. For example, take a minute to read the documentation for the ggplot function (?ggplot). Near the top of the help page you’ll see a description of how the function is called under “Usage”. Reading the Usage section you’ll see that the the “data” argument is the first positional argument to ggplot. Similarly, if you read the docs for the geom_point function you’ll see that mapping is the first positional argument for that function. The equivalent of our previous example, but now using positional arguments is: ggplot(iris) + # note we dropped the &quot;data = &quot; part # note we dropped the &quot;mapping = &quot; part from the geom_point call geom_point(aes(x = Sepal.Width, y = Petal.Width)) The upside of using positional arguments is that it means less typing, which is useful when working interactively at the console (or in an R Notebok). The downside to using positional arguments is you need to remember or lookup the order of the arguments. Using positional arguments can also make your code less “self documenting” in the sense that it is less explicit about how the inputs are being treated. While the argument “x” is the first argument to the aes function, I chose to explicitly include the argument name to make it clear what variable I’m plotting on the x-axis. We will cover function arguments in greater detail a class session or two from now, when we learn how to write our own functions. 5.5 Strip plots One of the simplest visualizations of a continuous variable is to draw points along a number line, where each point represent the value of one of the observations. This is sometimes called a “strip plot”. First, we’ll use the geom_point function as shown below to generate a strip plot for the Sepal.Width variable in the iris data set. ggplot(data = iris) + geom_point(aes(x = Sepal.Width, y = 0)) 5.5.1 Jittering data There should have been 150 points plotted in the figure above (one for each of the iris plants in the data set), but visually it looks like only about 25 or 30 points are shown. What’s going on? If you examine the iris data, you’ll see that the all the measures are rounded to the nearest tenth of a centimer, so that there are a large number of observations with identical values of Sepal.Width. This is a limitation of the precision of measurements that was used when generating the data set. To provide a visual clue that there are multiple observations that share the same value, we can slightly “jitter” the values (randomly move points a small amount in either in the vertical or horizontal direction). Jittering is used solely to enhance visualization, and any statistical analyses you carry out would be based on the original data. When presenting your data to someone else, should note when you’ve used jittering so as not to misconvey the actual data. Jittering can be accomplished using geom_jitter, which is derived from geom_point: ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = 0), width = 0.05, height = 0, alpha = 0.25) The width and height arguments specify the maximum amount (as fractions of the data) to jitter the observed data points in the horizontal (width) and vertical (height) directions. Here we only jitter the data in the horizontal direction. The alpha argument controls the transparency of the points – the valid range of alpha values is 0 to 1, where 0 means completely transparent and 1 is completely opaque. Within a geom, arguments outside of the aes mapping apply uniformly across the visualization (i.e. they are fixed values). For example, setting `alpha = 0.25’ made all the points transparent. 5.5.2 Adding categorical information Recall that are three different species represented in the data: Iris setosa, I. versicolor, and I. virginica. Let’s see how to generate a strip plot that also includes a breakdown by species. ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species), width=0.05, height=0.1, alpha=0.5) That was easy! All we had to do was change the aesthetic mapping in geom_jitter, specifying “Species” as the y variable. I also added a little vertical jitter as well to better separate the points. Now we have a much better sense of the data. In particular it’s clear that the I. setosa specimens generally have wider sepals than samples from the other two species. Let’s tweak this a little by also adding color information, to further emphasize the distinct groupings. We can do this by adding another argument to the aesthetic mapping in geom_jitter. ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species, color=Species), width=0.05, height=0.1, alpha=0.5) 5.5.3 Rotating plot coordinates What if we wanted to rotate this plot 90 degrees, depicting species on the x-axis and sepal width on the y-axis. For this example, it would be easy to do this by simpling swapping the variables in the aes mapping argument. However an alternate way to do this is with a coordinate transformation function. Here we use coord_flip to flip the x- and y-axes: ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species, color=Species), width=0.05, height=0.1, alpha=0.5) + coord_flip() We’ll see other uses of coordinate transformations in later lectures. 5.6 Histograms Histograms are probably the most common way to depict univariate data. In a histogram rather than showing individual observations, we divide the range of the data into a set of bins, and use vertical bars to depict the number (frequency) of observations that fall into each bin. This gives a good sense of the intervals in which most of the observations are found. The geom, geom_histogram, takes care of both the geometric representation and the statistical transformations of the data necessary to calculate the counts in each binn. Here’s the simplest way to use geom_histogram: ggplot(iris) + geom_histogram(aes(x = Sepal.Width)) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The default number of bins that geom_histogram uses is 30. For modest size data sets this is often too many bins, so it’s worth exploring how the histogram changes with different bin numbers: ggplot(iris) + geom_histogram(aes(x = Sepal.Width), bins = 10) ggplot(iris) + geom_histogram(aes(x = Sepal.Width), bins = 12) One important thing to note when looking at these histograms with different numbers of bins is that the number of bins used can change your perception of the data. For example, the number of peaks (modes) in the data can be very sensitive to the bin number as can the perception of gaps. 5.6.1 Variations on histograms when considering categorical data As before, we probably want to break the data down by species. Here we’re faced with some choices about how we depict that data. Do we generate a “stacked histogram” to where the colors indicate the number of observations in each bin that belong to each species? Do we generate side-by-side bars for each species? Or Do we generate separate histograms for each species, and show them overlapping? Stacked histograms are the default if we associate a categorical variable with the bar fill color: ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12) To get side-by-side bars, specify “dodge” as the position argument to geom_histogram. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12, position = &quot;dodge&quot;) If you want overlapping histograms, use position = &quot;identity&quot; instead. When generating overlapping histograms like this, you probably want to make the bars semi-transparent so you can can distinguish the overlapping data. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12, position = &quot;identity&quot;, alpha = 0.4) 5.7 Faceting to depict categorical information Yet another way to represent the histograms for the three species is to using faceting, the create subplots for each species. Faceting is the operation of subsetting the data with respect to a discrete or categorical variable of interest, and generating the same plot type for each subset. Here we use the “ncol” argument to the facet_wrap function to specify that the subplots should be drawn in a single vertical column to facilitate comparison of the distributions. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12) + facet_wrap(~Species, ncol = 1) 5.8 Density plots One shortcoming of histograms is that they are sensitive to the choice of bin margins and the number of bins. An alternative is a “density plot”, which you can think of as a smoothed version of a histogram. ggplot(iris) + geom_density(aes(x = Sepal.Width, fill = Species), alpha=0.25) Density plots still make some assumptions that affect the visualization, in particular a “smoothing bandwidth” (specified by the argument bw) which determines how course or granular the density estimation is. Note that the vertical scale on a density plot is no longer counts (frequency) but probability density. In a density plot, the total area under the plot adds up to one. Intervals in a density plot therefore have a probabilistic intepretation. 5.9 Violin or Beanplot A violin plot (sometimes called a bean plot) is closely related to a density plot. In fact you can think of a violin plot as a density plot rotated 90 degress and mirrored left/right. ggplot(iris) + geom_violin(aes(x = Species, y = Sepal.Width, color = Species, fill=Species), alpha = 0.25) 5.10 Boxplots Boxplots are another frequently used univariate visualization. Boxplots provide a compact summary of single variables, and are most often used for comparing distributions between groups. A standard box plot depicts five useful features of a set of observations: 1) the median (center most line); 2 and 3) the first and third quartiles (top and bottom of the box); 4) the whiskers of a boxplot extend from the first/third quartile to the highest value that is within 1.5 * IQR, where IQR is the inter-quartile range (distance between the first and third quartiles); 5) points outside of the whiskers are usually consider extremal points or outliers. There are many variants on box plots, particularly with respect to the “whiskers”. It’s always a good idea to be explicit about what a box plot you’ve created shows. ggplot(iris) + geom_boxplot(aes(x = Species, y = Sepal.Width, color = Species)) Boxplots are most commonly drawn with the cateogorical variable on the x-axis. 5.11 Building complex visualizations with layers All of our ggplot2 examples up to now have involved a single geom. We can think of geoms as “layers” of information in a plot. One of the powerful features of plotting useing ggplot2 is that it is trivial to combine layers to make more complex plots. The template for multi-layered plots is a simple extension of the single layer: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION1&gt;(mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION2&gt;(mapping = aes(&lt;MAPPINGS&gt;)) 5.12 Useful combination plots Boxplot or violin plots represent visual summaries/simplifications of the underlying data. This is useful but sometimes key information is lost in the process of summarizing. Combining these plots with a strip plot give you both the “birds eye view” as well as granular information. 5.12.1 Boxplot plus strip plot Here’s an example of combining box plots and strip plots: ggplot(iris) + # outlier.shape = NA suppresses the depiction of outlier points in the boxplot geom_boxplot(aes(x = Species, y = Sepal.Width), outlier.shape = NA) + # size sets the point size for the jitter plot geom_jitter(aes(x = Species, y = Sepal.Width), width=0.2, height=0.05, alpha=0.35, size=0.75) Note that I suppressed the plotting of outliers in geom_boxplot so as not to draw the same points twice (the individual data are drawn by geom_jitter). 5.12.2 Setting shared aesthetics The example above works well, but you might have noticed that there’s some repetition of code. In particular, we set the same aesthetic mapping in both geom_boxplot and geom_jitter. It turns out that creating layers that share some of the same aesthetic values is a common case. To deal with such cases, you can specify shared aesthetic mappings as an argument to the ggplot function and then set additional aesthetics specific to each layer in the individual geoms. Using this approach, our previous example can be written more compactly as follow. ggplot(iris, mapping = aes(x = Species, y = Sepal.Width)) + geom_boxplot(outlier.shape = NA) + # note how we specify a layer specific aesthetic in geom_jitter geom_jitter(aes(color = Species), width=0.2, height=0.05, alpha=0.5, size=0.75) 5.13 ggplot layers can be assigned to variables The function ggplot() returns a “plot object” that we can assign to a variable. The following example illustrates this: # create base plot object and assign to variable p # this does NOT draw the plot p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width)) In the code above we created a plot object and assigned it to the variable p. However, the plot wasn’t drawn. To draw the plot object we evaluate it as so: p # try to draw the plot object The code block above didn’t generate an image, because we haven’t added a geom to the plot to determine how our data should be drawn. We can add a geom to our pre-created plot object as so: # add a point geom to our base layer and draw the plot p + geom_boxplot() If we wanted to we could have assigned the geom to a variable as well: box.layer &lt;- geom_boxplot() p + box.layer In this case we don’t really gain anything by creating an intermediate variable, but for more complex plots or when considering different versions of a plot this can be very useful. 5.13.1 Violin plot plus strip plot Here is the principle of combining layers, applied to a combined violin plot + strip plot. Again, we set shared aesthetic mappings in ggplot function call and this time we assign individual layers of the plot to variables. p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width, color = Species)) violin.layer &lt;- geom_violin() jitter.layer &lt;- geom_jitter(width=0.15, height=0.05, alpha=0.5, size=0.75) p + violin.layer + jitter.layer # combined layers of plot and draw 5.14 Adding titles and tweaking axis labels ggplot2 automatically adds axis labels based on the variable names in the data frame passed to ggplot. Sometimes these are appropriate, but more presentable figures you’ll usually want to tweak the axis labs (e.g. adding units). The labs (short for labels) function allows you to do so, and also let’s you set a title for your plot. We’ll illustrate this by modifying our previous figure. Note that we save considerable amounts of re-typing since we had already assigned three of the plot layers to variables in the previous code block: p + violin.layer + jitter.layer + labs(x = &quot;Species&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Sepal Width Distributions for Three Iris Species&quot;) 5.15 ggplot2 themes By now you’re probably familiar with the default “look” of plots generated by ggplot2, in particular the ubiquitous gray background with a white grid. This default works fairly well in the context of RStudio notebooks and HTML output, but might not work as well for a published figure or a slide presentation. Almost every individual aspect of a plot can be tweaked, but ggplot2 provides an easier way to make consistent changes to a plot using “themes”. You can think of a theme as adding another layer to your plot. Themes should generally be applied after all the other graphical layers are created (geoms, facets, labels) so the changes they create affect all the prior layers. There are eight default themes included with ggplot2, which can be invoked by calling the corresponding theme functions: theme_gray, theme_bw, theme_linedraw, theme_light, theme_dark, theme_minimal, theme_classic, and theme_void (See http://ggplot2.tidyverse.org/reference/ggtheme.html for a visual tour of all the default themes) For example, let’s generate a boxplot using theme_bw which get’s rid of the gray background: # create another variable to hold combination of three previous # ggplot layers. I&#39;m doing this because I&#39;m going to keep re-using # the same plot in the following code blocks violin.plus.jitter &lt;- p + violin.layer + jitter.layer violin.plus.jitter + theme_bw() Another theme, theme_classic, remove the grid lines completely, and also gets rid of the top-most and right-most axis lines. violin.plus.jitter + theme_classic() 5.15.1 Further customization with ggplot2::theme In addition to the eight complete themes, there is a theme function in ggplot2 that allows you to tweak particular elements of a theme (see ?theme for all the possible options). For example, to tweak just the aspect ratio of a plot (the ratio of width to height), you can set the aspect.ratio argument in theme: violin.plus.jitter + theme_classic() + theme(aspect.ratio = 1) Theme related function calls can be combined to generate new themes. For example, let’s create a theme called my.theme by combining theme_classic with a call to theme: my.theme &lt;- theme_classic() + theme(aspect.ratio = 1) We can then apply this theme as so: violin.plus.jitter + my.theme 5.16 Other aspects of ggplots can be assigned to variables Plot objects, geoms and themes are not the only aspects of a figure that can be assigned to variables for later use. For example, we can create a label object: my.labels &lt;- labs(x = &quot;Species&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Sepal Width Distributions for Three Iris Species&quot;) Combining all of our variables as so, we generate our new plot: violin.plus.jitter + my.labels + my.theme 5.17 Bivariate plots Now we turn our attention to some useful representations of bivariate distributions. For the purposes of these illustrations I’m initially going to restrict my attention to just one of the three species represented in the iris data set – the I. setosa specimens. This allows us to introduce a vary useful base function called subset(). subset() will return subsets of a vector or data frames that meets the specified conditions. This can also be accomplished with conditional indexing but subset() is usually less verbose. # create a new data frame composed only of the I. setosa samples setosa.only &lt;- subset(iris, Species == &quot;setosa&quot;) In the examples that follow, I’m going to illustrate different ways of representing the same bivariate distribution – the joint distribution of Sepal Length and Sepal Width – over and over again. To avoid repitition, let’s assign the base ggplot layer to a variable as we did in our previous examples. We’ll also pre-create a label layer. setosa.sepals &lt;- ggplot(setosa.only, mapping = aes(x = Sepal.Length, y = Sepal.Width)) sepal.labels &lt;- labs(x = &quot;Sepal Length (cm)&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Relationship between Sepal Length and Width&quot;, caption = &quot;data from Anderson (1935)&quot;) 5.17.1 Scatter plots A scatter plot is one of the simplest representations of a bivariate distribution. Scatter plots are simple to create in ggplot2 by specifying the appropriate X and Y variables in the aesthetic mapping and using geom_point for the geometric mapping. setosa.sepals + geom_point() + sepal.labels 5.18 Bivariate density plots The density plot, which we introduced as a visualization for univariate data, can be extended to two-dimensional data. In a one dimensional density plot, the height of the curve was related to the relatively density of points in the surrounding region. In a 2D density plot, nested contours (or contours plus colors) indicate regions of higher local density. Let’s illustrate this with an example: setosa.sepals + geom_density2d() + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme The relationship between the 2D density plot and a scatter plot can be made clearer if we combine the two: setosa.sepals + geom_density_2d() + geom_jitter(alpha=0.35) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme 5.19 Combining Scatter Plots and Density Plots with Categorical Information As with many of the univariate visualizations we explored, it is often useful to depict bivariate relationships as we change a categorical variable. To illustrate this, we’ll go back to using the full iris data set. all.sepals &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) all.sepals + geom_point(aes(color = Species, shape = Species), size = 2, alpha = 0.6) + sepal.labels + labs(subtitle = &quot;All species&quot;) + my.theme Notice how in our aesthetic mapping we specified that both color and shape should be used to represent the species categories. The same thing can be accomplished with a 2D density plot. all.sepals + geom_density_2d(aes(color = Species)) + sepal.labels + labs(subtitle = &quot;All species&quot;) + my.theme As you can see, in the density plots above, when you have multiple categorical variables and there is significant overlap in the range of each sub-distribution, figures can become quite busy. As we’ve seen previously, faceting (conditioning) can be a good way to deal with this. Below a combination of scatter plots and 2D density plots, combined with faceting on the species variable. all.sepals + geom_density_2d(aes(color = Species), alpha = 0.5) + geom_point(aes(color = Species), alpha=0.5, size=1) + facet_wrap(~ Species) + sepal.labels + labs(subtitle = &quot;All species&quot;) + theme_bw() + theme(aspect.ratio = 1, legend.position = &quot;none&quot;) # get rid of legend In this example I went back to using a theme that includes grid lines to facilitate more accurate comparisons of the distributions across the facets. I also got rid of the legend, because the information there was redundant. 5.20 Density plots with fill Let’s revisit our earlier single species 2D density plot. Instead of simply drawing contour lines, let’s use color information to help guide the eye to areas of higher density. To draw filled contours, we use a sister function to geom_density_2d called stat_density_2d: setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme Using the default color scale, areas of low density are drawn in dark blue, whereas areas of high density are drawn in light blue. I personally find this dark -to-light color scale non-intuitive for density data, and would prefer that darker regions indicate area of higher density. If we want to change the color scale, we can use the a scale function (in this case scale_fill_continuous) to set the color values used for the low and high values (this function we’ll interpolate the intervening values for us). NOTE: when specifying color names, R accepts standard HTML color names (see the Wikipedia page on web colors for a list). We’ll also see other ways to set color values in a later class session. setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + # lavenderblush is the HTML standard name for a light purplish-pink color scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme The two contour plots we generated looked a little funny because the contours are cutoff due to the contour regions being outside the limits of the plot. To fix this, we can change the plot limits using the lims function as shown in the following code block. We’ll also add the scatter (jittered) to the emphasize the relationship between the levels, and we’ll change the title for the color legend on the right by specifying a text label associated with the fill arguments in the labs function. setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + geom_jitter(alpha=0.5, size = 1.1) + # customize labels, including legend label for fill labs(x = &quot;Sepal Length(cm)&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Relationship between sepal length and width&quot;, subtitle = &quot;I. setosa specimens only&quot;, fill = &quot;Density&quot;) + # Set plot limits lims(x = c(4,6), y = c(2.5, 4.5)) + my.theme 5.21 2D bin and hex plots Two dimensional bin and hex plots are alterative ways to represent the joint density of points in the Cartesian plane. Here are examples of to generate these plot types. Compare them to our previous examples. A 2D bin plot can be tought of as a 2D histogram: setosa.sepals + geom_bin2d(binwidth = 0.2) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme A hex plot is similar to a 2D bin plot but uses hexagonal regions instead of squares. Hexagonal bins are useful because they can somtimes avoid visual artefacts sometimes apparent with square bins: setosa.sepals + geom_hex(binwidth = 0.2) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme 5.22 The cowplot package A common task when preparing visualizations for scientific presentations and manuscripts is combining different plots as subfigures of a larger figure. To accomplish this we’ll use a package called cowplot that compliments the power of ggplot2. Install cowplot either via the command line or the R Studio GUI (see Section 2.10). library(cowplot) # assumes package has been installed cowplot allows us to create individual plots using ggplot, and then arrange them in a grid-like fashion with labels for each plot of interest, as you would typically see in publications. The core function of cowplot is plot_grid(), which allows the user to layout the sub-plots in an organized fashion and add labels as necesary. To illustrate plot_grid() let’s create three different representations of the distribution of sepal width in the irisu data set, and combine them into a single figure: p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width, color = Species)) # for the histogram we&#39;re going to override the mapping because # geom_histogram only takes an x argument plot.1 &lt;- p + geom_histogram(bins=12, mapping = aes(x = Sepal.Width), inherit.aes = FALSE) plot.2 &lt;- p + geom_boxplot() plot.3 &lt;- p + geom_violin() plot_grid(plot.1, plot.2, plot.3) If instead, we wanted to layout the plots in a single row we could change the call to plot_grid as so: plot_grid(plot.1, plot.2, plot.3, nrow = 1, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) Notice we also added labels to our sub-plots. "],
["introduction-to-the-dplyr-package.html", "Chapter 6 Introduction to the dplyr package 6.1 Libraries 6.2 Reading data with the readr package 6.3 A note on “tibbles” 6.4 Data filtering and transformation with dplyr 6.5 dplyr’s “verbs” 6.6 Pipes", " Chapter 6 Introduction to the dplyr package In today’s class we introduce a new package, dplyr, which, along with ggplot2 will be used in almost every class session. We will also explore in a little more depth the readr package, for reading tabular data. 6.1 Libraries Both readr and dplyr are members of the tidyverse, so a single invocation of library() makes the functions defined in these two packages available for our use: library(tidyverse) 6.2 Reading data with the readr package The readr package defines a number of functions for reading data tables from common file formats like Comma-Separated-Value (CSV) and Tab-Separated-Value (TSV) files. The two most frequently used readr functions we’ll use in this class are read_csv() and read_tsv() for reading CSV and TSV files respectively. There are some variants of these basic function, which you can read about by invoking the help system (?read_csv). 6.2.1 Example data: NC Births For today’s hands on session we’ll use a data set that contains information on 150 cases of mothers and their newborns in North Carolina in 2004. This data set is available at the following URL: https://github.com/Bio723-class/example-datasets/raw/master/nc-births.txt The births data is a TSV file, so we’ll use the read_tsv() function to read it: births &lt;- read_tsv(&quot;https://github.com/Bio723-class/example-datasets/raw/master/nc-births.txt&quot;) Parsed with column specification: cols( fAge = col_integer(), mAge = col_integer(), weeks = col_integer(), premature = col_character(), visits = col_integer(), gained = col_integer(), weight = col_double(), sexBaby = col_character(), smoke = col_character() ) Notice that when you used read_tsv() the function printed information about how it “parsed” the data (i.e. the types it assigned to each of the columns). The variables in the data set are: father’s age (fAge), mother’s age (mAge), weeks of gestation (weeks) whether the birth was premature or full term (premature) number of OB/GYN visits (visits) mother’s weight gained in pounds (gained) babies birth weight (weight) sex of the baby (sexBaby) whether the mother was a smoker (smoke). Notice too that we read the TSV file directly from a remote location via a URL. If instead, you wanted to load a local file on your computer you would specify the “path” – i.e. the location on your hard drive where you stored the file. For example, here is how I would load the same file if it was stored in the Downloads directory on my Mac laptop: # load the data from a local file births &lt;- read_tsv(&quot;/Users/pmagwene/Downloads/nc-births.txt&quot;) 6.2.2 Reading Excel files The tidyverse also includes a package called readxl which can be used to read Excel spreadsheets (recent versions with .xls and .xlsx extensions). Excel files are somewhat more complicated to deal with because they can include separate “sheets”. We won’t use readxl in this class, but documentation and examples of how readxl is used can be found at the page linked above. 6.3 A note on “tibbles” You may have noticed that most of the functions defined in tidyverse related packages return not data frames, but rather something called a “tibble”. You can think about tibbles as light-weight data frames. In fact if you ask about the “class” of a tibble you’ll see that it includes data.frame as one of it’s classes as well as tbl and tbl_df. class(births) [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; There are some minor differences between data frame and tibbles. For example, tibbles print differently in the console and don’t automatically change variable names and types in the same way that standard data frames do. Usually tibbles can be used wherever a standard data frame is expected, but you may occasionally find a function that only works with a standard data frame. It’s easy to convert a tibble to a standard data frame using the as.data.frame function: births.std.df &lt;- as.data.frame(births) For more details about tibbles, see the Tibbles chapter in R for Data Analysis. 6.4 Data filtering and transformation with dplyr dplyr is powerful tool for data filter and transformation. In the same way that ggplot2 attempts to provide a “grammar of graphics”, dplyr aims to provide a “grammar of data manipulation”. In today’s material we will see how dplyr complements and simplifies standard data frame indexing and subsetting operations. However, dplyr is focused only on data frames and doesn’t completely replace the basic subsetting operations, and so being adept with both dplyr and the indexing approaches we’ve seen previously is important. If you’re curious about the name “dplyr”, the package’s originator Hadley Wickham says it’s supposed to invoke the idea of pliers for data frames (Github: Meaning of dplyrs name) 6.5 dplyr’s “verbs” The primary functions in the dplyr package can be thought of as a set of “verbs”, each verb corresponding to a common data manipulation task. Some of the most frequently used verbs/functions in dplyr include: select – select columns filter – filter rows mutate – create new columns arrange– reorder rows summarize – summarize values group_by – split data frame on some grouping variable. Can be powerfully combined with summarize All of these functions return new data frames rather than modifying the existing data frame (though some of the functions support in place modification of data frames via optional arguments).We illustrate these below by example using the NC births data. 6.5.1 select The select function subsets the columns (variables) of a data frame. For example, to select just the weeks and weight columns from the births data set we could do: wks.weight &lt;- select(births, weeks, weight) dim(wks.weight) # dim should be 50 x 2 [1] 150 2 head(wks.weight) # A tibble: 6 x 2 weeks weight &lt;int&gt; &lt;dbl&gt; 1 39 6.880 2 39 7.690 3 40 8.880 4 40 9.000 5 40 7.940 6 40 8.250 The equivalent using standard indexing would be: wks.wt.alt &lt;- births[c(&quot;weeks&quot;, &quot;weight&quot;)] dim(wks.wt.alt) [1] 150 2 head(wks.wt.alt) # A tibble: 6 x 2 weeks weight &lt;int&gt; &lt;dbl&gt; 1 39 6.880 2 39 7.690 3 40 8.880 4 40 9.000 5 40 7.940 6 40 8.250 Notes: * The first argument to all of the dplyr functions is the data frame you’re operating on When using functions defined in dplyr and ggplot2 variable names are (usually) not quoted or used with the $ operator. This is a design feature of these libraries and makes it easier to carry out interactive analyes because it saves a fair amount of typing. 6.5.2 filter The filter function returns those rows of the data set that meet the given logical criterion. For example, to get all the premature babies in the data set we could use filter as so: premies &lt;- filter(births, premature == &quot;premie&quot;) dim(premies) [1] 21 9 The equivalent using standard indexing would be: premies.alt &lt;- births[births$premature == &quot;premie&quot;,] The filter function will work with more than one logical argument, and these are joined together using Boolean AND logic (i.e. intersection). For example, to find those babies that were premature and whose mothers were smokers we could do: smoking.premies &lt;- filter(births, premature == &quot;premie&quot;, smoke == &quot;smoker&quot;) The equivalent call using standard indexing is: # don&#39;t forget the trailing comma to indicate rows! smoking.premies.alt &lt;- births[(births$premature == &quot;premie&quot;) &amp; (births$smoke == &quot;smoker&quot;),] filter also accepts logical statements chained together using the standard Boolean operators. For example, to find babies who were premature or whose moms were older than 35 you could use the OR operator |: premies.or.oldmom &lt;- filter(births, premature == &quot;premie&quot; | fAge &gt; 35) 6.5.3 mutate The mutate function creates a new data frame that is the same as input data frame but with additional variables (columns) as specified by the function arguments. In the example below, I create two new variables, weight.in.kg and a mom.smoked: # to make code more readable it&#39;s sometime useful to spread out # function arguments over multiple lines like I&#39;ve done here births.plus &lt;- mutate(births, weight.in.kg = weight / 2.2, mom.smoked = (smoke == &quot;smoker&quot;)) head(births.plus) # A tibble: 6 x 11 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 31 30 39 full term 13 1 6.880 male smoker 2 34 36 39 full term 5 35 7.690 male nonsmoker 3 36 35 40 full term 12 29 8.880 male nonsmoker 4 41 40 40 full term 13 30 9.000 female nonsmoker 5 42 37 40 full term NA 10 7.940 male nonsmoker 6 37 28 40 full term 12 35 8.250 male smoker # ... with 2 more variables: weight.in.kg &lt;dbl&gt;, mom.smoked &lt;lgl&gt; The equivalent using standard indexing would be to create a new data frame from births, appending the new variables to the end as so: births.plus.alt &lt;- data.frame(births, weight.in.kg = births$weight / 2.2, mom.smoked = (births$smoke == &quot;smoker&quot;)) 6.5.4 arrange Arrange creates a new data frame where the rows are sorted according to their values for one or more variables. For example, to sort by mothers age we could do: young.moms.first &lt;- arrange(births, mAge) head(young.moms.first) # A tibble: 6 x 9 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 18 15 37 full term 12 76 8.440 male nonsmoker 2 NA 16 40 full term 4 12 6.000 female nonsmoker 3 21 16 38 full term 15 75 7.560 female smoker 4 26 17 38 full term 11 30 9.500 female nonsmoker 5 17 17 29 premie 4 10 2.630 female nonsmoker 6 20 17 40 full term 17 38 7.190 male nonsmoker The equivalent to arrange using standard indexing would be to use the information returned by the order function: young.moms.first.alt &lt;- births[order(births$mAge),] head(young.moms.first.alt) # A tibble: 6 x 9 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 18 15 37 full term 12 76 8.440 male nonsmoker 2 NA 16 40 full term 4 12 6.000 female nonsmoker 3 21 16 38 full term 15 75 7.560 female smoker 4 26 17 38 full term 11 30 9.500 female nonsmoker 5 17 17 29 premie 4 10 2.630 female nonsmoker 6 20 17 40 full term 17 38 7.190 male nonsmoker When using arrange, multiple sorting variables can be specified: sorted.by.moms.and.dads &lt;- arrange(births, mAge, fAge) head(sorted.by.moms.and.dads) # A tibble: 6 x 9 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 18 15 37 full term 12 76 8.440 male nonsmoker 2 21 16 38 full term 15 75 7.560 female smoker 3 NA 16 40 full term 4 12 6.000 female nonsmoker 4 17 17 29 premie 4 10 2.630 female nonsmoker 5 20 17 40 full term 17 38 7.190 male nonsmoker 6 26 17 38 full term 11 30 9.500 female nonsmoker If you want to sort in descending order, you can combing arrange with the desc (=descend) function, also defined in dplyr: old.moms.first &lt;- arrange(births, desc(mAge)) head(old.moms.first) # A tibble: 6 x 9 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 NA 41 33 premie 13 0 5.690 female nonsmoker 2 41 40 40 full term 13 30 9.000 female nonsmoker 3 33 40 36 premie 13 23 7.810 female nonsmoker 4 40 40 38 full term 13 38 7.310 male nonsmoker 5 46 39 38 full term 10 35 6.750 male smoker 6 NA 38 32 premie 10 16 2.190 female smoker 6.5.5 summarize summarize applies a function of interest to one or more variables in a data frame, reducing a vector of values to a single value and returning the results in a data frame. This is most often used to calculate statistics like means, medians, count, etc. As we’ll see below, this is powerful when combined with the group_by function. summarize(births, mean.wt = mean(weight), median.wks = median(weeks)) # A tibble: 1 x 2 mean.wt median.wks &lt;dbl&gt; &lt;dbl&gt; 1 7.046 39. You’ll need to be diligent if your data has missing values (NAs). For example, by default the mean function returns NA if any of the input values are NA: summarize(births, mean.gained = mean(gained)) # A tibble: 1 x 1 mean.gained &lt;dbl&gt; 1 NA However, if you read the mean docs (?mean) you’ll see that there is an na.rm argument that indicates whether NA values should be removed before computing the mean. This is what we want so we instead call summarize as follows: summarize(births, mean.gained = mean(gained, na.rm = TRUE)) # A tibble: 1 x 1 mean.gained &lt;dbl&gt; 1 32.45 6.5.6 group_by The group_by function implicitly adds grouping information to a data frame. # group the births by whether mom smoked or not by_smoking &lt;- group_by(births, smoke) The object returned by group_by is a “grouped data frame”: class(by_smoking) [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Some functions, like count() and summarize() (see below) know how to use the grouping information. For example, to count the number of births conditional on mother smoking status we could do: count(by_smoking) # A tibble: 2 x 2 # Groups: smoke [2] smoke n &lt;chr&gt; &lt;int&gt; 1 nonsmoker 100 2 smoker 50 group_by also works with multiple grouping variables, with each added grouping variable specified as an additional argument: by_smoking.and.mAge &lt;- group_by(births, smoke, mAge &gt; 35) 6.5.7 Combining grouping and summarizing Grouped data frames can be combined with the summarize function we saw above. For example, if we wanted to calculate mean birth weight, broken down by whether the baby’s mother smoked or not we could call summarize with our by_smoking grouped data frame: summarize(by_smoking, mean.wt = mean(weight)) # A tibble: 2 x 2 smoke mean.wt &lt;chr&gt; &lt;dbl&gt; 1 nonsmoker 7.180 2 smoker 6.779 Similarly to get the mean birth weight of children conditioned on mothers smoking status and age: summarize(by_smoking.and.mAge, mean(weight)) # A tibble: 4 x 3 # Groups: smoke [?] smoke `mAge &gt; 35` `mean(weight)` &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; 1 nonsmoker FALSE 7.171 2 nonsmoker TRUE 7.258 3 smoker FALSE 6.832 4 smoker TRUE 6.302 6.5.8 Scoped variants of mutate and summarize Both the mutate() and summarize() functions provide “scoped” alternatives, that allow us to apply the operation on a selection of variables. These variants are often used in combination with grouping. We’ll look at the summarize versions – summarize_all(), summarize_at(), and summarize_if(). See the documentation (?mutate_all) for descriptions of the mutate versions. 6.5.8.1 summarize_all() summarize_all() applies a one or more functions to all columns in a data frame. Here we illustrate a simple version of this with the iris data: # group by species by_species &lt;- group_by(iris, Species) # calculate the mean of every variable, grouped by species summarize_all(by_species, mean) # A tibble: 3 x 5 Species Sepal.Length Sepal.Width Petal.Length Petal.Width &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 setosa 5.006 3.428 1.462 0.2460 2 versicolor 5.936 2.770 4.260 1.326 3 virginica 6.588 2.974 5.552 2.026 Note that if we try and apply summarize_all() in the same way to the grouped data frame by_smoking we’ll get a bunch of warning messages: summarize_all(by_smoking, mean) # A tibble: 2 x 9 smoke fAge mAge weeks premature visits gained weight sexBaby &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker NA 26.90 38.55 NA NA NA 7.180 NA 2 smoker NA 26.00 38.54 NA 10.80 NA 6.779 NA Here’s an example of one of these warnings: Warning messages: 1: In mean.default(premature) : argument is not numeric or logical: returning NA This message is telling us that we can’t apply the mean() function to the data frame column premature because this is not a numerical or logical vector. Despite this and the other similar warnings, summarize_all() does return a result, but the means for any non-numeric values are replaced with NAs, as shown below: # A tibble: 2 x 9 smoke fAge mAge weeks premature visits gained weight sexBaby &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker NA 26.9 38.6 NA NA NA 7.18 NA 2 smoker NA 26.0 38.5 NA 10.8 NA 6.78 NA If you examine the output above, you’ll see that there are several variables that are numeric, however we still got NAs when we calculated the grouped means. This is because those variables contain NA values. The mean function has an optional argument, na.rm, which tells the function to remove any missing data before calculating the mean. Thus we can modify our call to summarize_all as follows: # calculate mean of all variables, grouped by smoking status summarize_all(by_smoking, mean, na.rm = TRUE) # A tibble: 2 x 9 smoke fAge mAge weeks premature visits gained weight sexBaby &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker 29.81 26.90 38.55 NA 11.86 32.55 7.180 NA 2 smoker 29.71 26.00 38.54 NA 10.80 32.27 6.779 NA Note that the non-numeric data columns still lead to NA values. 6.5.8.2 summarize_if() summarize_if() is similar to summarize_all(), except it only applies the function of interest to those variables that match a particular predicate (i.e. are TRUE for a particular TRUE/FALSE test). Here we use summarize_if() to apply the mean() function to only those variables (columns) that are numeric. # calculate mean of all numeric variables, grouped by smoking status summarize_if(by_smoking, is.numeric, mean, na.rm = TRUE) # A tibble: 2 x 7 smoke fAge mAge weeks visits gained weight &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker 29.81 26.90 38.55 11.86 32.55 7.180 2 smoker 29.71 26.00 38.54 10.80 32.27 6.779 6.5.8.3 summarize_at() summarize_at() allows us to apply functions of interest only to specific variables. # calculate mean of gained and weight variables, grouped by smoking status summarize_at(by_smoking, c(&quot;gained&quot;, &quot;weight&quot;), mean, na.rm = TRUE) # A tibble: 2 x 3 smoke gained weight &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker 32.55 7.180 2 smoker 32.27 6.779 All three of the scoped summarize functions can also be used to apply multiple functions, by wrapping the function names in a call to dplyr::funs(): # calculate mean and std deviation of # gained and weight variables, grouped by smoking status summarize_at(by_smoking, c(&quot;gained&quot;, &quot;weight&quot;), funs(mean, sd), na.rm = TRUE) # A tibble: 2 x 5 smoke gained_mean weight_mean gained_sd weight_sd &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker 32.55 7.180 15.23 1.434 2 smoker 32.27 6.779 16.65 1.597 summarize_at() accepts as the the argument for variables a character vector of column names, a numeric vector of column positions, or a list of columns generated by the dplyr::vars() function, which can be be used as so: # reformatted to promote readability of arguments summarize_at(by_smoking, vars(gained, weight), funs(mean, sd), na.rm = TRUE) # A tibble: 2 x 5 smoke gained_mean weight_mean gained_sd weight_sd &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker 32.55 7.180 15.23 1.434 2 smoker 32.27 6.779 16.65 1.597 6.5.9 Combining summarize with grouping aesthetics in ggplot2 We’ve already seen an instance of grouping (conditioning) when we used aesthetics like color or fill to distinguish subgroups in different types of statistical graphics. Below is an example where we integrate information from a group_by/summarize operation into a plot: # calculate mean weights, conditioned on smoking status wt.by.smoking &lt;- summarize(by_smoking, mean_weight = mean(weight, na.rm = TRUE)) # create density plot for all the data # and then use geom_vline to draw vertical lines at the means for # each group ggplot(births) + geom_density(aes(x = weight, color = smoke)) + # data drawn from births geom_vline(data = wt.by.smoking, # note use of different data frame! mapping = aes(xintercept = mean_weight, color = smoke), linetype = &#39;dashed&#39;) 6.6 Pipes dplyr includes a very useful operator available called a pipe available to us. Pipes are powerful because they allow us to chain together sets of operations in a very intuitive fashion while minimizing nested function calls. We can think of pipes as taking the output of one function and feeding it as the first argument to another function call, where we’ve already specified the subsequent arguments. Pipes are actually defined in another packaged called magrittr. We’ll look at the basic pipe operator and then look at a few additional “special” pipes that magrittr provides. 6.6.1 Install and load magrittr In magrittr in not already installed, install it via the command line or the RStudio GUI. Having done so, you will need to load magrittr via the library() function: library(magrittr) 6.6.2 The basic pipe operator The pipe operator is designated by %&gt;%. Using pipes, the expression x %&gt;% f() is equivalent to f(x) and the expression x %&gt;% f(y) is equivalent to f(x,y). The documentation on pipes (see ?magrittr) uses the notation lhs %&gt;% rhs where lhs and rhs are short for “left-hand side” and “right-hand side” respectively. I’ll use this same notation in some of the explanations that follow. births %&gt;% head() # same as head(births) # A tibble: 6 x 9 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 31 30 39 full term 13 1 6.880 male smoker 2 34 36 39 full term 5 35 7.690 male nonsmoker 3 36 35 40 full term 12 29 8.880 male nonsmoker 4 41 40 40 full term 13 30 9.000 female nonsmoker 5 42 37 40 full term NA 10 7.940 male nonsmoker 6 37 28 40 full term 12 35 8.250 male smoker births %&gt;% head # you can even leave the parentheses out # A tibble: 6 x 9 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 31 30 39 full term 13 1 6.880 male smoker 2 34 36 39 full term 5 35 7.690 male nonsmoker 3 36 35 40 full term 12 29 8.880 male nonsmoker 4 41 40 40 full term 13 30 9.000 female nonsmoker 5 42 37 40 full term NA 10 7.940 male nonsmoker 6 37 28 40 full term 12 35 8.250 male smoker births %&gt;% head(10) # same as head(births, 10) # A tibble: 10 x 9 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 31 30 39 full term 13 1 6.880 male smoker 2 34 36 39 full term 5 35 7.690 male nonsmoker 3 36 35 40 full term 12 29 8.880 male nonsmoker 4 41 40 40 full term 13 30 9.000 female nonsmoker 5 42 37 40 full term NA 10 7.940 male nonsmoker 6 37 28 40 full term 12 35 8.250 male smoker 7 35 35 28 premie 6 29 1.630 female nonsmoker 8 28 21 35 premie 9 15 5.500 female smoker 9 22 20 32 premie 5 40 2.690 male smoker 10 36 25 40 full term 13 34 8.750 female nonsmoker Multiple pipes can be chained together, such that x %&gt;% f() %&gt;% g() %&gt;% h() is equivalent to h(g(f(x))). # equivalent to: head(arrange(births, weight), 10) births %&gt;% arrange(weight) %&gt;% head(10) # A tibble: 10 x 9 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 35 35 28 premie 6 29 1.630 female nonsmoker 2 NA 18 33 premie 7 40 1.690 male smoker 3 NA 38 32 premie 10 16 2.190 female smoker 4 17 17 29 premie 4 10 2.630 female nonsmoker 5 22 20 32 premie 5 40 2.690 male smoker 6 38 37 26 premie 5 25 3.630 male nonsmoker 7 25 22 34 premie 10 20 3.750 male nonsmoker 8 NA 24 38 full term 16 50 3.750 female nonsmoker 9 30 25 35 premie 15 40 4.500 male smoker 10 19 20 34 premie 13 6 4.500 male nonsmoker When there are multiple piping operations, I like to arrange the statements vertically to help emphasize the flow of processing and to facilitate debugging and/or modification. I would usually rearrange the above code block as follows: births %&gt;% arrange(weight) %&gt;% head(10) # A tibble: 10 x 9 fAge mAge weeks premature visits gained weight sexBaby smoke &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; 1 35 35 28 premie 6 29 1.630 female nonsmoker 2 NA 18 33 premie 7 40 1.690 male smoker 3 NA 38 32 premie 10 16 2.190 female smoker 4 17 17 29 premie 4 10 2.630 female nonsmoker 5 22 20 32 premie 5 40 2.690 male smoker 6 38 37 26 premie 5 25 3.630 male nonsmoker 7 25 22 34 premie 10 20 3.750 male nonsmoker 8 NA 24 38 full term 16 50 3.750 female nonsmoker 9 30 25 35 premie 15 40 4.500 male smoker 10 19 20 34 premie 13 6 4.500 male nonsmoker 6.6.3 An example without pipes To illustrate how pipes help us, first let’s look at an example set of analysis steps without using pipes. Let’s say we wanted to explore the relationship between father’s age and baby’s birth weight. We’ll start this process of exploration by generating a bivariate scatter plot. Being good scientists we want to express our data in SI units, so we’ll need to converts pounds to kilograms. You’ll also recall that a number of the cases have missing data on father’s age, so we’ll want to remove those before we plot them. Here’s how we might accomplish these steps: # add a new column for weight in kg births.kg &lt;- mutate(births, weight.kg = weight / 2.2) # filter out the NA fathers filtered.births &lt;- filter(births.kg, !is.na(fAge)) # create our plot ggplot(filtered.births, aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) Notice that we created two “temporary” data frames along the way – births.kg and filtered.births. These probably aren’t of particular interest to us, but we needed to generate them to build the plot we wanted. If you were particularly masochistic you could avoid these temporary data frames by using nested functions call like this: # You SHOULD NOT write nested code like this. # Code like this is hard to debug and understand! ggplot(filter(mutate(births, weight.kg = weight / 2.2), !is.na(fAge)), aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) 6.6.4 The same example using pipes The pipe operator makes the output of one statement (lhs) as the first input of a following function (rhs). This simplifies the above example to: births %&gt;% mutate(weight.kg = weight / 2.2) %&gt;% filter(!is.na(fAge)) %&gt;% ggplot(aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) In the example above, we feed the data frame into the mutate function. mutate expects a data frame as a first argument, and subsequent arguments specify the new variables to be created. births %&gt;% mutate(weight.kg = weight / 2.2) is thus equivalent to mutate(births, weight.kg = weight / 2.2)). We then pipe the output to filter, removing NA fathers, and then pipe that output as the input to ggplot. As mentioned previously, it’s good coding style to write each discrete step as its own line when using piping. This make it easier to understand what the steps of the analysis are as well as facilitating changes to the code (commenting out lines, adding lines, etc) 6.6.5 Assigning the output of a statement involving pipes to a variable It’s important to recognize that pipes are simply a convenient way to chain together a series of expression. Just like any other compound expression, the output of a series of pipe statements can be assigned to a variable, like so: stats.old.moms &lt;- births %&gt;% filter(mAge &gt; 35) %&gt;% summarize(median.gestation = median(weeks), mean.weight = mean(weight)) stats.old.moms # A tibble: 1 x 2 median.gestation mean.weight &lt;int&gt; &lt;dbl&gt; 1 38 6.939 Note that our summary table, stats.old.moms, is itself a data frame. 6.6.6 Compound assignment pipe operator A fairly common operation when working interactively in R is to update an existing data frame. magrittr defines another pipe operator – %&lt;&gt;% – called the “compound assignment” pipe operator, to facilitate this. The compound assignment pipe operator has the basic usage lhs %&lt;&gt;% rhs. This operator evaluates the function on the rhs using the lhs as the first argument, and then updates the lhs with the resulting value. This is simply shorthand for writing lhs &lt;- lhs %&gt;% rhs. stats.old.moms %&lt;&gt;% # note compound pipe operator! mutate(mean.weight.kg = mean.weight / 2.2) 6.6.7 The dot operator with pipes When working with pipes, sometimes you’ll want to use the lhs in multiple places on the rhs, or as something other than the first argument to the rhs. magrittr provides for this situation by using the dot (.) operator as a placeholder. Using the dot operator, the expression y %&gt;% f(x, .) is equivalent to f(x,y). c(&quot;dog&quot;, &quot;cakes&quot;, &quot;sauce&quot;, &quot;house&quot;) %&gt;% # create a vector sample(1) %&gt;% # pick a random single element of that vector str_c(&quot;hot&quot;, .) # string concatenate the pick with the word &quot;hot&quot; [1] &quot;hothouse&quot; 6.6.8 The exposition pipe operator magrittr defines another operator called the “exposition pipe operator”, designed %$%. This operator exposes the names in the lhs to the expression on the rhs. Here is an example of using the exposition pipe operator to simply return the vector of weights: births %&gt;% filter(premature == &quot;premie&quot;) %$% # note the different pipe operator! weight [1] 1.63 5.50 2.69 6.50 7.81 4.75 3.75 2.19 6.81 4.69 6.75 4.50 5.94 4.50 [15] 5.06 5.69 1.69 6.31 2.63 5.88 3.63 If we wanted to calculate the minimum and maximum weight of premature babies in the data set we could do the following (though I’d usually prefer summarize() unless I needed the results in the form of a vector): births %&gt;% filter(mAge &gt; 35) %$% # note the different pipe operator! c(min(weight), max(weight)) [1] 2.19 10.13 "],
["data-wrangling.html", "Chapter 7 Data wrangling 7.1 Libraries 7.2 Data 7.3 Renaming data frame columms 7.4 Dropping unneeded columns 7.5 Reshaping data with tidyr 7.6 Using your tidy data 7.7 Heat maps", " Chapter 7 Data wrangling In the real world you’ll often create a data set (or be given one) in a format that is less than ideal for analysis. This can happen for a number of reasons. For example, the data may have been recorded in a manner convenient for collection and visual inspection, but which does not work well for analysis and plotting. Or the data may be an amalgamation of multiple experiments, in which each of the experimenters used slightly different naming conventions. Or the data may have been produced by an instrument that produces output with a fixed format. Sometimes important experimental information is included in the column headers of a spreadsheet. Whatever the case, we often find ourselves in the situation where we need to “wrangle” our data into a “tidy” format before we can proceed with visualization and analysis. The “R for Data Science” text discusses some desirable rules for “tidy” data in order to facilitate downstream analyses. These are: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In this lecture we’re going to walk through an extended example of wrangling some data into a “tidy” format. 7.1 Libraries library(magrittr) library(stringr) library(tidyverse) library(cowplot) 7.2 Data To illustrate a standard data wrangling pipeline, we’re going to use a gene expression microarray data set, based on the following paper: Spellman PT, et al. 1998. Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization. Mol Biol Cell 9(12): 3273-97. In this paper, Spellman and colleagues tried to identify all the genes in the yeast genome (&gt;6000 genes) that exhibited oscillatory behaviors suggestive of cell cycle regulation. To do so, they combined gene expression measurements from six different types of cell cycle synchronization experiments. Download the Spellman data to your filesystem from this link (right-click the “Download” button and save to your Downloads folder or similar). I suggest that once you download the data, you open it in a spreadsheet program (e.g. Excel) or use the RStudio Data Viewer to get a sense of what the data looks like. Let’s load it into R, using the read_tsv() function, using the appropriate file path. # the filepath may differ on your computer spellman &lt;- read_tsv(&quot;~/Downloads/spellman-combined.txt&quot;) Parsed with column specification: cols( .default = col_double(), X1 = col_character(), clb = col_character(), alpha = col_character(), cdc15 = col_character(), cdc28 = col_character(), elu = col_character() ) See spec(...) for full column specifications. The initial dimenions of the data frame are: dim(spellman) [1] 6178 83 The six types of cell cycle synchronization experiments are: synchronization by alpha-factor = “alpha” synchronization by cdc15 temperature sensitive mutants = “cdc15” synchronization by cdc28 temperature sensitive mutants = “cdc28” synchronization by elutration = “elu” synchronization by cln3 mutatant strains = “cln3” synchronization by clb2 mutant strains = “clb2” 7.3 Renaming data frame columms Notice that when we imported the data we got a warning message: Missing column names filled in: 'X1' [1]. In a data frame, every column must have a name. The first column of our data set did not have a name in the header, so read_tsv automatically gave it the name X1. Our first task is to give the first column a more meaningful name. This column gives “systematic gene names” – a standardized naming scheme for genes in the yeast genome. We’ll use dplyr::rename to do rename X1 to gene. Note that rename can take multiple arguments if you need to rename multiple columns simultaneously. spellman.clean &lt;- spellman %&gt;% rename(gene = X1) 7.4 Dropping unneeded columns Take a look at the Spellman data again in your spreadsheet program (or the R Studio data viewer). You’ll notice there are some blank columns. These are simply visual organizing elements that the creator of the spreadsheet added to separate the different experiments that are included in the data set. We can use dplyr::select to drop columns by prependingcolumn names with the negative sign: # drop the alpha column keeping all others spellman.clean %&lt;&gt;% select(-alpha) Note that usually select() keeps only the variables you specify. However if the first expression is negative, select will instead automatically keep all variables, dropping only those you specify. 7.4.1 Finding all empty columns In the example above, we looked at the data and saw that the “alpha” column was empty, and thus dropped it. This worked because there are only a modest number of columns in the data frame in it’s initial form. However, if our data frame contained thousands of columns, this “look and see” procedure would not be efficient. Can we come up with a general solution for removing empty columns from a data frame? When you load a data frame from a spreadsheet, empty cells are given the value NA. In previous class sessions we were introduced to the function is.na() which tests each value in a vector or data frame for whether it’s NA or not. We can count NA values in a vector by summing the output of is.na(). Conversely we can count the number of “not NA” items by using the negation operator (!): # count number of NA values in the alpha0 column sum(is.na(spellman$alpha0)) [1] 165 # count number of values that are NOT NA in alpha0 sum(!is.na(spellman$alpha0)) [1] 6013 This seems like it should get us close to a solution but sum(is.na(..)) when applied to a data frame counts NAs across the entire data frame, not column-by-column. # doesn&#39;t do what we hoped! sum(is.na(spellman)) [1] 59017 If we want sums of NAs by column, we instead use the colSums() function: # get number of NAs by column colSums(is.na(spellman)) X1 cln3-1 cln3-2 clb clb2-2 clb2-1 alpha 0 193 365 6178 454 142 6178 alpha0 alpha7 alpha14 alpha21 alpha28 alpha35 alpha42 165 525 191 312 267 207 123 alpha49 alpha56 alpha63 alpha70 alpha77 alpha84 alpha91 257 147 186 185 178 155 329 alpha98 alpha105 alpha112 alpha119 cdc15 cdc15_10 cdc15_30 209 174 222 251 6178 677 477 cdc15_50 cdc15_70 cdc15_80 cdc15_90 cdc15_100 cdc15_110 cdc15_120 501 608 573 562 606 570 611 cdc15_130 cdc15_140 cdc15_150 cdc15_160 cdc15_170 cdc15_180 cdc15_190 495 574 811 583 571 803 613 cdc15_200 cdc15_210 cdc15_220 cdc15_230 cdc15_240 cdc15_250 cdc15_270 1014 573 741 596 847 379 537 cdc15_290 cdc28 cdc28_0 cdc28_10 cdc28_20 cdc28_30 cdc28_40 426 6178 122 72 67 55 66 cdc28_50 cdc28_60 cdc28_70 cdc28_80 cdc28_90 cdc28_100 cdc28_110 56 82 84 75 237 165 319 cdc28_120 cdc28_130 cdc28_140 cdc28_150 cdc28_160 elu elu0 312 1439 2159 521 543 6178 122 elu30 elu60 elu90 elu120 elu150 elu180 elu210 153 175 132 103 119 111 118 elu240 elu270 elu300 elu330 elu360 elu390 131 110 112 112 156 114 Columns with all missing values can be more conveniently found by asking for those columns where the number of “not missing” values is zero: # get names of all columns for which all rows are NA # useing standard indexing names(spellman)[colSums(!is.na(spellman)) == 0] [1] &quot;clb&quot; &quot;alpha&quot; &quot;cdc15&quot; &quot;cdc28&quot; &quot;elu&quot; We can combine the colSums(!is.na()) idiom with the dplyr::select_if function to quickly remove all empty columns as so: spellman.clean %&lt;&gt;% # keep ONLY the non-empty columns select_if(colSums(!is.na(.)) &gt; 0) 7.4.2 Dropping columns by matching names Only two time points from the cln3 and clb2 experiments were reported in the original publication. Since complete time series are unavailable for these two experimental conditions we will drop them from further consideration. select() can be called be called with a number of “helper function” (?select_helpers). Here we’ll illustrate the matches() helper function which matches column names to a “regular expression”. Regular expressions (also referred to as “regex” or “regexp”) are a way of specifying patterns in string. For the purposes of this document we’ll illustrate regexs by example; for a more detailed explanation of regular expressions see the the regex help(?regex) and the Chapter on Strings in “R for Data Analysis”: Let’s see how to drop all the “cln3” and “clb2” columns from the data frame using matches(): spellman.clean %&lt;&gt;% select(-matches(&quot;cln3&quot;)) %&gt;% select(-matches(&quot;clb2&quot;)) If we wanted we could have collapsed our two match statements into one as follows: spellman.clean %&lt;&gt;% select(-matches(&quot;cln3|clb2&quot;)) In this second example, the character “|” is specifing an OR match within the regular expression, so this regular expression matches column names that contain “cln3” OR “clb2”. 7.5 Reshaping data with tidyr The tidyr package provides functions for reshaping or tidying data frames. tidyr is yet another component of the tidyverse, and thus was loaded by the library(tidyverse). Today we’re going to look at two functions tidyr::gather() and tidyr::extract(), and how they can be combined with now familiar dplyr functions we’ve seen previously. The reading assignment for today’s class session covers a variety of other functions defined in tidyr. The Spellman data, as I provided it to you, is in what we would call “wide” format. Each column (besides the gene column) corresponds to an experimental condition and time point. For example, “alpha0” is the alpha-factor experiment at time point 0 mins; “alpha7” is the alpha-factor experiment at time point 7 mins, etc. The cells within each column correspond to the expression of a corresponding gene (given by the first column which we renamed gene) in that particular experiment at that particular time point. In every column (except “gene”), the cellsrepresents the same abstract property of interest – the expression of a gene of interest in a particular experiment/time point. Our first task will be to rearrange our “wide” data frame that consists of many different columns representing gene expression into a “long” data frame with just a single column representing expression. We’ll also create a new column to keep track of the which experiment and time point the measurement came from. 7.5.1 Wide to long conversions using tidyr::gather tidyr::gather() takes multiple columns, and collapses them together into a smaller number of new columns. When using gather() you give the names of the new columns to create, as well as the names of any existing columns gather should not collect together. Here we want to collapse all 73 or the expression columns – “alpha0” to “elu390” – into two columns: 1) a column to represent the expt/time point of the measurement, and 2) a column to represent the corresponding expression value. The only column we don’t want to touch is the gene column with our gene names. # convert &quot;wide&quot; data to &quot;long&quot; spellman.long &lt;- spellman.clean %&gt;% gather(expt.and.time, expression, -gene) Take a moment to look at the data in the “long format”: head(spellman.long) # A tibble: 6 x 3 gene expt.and.time expression &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 YAL001C alpha0 -0.1500 2 YAL002W alpha0 -0.1100 3 YAL003W alpha0 -0.1400 4 YAL004W alpha0 -0.02000 5 YAL005C alpha0 -0.05000 6 YAL007C alpha0 -0.6000 And compare the dimensions of the wide data to the new data: dim(spellman.clean) # for comparison [1] 6178 74 dim(spellman.long) [1] 450994 3 As you see, we’ve gone from a data frame with 6178 rows and 74 columns (wide format), to a new data frame with 450994 rows and 3 columns (long format). 7.5.2 Extracting information from combined variables using tidyr::extract The column expt.and.time violates one of our principles of tidy data: “Each variable must have its own column.”. This column conflates two different types of information – the experiment type and the time point of the measurement. Our next task is to split this information up into two new variables, which will help to facilitate downstream plotting and analysis. One complicating factor is that the different experiments/time combinations have different naming conventions: The “alpha” and “elu” experiments are of the form “alpha0”, “alpha7”, “elu0”, “elu30”, etc. In this case, the first part of the string gives the experiment type (either alpha or elu) and the following digits give the time point. In the “cdc15” and “cdc28” experiments the convention is slightly different; they are of the form “cdc15_0”, “cdc15_10”, “cdc28_0”, “cdc28_10”, etc. Here the part of the string before the underscore gives the experiment type, and the digits after the underscore give the time point. Because of the differences in naming conventions, we will find it easiest to break up spellman.long into a series of sub-data sets corresponding to each experiment type in order to extract out the experiment and time information. After processing each data subset separately, we will join the modified sub-data frames back together. 7.5.3 Subsetting rows Let’s start by getting just the rows corresponding to the “alpha” experiment/times. Here we use dplyr::filter in combination with stringr::str_detect to get all those rows in which the expt.and.time variable contains the string “alpha”. alpha.long &lt;- spellman.long %&gt;% filter(str_detect(expt.and.time, &quot;alpha&quot;)) # look at the new data frame dim(alpha.long) [1] 111204 3 head(alpha.long, n = 10) # A tibble: 10 x 3 gene expt.and.time expression &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 YAL001C alpha0 -0.1500 2 YAL002W alpha0 -0.1100 3 YAL003W alpha0 -0.1400 4 YAL004W alpha0 -0.02000 5 YAL005C alpha0 -0.05000 6 YAL007C alpha0 -0.6000 7 YAL008W alpha0 -0.2800 8 YAL009W alpha0 -0.03000 9 YAL010C alpha0 -0.05000 10 YAL011W alpha0 -0.3100 7.5.4 Splitting columns Having subsetted the data, we can now split expt.and.time into two new variables – expt and time. To do this we use tidyr::extract. alpha.long %&lt;&gt;% tidyr::extract(expt.and.time, # column we&#39;re extracting from c(&quot;expt&quot;, &quot;time&quot;), # new columns we&#39;re creating regex=&quot;(alpha)([[:digit:]]+)&quot;, # regexp (see below) convert=TRUE) # automatically convert column types # NOTE: I&#39;m being explict about saying tidyr::extract because the # magrittr package defines a different extract function Let’s take a moment to look at the regex argument to extract – regex=&quot;(alpha)([[:digit:]]+)&quot;. The regex is specified as a character string. Each part we want to match and extract is surround by parentheses. In this case we have two sets of parentheses corresponding to the two matches we want to make. The first part of the regex is (alpha); here we’re looking to make an exact match to the string “alpha”. The second part of the regex reads ([[:digit:]]+). [[:digit:]] indicates we’re looking for a numeric digit. The + after [[:digit:]] indicates that we want to match one or more digits (i.e. to get a match we need to find at least one digit, but more than one digit should also be a match). Let’s take a look at the new version of alpha.long following application of extract: head(alpha.long, n = 10) # A tibble: 10 x 4 gene expt time expression &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 YAL001C alpha 0 -0.1500 2 YAL002W alpha 0 -0.1100 3 YAL003W alpha 0 -0.1400 4 YAL004W alpha 0 -0.02000 5 YAL005C alpha 0 -0.05000 6 YAL007C alpha 0 -0.6000 7 YAL008W alpha 0 -0.2800 8 YAL009W alpha 0 -0.03000 9 YAL010C alpha 0 -0.05000 10 YAL011W alpha 0 -0.3100 Notice our two new variables, both of which have appropriate types! A data frame for the elutriation data can be created similarly: elu.long &lt;- spellman.long %&gt;% filter(str_detect(expt.and.time, &quot;elu&quot;)) %&gt;% tidyr::extract(expt.and.time, # column we&#39;re extracting from c(&quot;expt&quot;, &quot;time&quot;), # new columns we&#39;re creating regex=&quot;(elu)([[:digit:]]+)&quot;, # regexp (see below) convert=TRUE) # automatically convert column types 7.5.4.1 A fancier regex for the cdc experiments Now let’s process the cdc experiments (cdc15 and cdc28). As before we extract the corresponding rows of the data frame using filter and str_detect. We then split expt.and.time using tidyr::extract. In this case we carry out the two steps in a single code block using pipes: cdc.long &lt;- spellman.long %&gt;% # both cdc15 and cdc28 contain &quot;cdc&quot; as a sub-string filter(str_detect(expt.and.time, &quot;cdc&quot;)) %&gt;% tidyr::extract(expt.and.time, c(&quot;expt&quot;, &quot;time&quot;), regex=&quot;(cdc15|cdc28)_([[:digit:]]+)&quot;, # note the fancier regex convert=TRUE) The regex – &quot;(cdc15|cdc28)_([[:digit:]]+)&quot; – is slightly fancier in this example. As before there are two parts we’re extracting: (cdc15|cdc28) and ([[:digit:]]+). The first parenthesized regexp is an “OR” – i.e. match “cdc15” or “cdc28”. The second parenthesized regexp is the same as we saw previously. Separating the two parenthesized regexps is an underscore (_). The underscore isn’t parenthesized because we only want to use it to make a match not to extract the corresponding match. 7.5.5 Combining data frame rows If you have two or more data frames with identical columns, the rows of the data frame can be combined using rbind (defined in the base package). For example, to reassemble the alpha.long, elu.long, and cdc.long data frames into a single data frame we do: spellman.final &lt;- rbind(alpha.long, elu.long, cdc.long) # check the dimensions of the new data frame dim(spellman.final) [1] 450994 4 7.5.6 Sorting data frame rows Currently the spellman.final data frame is sorted by time point and experiment. head(spellman.final, n = 10) # A tibble: 10 x 4 gene expt time expression &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 YAL001C alpha 0 -0.1500 2 YAL002W alpha 0 -0.1100 3 YAL003W alpha 0 -0.1400 4 YAL004W alpha 0 -0.02000 5 YAL005C alpha 0 -0.05000 6 YAL007C alpha 0 -0.6000 7 YAL008W alpha 0 -0.2800 8 YAL009W alpha 0 -0.03000 9 YAL010C alpha 0 -0.05000 10 YAL011W alpha 0 -0.3100 It might be useful instead to sort by gene and experiment. To do this we can use dplyr::arrange: spellman.final %&lt;&gt;% arrange(gene, expt) # look again at the rearranged data head(spellman.final, n = 10) # A tibble: 10 x 4 gene expt time expression &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 YAL001C alpha 0 -0.1500 2 YAL001C alpha 7 -0.1500 3 YAL001C alpha 14 -0.2100 4 YAL001C alpha 21 0.1700 5 YAL001C alpha 28 -0.4200 6 YAL001C alpha 35 -0.4400 7 YAL001C alpha 42 -0.1500 8 YAL001C alpha 49 0.2400 9 YAL001C alpha 56 -0.1000 10 YAL001C alpha 63 NA 7.5.7 Collapsing columns using tidyr::unite unite combines the data from multiple columns into one, separating the column data with a consistent separator character (the underscore character “_&quot; by default). For example, if we wanted to combined the experiment and time columns again (recall that we previously separated with with extract) we could do: spellman.recombined &lt;- spellman.final %&gt;% unite(expt.and.time, expt, time) # give new column name first # then give columns to combine dim(spellman.recombined) [1] 450994 3 head(spellman.recombined, n = 10) # A tibble: 10 x 3 gene expt.and.time expression &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 YAL001C alpha_0 -0.1500 2 YAL001C alpha_7 -0.1500 3 YAL001C alpha_14 -0.2100 4 YAL001C alpha_21 0.1700 5 YAL001C alpha_28 -0.4200 6 YAL001C alpha_35 -0.4400 7 YAL001C alpha_42 -0.1500 8 YAL001C alpha_49 0.2400 9 YAL001C alpha_56 -0.1000 10 YAL001C alpha_63 NA 7.5.8 Splitting columns using tidyr::separate separate splits a column into multiple columns. It’s very much like extract except that it only takes a single regex for the separator, which by default is any non-alphabetic or non-numeric character. For example, to re-split expt.and.time in spellman.recombined we could do: spellman.respread &lt;- spellman.recombined %&gt;% separate(expt.and.time, c(&quot;expt&quot;, &quot;time&quot;), convert = TRUE) dim(spellman.respread) [1] 450994 4 head(spellman.respread,n= 10) # A tibble: 10 x 4 gene expt time expression &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; 1 YAL001C alpha 0 -0.1500 2 YAL001C alpha 7 -0.1500 3 YAL001C alpha 14 -0.2100 4 YAL001C alpha 21 0.1700 5 YAL001C alpha 28 -0.4200 6 YAL001C alpha 35 -0.4400 7 YAL001C alpha 42 -0.1500 8 YAL001C alpha 49 0.2400 9 YAL001C alpha 56 -0.1000 10 YAL001C alpha 63 NA Faced with the task of splitting columns, try tidyr::separate first and only consider tidyr::extract if you need to defines more sophisticated rules for splitting. 7.5.9 Long-to-wide conversion using tidyr::spread tidyr::spread is the inverse of tidyr::gather. gather() took multiple columns and collapsed them together into a smaller number of new columns. The tidyr documentation calls this “collapsing into key-value pairs”. By contrast, spread() creates new columns by spreading “key-value pairs” (a column representing the “keys” and a column reprsenting the “values”) into multiple columns. Here let’s use spread to use the gene names (the “key”) and expression measures (the “values”) to create a new data frame where the genes are the primary variables (columns) of the data. spellman.gene.focused &lt;- spellman.final %&gt;% spread(gene, expression) dim(spellman.gene.focused) [1] 73 6180 Both “long” and “wide” tidy versions of the same data are useful, depending on the specific analyses and/or visualiztions you want to accomplish. We’ll look at applications of both in the sections that follow. 7.6 Using your tidy data Whew – that was a fair amount of work to tidy our data! But having done so we can now carry out a wide variety of very powerful analyses. 7.6.1 Visualizing gene expression time series Let’s start by walking through a series of visualizations of gene expression time series. Each plot will show the expression of one or more genes, at different time points, in one or more experimental conditions. Our initial visualizations exploit the “long” versions of the tidy data. First a single gene in a single experimental condition: spellman.final %&gt;% filter(expt == &quot;alpha&quot;, gene == &quot;YAL022C&quot;) %&gt;% ggplot(aes(x = time, y = expression)) + geom_line() + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression of YAL022C&quot;) We can easily modify the above code block to visualize the expression of multiple genes of interest: genes.of.interest &lt;- c(&quot;YAL022C&quot;, &quot;YAR018C&quot;, &quot;YGR188C&quot;) spellman.final %&gt;% filter(gene %in% genes.of.interest, expt == &quot;alpha&quot;) %&gt;% ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of multiple genes\\nfollowing synchronization by alpha factor&quot;) By employing facet_wrap() we can visualize the relationship between this set of genes in each of the experiment types: spellman.final %&gt;% filter(gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + facet_wrap(~ expt) + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of Multiple Genes\\nAcross experiments&quot;) The different experimental treatments were carried out for varying lengths of time due to the differences in their physiological effects. Plotting them all on the same time scale can obscure that patterns of oscillation we might be interested in, so let’s modify our code block so that plots that share the same y-axis, but have differently scaled x-axes. spellman.final %&gt;% filter(gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + facet_wrap(~ expt, scales = &quot;free_x&quot;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of Multiple Genes\\nAcross experiments&quot;) 7.6.2 Finding the most variable genes When dealing with vary large data sets, one ad hoc filtering criteria that is often employed is to focus on those variables that exhibit that greatest variation. To do this, we first need to order our variables (genes) by their variation. Let’s see how we can accomplish this using our long data frame: by.variance &lt;- spellman.final %&gt;% group_by(gene) %&gt;% summarize(expression.var = var(expression, na.rm = TRUE)) %&gt;% arrange(desc(expression.var)) head(by.variance) # A tibble: 6 x 2 gene expression.var &lt;chr&gt; &lt;dbl&gt; 1 YLR286C 2.157 2 YNR067C 1.733 3 YNL327W 1.653 4 YGL028C 1.571 5 YHL028W 1.521 6 YKL164C 1.515 The code above calculates variance but ignores the fact that we have different experimental conditions. To take into account the experimental design of the data at hand, let’s calculate the average variance across the experimental conditions: by.avg.variance &lt;- spellman.final %&gt;% group_by(gene, expt) %&gt;% summarize(expression.var = var(expression, na.rm = TRUE)) %&gt;% group_by(gene) %&gt;% summarize(avg.expression.var = mean(expression.var)) %&gt;% arrange(desc(avg.expression.var)) head(by.avg.variance) # A tibble: 6 x 2 gene avg.expression.var &lt;chr&gt; &lt;dbl&gt; 1 YFR014C 3.579 2 YFR053C 2.377 3 YBL032W 2.299 4 YDR274C 2.173 5 YLR286C 2.128 6 YMR206W 1.937 Based on the average experession variance across experimental conditions, let’s get the names of the 1000 most variable genes: top.genes.1k &lt;- by.avg.variance[1:1000,]$gene head(top.genes.1k) [1] &quot;YFR014C&quot; &quot;YFR053C&quot; &quot;YBL032W&quot; &quot;YDR274C&quot; &quot;YLR286C&quot; &quot;YMR206W&quot; 7.6.3 Exploring bivariate relationships using “wide” data The “long” version of our data frame proved useful for exploring how gene expression changed over time. By contrast, our “wide” data frame is more convient for exploring how pairs of genes covary together. For example, we can generate bivariate scatter plots depicting the relationship between two genes in the four different experimental conditions: spellman.gene.focused %&gt;% filter(!is.na(YAL022C) &amp; !is.na(YAR018C)) %&gt;% # filter any missing values ggplot(aes(x = YAL022C, y = YAR018C)) + geom_point() + theme(aspect.ratio = 1) + facet_wrap(~expt, nrow = 2, ncol = 2) 7.6.4 Calculating correlations between genes Let’s explore more global bivariate relationships by calculating all the pairwise correlations between genes, focusing our attention on only the top 1000 most variable genes (as calculated previously). Correlation is a measure of linear association between a pair of variables, and ranges from -1 to 1. A value near zero indicates the variables are uncorrelated (no linear association), while values approaching +1 indicate a strong positive association (the variables tend to get bigger or smaller together) while values near -1 indicate strong negative association (when one variable is larger, the other tends to be small). First, let’s reduce our “wide” data frame to including only the top 1000 genes: top.1k.wide &lt;- spellman.gene.focused %&gt;% # get expt and time column plus top 1000 genes select(expt, time, top.genes.1k) dim(top.1k.wide) [1] 73 1002 Now, we calculate the correlations between all those genes. We need to drop two columns from the data frame, corresponding to “expt” and “time”, and we then use the cor function to calculate the pairwise correlations. Since there is missing data (indicated with NAs) we also need to tell the cor function to only use pairwise complete observations when calculating correlations. spellman.cor &lt;- top.1k.wide %&gt;% select(-expt, -time) %&gt;% # drop expt and time cor(use = &quot;pairwise.complete.obs&quot;) The correlation matrix is a square matrix with the number of rows and columns equal to the number of variables: dim(spellman.cor) [1] 1000 1000 To get the correlations with a gene of interest, we can index with the gene name on the rows of the correlation matrix. For example, to get all the correlations between the gene YAL022C and the other genes in the top 1000 set: spellman.cor[&quot;YAL022C&quot;,] YFR014C YFR053C YBL032W YDR274C YLR286C -0.0286883534 0.1061796423 0.3143767286 -0.3151280104 -0.0844089765 YMR206W YCRX11W YNR067C YDR337W YNL327W -0.2115248731 0.1159781117 0.3065585116 0.4778333353 0.2868559029 YCR104W YMR218C YGL028C YHL028W YER124C -0.0440531822 0.0381755416 0.1330163451 0.4289999121 0.0359292260 YGR117C YKL164C YGL037C YBR090C YCR100C 0.0683951306 0.2656815642 -0.0771173501 0.0833816882 0.0945617330 YDL191W YNL018C YBR038W YNR044W YDL037C 0.0114585067 -0.1370538983 0.6767483573 -0.0501138796 0.1399467371 YBR054W YHR143W YNL296W YBR009C YGL055W 0.5874136129 -0.1017829045 0.2414025877 -0.0815754480 -0.1342443192 YHR152W YPR119W YBR040W YER070W YKL125W 0.5685684046 0.4944216448 -0.2220889263 -0.0625151935 -0.2427881831 YKL185W YOR255W YBL002W YGR108W YMR244W 0.3534584753 0.0513256848 -0.0288862913 0.5262281195 -0.4018006560 YPL061W YNL160W YDR033W YPR149W YER150W 0.3761353441 0.4869534960 0.6454767678 0.4839791735 -0.2357127137 YPL256C YNL030W YCRX16C YOL101C YLR183C -0.2039214092 -0.1070530317 0.0638333407 -0.2199840099 -0.1128581074 YBR051W YMR297W YHR160C YPL163C YMR032W -0.3088116505 0.0430893117 -0.2429794546 -0.0212938980 0.5425463399 YDR461W YBR088C YDR225W YKR039W YCR098C -0.1908170286 -0.1650108329 -0.1242027461 0.2969274750 -0.1891684344 YKL163W YGR087C YBR158W YOL007C YMR058W 0.2996797052 -0.1152862904 0.1378974832 -0.2469726169 0.3907602438 YKR013W YFR025C YBR089W YGR044C YBR010W -0.3376738415 0.1468963065 -0.1181068439 -0.0311776847 -0.1542630984 YBR092C YMR001C YNL020C YPL158C YDR224C 0.6833823397 0.4793964162 0.3016275342 0.3375996437 -0.1941701493 YPL021W YMR305C YKL096W YBL003C YDL125C -0.1035945658 -0.0593409840 0.2244099811 -0.1246019635 0.1947516335 YNL002C YBL073W YGL089C YGR189C YDL003W 0.0425075311 0.3602637335 0.1429082759 0.0162498806 -0.2185164033 YDR327W YGR109C YHR162W YBL037W YMR215W 0.1421919404 0.0182997926 -0.0552174064 0.1453925831 0.1600066978 YML027W YHR055C YHR053C YJL153C YCL064C -0.1771427528 -0.3997746469 -0.3781016372 -0.2530899200 0.0023790988 YBR072W YLR308W YPL127C YMR226C YOR247W -0.2851121860 0.0133499022 -0.1847228615 0.0776092853 0.3205073786 YDL149W YPR156C YNL058C YGL188C YPL265W -0.0256392164 0.5703507173 0.7406816267 0.0826286206 0.2582815082 YLL026W YCR021C YJL158C YHR178W YDL039C -0.1537408516 -0.0739169450 0.3691184823 0.0458664853 0.1323709280 YBR108W YBL036C YKL086W YHR215W YIL140W 0.1310492504 0.2746395513 -0.3122257240 0.4479302089 -0.2289151564 YBL035C YDR055W YMR205C YNR009W YMR232W -0.0798156989 0.3615737685 0.5013289098 -0.0741334683 -0.2504976667 YAL005C YPL267W YDR340W YJR082C YJL157C 0.0773313597 -0.2090532185 0.0391939977 -0.1413973109 0.5375692469 YNL279W YCR007C YLR190W YBR071W YCL040W -0.0584967941 0.0694421074 0.5699897298 0.0834237477 0.0855301516 YJL159W YJL079C YNL057W YDL038C YLR049C 0.3292349327 0.3181403942 0.7078795864 -0.0202989479 0.1469737443 YMR199W YBR110W YMR250W YBR067C YAR071W -0.0561794307 0.1870388826 -0.0454105037 0.0406480035 0.5579207051 YOR263C YPL240C YJL170C YER001W YMR107W 0.2380858365 -0.2140106688 -0.0169081501 -0.3153651235 0.0577110087 YIL141W YFL012W YOR248W YOR025W YJR004C -0.2273536880 0.0127273347 0.2596667783 0.4540887836 0.1078818448 YNL078W YML047C YER081W YGL032C YDR475C 0.4099708355 -0.0357221844 -0.3001572147 -0.0896118230 -0.1542293261 YBL051C YNL082W YGL021W YDR097C YHR137W 0.1206507331 0.0897341739 0.4312426547 -0.1605522395 0.2964205629 YNL300W YNL031C YBR202W YFL066C YHR217C -0.2778721995 -0.2167212809 0.6889264789 0.2399656802 -0.1037777337 YKR086W YPL186C YPR160W YKL178C YDR146C -0.1532220801 0.0873683068 0.0853699755 0.2694210070 0.5687971948 YOR264W YFL014W YLL024C YIL123W YAL067C 0.2919668997 -0.3532417042 0.0591293552 0.2781267792 -0.2441177846 YLR300W YPL058C YBR124W YML058W YLR079W 0.2096401531 0.1924970262 0.1650570585 0.3104729553 0.1367913935 YAL004W YFL052W YDR258C YPR089W YCR005C 0.1192950712 0.1557951261 -0.1856847015 -0.2566943201 0.0515486602 YOL132W YOR382W YOL090W YOR315W YOR074C -0.0863263324 -0.0182262059 -0.2418699878 0.6261722221 -0.0821462975 YMR179W YCL024W YPL187W YLR131C YMR189W -0.0343346047 -0.1592876382 0.1555191336 0.4432887893 0.3938863771 YIL066C YPL121C YMR031C YDR278C YKR077W -0.1864322080 -0.1361752489 0.3642248838 0.1425420496 0.0725875637 YDL123W YDL055C YMR317W YKR012C YJL115W -0.0255725388 -0.0842054357 0.0261073152 -0.2689016878 -0.3354834652 YCL022C YLR178C YNR069C YPL062W YDR215C -0.1228209908 -0.2706472546 -0.0708256297 0.1775223075 0.0624842812 YLR194C YPL153C YGR248W YIL158W YGL184C 0.0574184113 -0.1250398451 -0.1519125497 0.3695412028 -0.3261795208 YOR313C YDR451C YML034W YLR013W YHR182W 0.7087445727 0.1223004252 0.3747740962 0.1155198497 0.1153829979 YDR171W YDR380W YDR355C YHR066W YLR413W -0.2177169286 0.3983529588 -0.0521061808 0.2570740438 0.1160772861 YBL013W YAR007C YLR217W YNL192W YDL222C 0.2255186281 -0.1966626306 -0.3889552994 0.2105866451 -0.1964242333 YGR227W YPR031W YBR078W YNL066W YFR015C 0.2710207098 -0.2431605366 0.4682509499 0.5015769081 0.1452400258 YLR136C YNL007C YDL179W YCL027W YGL121C -0.2171544492 -0.0814742528 0.3428984751 -0.0224610024 -0.3288397352 YJR132W YCLX07W YJR119C YGL182C YDL160C -0.0521454890 -0.0691820776 -0.1514407636 -0.1085282368 0.1211280108 YML128C YER053C YDR269C YDR535C YHR210C 0.0208170384 0.0131635837 0.0342189899 0.3190638098 -0.2171551339 YDL227C YGR142W YEL065W YGL116W YDL127W 0.0349337766 -0.1824083842 0.5020000273 0.5475455794 -0.0955093008 YGL238W YLR142W YBR083W YHR140W YNL036W 0.3341567242 -0.0070662369 0.2623249676 0.1395351562 0.1617281941 YEL011W YDR309C YOR383C YDR507C YDR039C 0.1664855038 0.0237342685 0.1406353361 0.2244170615 0.2026812879 YCL042W YPL165C YGR221C YBR138C YLL065W 0.0133317858 -0.1657346436 -0.1171675233 0.4017450077 0.2027991992 YDL163W YMR245W YIL129C YBR162C YKL132C 0.0184571398 -0.0484193698 -0.0004762559 0.4887184904 -0.1394617233 YML119W YMR326C YJR092W YPL141C YKL103C 0.5686976677 0.1615925226 0.4984168377 0.4123613340 -0.1781059131 YPL136W YLR109W YML100W YLR303W YKL096W-A -0.0581992754 -0.1487032106 0.0561414368 -0.2176242658 0.3041131057 YGL240W YHR005C YCL055W YHR125W YPR028W 0.1172025071 0.2677456569 -0.1810688595 -0.3677622706 0.0704531199 YJL051W YKL172W YJL078C YGL230C YHR023W 0.4711181785 -0.2721991245 -0.0678252090 -0.4688018634 0.5911430711 YIL029C YAR018C YOR024W YGR092W YBR183W 0.1402047310 0.6915321996 0.0876010033 0.6391449885 0.3918884624 YHL021C YDL164C YBL108W YAL054C YMR011W 0.0925680240 0.1199445452 0.4126670740 0.3895593119 0.2688081349 YHR126C YNL270C YMR186W YPL223C YCL013W 0.1049504910 0.0728111686 -0.0454025754 -0.0303845805 0.3194041454 YDR300C YGR152C YJR150C YCR097WA YAL066W 0.1400593164 -0.2767921070 0.0965685215 0.2043400848 -0.0170308913 YJR010W YPR018W YJL116C YIL025C YMR210W -0.2648643460 -0.0732719999 0.0140673365 -0.1236160878 0.1427579604 YNL102W YLR258W YLR235C YBL078C YMR241W -0.2677697358 0.0224981675 -0.1399501977 -0.0022903055 0.3729657962 YOL084W YPR096C YBR093C YDL083C YGR008C -0.0990741926 -0.0743702551 0.5109771486 -0.1311365154 -0.0970744867 YFL056C YFR057W YPL130W YDR273W YGR041W -0.1323317470 -0.2684767856 -0.1524079708 0.0185778762 0.0776420058 YNL283C YPL036W YJR128W YLR327C YKL177W -0.1123323508 0.4532999780 0.2113244606 -0.2673150709 0.2493318807 YDL158C YHR096C YCR024C-A YLR313C YNL289W -0.2429262961 -0.0638636653 0.6119968093 -0.2012132262 -0.1125286079 YOR348C YIL131C YDL023C YFL011W YBR144C 0.4959467801 0.0187661671 -0.1012676099 0.4561695374 0.0681327565 YGR279C YCL014W YHL018W YIR019C YOR314W 0.4353506460 0.2193095468 0.1337808337 -0.0915904284 0.6125448152 YGL094C YGR291C YPL242C YCL025C YER079W -0.0096790827 0.1849534110 0.6205437471 0.2481660517 0.0471726391 YOR066W YDR528W YER103W YOL155C YKL221W 0.2719683473 0.0134378978 -0.2314315566 -0.1025696700 -0.2295842331 YIL037C YLR452C YMR105C YKR093W YOR173W -0.0957571965 0.0021728878 -0.2328899895 0.4295286499 -0.2057425788 YNL004W YDL009C YNL145W YER095W YFR012W 0.0957906792 -0.0770058636 0.1853350841 -0.1351268061 -0.4075311215 YLR164W YGL008C YHR164C YAR037W YPL123C 0.1235426772 0.6250411101 0.1285009879 0.3117726969 0.0315596856 YNL200C YPL192C YDR536W YDR102C YAR040C 0.1368255160 -0.1619905827 0.0300023802 0.1402814467 0.3978319390 YDL239C YLR274W YMR173W YOR153W YLR119W 0.1021506249 0.4038939444 -0.4424445064 0.3004830136 0.0488849797 YML054C YDL022W YBR106W YDR010C YGR151C -0.2662514487 -0.0356716913 0.4682243347 -0.2448715036 -0.2553008768 YBR070C YGR035C YHR189W YLR365W YBR157C -0.0741180746 0.0537186873 0.0081871349 0.0294451733 0.0711230451 YER106W YLR103C YOL058W YGR160W YML084W -0.1099489906 -0.2249608855 -0.1091161131 -0.0421362911 -0.1359581020 YGL259W YML123C YOR374W YPR142C YOR237W 0.0566424272 0.3249126268 0.0462493659 -0.3923410121 -0.1317747253 YGR088W YJL074C YDL124W YFL026W YMR078C -0.1783575251 -0.0980414466 -0.1632200366 0.0593077220 0.1238530280 YOR052C YLR214W YBL021C YFR023W YGL170C 0.1411149498 0.3308321431 0.3064699451 -0.3133130397 -0.3352409661 YMR280C YJL194W YCL060C YBL065W YMR193W 0.0736936253 0.3295477584 -0.2162226073 -0.0903457244 -0.0337282249 YBR267W YDL214C YIL009W YGL033W YLL028W -0.1243396792 -0.3096142437 0.2782212820 -0.1952406345 0.3868741336 YMR159C YCL008C YCL012W YHR153C YKL043W 0.1033460465 -0.1027070022 0.3211380078 -0.1302678321 0.2947125490 YOR338W YOL017W YDL117W YNL173C YEL035C -0.3307116633 -0.2127698409 0.4627775749 0.1741116877 -0.1335662092 YLR304C YBR056W YAL061W YHR095W YBR296C 0.3332536733 0.2112324346 0.1182255826 -0.1954111464 -0.1298652913 YGR273C YHR109W YFL051C YKL045W YPR175W -0.0120357702 0.1036574707 0.2207478044 -0.1552190451 -0.1934776849 YNL046W YLR121C YPL092W YER091C YDL024C 0.4363353327 -0.3590433809 0.2262713737 -0.0283736282 0.0917133927 YMR071C YGR236C YNL142W YMR198W YCL076W 0.0020979607 0.0312216382 0.3403679728 -0.0533251185 0.1024406429 YNL194C YIL100W YCL075W YDR281C YNL065W -0.0902833613 -0.2444417136 0.1504961987 -0.0842010022 -0.1680388051 YGR159C YOR273C YML033W YBR285W YMR195W -0.0153855534 0.3050982660 0.3657611982 -0.1781703060 0.1011029028 YER054C YJL114W YNL318C YKL152C YMR203W -0.1812507152 -0.0832641390 -0.0216772191 -0.0994651792 0.3099329921 YCR010C YNR019W YDR339C YDL234C YLR216C 0.3959373799 0.2653578819 -0.0108908279 -0.0195281178 -0.3299933655 YHR185C YKL026C YDR382W YOR220W YPR203W -0.0873826579 -0.1841897437 0.2582009222 -0.2035233516 0.0836167519 YDR070C YDL204W YDR343C YJR048W YNL022C -0.3517625690 -0.1185897539 0.2976565559 0.1832709085 0.1361726556 YAL003W YNL195C YER062C YGR225W YKL029C 0.3182406985 -0.1596760627 -0.2816092594 0.1154407509 -0.0771589517 YOR386W YDR019C YJL190C YJR160C YMR316W -0.0020657625 0.5192861196 0.1216579003 -0.1018574457 -0.3407524789 YPR106W YJR079W YBR073W YJL028W YBL049W -0.3100498543 -0.0265760420 -0.1146425926 -0.1750640129 -0.0022388058 YDL182W YBR169C YOR298W YCLX09W YPL250C 0.2450049918 -0.2480494885 0.0345725531 0.2279446999 0.0763432824 YCR097WB YAL022C YDL175C YKR103W YNL141W 0.0753779687 1.0000000000 -0.1240959120 0.3016612680 -0.1536980256 YGR143W YPR039W YNL205C YJL166W YNR066C 0.4852912327 -0.2907586159 -0.1285507665 -0.0451235373 -0.0928617100 YDL198C YLL035W YPR141C YIL053W YGL034C 0.3311884934 0.0978175944 -0.1693149309 0.0066941634 0.1588912747 YLR463C YJL033W YHL049C YKL047W YOL126C 0.0074153530 -0.1210940898 0.1588113850 -0.2683146636 0.4193621676 YPL222W YBL112C YDR113C YNL134C YPL093W -0.1489950300 0.2491406611 -0.4204508484 -0.0070737926 0.0850157509 YGL189C YBR031W YMR003W YKL108W YPL171C -0.2643915024 0.3868470149 -0.2342296173 -0.2277813945 0.1108765822 YJL038C YPR112C YFR017C YGR138C YLR406C -0.2477970346 -0.2053491107 -0.1075600539 0.0531462687 0.2080488697 YLR438W YFL064C YOR307C YER145C YLR340W 0.2413475155 0.1294974308 0.1508481729 0.2260111202 0.2427754945 YDR501W YOL019W YNL112W YOL158C YJL007C -0.2013473639 -0.0400500840 -0.0115696542 0.5575230739 0.0782228348 YOR289W YCR012W YDL173W YDL169C YLR198C -0.2760190155 0.3352891755 0.0806025364 -0.0850153057 -0.0730561029 YOR305W YLR126C YNL333W YLR255C YDR012W 0.1391350929 -0.1987578299 -0.3562597193 -0.2468157722 0.0262580984 YDL081C YDL075W YDL061C YJL205C-A YDR418W 0.2324101215 -0.0384625714 0.2272642816 -0.0239053749 0.0602411328 YPR157W YDL048C YHR158C YMR048W YDR525W 0.3213687055 -0.0392957221 0.1000145753 -0.0836989353 0.2432478969 YGR043C YFL033C YDR015C YJL164C YDR313C -0.2191209881 -0.1497238138 -0.1589843849 0.2642468242 -0.1402294085 YJL196C YGR103W YOL069W YOR135C YNL042W 0.3980307062 -0.1547829581 0.1698733701 0.2530762972 -0.0350372913 YBL023C YGL031C YLR058C YOR293W YLR004C 0.5940455669 -0.1824712639 0.3797759775 -0.0561342506 -0.0575076335 YGR067C YKR042W YJR030C YHL047C YDL018C 0.1599652429 0.2985125857 -0.1929857093 0.0337705080 -0.2350767640 YPL146C YHR107C YGL158W YER139C YNL077W -0.2165028601 0.1851828292 -0.2156654873 0.1930677132 -0.1554539742 YEL039C YDL096C YLR048W YKL151C YGR290W 0.1039234601 0.0111438988 0.1680572976 0.1590248890 0.0705863167 YMR090W YDR358W YOR365C YLR054C YOR308C -0.1645030988 0.1039372305 -0.2050535783 -0.0357656357 -0.0040816230 YDL213C YPL088W YPR192W YPL090C YCR089W 0.0181380487 -0.1925753294 0.0635971187 0.2252144371 0.1628330050 YMR096W YOR136W YJL045W YMR196W YHL037C 0.2450831594 0.1803068929 -0.2400930169 0.2877922806 -0.1808651526 YOR376W YOR214C YNR056C YDR453C YOL012C -0.1205740953 0.1262113062 0.0003107212 0.1417024260 0.2321485275 YKL035W YIL106W YJL144W YFL037W YLR124W 0.0746189175 0.3642290395 -0.2234743890 0.2888710643 -0.0095850121 YCR085W YIL167W YKL109W YOL109W YCR065W -0.0080854253 0.4019825678 0.1109732616 -0.0422770563 -0.0325633470 YPR174C YML060W YLR223C YKL142W YIL136W -0.1314829841 -0.2614855100 -0.2288183441 -0.2203866250 -0.1513205079 YNL262W YER135C YDR523C YLR333C YGR055W -0.1320123817 -0.1670435697 -0.0016627085 -0.0164488754 0.0450438634 YPR006C YKL009W YER080W YPL132W YHR010W 0.3674955086 -0.1655186083 0.0165046294 0.1025320698 -0.0624608752 YCR013C YHR061C YBR116C YBL064C YCR101C 0.3081825848 -0.2679556927 -0.1047680021 0.0124850069 0.2761553832 YPR002W YNL208W YDR218C YLL059C YIR017C 0.3361093933 -0.1466419697 -0.0789702963 -0.2117711575 -0.3276105997 YOR100C YPR158W YDL077C YDR018C YNL037C 0.2712857388 -0.0932927073 0.0207133859 -0.0640440715 0.2562815942 YNL015W YJL187C YPL004C YOR027W YBL012C -0.1604511549 -0.1880758696 0.3163450676 -0.2365835100 0.0172012731 YER185W YDR450W YGR286C YMR246W YOR256C -0.0832803307 0.1317957856 -0.3111905571 -0.0811341820 0.2417881618 YMR095C YIL102C YLR056W YBR126C YBR019C -0.0777476458 0.1405349762 0.2377638841 -0.1378886696 -0.0349753295 YCL074W YPR019W YJL172W YJL219W YPR078C 0.0079256996 0.5614892319 0.4842547457 0.1099039408 -0.1054934363 YMR323W YPL012W YBR250W YGL225W YHR149C 0.2141990627 0.0393315430 -0.1256912901 0.1983062627 0.0393305365 YJL092W YDR025W YEL060C YLR307W YLR110C 0.0935392211 0.1553105916 -0.4405397392 -0.1001015527 0.3764655382 YAL038W YDR285W YNL339C YNL309W YBL109W 0.0285008124 -0.1349643377 0.1435032852 -0.2637472003 0.0410542849 YLR249W YGL062W YLR247C YER111C YGR188C 0.3008056507 0.0746277873 0.1453713581 0.0201345875 -0.4563998053 YPR120C YLR441C YCR039C YAL062W YBR184W -0.0950955961 -0.1116331712 -0.1841309439 -0.3663836098 -0.1603599444 YDL176W YPL191C YJR148W YBR189W YER153C 0.0547663691 -0.2478675194 0.0348375119 0.1356618612 -0.1333441548 YMR072W YNR014W YKL091C YBR191W YOL160W 0.1756777470 -0.1466754765 0.0330376908 0.2145967754 -0.3732563375 YBR243C YIL082W YGR059W YER003C YAR062W 0.1797519974 -0.0898942274 -0.0427629476 -0.1643077836 0.3069126262 YBR118W YIL119C YAL018C YAL065C YEL032W 0.2332684874 0.2085215197 0.0307067440 0.1628335162 0.4245075135 YKL067W YIL117C YOR114W YAL025C YML042W -0.3060411291 -0.1239500941 -0.3350434771 -0.1149068147 0.2715623526 YNL009W YFR026C YMR076C YCL026C YGL090W 0.0273621001 -0.0216969397 0.0008008518 -0.0259257059 0.1106609829 YLR344W YML110C YLR061W YPL044C YDR250C -0.0922629314 0.0668334030 -0.0499234633 -0.1368485113 0.0865348072 YCR031C YPR124W YNL276C YGR086C YNL126W 0.1483802379 0.1861815567 -0.1558542529 0.1466462551 -0.0573266366 YGR148C YJR109C YEL075C YNL269W YMR269W -0.1912792665 0.3148074083 0.0937848388 0.0140046343 -0.2136840076 YDR191W YAR008W YFL059W YBR214W YBL113C 0.3766034739 -0.0395493093 -0.2813204810 -0.1502025520 0.2717806327 YDL130W YGL138C YIL057C YGR280C YJL037W 0.1389708034 0.1018121335 0.1345948557 -0.2309113396 -0.2252513411 YBR230C YHR218W YCL048W YMR015C YKR011C 0.2296307774 0.1706201864 0.1398425833 0.2787421167 -0.0678805200 YHL045W YGL204C YGR224W YHR097C YNL073W -0.0019326745 -0.1968206948 0.4021516370 -0.3395960036 0.2141287635 YHR018C YCL062W YJL216C YOR058C YJL214W 0.1111401000 0.0730572532 0.2183090917 0.1394356932 0.3977814168 YPR204W YOL023W YPR077C YBR247C YPR150W 0.1936245905 0.0734747017 -0.0802740414 -0.0472740025 -0.1418420626 YLR197W YKL078W YMR145C YKL116C YOR083W -0.0621797609 -0.1471203549 0.5079851829 0.3097069037 0.0589035001 YER131W YOL016C YJL089W YDR044W YAL060W -0.1640640905 -0.3002781310 0.2127703663 -0.1655642307 0.3858333051 YFL053W YBR188C YHL041W YJR095W YPL054W 0.3128835919 0.0121671863 0.2153828146 0.3985896266 -0.5092294056 YHR110W YOR303W YOL118C YOR065W YAR009C -0.3716775946 0.2042701847 0.1568516725 0.1254604970 0.0333297210 YBR241C YMR239C YMR321C YMR128W YIR027C -0.0417405866 0.1720322649 0.2572081000 -0.2078147453 -0.3185905595 YPL281C YGR014W YCR091W YLR029C YKL066W 0.0789068758 0.1111947556 0.2210304214 -0.0449059940 -0.1380473715 YIL104C YKL001C YCL061C YNR074C YOL059W -0.1320158357 -0.2889322399 -0.2208711543 0.2593177399 0.1264937448 YLL061W YPL266W YER065C YLR383W YBL111C -0.1118581554 -0.2152943487 0.2556167100 -0.2864827411 0.2764584016 YHR022C YDR516C YCRX09C YLR466W YGR084C 0.2179272644 0.1560488279 0.0815185074 0.1490697953 -0.0462873093 YIL015W YPL057C YKR091W YBR209W YPR029C 0.1361778150 -0.1479436060 -0.1077362566 0.3769450912 -0.1373041150 YLR372W YOR312C YCR009C YDR399W YOR134W 0.0936782038 -0.1037106095 0.1197879116 0.0393015887 -0.0149337658 YER032W YLR081W YBR117C YDR042C YDR136C 0.0485483705 0.2648434351 -0.2761839624 -0.0985121147 -0.0268150407 YAR043C YGL096W YAL051W YDR297W YKL101W 0.3569783388 -0.3181403369 -0.3109300965 -0.1080769976 -0.1120278976 YEL054C YLR374C YOR183W YJR146W YLR342W 0.3746970226 -0.2273675049 0.1132929216 -0.0502010724 -0.0241602473 YPL142C YKL090W YOL150C YNL312W YPR032W 0.4649178256 -0.1329417315 0.3283427655 -0.1676677425 0.3867057830 YBR266C YPR030W YJL088W YCR034W YCL067C -0.1669415781 0.0148142909 0.0787666250 0.2940652651 -0.2236403322 YKR076W YJL215C YOR394W YLR467W YOR206W -0.1381714108 0.2448770304 -0.2251683095 0.1681982698 -0.1961140804 YPL087W YOR391C YDR256C YPL033C YDR374C 0.0133421522 -0.2183401873 0.1225832295 -0.2482778086 -0.0917347639 YER102W YNR001C YIL175W YKL008C YGR039W -0.0300505616 0.2550147886 -0.1248154091 -0.2530985066 -0.1923337778 YER110C YER043C YBL063W YMR049C YDL228C 0.3046050770 0.3105959406 -0.2034666934 0.0069776775 0.3571656370 YHR209W YKR041W YPL102C YOL014W YER190W 0.2620384201 0.0371119315 -0.0279897043 0.1056049701 0.1255554679 YBL075C YOR029W YDR046C YJR037W YDR040C 0.0390750261 0.2139944224 0.3951934418 0.1008045420 0.0319597826 YAR064W YLR167W YLR045C YOR344C YAL028W 0.1819505950 -0.0673020849 -0.0379580312 -0.1131816750 0.1237026718 YHR139C YLR236C YKL153W YCL059C YLR212C -0.2757014954 0.0006889656 0.0447586493 -0.1629682607 0.0685096540 YLL046C YLR149C YPR151C YOR388C YGL209W -0.1207887412 0.0729031919 -0.2505535151 -0.1051335029 0.2395956063 YOR347C YDR214W YBR238C YLR162W YCL030C 0.1959635756 -0.0266453978 0.1969892259 -0.1630248849 0.1645320045 YAR073W YIL126W YML045W YCL023C YGL255W 0.4345615934 -0.1723045257 -0.2460875611 -0.1200342196 0.3274183304 YPR035W YNL117W YJR047C YGL103W YJR008W -0.2518503203 0.2071469205 0.1600389474 -0.0645014970 -0.1135789649 YJL195C YPR201W YLR318W YDR446W YNL175C 0.3378482028 0.0555207851 0.0499707855 -0.0394964115 0.1571383157 YPR047W YLL002W YGR245C YKL030W YDL063C 0.3028269956 -0.0546395854 -0.0362406428 -0.0042540216 -0.0844669466 YJL163C YMR081C YOR182C YNL072W YGL228W 0.1633698773 -0.2148284098 -0.2597132559 0.1246731584 0.0192954760 YDL218W YPR184W YHR037W YER189W YBL039C 0.0752723458 -0.0786889173 0.1668945669 0.1581546399 0.2235528391 YMR295C YKR105C YFL067W YOR235W YCLX02C -0.0654890777 -0.0229559592 0.4094981264 0.0182779307 -0.1755292354 YHR092C YLL022C YJL118W YDL102W YLR270W 0.3286916445 -0.3351218696 -0.1537702984 0.1801550890 0.0028156806 YDL128W YHR172W YDR345C YDR112W YLR353W 0.5251076083 -0.1624860646 0.4098222863 -0.0146686904 0.4575062062 YML120C YBR076W YJL108C YJL217W YDR540C 0.3085445399 0.1526587348 -0.1626661949 0.0198431863 0.3624761479 YLR355C YNL274C YPL280W YPR194C YMR069W 0.3874400681 -0.0115377428 0.0322258646 0.2890661764 -0.2232485453 YDL244W YBR074W YFL016C YOR385W YOL077C 0.2104042212 0.3489285917 0.1224921408 -0.1086568943 -0.1686662376 YPR045C YLR462W YAL035W YNL044W YOR096W 0.0926815132 0.1309320155 0.1369497149 0.2149857566 0.1689311218 YHR180W YJL148W YOL119C YGR003W YCR042C -0.2770801725 -0.2495145484 0.3723574161 0.1657071382 0.4567219046 YJL135W YAL044C YNL028W YFR054C YLR273C -0.0275517314 0.1595257918 0.0610316933 -0.0054161479 0.2018959878 YEL076C YGR205W YIL012W YDL056W YLL047W 0.1807176952 0.2386590341 0.0095510031 0.2211287903 -0.0908766519 YJL211C YDL147W YOL041C YOL113W YBL009W 0.1891826624 0.1871156140 -0.2545846899 0.2596401285 -0.0307749383 In the next statement we extract the names of the genes that have correlations with YAL022C greater than 0.6. First we test genes to see if they have a correlation with YAL022C greater than 0.6, which returns a vector of TRUE or FALSE values. This vector of Boolean values is than used to index into the rownames of the correlation matrix, pulling out the gene names where the statement was true. pos.corr.YAL022C &lt;- rownames(spellman.cor)[spellman.cor[&quot;YAL022C&quot;,] &gt; 0.6] length(pos.corr.YAL022C) [1] 15 We then return to our wide data to show this set of genes that are strongly positively correlated with YAL022C, using faceted line graphs: spellman.final %&gt;% filter(gene %in% pos.corr.YAL022C) %&gt;% ggplot(aes(x = time, y = expression, color = expt, group = gene)) + geom_line(alpha = 0.33) + facet_wrap(~ expt, scales = &quot;free_x&quot;, nrow = 2, ncol = 2) + theme(legend.position = &quot;none&quot;) We can similarly filter for genes that have negative correlations with YAL022C. neg.corr.YAL022C &lt;- colnames(spellman.cor)[spellman.cor[&quot;YAL022C&quot;,] &lt;= -0.4] As before we generate a faceted line plot showing these genes by experiment: spellman.final %&gt;% filter(gene %in% neg.corr.YAL022C) %&gt;% ggplot(aes(x = time, y = expression, color = expt, group = gene)) + geom_line(alpha = 0.33) + facet_wrap(~ expt, scales = &quot;free_x&quot;, nrow = 2, ncol = 2) + theme(legend.position = &quot;none&quot;) 7.6.5 Adding new columns and combining filtered data frames Now let’s create a new data frame by: 1) filtering on our list of genes that have strong positive and negative correlations with YAL022C; and 2) creating a new variable, “corr.with.YAL022C”, which indicates the sign of the correlation. We’ll use this new variable to group genes when we create the plot. pos.corr.df &lt;- spellman.final %&gt;% filter(gene %in% pos.corr.YAL022C) %&gt;% mutate(corr.with.YAL022C = &quot;positive&quot;) neg.corr.df &lt;- spellman.final %&gt;% filter(gene %in% neg.corr.YAL022C) %&gt;% mutate(corr.with.YAL022C = &quot;negative&quot;) combined.pos.neg &lt;- rbind(pos.corr.df, neg.corr.df) Finally, we plot the data, colored according to the correlation with YAL022C. ggplot(combined.pos.neg, aes(x = time, y = expression, group = gene, color = corr.with.YAL022C)) + geom_line(alpha=0.5) + facet_wrap(~ expt, nrow =2, ncol = 2, scales = &quot;free_x&quot;) + # changes legend title for discrete color legends scale_color_discrete(name = &quot;Correlation with YAL022C&quot;) 7.7 Heat maps Line plots can become visually busy very quickly, and hence aren’t very useful for more than a few tens of genes. An alternative is a “heat map” which depicts data in a grid or table like form, with values indicated by color. Heat maps are good for depicting large amounts of data and providing a coarse “10,000 foot view”. We’ll be looking at lots of heat maps when we get into clustering, but here’s a simple example based on the genes that were positively and negatively correlated with YAL022C. 7.7.1 RColorBrewer The RColorBrewer packages provides nice color schemes that are useful for creating heat maps. RColorBrewer defines a set of color palettes that have been optimized for color discrimination, to be color blind friendly, etc. Once you’ve installed the RColorBrewer package you can see the available color palettes as so: library(RColorBrewer) # show representations of the palettes par(cex = 0.5) # reduce size of text in the follow plot display.brewer.all() 7.7.2 Creating a heat map using geom_tile We’ll use the geom_tile geom to create the heat map. We’ll depict genes in rows, time points in columns, and expression values by the fill color of each tile. We’ll use the Red-to-Blue (“RdBu”) color scheme defined in RColorBrewer, however we’ll reverse the scheme so blues represent low expression and reds represent high expression. # generate the color scheme to use color.scheme &lt;- rev(brewer.pal(8,&quot;RdBu&quot;)) # re-factor gene names so positive and negative genes are spatially distinct in plot combined.pos.neg$gene &lt;- factor(combined.pos.neg$gene, levels = c(pos.corr.YAL022C, neg.corr.YAL022C)) combined.pos.neg %&gt;% filter(expt == &quot;cdc28&quot;) %&gt;% # focus only on one condition ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme) + xlab(&quot;Time (mins)&quot;) Can you visually see the breakpoint between the positively and negatively correlated sets of genes? 7.7.3 A “fancy” figure If we want to get really fancy, we can use cowplot’s draw_plot function that allows us to place plots at arbitrary locations and at arbitrary sizes onto the canvas. The coordinates of the canvas run from 0 to 1, and the point (0, 0) is in the lower left corner of the canvas. We’ll use draw_plot to draw a complex figure with a heatmap on the left, and two smaller line plots on the right. cdc28.filtered &lt;- filter(combined.pos.neg, expt == &quot;cdc28&quot;) pos.corr.lineplot &lt;- cdc28.filtered %&gt;% filter(gene %in% pos.corr.YAL022C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33, color = &#39;red&#39;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;, title = &quot;Genes Positively correlated\\nwith YAL022C&quot;) neg.corr.lineplot &lt;- cdc28.filtered %&gt;% filter(gene %in% neg.corr.YAL022C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33, color = &#39;blue&#39;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;, title = &quot;Genes negatively correlated\\nwith YAL022C&quot;) heat.map &lt;- ggplot(cdc28.filtered, aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme) + labs(x = &quot;Time (mins)&quot;, y = &quot;Gene&quot;) + theme(legend.position = &quot;none&quot;) fancy.plot &lt;- ggdraw() + draw_plot(heat.map, 0, 0, width = 0.6) + draw_plot(neg.corr.lineplot, 0.6, 0.5, width = 0.4, height = 0.5) + draw_plot(pos.corr.lineplot, 0.6, 0, width = 0.4, height = 0.5) + draw_plot_label(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c(0, 0.6, 0.6), c(1, 1, 0.5), size = 15) fancy.plot 7.7.4 Sorting and visualizing genes by time of maximum expression In a time series data one might be interested in examing genes with respect to some property of their dynamic expression. Here we’ll focus on the 1000 most variable genes, sort the data from the alpha factor synchronization experiment by time of maximum expression. First let’s filter our tidy, long data frame to include only our top 1000 genes and the alpha factor measurements: top1k.alpha.long &lt;- spellman.final %&gt;% filter(gene %in% top.genes.1k, expt == &quot;alpha&quot;) To find the time of maximum expression we’ll employ the function which.max (which.min), which finds the index of the maximum (minimum) element of a vector. For example to find the index of the maximum expression measurement for YAL022C we could do: top1k.alpha.long %&gt;% filter(gene == &quot;YAL022C&quot;) %$% # note the exposition pipe operator! which.max(expression) [1] 18 From the code above we find that the index of the observation at which YAL022C is maximal is 18. To get the corresponding time point we can do something like this: top1k.alpha.long %&gt;% filter(gene == &quot;YAL022C&quot;) %$% # again note the exposition pipe operator! time[which.max(expression)] [1] 119 Thus YAL022C expression peaks at 119 minutes in the alpha factor experiment. To find the index of maximal expression of all genes we can apply the dplyr::group_by() and dplyr::summarize() functions peak.expression &lt;- top1k.alpha.long %&gt;% group_by(gene) %&gt;% summarise(peak = which.max(expression)) dim(peak.expression) [1] 1000 2 head(peak.expression, n = 10) # A tibble: 10 x 2 gene peak &lt;chr&gt; &lt;int&gt; 1 YAL003W 10 2 YAL004W 8 3 YAL005C 16 4 YAL018C 1 5 YAL022C 18 6 YAL025C 8 7 YAL028W 16 8 YAL035W 16 9 YAL038W 18 10 YAL044C 17 We can then generate a heatmap where we sort the rows (genes) of the heatmap by their time of peak expression. We introduce a new geom – geom_raster – which is like geom_tile but better suited for large data. The explicit sorting of the data by peak expression is carried out in the call to scale_y_discrete where the limits (and order) of this axis are set with the limits argument (see scale_y_discrete and discrete_scale in the ggplot2 docs). top1k.alpha.long %&gt;% ggplot(aes(x = time, y = gene)) + geom_raster(aes(fill = expression)) + # scale_fill_gradientn(limits=c(-2,2),colors=color.scheme) + scale_y_discrete(limits=peak.expression$gene[rev(order(peak.expression$peak))]) + labs(x = &quot;Time (mins)&quot;, y = &quot;Genes&quot;, title = &quot;1000 most variable genes&quot;, subtitle = &quot;Sorted by time of peak expression&quot;) + # the following line suppresses tick and labels on y-axi theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) Figure 7.1: A Heatmap showing genes in the alpha-factor experiment, sorted by peak expression "],
["vectors-and-functions.html", "Chapter 8 Vectors and Functions 8.1 Libraries 8.2 Vector Mathematics in R 8.3 Writing Your Own Functions in R 8.4 Function arguments 8.5 Writing functions with optional arguments 8.6 Putting R functions in Scripts 8.7 Simple statistics in vector form", " Chapter 8 Vectors and Functions 8.1 Libraries library(tidyverse) library(magrittr) 8.2 Vector Mathematics in R R vectors support basic arithmetic operations that correspond to the same operations on geometric vectors. For example: &gt; x &lt;- 1:15 &gt; y &lt;- 10:24 &gt; x [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 &gt; y [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 &gt; x + y # vector addition [1] 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 &gt; x - y # vector subtraction [1] -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 &gt; x * 3 # multiplication by a scalar [1] 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 R also has an operator for the dot product, denoted %*%. This operator also designates matrix multiplication, which we will discuss next week. By default this operator returns an object of the R matrix class. If you want a scalar (or the R equivalent of a scalar, i.e. a vector of length 1) you need to use the drop() function. &gt; z &lt;- x %*% x &gt; class(z) # note use of class() function [1] &quot;matrix&quot; &gt; z [,1] [1,] 1240 &gt; drop(z) [1] 1240 In lecture we saw that many useful geometric properties of vectors could be expressed in the form of dot products. Let’s start with some two-dimensional vectors where the geometry is easy to visualize: &gt; a &lt;- c(2, 0) # the point (2,0) &gt; b &lt;- c(1, 3) # the point (1,3) To draw our vectors using ggplot, we’ll need to create a data frame with columns representing the x,y coordinates of the end-points of our vectors: df &lt;- data.frame(x.end = c(a[1], b[1]), y.end = c(a[2], b[2]), label = c(&#39;a&#39;, &#39;b&#39;)) ggplot(df) + geom_segment(aes(x=0, y = 0, xend = x.end, yend = y.end, color=label), arrow = arrow()) + labs(x = &quot;x-coordinate&quot;, y = &quot;y-coordinate&quot;) + coord_fixed(ratio = 1) + # insures x and y axis scale are same theme_bw() Let’s see what the dot product can tell us about these vectors. First recall that we can calculate the length of a vector as the square-root of the dot product of the vector with itself (\\(\\vert\\vec{a}\\vert^2 = \\vec{a} \\cdot \\vec{a}\\)) &gt; len.a &lt;- drop(sqrt(a %*% a)) &gt; len.a [1] 2 &gt; len.b &lt;- drop(sqrt(b %*% b)) &gt; len.b [1] 3.162278 How about the angle between \\(a\\) and \\(b\\)? First we can use the dot product and the previously calculated lengths to calculate the cosine of the angle between the vectors: &gt; cos.ab &lt;- (a %*% b)/(len.a * len.b) &gt; cos.ab [,1] [1,] 0.3162278 To go from the cosine of the angle to the angle (in radians) we need the arc-cosine function, acos(): &gt; acos(cos.ab) # given angle in radians [,1] [1,] 1.249046 8.3 Writing Your Own Functions in R So far we’ve been using a variety of built in functions in R. However the real power of a programming language is the ability to write your own functions. Functions are a mechanism for organizing and abstracting a set of related computations. We usually write functions to represent sets of computations that we apply frequently, or to represent some conceptually coherent set of manipulations to data. The general form of an R function is as follows: funcname &lt;- function(arg1, arg2) { # one or more expressions that operate on the fxn arguments # last expression is the object returned # or you can explicitly return an object } To make this concrete, here’s an example where we define a function to calculate the geometric length of a vector using dot products: vec.length &lt;- function(x) { return(sqrt(drop(x %*% x))) } Since R returns the value of the last expression in the function, the return call is optional and we could have simply written: vec.length &lt;- function(x) { sqrt(drop(x %*% x)) } Very short and concise functions are often written as a single line. Here’s a single line version in which I also use pipes to unpack the nested function calls vec.length &lt;- function(x) {(x %*% x) %&gt;% drop %&gt;% sqrt } The vec.length function takes one argument, x, and calculates the length of the vector represented by x. Having defined the function we can immediately put it to use: vec.length(b) [1] 3.162278 If you type a function name without parentheses R shows you the function’s definition. This works for built-in functions as well (thought sometimes these functions are defined in C code in which case R will tell you that the function is a .Primitive). 8.4 Function arguments Function arguments can specify the data that a function operates on or parameters that the function uses. Function arguments can be either required or optional. In the case of optional arguments, a default value is assigned if the argument is not given. Take for example the log function. If you examine the help file for the log function (type ?log now) you’ll see that it takes two arguments, refered to as x and base. The argument x represents the numeric vector you pass to the function and is a required argument (see what happens when you type log() without giving an argument). The argument base is optional. By default the value of base is \\(e = 2.71828\\ldots\\). Therefore by default the log function returns natural logarithms. If you want logarithms to a different base you can change the base argument as in the following examples: &gt; log(2) # log of 2, base e [1] 0.6931472 &gt; log(2,2) # log of 2, base 2 [1] 1 &gt; log(2, 4) # log of 2, base 4 [1] 0.5 Because base 2 and base 10 logarithms are fairly commonly used, there are convenient aliases for calling log with these bases. &gt; log2(8) [1] 3 &gt; log10(100) [1] 2 8.5 Writing functions with optional arguments To write a function that has an optional argument, you can simply specify the optional argument and its default value in the function definition as so: # a function to substitute missing values in a vector sub.missing &lt;- function(x, sub.value = -99){ x[is.na(x)] &lt;- sub.value return(x) } You can then use this function as so: &gt; m &lt;- c(1, 2, NA, 4) &gt; sub.missing(m, -999) # explicitly define sub.value [1] 1 2 -999 4 &gt; sub.missing(m, sub.value = -333) # more explicit syntax [1] 1 2 -333 4 &gt; sub.missing(m) # use default sub.value [1] 1 2 -99 4 &gt; m # notice that m wasn&#39;t modified within the function [1] 1 2 NA 4 Notice that when we called sub.missing with our vector m, the vector did not get modified in the function body. Rather a new vector, x was created within the function and returned. However, if you did the missing value subsitute outside of a function call, then the vector would be modified: &gt; n &lt;- c(1, 2, NA, 4) &gt; n[is.na(n)] &lt;- -99 &gt; n [1] 1 2 -99 4 8.6 Putting R functions in Scripts When you define a function at the interactive prompt and then close the interpreter your function definition will be lost. If you put your functions in an R notebook, you have a record of your functions but you can’t use those function in another notebook without copying and pasting codeblock. The simple way around this is to define your R functions in a script that you can than access at any time. In RStudio choose File &gt; New File &gt; R Script. This will bring up a blank editor window. Type your function(s) into the editor. Everything in this file will be interpretted as R code, so you should not use the code block notation that is used in Markdown notebooks. Save the source file in your R working directory with a name like vecgeom.R. # functions defined in vecgeom.R vec.length &lt;- function(x) { # calculate the geometric length of a vecotr sqrt(drop(x %*% x)) } unit.vector &lt;- function(x) { # Return a unit vector in the same direction as x x/vec.length(x) } Notice that it is possible (and in fact typical) to use one function you define in writing another function (in this case we used vec.length() to write unit.vector()) Once your functions are in a script file you can make them accesible by using the source function, which reads the named file as input and evaluates any definitions or statements in the input file (See also the Source button in the R Studio GUI): source(&quot;vecgeom.R&quot;) Note that if you change the source file, such as correcting a mistake or adding a new function, you need to call the source function again to make those changes available. Having sourced the file you can now use your functions like so: &gt; x &lt;- c(1, 0.4) &gt; vec.length(x) [1] 1.077033 &gt; ux &lt;- unit.vector(x) &gt; ux [1] 0.9284767 0.3713907 &gt; a &lt;- 1:5 &gt; vec.length(a) [1] 7.416198 &gt; unit.vector(a) [1] 0.1348400 0.2696799 0.4045199 0.5393599 0.6741999 Note that our functions, vec.length() and unit.vector(), work with vectors of arbitrary dimension. 8.7 Simple statistics in vector form Now let’s turn our attention to seeing how to calculate a variety of simple statistics such as the mean, variance, etc. in terms of vector operations. To illustrate these oeprations we’ll use the I. setosa data from the iris examplar data set. setosa &lt;- filter(iris, Species == &quot;setosa&quot;) 8.7.1 Mean First let’s calculate the mean for the Sepal.Length variable. Referring back to the slides for today’s lecture, we see we can calculate the mean as: \\[ \\bar{x} = \\frac{\\vec{1} \\cdot \\vec{x}}{\\vec{1} \\cdot \\vec{1}} \\] &gt; sepal.length &lt;- setosa$Sepal.Length &gt; ones &lt;- rep(1, length(sepal.length)) # 1-vector of length n &gt; mean.sepal.length &lt;- (ones %*% sepal.length)/(ones %*% ones) &gt; mean.sepal.length %&lt;&gt;% drop # use drop to convert back to scalar &gt; mean.sepal.length [1] 5.006 Let’s compare our calculation against the built-in mean function: &gt; mean(sepal.length) [1] 5.006 8.7.2 Variance and standard deviation Now let’s create a mean centered vector from sepal.length, which we’ll refer to as the vector of deviates about the mean: &gt; sepal.length.deviates &lt;- sepal.length - mean.sepal.length Using the vector of deviates we can easily calculate the variance and standard deviation of sepal.length: &gt; n &lt;- length(sepal.length.deviates) &gt; var.sepal.length &lt;- (sepal.length.deviates %*% sepal.length.deviates)/(n-1) &gt; var.sepal.length [,1] [1,] 0.124249 &gt; sd.sepal.length &lt;- sqrt(var.sepal.length) &gt; sd.sepal.length [,1] [1,] 0.3524897 Again, we can compare our calculations to the built-in var() and sd() functions: &gt; var(sepal.length) [1] 0.124249 &gt; sd(sepal.length) [1] 0.3524897 8.7.3 Covariance and correlation Now let’s consider the common measures of bivariate association, covariance and correlation. We’ll examine the relationship between sepal length and width: sepal.width &lt;- setosa$Sepal.Width mean.sepal.width &lt;- drop((ones %*% sepal.width)/(ones %*% ones)) sepal.width.deviates &lt;- sepal.width - mean.sepal.width var.sepal.width &lt;- drop((sepal.width.deviates %*% sepal.width.deviates)/(n-1)) sd.sepal.width &lt;- drop(sqrt(var.sepal.width)) With the vector of sepal width deviates in hand we can now calculate covariances: &gt; cov.swidth.slength &lt;- (sepal.length.deviates %*% sepal.width.deviates)/(n-1) &gt; cov.swidth.slength [,1] [1,] 0.09921633 &gt; cov(sepal.length, sepal.width) # and compare to built-in covariance [1] 0.09921633 And correlations: &gt; corr.swidth.slength &lt;- + (sepal.length.deviates %*% sepal.width.deviates) / (vec.length(sepal.length.deviates) * vec.length(sepal.width.deviates)) &gt; corr.swidth.slength [,1] [1,] 0.7425467 &gt; cor(sepal.length, sepal.width) # and compare to built-in correlation [1] 0.7425467 Alternately, we could have calculated the correlation more simply as follows: &gt; cov.swidth.slength/(sd.sepal.length * sd.sepal.width) [,1] [1,] 0.7425467 "],
["bivariate-linear-regression.html", "Chapter 9 Bivariate Linear Regression 9.1 Regression terminology 9.2 Libraries 9.3 Illustrating linear regression with simulated data 9.4 Specifying Regression Models in R 9.5 Residuals 9.6 Regression as sum-of-squares decomposition 9.7 Variance “explained” by a regression model 9.8 Broom: a library for converting model results into data frames 9.9 Bivariate linear regression on a real world data set", " Chapter 9 Bivariate Linear Regression Statistical models can be thought of as quantitative statements about how we think the variables under consideration are related to each other. Linear models are among the simplest statistical models. In a linear model relating two variables \\(X\\) and \\(Y\\), the general form of the model can be stated as “I assume that \\(Y\\) can be expressed as a linear function of \\(X\\)”. The process of model fitting is then the task of finding the coefficients (parameters) of the linear model which best fit the observed data. Linear functions are those whose graphs are straight lines. A linear function of a variable \\(X\\) is usually written as: \\[ \\hat{Y} = f(X) = a + bX \\] where \\(a\\) and \\(b\\) are constants. In geometric terms \\(b\\) is the slope of the line and \\(a\\) is the value of the function when \\(X\\) is zero (usually the referred to as the “Y-intercept”). The slope tells you have much \\(Y\\) changes per unit change of \\(X\\). In vector terms, the above linear model can be written as: \\[ \\vec{\\hat{\\mathbf{y}}} = a\\vec{\\mathbf{1}} + b\\vec{\\mathbf{x}} \\] In order to fit a model to data, we have to specify some criterion for judging how well alternate models perform. In linear regression, the optimality criterion can be expressed as “Find the linear function, \\(f(X)\\), that minimizes the following quantity: \\[ \\sum (y_i - f(x_i))^2 \\] That is, our goal is to find the linear function of \\(X\\) that minimizes the squared deviations in the \\(Y\\) direction. With a little calculus and linear algebra one can show that the values of \\(b\\) (slope) and \\(a\\) (intercept) that minimize the sum of squared deviations described above are: \\[\\begin{align} b &amp;= \\frac{\\vec{\\mathbf{x}} \\cdot \\vec{\\mathbf{y}}}{\\vec{\\mathbf{x}} \\cdot \\vec{\\mathbf{x}}} \\\\ &amp;= \\frac{s_{xy}}{s^2_x} \\\\ &amp;= r_{xy}\\frac{s_y}{s_x}\\\\ \\\\ a &amp;= \\overline{Y} - b\\overline{X} \\end{align}\\] where \\(r_{xy}\\) is the correlation coefficient between \\(X\\) and \\(Y\\), and \\(s_x\\) and \\(s_y\\) are the standard deviations of \\(X\\) and \\(Y\\) respectively. 9.1 Regression terminology Predictors, explanatory, or independent variable – the variables from which we want to make our prediction. Outcomes, dependent, or response variable – the variable we are trying to predict in our regression. 9.2 Libraries library(tidyverse) 9.3 Illustrating linear regression with simulated data To illustrate how regression works, we’ll use a simulated data set where we specify the relationship between two variables, \\(X\\) and \\(Y\\). Using a simulation is desirable because it allows us to know what the “true” underlying model that relates \\(X\\) and \\(Y\\) is, so we can evaluate how well we do in terms of recovering the model. Let’s generate two vectors representing the variable, \\(X\\) and \\(Y\\), where \\(Y\\) is a function of \\(X\\) plus some independent noise. As specified below, the “true” model is \\(Y = 1.5X + 1.0 + \\epsilon_y\\) where \\(\\epsilon_y\\) is a noise term. # this seeds our random number generator # by setting a seed, we can make random number generation reproducible set.seed(20160921) npts &lt;- 50 X &lt;- seq(1, 5, length.out = npts) + rnorm(npts) a &lt;- 1.0 b &lt;- 1.5 Y &lt;- b*X + a + rnorm(npts, sd = 2) # Y = 1.5X + 1.0 + noise df.xy &lt;- data.frame(X = X, Y = Y) Having generated some simulated data, let’s visualize it. library(ggExtra) # a new library, provides ggMarginal plot # install if you don&#39;t already have it p &lt;- ggplot(df.xy, aes(x = X, y = Y)) + geom_point() ggMarginal(p, type = &quot;histogram&quot;, bins = 11) 9.4 Specifying Regression Models in R R, of course, has a built-in function for fitting linear regression models. The function lm() can be used not only to carry out bivariate linear regression but a wide range of linear models, including multiple regression, analysis of variance, analysis of covariance, and others. fit.xy &lt;- lm(Y ~ X, df.xy) The first argument to lm is an R “formula”, the second argument is a data frame. Formulas are R’s way of specifying models, though they find other uses as well (e.g. we saw the formula syntax when we introduced the facet_wrap and facet_grid functions from ggplot). The general form of a formula in R is response variable ~ explanatory variables. In the code example above, we have only a single explanatory variable, and thus our response variable is Y and our explanatory variable is X. We’ll see more advanced R formulas in later lectures, but for now just remember that the variable to the left of the tilde is the one you’re trying to predict and the variable to the right is the explanatory variable. The lm function returns a list with a number of different components. The ones of most interest to us are fitted.values, coefficients, residuals, and (see the lm documentation for full details.) fit.xy Call: lm(formula = Y ~ X, data = df.xy) Coefficients: (Intercept) X 0.7688 1.5511 Calling summary on a fit model provides more detailed output: summary(fit.xy) Call: lm(formula = Y ~ X, data = df.xy) Residuals: Min 1Q Median 3Q Max -4.3723 -1.2391 -0.1677 1.4593 3.1808 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.7688 0.5755 1.336 0.188 X 1.5511 0.1699 9.129 4.58e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 1.772 on 48 degrees of freedom Multiple R-squared: 0.6345, Adjusted R-squared: 0.6269 F-statistic: 83.34 on 1 and 48 DF, p-value: 4.581e-12 The model fit object is actually a list, typeof(fit.xy) [1] &quot;list&quot; names(fit.xy) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; For example, to get back the list of regression coefficients (intercept and slope) as a vector we can index this list with the appropriate name: fit.xy$coefficients (Intercept) X 0.7687897 1.5510691 9.4.1 Fitted values The component fitted.values gives the predicted values of \\(Y\\) (\\(\\hat{Y}\\) in the equations above) for each observed value of \\(X\\). We can plot these predicted values of \\(Y\\), as shown below: ggplot(df.xy, aes(x = X, y = Y)) + geom_point(alpha=0.7) + # observed data geom_point(aes(x = X, y = fit.xy$fitted.values), # predicted data color=&#39;red&#39;, alpha=0.5) 9.4.2 Getting the model coefficients The coefficients components gives the value of the model parameters, namely the intercept and slope. &gt; fit.xy$coefficients (Intercept) X 0.7687897 1.5510691 As shown above, the estimated slope is 1.5510691 and the estimated intercept is 0.7687897. Recall that because this is a synthetic example, we know the “true” underlying model, which is \\(Y = 1.5X + 1.0 + \\epsilon_x\\). On the face of it, it appears our regression model is doing a decent job of estimating the true model. With this information in hand we can draw the regression line as so: ggplot(df.xy, aes(x = X, y = Y)) + geom_point(alpha=0.7) + # observed data geom_abline(slope = fit.xy$coefficients[[2]], intercept = fit.xy$coefficients[[1]], color=&#39;red&#39;, alpha=0.5) Since linear model fitting is a fairly common task, the ggplot library includes a geometric mapping, geom_smooth, that will fit a linear model for us and generate the corresponding regression plot. ggplot(df.xy, aes(x = X, y = Y)) + geom_point(alpha = 0.75) + geom_smooth(method=&quot;lm&quot;, color = &#39;red&#39;) By default, geom_smooth draws confidence intervals for the regression model (the shaded gray area around the regression line). 9.5 Residuals Residuals are the difference between the observed values of \\(Y\\) and the predicted values. You can think of residuals as the proportion of \\(Y\\) unaccounted for by the model. \\[ \\mbox{residuals} =\\vec{\\mathbf{e}} = \\vec{\\mathbf{y}} - \\vec{\\hat{\\mathbf{y}}} \\] We can draw the residuals from our model like so: ggplot(df.xy, aes(x = X)) + geom_point(aes(y = fit.xy$residuals)) + geom_hline(yintercept = 0, color = &#39;red&#39;, linetype = &quot;dashed&quot;) + labs(x = &quot;X&quot;, y = &quot;Residuals&quot;) When the linear regression model is appropriate, residuals should be centered around zero and should show no strong trends or extreme differences in spread for different values of \\(X\\). 9.6 Regression as sum-of-squares decomposition Regression can be viewed as a decomposition of the sum-of-squared deviations. In vector terms this decomposition is: \\[ |\\vec{\\mathbf{y}}|^2 = |\\vec{\\hat{\\mathbf{y}}}|^2 + |\\vec{\\mathbf{e}}|^2 \\] More typically, this is written as: \\[ ss(Y) = ss(\\hat{Y}) + ss(\\mbox{residuals}) \\] Let’s check this for our example: &gt; ss.Y &lt;- sum((Y - mean(Y))^2) &gt; ss.Yhat &lt;- sum((fit.xy$fitted.values - mean(Y))^2) &gt; ss.residuals &lt;- sum(fit.xy$residuals^2) &gt; ss.Y [1] 412.6367 &gt; ss.Yhat + ss.residuals [1] 412.6367 9.7 Variance “explained” by a regression model We can use the sum-of-square decomposition to understand the relative proportion of variance “explained” (accounted for) by the regression model. We call this quantity the “Coefficient of Determination”, designated \\(R^2\\). \\[ R^2 = \\left( 1 - \\frac{SS_{residuals}}{SS_{total}} \\right) \\] For this particular example we can estimate \\(R^2\\) as follows: R2 &lt;- 1.0 - (ss.residuals/ss.Y) R2 [1] 0.6345462 In this particular example, we find our linear model accounts for about 63% of the variance in \\(Y\\). Note that the coefficient of determination is also reported when you apply the summary function to a linear model. 9.8 Broom: a library for converting model results into data frames The model fit object we got back when we used the lm function to carry out linear regression, carries lots of useful information it isn’t a particularly “tidy” way to access the data. The R package Broom converts “statistical analysis objects from R into tidy data frames, so that they can more easily be combined, reshaped and otherwise processed with tools like ‘dplyr’, ‘tidyr’ and ‘ggplot2’. The discussion of Broom below is drawn from the Introduction to Broom Install the broom package before proceeding. library(broom) There are three broom functions that are particularly useful for our purposes. They are: tidy – constructs a data frame that summarizes the model’s statistical findings. augment – add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments. glance – construct a concise one-row summary of the model. 9.8.1 broom::tidy tidy applied to a regression model object returns a table giving the estimated coefficients and other information about the uncertainty of those estimates and corresponding p-values. For now we’re just interested in the estiamtes, the other values will be described in detail when we get to statistical inference. tidy(fit.xy) term estimate std.error statistic p.value 1 (Intercept) 0.7687897 0.5754650 1.335945 1.878668e-01 2 X 1.5510691 0.1699007 9.129269 4.580723e-12 9.8.2 broom::augment augment creates a data frame that combines the original data with related information from the model fit. df.xy.augmented &lt;- augment(fit.xy, df.xy) head(df.xy.augmented) X Y .fitted .se.fit .resid .hat .sigma 1 2.0862011 3.025964557 4.004632 0.2993053 -0.9786672 0.02851475 1.785365 2 1.1202086 1.809581474 2.506311 0.4125607 -0.6967292 0.05417720 1.788179 3 1.8077907 4.985202523 3.572798 0.3275579 1.4124045 0.03415207 1.778921 4 1.3753651 3.227592763 2.902076 0.3790450 0.3255168 0.04573224 1.790570 5 1.0030005 -0.009990412 2.324513 0.4285481 -2.3345032 0.05845748 1.756516 6 0.8915551 -0.944425977 2.151653 0.4440440 -3.0960792 0.06276144 1.729420 .cooksd .std.resid 1 0.0046055129 -0.5601926 2 0.0046788147 -0.4041844 3 0.0116232759 0.8108213 4 0.0008469145 0.1880004 5 0.0571952511 -1.3573596 6 0.1090002740 -1.8042944 Now, in addition to the X and Y variables of the original data, we have columns like .fitted (value of Y predicted by the model for the corresponding value of X), .resid (difference between the actual Y and the predicted value), and a variety of other information for evalulating model uncertainty. One thing we can do with this “augmented” data frame is to use it to better visualize and explore the model. For example, if we wanted to generate a figure highlighting the deviations from the model using vertical lines emanating from the regression line, we could do something like this: ggplot(df.xy.augmented, aes(X, Y)) + geom_point() + geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;,se=FALSE) + geom_segment(aes(xend = X, yend = .fitted), linetype=&quot;dashed&quot;) An another example, we can recreate our residual plot using the augmented data frame as so: ggplot(df.xy.augmented, aes(X, .resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, linetype=&#39;dashed&#39;) + labs(y = &quot;Residuals&quot;, title = &quot;Residual plot for synthetic data example.&quot;) 9.8.3 broom::glance glance() provides summary information about the goodness of fit of the model. Most relevant for our current discussion is the column giving the coefficient of determination (r.squared): glance(fit.xy) r.squared adj.r.squared sigma statistic p.value df logLik 1 0.6345462 0.6269326 1.772473 83.34355 4.580723e-12 2 -98.54516 AIC BIC deviance df.residual 1 203.0903 208.8264 150.7997 48 9.9 Bivariate linear regression on a real world data set Having walked through a simulation example, let’s now turn to a real world data set. A study by Whitman et al. (2004) showed that the amount of black coloring on the nose of male lions increases with age, and suggested that this might be used to estimate the age of unknown lions. To establish the relationship between these variables they measured the black coloring on the noses of male lions of known age (represented as a proportion), giving the bivariate relationship (and fitted model) shown below: lions &lt;- read_csv(&quot;https://github.com/bio304-class/bio304-fall2017/raw/master/datasets/ABD-lion-noses.csv&quot;) Parsed with column specification: cols( proportionBlack = col_double(), ageInYears = col_double() ) ggplot(lions, aes(x = proportionBlack, y = ageInYears)) + geom_point() + geom_smooth(method=&quot;lm&quot;, color = &#39;red&#39;) By eye, the linear model looks like a pretty good fit. Let’s take a look at the quantitative values of the regression model, using the various Broom functions to produce nice output. lion.model &lt;- lm(ageInYears ~ proportionBlack, data = lions) tidy(lion.model) term estimate std.error statistic p.value 1 (Intercept) 0.8790062 0.5688171 1.545323 1.327530e-01 2 proportionBlack 10.6471194 1.5095005 7.053406 7.677005e-08 glance(lion.model) r.squared adj.r.squared sigma statistic p.value df logLik 1 0.623827 0.6112879 1.668764 49.75053 7.677005e-08 2 -60.76008 AIC BIC deviance df.residual 1 127.5202 131.9174 83.54321 30 We then augment our data set with information from the model fit and plots a residual plot. lions.augmented &lt;- augment(lion.model, lions) ggplot(lions.augmented, aes(proportionBlack, .resid)) + geom_point() + geom_hline(yintercept = 0, color=&quot;red&quot;, linetype=&quot;dashed&quot;) "],
["matrices-in-r.html", "Chapter 10 Matrices in R 10.1 Creating matrices in R 10.2 Matrix arithmetic operations in R 10.3 Descriptive statistics as matrix functions 10.4 Matrix Inverse", " Chapter 10 Matrices in R In R matrices are two-dimensional collections of elements all of which have the same mode or type. This is different than a data frame in which the columns of the frame can hold elements of different type (but all of the same length), or from a list which can hold objects of arbitrary type and length. Matrices are more efficient for carrying out most numerical operations, so if you’re working with a very large data set that is amenable to representation by a matrix you should consider using this data structure. library(tidyverse) 10.1 Creating matrices in R There are a number of different ways to create matrices in R. For creating small matrices at the command line you can use the matrix() function. &gt; x &lt;- matrix(1:5) # creates a column vector &gt; x [,1] [1,] 1 [2,] 2 [3,] 3 [4,] 4 [5,] 5 &gt; X &lt;- matrix(1:12, nrow=4) # creates a matr &gt; X [,1] [,2] [,3] [1,] 1 5 9 [2,] 2 6 10 [3,] 3 7 11 [4,] 4 8 12 &gt; dim(X) # give the shape of the matrix [1] 4 3 matrix() takes a data vector as input and the shape of the matrix to be created is specified by using the nrow and ncol arguments. If the number of elements in the input data vector is less than nrows \\(\\times\\) ncols the elements will be ‘recycled’ as discussed in previous chapters. Without any shape arguments the matrix() function will create a column vector as shown above. By default the matrix() function fills in the matrix in a column-wise fashion. To fill in the matrix in a row-wise fashion use the argument byrow=T. If you have a pre-existing data set in a list or data frame you can use the as.matrix() function to convert it to a matrix. &gt; iris.mtx &lt;- as.matrix(iris) &gt; head(iris.mtx) # NOTE: the elements were all converted to character Sepal.Length Sepal.Width Petal.Length Petal.Width Species [1,] &quot;5.1&quot; &quot;3.5&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; [2,] &quot;4.9&quot; &quot;3.0&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; [3,] &quot;4.7&quot; &quot;3.2&quot; &quot;1.3&quot; &quot;0.2&quot; &quot;setosa&quot; [4,] &quot;4.6&quot; &quot;3.1&quot; &quot;1.5&quot; &quot;0.2&quot; &quot;setosa&quot; [5,] &quot;5.0&quot; &quot;3.6&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; [6,] &quot;5.4&quot; &quot;3.9&quot; &quot;1.7&quot; &quot;0.4&quot; &quot;setosa&quot; Since all elements of an R matrix must be of the same type, when we passed the iris data frame to as.matrix(), everything was converted to a character due to the presence of the Species column in the data frame. &gt; # This is probably more along the lines of what you want &gt; iris.mtx &lt;- iris %&gt;% select(-Species) %&gt;% as.matrix &gt; head(iris.mtx) Sepal.Length Sepal.Width Petal.Length Petal.Width [1,] 5.1 3.5 1.4 0.2 [2,] 4.9 3.0 1.4 0.2 [3,] 4.7 3.2 1.3 0.2 [4,] 4.6 3.1 1.5 0.2 [5,] 5.0 3.6 1.4 0.2 [6,] 5.4 3.9 1.7 0.4 You can use the various indexing operations to get particular rows, columns, or elements. Here are some examples: &gt; X &lt;- matrix(1:12, nrow=4) &gt; X [,1] [,2] [,3] [1,] 1 5 9 [2,] 2 6 10 [3,] 3 7 11 [4,] 4 8 12 &gt; X[1,] # get the first row [1] 1 5 9 &gt; X[,1] # get the first column [1] 1 2 3 4 &gt; X[1:2,] # get the first two rows [,1] [,2] [,3] [1,] 1 5 9 [2,] 2 6 10 &gt; X[,2:3] # get the second and third columns [,1] [,2] [1,] 5 9 [2,] 6 10 [3,] 7 11 [4,] 8 12 &gt; Y &lt;- matrix(1:12, byrow=T, nrow=4) &gt; Y [,1] [,2] [,3] [1,] 1 2 3 [2,] 4 5 6 [3,] 7 8 9 [4,] 10 11 12 &gt; Y[4] # see explanation below [1] 10 &gt; Y[5] [1] 2 &gt; dim(Y) &lt;- c(2,6) # reshape Y &gt; Y [,1] [,2] [,3] [,4] [,5] [,6] [1,] 1 7 2 8 3 9 [2,] 4 10 5 11 6 12 &gt; Y[5] [1] 2 The example above where we create a matrix Y is meant to show that matrices are stored internally in a column wise fashion (think of the columns stacked one atop the other), regardless of whether we use the byrow=T argument. Therefore using single indices returns the elements with respect to this arrangement. Note also the use of assignment operator in conjuction with the dim() function to reshape the matrix. Despite the reshaping, the internal representation in memory hasn’t changed so Y[5] still gives the same element. You can use the diag() function to get the diagonal of a matrix or to create a diagonal matrix as show below: &gt; Z &lt;- matrix(rnorm(16), ncol=4) &gt; Z [,1] [,2] [,3] [,4] [1,] 0.2813681 -1.4982809 0.9894849 1.4907441 [2,] -0.2200975 -0.6530464 -1.5459291 -0.3988433 [3,] 0.9326019 -0.5406263 0.8606377 -1.3769548 [4,] -2.0768741 -0.8227990 1.0986354 0.6869724 &gt; diag(Z) [1] 0.2813681 -0.6530464 0.8606377 0.6869724 &gt; diag(5) # create the 5 x 5 identity matrix [,1] [,2] [,3] [,4] [,5] [1,] 1 0 0 0 0 [2,] 0 1 0 0 0 [3,] 0 0 1 0 0 [4,] 0 0 0 1 0 [5,] 0 0 0 0 1 &gt; s &lt;- sqrt(10:13) &gt; diag(s) [,1] [,2] [,3] [,4] [1,] 3.162278 0.000000 0.000000 0.000000 [2,] 0.000000 3.316625 0.000000 0.000000 [3,] 0.000000 0.000000 3.464102 0.000000 [4,] 0.000000 0.000000 0.000000 3.605551 10.2 Matrix arithmetic operations in R The standard mathematical operations of addition and subtraction and scalar multiplication work element-wise for matrices in the same way as they did for vectors. Matrix multiplication uses the operator %*% which you saw last week for the dot product. To get the transpose of a matrix use the function t(). &gt; A &lt;- matrix(1:12, nrow=4) &gt; A [,1] [,2] [,3] [1,] 1 5 9 [2,] 2 6 10 [3,] 3 7 11 [4,] 4 8 12 &gt; &gt; B &lt;- matrix(rnorm(12), nrow=4) &gt; B [,1] [,2] [,3] [1,] 0.2130817 -0.7822787 -2.5833688 [2,] -2.0990084 -0.3766951 1.1850931 [3,] -0.5428810 -0.2586100 -0.6334821 [4,] 0.9451671 0.6230917 -0.6260841 &gt; &gt; t(A) # transpose [,1] [,2] [,3] [,4] [1,] 1 2 3 4 [2,] 5 6 7 8 [3,] 9 10 11 12 &gt; A + B # matrix addition [,1] [,2] [,3] [1,] 1.21308172 4.217721 6.416631 [2,] -0.09900841 5.623305 11.185093 [3,] 2.45711896 6.741390 10.366518 [4,] 4.94516711 8.623092 11.373916 &gt; A - B # matrix subtraction [,1] [,2] [,3] [1,] 0.7869183 5.782279 11.583369 [2,] 4.0990084 6.376695 8.814907 [3,] 3.5428810 7.258610 11.633482 [4,] 3.0548329 7.376908 12.626084 &gt; 5 * A # multiplication by a scalar [,1] [,2] [,3] [1,] 5 25 45 [2,] 10 30 50 [3,] 15 35 55 [4,] 20 40 60 When applying matrix multiplication, the dimensions of the matrices involved must be conformable. For example, you can’t do this: A %*% B # do you understand why this generates an error? But this works: &gt; A %*% t(B) [,1] [,2] [,3] [,4] [1,] -26.94863 6.683354 -7.537270 -1.5741315 [2,] -30.10120 5.392744 -8.972243 -0.6319568 [3,] -33.25376 4.102133 -10.407216 0.3102179 [4,] -36.40633 2.811523 -11.842190 1.2523925 10.3 Descriptive statistics as matrix functions Assume you have a data set represented as a \\(n \\times p\\) matrix, \\(X\\), with observations in rows and variables in columns. Below I give formulae for calculating some descriptive statistics as matrix functions. 10.3.1 Mean vector and matrix You can calculate a row vector of means, \\(\\mathbf{m}\\), as: \\[ \\mathbf{m} = \\frac{1}{n} \\mathbf{1}^T X \\] where \\(1\\) is a \\(n \\times 1\\) vector of ones. A \\(n \\times p\\) matrix \\(M\\) where each column is filled with the mean value for that column is: \\[ M = \\mathbf{1}\\mathbf{m} \\] 10.3.2 Deviation matrix To re-express each value as the deviation from the variable means (i.e.~each columns is a mean centered vector) we calculate a deviation matrix: \\[ D = X - M \\] 10.3.3 Covariance matrix The \\(p \\times p\\) covariance matrix can be expressed as a matrix product of the deviation matrix: \\[ S = \\frac{1}{n-1} D^T D \\] 10.3.4 Correlation matrix The correlation matrix, \\(R\\), can be calculated from the covariance matrix by: \\[ R = V S V \\] where \\(V\\) is a \\(p \\times p\\) diagonal matrix where \\(V_{ii} = 1/\\sqrt{S_{ii}}\\). 10.4 Matrix Inverse The function solve() can be used to find matrix inverses in R. A &lt;- matrix(1:4, nrow=2) A [,1] [,2] [1,] 1 3 [2,] 2 4 Ainv &lt;- solve(A) Ainv [,1] [,2] [1,] -2 1.5 [2,] 1 -0.5 A %*% Ainv # should give identity matrix [,1] [,2] [1,] 1 0 [2,] 0 1 Ainv %*% A # should also result in identity matrix [,1] [,2] [1,] 1 0 [2,] 0 1 Keep in mind that not all square matrices are invertible: C &lt;- matrix(1:16, nrow=4) C [,1] [,2] [,3] [,4] [1,] 1 5 9 13 [2,] 2 6 10 14 [3,] 3 7 11 15 [4,] 4 8 12 16 Cinv &lt;- solve(C) Error in solve.default(C): Lapack routine dgesv: system is exactly singular: U[3,3] = 0 "],
["multiple-regression.html", "Chapter 11 Multiple Regression 11.1 New Libraries to install 11.2 Libraries 11.3 Examplar data 11.4 Data exploration 11.5 3D Plots 11.6 Fitting the regression model 11.7 Interpretting the regression model 11.8 Exploring the Vector Geometry of the Regression Model 11.9 Exploring the Residuals from the Model Fit 11.10 An alternate model 11.11 Fitting a curvilinear model using lm() 11.12 Exploring the impact of nearly collinear predictors on regression", " Chapter 11 Multiple Regression 11.1 New Libraries to install We’ll be using several new packages for this class session. Install the following packages via one of the standard install mechanisms: scatterplot3d rgl – NOTE: On OS X, rgl requires you to install a program called XQuartz. XQuartz can be downloaded from the XQuartz Home Page. If you’re on a Mac, install XQuartz before installing rgl. You may have to reboot your computer after installing XQuartz. 11.2 Libraries library(tidyverse) library(broom) # for working w/lm output 11.3 Examplar data To illustrate multiple regression in R we’ll use a built in dataset called trees. trees consists of measurements of the girth, height, and volume of 31 black cherry trees (?trees for more info). Let’s assume we’re lumberjacks, but our permit only allows us to harvest a fixed number of trees. We get paid by the total volume of wood we harvest, so we’re interested in predicting a tree’s volume (hard to measure directly) as a function of its girth and height (relatively easy to measure), so we can pick the best trees to harvest. We’ll therefore calculate a multiple regression of volume on height and width. 11.4 Data exploration We’ll start with some summary tables and diagnostic plots to familiarize ourselves with the data: names(trees) [1] &quot;Girth&quot; &quot;Height&quot; &quot;Volume&quot; dim(trees) [1] 31 3 summary(trees) Girth Height Volume Min. : 8.30 Min. :63 Min. :10.20 1st Qu.:11.05 1st Qu.:72 1st Qu.:19.40 Median :12.90 Median :76 Median :24.20 Mean :13.25 Mean :76 Mean :30.17 3rd Qu.:15.25 3rd Qu.:80 3rd Qu.:37.30 Max. :20.60 Max. :87 Max. :77.00 We’ll use the GGally::ggpairs() function introduced in problem set 01 to create a scatterplot matrix depicting the pairwise relationships between all the variables library(GGally) ggpairs(trees) As one might expect for morphological measurements related to size, the scatterplot matrix shows that all the variables are positively correlated, and girth and volume have a particularly strong correlation. 11.5 3D Plots ggplot has no built in facilities for 3D scatter plots so we’ll use two new packages, scatterplot3D and rgl, to generate 3D visualizations. 11.5.1 scatterplot3d library(scatterplot3d) # install this package first if needed scatterplot3d(trees, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) The argument pch sets the type of plotting character to use in the plot (for a graphical key of the available plotting characters see this link) and color sets plotting character colors. We can change the angle of the 3D plot using the angle argument: scatterplot3d(trees, pch = 16, color=&quot;steelblue&quot;, angle=75, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) We can add vertical lines to the plot using the type argument and remove the box around the plot: scatterplot3d(trees, pch = 16, color=&quot;steelblue&quot;, angle=75, box = FALSE, type = &quot;h&quot;, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) For more examples of how you can modify plots generated with the scatterplot3d package see this web page). 11.5.2 rgl The package rgl is another package that we can use for 3D visualization. rgl is powerful because it lets us create interactive plots we can rotate and zoom in/out on. To make rgl plots work in HTML document, make your codeblock start with this header: {r, results = &quot;asis&quot;} You can then create an interactive 3D plot as so: library(rgl) # create 3D scatter, using spheres to draw points plot3d(trees$Girth, trees$Height, trees$Volume, xlab = &quot;Girth&quot;, ylab = &quot;Height&quot;, zlab = &quot;Volume&quot;, type = &quot;s&quot;, size = 1.5, col = &quot;red&quot;) rglwidget() # only need to include this line if using in a markdown document 11.6 Fitting the regression model From the 3D scatter plot it looks like we ought to be able to find a plane through the data that fits the scatter fairly well. Let’s use the lm() function to calculate the multiple regression and summary() to get the details of the model: fit.trees &lt;- lm(Volume ~ Girth + Height, data=trees) summary(fit.trees) Call: lm(formula = Volume ~ Girth + Height, data = trees) Residuals: Min 1Q Median 3Q Max -6.4065 -2.6493 -0.2876 2.2003 8.4847 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -57.9877 8.6382 -6.713 2.75e-07 *** Girth 4.7082 0.2643 17.816 &lt; 2e-16 *** Height 0.3393 0.1302 2.607 0.0145 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 3.882 on 28 degrees of freedom Multiple R-squared: 0.948, Adjusted R-squared: 0.9442 F-statistic: 255 on 2 and 28 DF, p-value: &lt; 2.2e-16 11.6.1 Visualizing the regression model in scatterplot3d To visualize the multiple regression, let’s use the scatterplot3d package to draw the 3D scatter of plots and the plane that corresponds to the regression model: p &lt;- scatterplot3d(trees, angle=55,type=&#39;h&#39;, pch = 16, color = &quot;steelblue&quot;, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) # add a plane representing the fit of the model p$plane3d(fit.trees, col=&#39;orangered&#39;) If instead of scatterplot3d, we wanted to use rgl to depict the model fit we can use the rgl.planes function as shown below. coefs &lt;- coef(fit.trees) b1 &lt;- coefs[&quot;Girth&quot;] b2 &lt;- coefs[&quot;Height&quot;] c &lt;- -1 a &lt;- coefs[&quot;(Intercept)&quot;] plot3d(trees$Girth, trees$Height, trees$Volume, xlab = &quot;Girth&quot;, ylab = &quot;Height&quot;, zlab = &quot;Volume&quot;, type = &quot;s&quot;, size = 1.5, col = &quot;red&quot;) rgl.planes(b1, b2, c, a, alpha = 0.9, color = &quot;gray&quot;) rglwidget() From the figures it looks like the regression model fits pretty well, as we anticipated from the pairwise relationships. 11.7 Interpretting the regression model The regression equation is: \\(\\hat{y}\\) = + \\(x_1\\) +\\(x_2\\), where \\(y\\) is Volume, and \\(x_1\\) and \\(x_2\\) are Girth and Height respectively. Since they’re on different scales the coefficients for Girth and Height aren’t directly comparable. Both coefficients are significant at the \\(p&lt;0.05\\) level, but note that Girth is the much stronger predictor. In fact the addition of height explains only a minor additional fraction of variation in tree volume, so from the lumberjack’s perspective the additional trouble of measuring height probably isn’t worth it. 11.8 Exploring the Vector Geometry of the Regression Model The object returned by the lm() function hold lots of useful information: names(fit.trees) [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; The fitted.values correspond to the predicted values of the outcome variable (\\(\\hat{y}\\)). Alternate we can get this information in useful table form using functions from the broom library: Recall the broom:tidy produces a tabular summary of the coefficients of the model and their associated statistics: broom::tidy(fit.trees) term estimate std.error statistic p.value 1 (Intercept) -57.9876589 8.6382259 -6.712913 2.749507e-07 2 Girth 4.7081605 0.2642646 17.816084 8.223304e-17 3 Height 0.3392512 0.1301512 2.606594 1.449097e-02 broom:glance provides information about the fit of the model: broom::glance(fit.trees) r.squared adj.r.squared sigma statistic p.value df logLik 1 0.94795 0.9442322 3.881832 254.9723 1.071238e-18 3 -84.45499 AIC BIC deviance df.residual 1 176.91 182.6459 421.9214 28 Let’s use our knowledge of vector geometry to further explore the relationship between the predicted Volume and the predictor variables. By definition the vector representing the predicted values lies in the subspace (in this case a plane) defined by Height and Girth, so let’s do some simple calculations to understand their length and angular relationships: # proportional to length of vectors sd(fit.trees$fitted.values) [1] 16.00434 sd(trees$Height) [1] 6.371813 sd(trees$Girth) [1] 3.138139 # cosines of angles btw vectors cor(trees$Height, trees$Girth) [1] 0.5192801 cor(trees$Girth, fit.trees$fitted.values) [1] 0.9933158 cor(trees$Height, fit.trees$fitted.values) [1] 0.6144545 # angles btw vectors in degrees acos(cor(trees$Girth, trees$Height)) * (180/pi) [1] 58.71603 acos(cor(trees$Girth, fit.trees$fitted.values)) * (180/pi) [1] 6.628322 acos(cor(trees$Height, fit.trees$fitted.values)) * (180/pi) [1] 52.08771 11.9 Exploring the Residuals from the Model Fit Now let’s look at the residuals from the regression. The residuals represent the `unexplained’ variance: trees.augmented &lt;- augment(fit.trees, trees) ggplot(trees.augmented, aes(x = Girth, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color=&#39;red&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;Girth&quot;, y = &quot;Residuals&quot;) Ideally the residuals should be evenly scattered around zero, with no trends as we go from high to low values of the dependent variable. As you can see, the residuals are somewhat u-shaped or j-shaped suggesting that there may be a non-linear aspect of the relationship that our model isn’t capturing. 11.10 An alternate model Let’s think about the relationships we’re actually modeling for a few minutes. For the sake of simplicity let’s consider the trunk of a tree to be a cylinder. How do the dimensions of this cylinder relate to its volume? You can look up the formula for the volume of a cylinder, but the key thing you’ll want to note is that volume of the cylinder should be proportional to a characteristic length of the cylinder cubed (\\(V \\propto \\mathrm{L}^3\\)). This suggests that if we want to fit a linear model we should relate Girth and Height to \\(\\sqrt[3]{\\mathrm{Volume}}\\): trees.cuberoot &lt;- mutate(trees, cuberoot.Volume = Volume^0.33) fit.trees.cuberoot &lt;- lm(cuberoot.Volume ~ Girth + Height, data = trees.cuberoot) broom::glance(fit.trees) # summary of fit of original model r.squared adj.r.squared sigma statistic p.value df logLik 1 0.94795 0.9442322 3.881832 254.9723 1.071238e-18 3 -84.45499 AIC BIC deviance df.residual 1 176.91 182.6459 421.9214 28 broom::glance(fit.trees.cuberoot) # summary of fit of alternate model r.squared adj.r.squared sigma statistic p.value df logLik 1 0.9776484 0.9760518 0.08108365 612.3532 7.768237e-24 3 35.47103 AIC BIC deviance df.residual 1 -62.94206 -57.20611 0.1840877 28 Comparing the summary tables, we see indeed that using the cube root of Volume improves the fit of our model some. Let’s examine the residuals of this alternate model. trees.cuberoot &lt;- broom::augment(fit.trees.cuberoot, trees.cuberoot) ggplot(trees.cuberoot, aes(x = cuberoot.Volume, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color=&#39;red&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;Girth&quot;, y = &quot;Residuals&quot;) As we can see the transformation we applied to the data did seem to make our residuals more uniform across the range of observations. 11.11 Fitting a curvilinear model using lm() Above we transformed the volume data in order to fit a straight line relationship between \\(\\sqrt[3]{V}\\) and Girth and Hieght. However, we could just as easily have applied a cubic regression to the original variables (remember this is still linear in the coefficients). Since Height didn’t add much to additional information, we’ll simplify the model to consider only Girth. fit.curvilinear &lt;- lm(Volume ~ I(Girth^3), data=trees) broom::tidy(fit.curvilinear) term estimate std.error statistic p.value 1 (Intercept) 8.042696007 1.0426698301 7.71356 1.662577e-08 2 I(Girth^3) 0.008136533 0.0003117691 26.09795 1.086600e-21 broom::glance(fit.curvilinear) r.squared adj.r.squared sigma statistic p.value df logLik 1 0.9591608 0.9577526 3.378663 681.1029 1.0866e-21 2 -80.69526 AIC BIC deviance df.residual 1 167.3905 171.6925 331.0456 29 Here’s how we can visualize the corresponding curvilinear regression using ggplot: ggplot(trees, aes(x = Girth, y = Volume)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ I(x^3), se = FALSE) The I() function used above requires a little explanation. Normally, the R formula syntax (see ?formula) treats the carat symbol, ^, as short-hand for factor crossing to the specified degree. For example, the formula (a+b+c)^2 would be interpretted as the model with main effects and all second order interaction terms, i.e. a + b + c + a:b + a:c + b:c where the colons indicate interactions. The I() function `protects’ the object in it’s argument; in this case telling the regression function to treat this as Girth raised to the third power as opposed to trying to construct interaction terms for Girth. 11.12 Exploring the impact of nearly collinear predictors on regression In lecture we discussed the problems that can arise in regression when your predictor variables are nearly collinear. In this section we’ll illustrate some of these issues. Consider again the trees data set. Recall that two of the variables – Girth and Volume – are highly correlated and thus nearly collinear. cor(trees) Girth Height Volume Girth 1.0000000 0.5192801 0.9671194 Height 0.5192801 1.0000000 0.5982497 Volume 0.9671194 0.5982497 1.0000000 Let’s explore what happens when we treat Height as the dependent variable, and Girth and Volume as the predictor variables. fit.Height &lt;- lm(Height ~ Girth + Volume, data = trees) broom::glance(fit.Height) r.squared adj.r.squared sigma statistic p.value df logLik 1 0.4122678 0.3702869 5.056318 9.820371 0.0005868313 3 -92.64926 AIC BIC deviance df.residual 1 193.2985 199.0345 715.8579 28 We can, of course, fit the linear model despite the near collinearity, and we find that the model does have some predictive power, with \\(R^2 = 0.41\\), and with Volume being the more significant predictor. Now, let’s created a slightly different version of the trees data set by add some noise to the three variables. Our goal here is to simulate a data set we might have created had we measured a slightly different set of trees during our sampling. We’ll use the jitter() function to add uniform noise to the data set. jitter.Girth &lt;- jitter(trees$Girth, amount= 0.5 * sd(trees$Girth)) jitter.Height &lt;- jitter(trees$Height, amount= 0.5 * sd(trees$Height)) jitter.Volume &lt;- jitter(trees$Volume, amount= 0.5 * sd(trees$Volume)) jitter.trees &lt;- data.frame(Girth = jitter.Girth, Height = jitter.Height, Volume = jitter.Volume) Here we added uniform noise proportional to the one-quarter the standard deviation of each variable. Let’s take a moment to convince ourselves that our new data set, jitter.trees, is not too different from the trees data set from which it was derived. set.seed(20180228) # compare this to broom::tidy(trees) broom::tidy(jitter.trees) column n mean sd median trimmed mad min 1 Girth 31 13.12941 3.254615 12.68101 12.93121 3.141887 7.899714 2 Height 31 75.61539 6.833216 75.19582 75.73138 6.742719 61.893582 3 Volume 31 30.38463 17.022313 21.64749 28.56103 8.936935 10.057182 max range skew kurtosis se 1 21.93581 14.03610 0.65775952 -0.08395162 0.5845461 2 88.12591 26.23232 -0.08102373 -0.88115858 1.2272818 3 71.95424 61.89706 0.84178093 -0.64817524 3.0572977 # correlations among jittered variables are # similar to those of the original variables cor(jitter.trees) Girth Height Volume Girth 1.0000000 0.5020319 0.8609323 Height 0.5020319 1.0000000 0.4845783 Volume 0.8609323 0.4845783 1.0000000 ## jittered variables are highly correlatd with original variables cor(trees$Height, jitter.trees$Height) [1] 0.9618093 cor(trees$Girth, jitter.trees$Girth) [1] 0.9594642 cor(trees$Volume, jitter.trees$Volume) [1] 0.9518737 Now that we’ve convinced ourselves that our jittered data set is a decent approximation to our original data set, let’s re-calculate the linear regression, and compare the coefficients of the jittered model to the original model: fit.Height.jitter &lt;- lm(Height ~ Girth + Volume, data = jitter.trees) broom::tidy(fit.Height) term estimate std.error statistic p.value 1 (Intercept) 83.2957705 9.0865753 9.166905 6.333488e-10 2 Girth -1.8615109 1.1566879 -1.609346 1.187591e-01 3 Volume 0.5755946 0.2208225 2.606594 1.449097e-02 broom::tidy(fit.Height.jitter) term estimate std.error statistic p.value 1 (Intercept) 64.11040674 5.8941259 10.8770000 1.457665e-11 2 Girth 0.68830907 0.6697467 1.0277155 3.128805e-01 3 Volume 0.08122189 0.1280535 0.6342806 5.310461e-01 We see that the coefficients of the linear model have changed substantially between the original data and the jittered data. Our model is unstable to relatively modest changes to the data! Let’s draw some plots to illustrate how different the models fit to the original and jittered data are: # draw 3d scatter plots with small points so as not to obscure regression planes p &lt;- scatterplot3d(x=trees$Girth, y=trees$Volume, z=trees$Height, angle=15, type=&#39;p&#39;, pch=&#39;.&#39;) # original model p$plane3d(fit.Height, col=&#39;orangered&#39;) # jittered model p$plane3d(fit.Height.jitter, col=&#39;blue&#39;) Let’s do the same comparison for the multiple regression of Volume on Height and Girth. In this case the predictor variables are nearly collinear. fit.Volume &lt;- lm(Volume ~ Girth + Height, data = trees) fit.Volume.jitter &lt;- lm(Volume ~ Girth + Height, data = jitter.trees) coefficients(fit.Volume) (Intercept) Girth Height -57.9876589 4.7081605 0.3392512 coefficients(fit.Volume.jitter) (Intercept) Girth Height -39.5087487 4.3190348 0.1743959 For this model, we see that the coefficients have changed only a small amount. The underlying data, jitter.trees, is the same in both cases, but now our model is stable because the predictor variables are only modestly correlated with each other. Let’s generate another plot to illustrate the similarity of the models fit to the original and jittered data when Girth and Height are used to predict Volume. p &lt;- scatterplot3d(x=trees$Girth, y=trees$Height, z=trees$Volume, angle=55, type=&#39;p&#39;, pch=&#39;.&#39;) p$plane3d(fit.Volume, col=&#39;orangered&#39;) p$plane3d(fit.Volume.jitter, col=&#39;blue&#39;) Finally, let’s do some vector calculations to quantify how the angular deviation between the fit data and the predictor variables changes between the original and jittered data set for the two different multiple regressions: # write a quickie fxn to express angle between vectors in degrees vec.angle &lt;- function(x,y) { acos(cor(x,y)) * (180/pi)} # vector angles for fit of Height ~ Girth + Volume (orig) vec.angle(fit.Height$fitted.values, trees$Girth) [1] 36.02644 vec.angle(fit.Height$fitted.values, trees$Volume) [1] 21.29297 # vector angles for fit of Height ~ Girth + Volume (jittered) vec.angle(fit.Height.jitter$fitted.values, jitter.trees$Girth) [1] 11.58668 vec.angle(fit.Height.jitter$fitted.values, jitter.trees$Volume) [1] 18.9919 Now the same comparison for the non-collinear model Volume ~ Girth + Height. # vector angles for fit of Volume ~ Girth + Height (orig) vec.angle(fit.Volume$fitted.values, trees$Girth) [1] 6.628322 vec.angle(fit.Volume$fitted.values, trees$Height) [1] 52.08771 # vector angles for fit of Volume ~ Girth + Height (jittered) vec.angle(fit.Volume.jitter$fitted.values, jitter.trees$Girth) [1] 4.022743 vec.angle(fit.Volume.jitter$fitted.values, jitter.trees$Height) [1] 55.84274 "],
["principal-components-analysis.html", "Chapter 12 Principal Components Analysis 12.1 Libraries 12.2 Matrices as linear transformations 12.3 Eigenanalysis in R 12.4 Principal Components Analysis in R 12.5 Drawing Figures to Represent PCA", " Chapter 12 Principal Components Analysis 12.1 Libraries library(tidyverse) library(broom) library(GGally) # provides ggpairs library(cowplot) 12.2 Matrices as linear transformations In lecture we introduced the notion that pre-multiplying a vector, \\(x\\), by a matrix \\(\\mathbf{A}\\) represents a linear transformation of \\(x\\). Let’s explore that visually for 2D vectors. For illustration let’s generate a 2D vector representing the coordinates of points on the sine function. sin.xy &lt;- data_frame(x = seq(0,2*pi, length.out = 50) - pi, y = sin(x)) ggplot(sin.xy, aes(x = x, y= y)) + geom_point() Let’s start with some of the transformations we discussed in lecture. First we look at reflection in the x-axis: A &lt;- matrix(c(1, 0, 0, -1), byrow=TRUE, nrow=2) Ax &lt;- A %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) ggplot(Ax.df, aes(x = x, y= y)) + geom_point() Now shear: A &lt;- matrix(c(1, 1.5, 0, 1), byrow=TRUE, nrow=2) Ax &lt;- A %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) ggplot(Ax.df, aes(x = x, y= y)) + geom_point() And finally rotation: A &lt;- matrix(c(cos(pi/2), -sin(pi/2), sin(pi/2), cos(pi/2)), byrow=TRUE, nrow=2) Ax &lt;- A %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) ggplot(Ax.df, aes(x = x, y= y)) + geom_point() 12.3 Eigenanalysis in R As we discussed in lecture, the eigenvectors of a square matrix, \\(\\mathbf{A}\\), point in the directions that are unchanged by the transformation specified by \\(\\mathbf{A}\\). The following relationships relate \\(\\mathbf{A}\\) to it’s eigenvectors and eigenvalues: \\[\\mathbf{V}^{-1}\\mathbf{A}\\mathbf{V} = \\mathbf{D} \\] \\[\\mathbf{A} = \\mathbf{V}\\mathbf{D}\\mathbf{V}^{-1}\\] where \\(\\mathbf{V}\\) is a matrix where the columns represent the eigenvectors, and \\(\\mathbf{D}\\) is a diagonal matrix of eigenvalues. The eigen() function computes the eigenvalues and eigenvectors of a square matrix. A &lt;- matrix(c(2,1,2,3),nrow=2) A [,1] [,2] [1,] 2 2 [2,] 1 3 eigen.A &lt;- eigen(A) eigen.A eigen() decomposition $values [1] 4 1 $vectors [,1] [,2] [1,] -0.7071068 -0.8944272 [2,] -0.7071068 0.4472136 V &lt;- eigen.A$vectors D &lt;- diag(eigen.A$values) # diagonal matrix of eigenvalues Vinv &lt;- solve(V) V %*% D %*% Vinv # reconstruct our original matrix (see lecture slides) [,1] [,2] [1,] 2 2 [2,] 1 3 Vinv %*% A %*% V [,1] [,2] [1,] 4.000000e+00 0 [2,] 2.220446e-16 1 all.equal(Vinv %*% A %*% V, D) # test &#39;near equality&#39; [1] TRUE V[,1] %*% V[,2] # note that the eigenvectors are NOT orthogonal. Why? [,1] [1,] 0.3162278 B &lt;- matrix(c(2,2,2,3),nrow=2) # define another tranformation B [,1] [,2] [1,] 2 2 [2,] 2 3 eigen.B &lt;- eigen(B) eigen.B$values [1] 4.5615528 0.4384472 eigen.B$vectors [,1] [,2] [1,] 0.6154122 -0.7882054 [2,] 0.7882054 0.6154122 Vb &lt;- eigen.B$vectors Vb[,1] %*% Vb[,2] # these eigenvectors ARE orthogonal. [,1] [1,] 0 Since \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) represent 2D transformations we can visualize the effect of these transformations using points in the plane. We’ll show how they distort a set of points that make up a square. eigenvec.A.slope1 = eigen.A$vectors[1,1]/eigen.A$vectors[2,1] eigenvec.A.slope2 = eigen.A$vectors[2,1]/eigen.A$vectors[2,2] eigenvec.B.slope1 = eigen.B$vectors[1,1]/eigen.B$vectors[2,1] eigenvec.B.slope2 = eigen.B$vectors[2,1]/eigen.B$vectors[2,2] Ax &lt;- A %*% t(sin.xy) Bx &lt;- B %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) Bx.df &lt;- as.data.frame(t(Bx)) %&gt;% rename(x = V1, y = V2) plot.transform.AB &lt;- ggplot(sin.xy, aes(x = x, y = y)) + geom_point(alpha=0.5) + geom_point(data = Ax.df, color=&#39;red&#39;) + geom_point(data = Bx.df, color=&#39;blue&#39;) + coord_fixed() plot.transform.AB By plotting the eigenvectors we can make a similar figure to show the directions that are unaffected by the transformation represented by the matrix \\(\\mathbf{A}\\): ggplot(sin.xy, aes(x = x, y = y)) + geom_point(alpha=0.5) + geom_point(data = Ax.df, color=&#39;red&#39;) + geom_abline(aes(slope = eigenvec.A.slope1,intercept=0), color=&#39;red&#39;,linetype=&#39;dashed&#39;) + geom_abline(aes(slope = eigenvec.A.slope2,intercept=0), color=&#39;red&#39;,linetype=&#39;dashed&#39;) + coord_fixed() 12.4 Principal Components Analysis in R There are two functions in R for carrying out PCA - princomp() and prcomp(). The princomp() function uses the eigen() function to carry out the analysis on the covariance matrix or correlation matrix, while carries out an equivalent analysis, starting from a data matrix, using a technique called singular value decomposition (SVD). The SVD routine has greater numerical accuracy, so the prcomp() function should generally be preferred. The princomp() function is useful when you don’t have access to the original data, but you do have a covariance or correlation matrix (a frequent situation when re-analyzing data from the literature). We’ll concentrate on using the prcomp() function. 12.4.1 Bioenv dataset To demonstrate PCA we’ll use a dataset called `bioenv.txt’ (see class wiki), obtained from a book called “Biplots in Practice” (M. Greenacre, 2010). Here is Greenacre’s description of the dataset: The context is in marine biology and the data consist of two sets of variables observed at the same locations on the sea-bed: the first is a set of biological variables, the counts of five groups of species, and the second is a set of four environmental variables. The data set, called “bioenv”, is shown in Exhibit 2.1. The species groups are abbreviated as “a” to “e”. The environmental variables are “pollution”, a composite index of pollution combining measurements of heavy metal concentrations and hydrocarbons; depth, the depth in metres of the sea-bed where the sample was taken; “temperature”, the temperature of the water at the sampling point; and “sediment”, a classification of the substrate of the sample into one of three sediment categories. We’ll start by reading the bioenv.txt data set from the Github repository: bioenv &lt;- read_tsv(&#39;https://github.com/Bio723-class/example-datasets/raw/master/bioenv.txt&#39;) Parsed with column specification: cols( X1 = col_character(), a = col_integer(), b = col_integer(), c = col_integer(), d = col_integer(), e = col_integer(), Pollution = col_double(), Depth = col_integer(), Temperature = col_double(), Sediment = col_character() ) names(bioenv) [1] &quot;X1&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; [6] &quot;e&quot; &quot;Pollution&quot; &quot;Depth&quot; &quot;Temperature&quot; &quot;Sediment&quot; Notice that the first column got assigned the generic name X1. This is because there is a missing column header in the bioenv.txt file. This first column corresponds to the sampling sites. Before we move on let’s give this column a more meaningful name: bioenv &lt;- bioenv %&gt;% rename(Site = X1) names(bioenv) [1] &quot;Site&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; [6] &quot;e&quot; &quot;Pollution&quot; &quot;Depth&quot; &quot;Temperature&quot; &quot;Sediment&quot; The columns labeled a to e contain the counts of the five species at each site, while the remaining columns give additional information about the physical properties of each sampling site. For our purposes today we’ll confine our attention to the abundance data. abundance &lt;- bioenv %&gt;% select(Site, a, b, c, d, e) head(abundance) # A tibble: 6 x 6 Site a b c d e &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 s1 0 2 9 14 2 2 s2 26 4 13 11 0 3 s3 0 10 9 8 0 4 s4 0 0 15 3 0 5 s5 13 5 3 10 7 6 s6 31 21 13 16 5 The data is currently in a “wide” format. For the purposes of plotting it will be more convenient to generate a “long” version of the data using functions from the tidyr library (see the Data Wrangling chapter). long.abundance &lt;- abundance %&gt;% tidyr::gather(Species, Count, -Site) head(long.abundance) # A tibble: 6 x 3 Site Species Count &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 s1 a 0 2 s2 a 26 3 s3 a 0 4 s4 a 0 5 s5 a 13 6 s6 a 31 ggplot(long.abundance, aes(x = Species, y = Count)) + geom_boxplot() + labs(x = &quot;Species&quot;, y = &quot;Count&quot;, title=&quot;Distribution of\\nSpecies Counts per Site&quot;) From the boxplot it looks like the counts for species `e’ are smaller on average, and less variable. The mean and variance functions confirm that. long.abundance %&gt;% group_by(Species) %&gt;% summarize(mean(Count), var(Count)) # A tibble: 5 x 3 Species `mean(Count)` `var(Count)` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 a 13.47 157.6 2 b 8.733 83.44 3 c 8.400 73.63 4 d 10.90 44.44 5 e 2.967 15.69 A correlation matrix suggests weak to moderate associations between the variables: abundance.only &lt;- abundance %&gt;% select(-Site) # drop the Site column cor(abundance.only) a b c d e a 1.0000000 0.67339954 -0.23992888 0.358192050 0.273522301 b 0.6733995 1.00000000 -0.08041947 0.501834036 0.036914702 c -0.2399289 -0.08041947 1.00000000 0.081504483 -0.343540453 d 0.3581921 0.50183404 0.08150448 1.000000000 -0.004048517 e 0.2735223 0.03691470 -0.34354045 -0.004048517 1.000000000 However a scatterplot matrix generated by the GGally::ggapirs() function suggests that many of the relationships have a strong non-linear element. ggpairs(abundance.only) 12.4.2 PCA of the Bioenv dataset Linearity is not a requirement for PCA, as it’s simply a rigid rotation of the original data. So we’ll continue with our analysis after taking a moment to read the help on the prcomp() function that is used to carry-out PCA in R. abundance.pca &lt;- prcomp(abundance.only, center=TRUE, retx=TRUE) # center=TRUE mean centers the data # retx=TRUE returns the PC scores # if you want to do PCA on the correlation matrix set scale.=TRUE # -- notice the period after scale! summary(abundance.pca) Importance of components: PC1 PC2 PC3 PC4 PC5 Standard deviation 14.8653 8.8149 6.2193 5.03477 3.48231 Proportion of Variance 0.5895 0.2073 0.1032 0.06763 0.03235 Cumulative Proportion 0.5895 0.7968 0.9000 0.96765 1.00000 We see that approximately 59% of the variance in the data is capture by the first PC, and approximately 90% by the first three PCs. Let’s compare the values return by PCA to what we would get if we carried out eigenanalysis of the covariance matrix that corresponds to our data. First the list object return by prcomp(): abundance.pca Standard deviations (1, .., p=5): [1] 14.865306 8.814912 6.219250 5.034774 3.482308 Rotation (n x k) = (5 x 5): PC1 PC2 PC3 PC4 PC5 a 0.81064462 0.07052882 -0.53108427 0.18442140 -0.14771336 b 0.51264394 -0.27799671 0.47711910 -0.63418946 0.17342177 c -0.16235135 -0.88665551 -0.40897655 -0.01149647 0.14173943 d 0.22207108 -0.31665237 0.56250980 0.72941223 -0.04422938 e 0.06616623 0.17696554 -0.08141111 0.17781482 0.96231977 And now the corresponding values returned by eigenanaysis of the covariance matrix generated from the abundance data: eig.abundance &lt;- eigen(cov(abundance.only)) eig.abundance$vectors # compare to rotation matrix of PCA [,1] [,2] [,3] [,4] [,5] [1,] 0.81064462 -0.07052882 0.53108427 0.18442140 -0.14771336 [2,] 0.51264394 0.27799671 -0.47711910 -0.63418946 0.17342177 [3,] -0.16235135 0.88665551 0.40897655 -0.01149647 0.14173943 [4,] 0.22207108 0.31665237 -0.56250980 0.72941223 -0.04422938 [5,] 0.06616623 -0.17696554 0.08141111 0.17781482 0.96231977 sqrt(eig.abundance$values) # compare to sdev of PCA [1] 14.865306 8.814912 6.219250 5.034774 3.482308 Notice that the rotation object returned by the prcomp() represents the scaled eigenvectors (scaled to have length 1). The standard deviations of the PCA are the square roots of the eigenvalues of the covariance matrix. 12.4.3 Calculating Factor Loadings Let’s calculate the “factor loadings” associated with the PCs: V &lt;- abundance.pca$rotation # eigenvectors L &lt;- diag(abundance.pca$sdev) # diag mtx w/sqrts of eigenvalues on diag. abundance.loadings &lt;- V %*% L abundance.loadings [,1] [,2] [,3] [,4] [,5] a 12.0504801 0.6217053 -3.3029460 0.92852016 -0.5143835 b 7.6206090 -2.4505164 2.9673232 -3.19300085 0.6039081 c -2.4134024 -7.8157898 -2.5435276 -0.05788214 0.4935804 d 3.3011545 -2.7912626 3.4983893 3.67242602 -0.1540203 e 0.9835813 1.5599356 -0.5063161 0.89525751 3.3510942 The magnitude of the factor loadings is what you want to focus on. For example, species a and b contribute most to the first PC, while species c has the largest influence on PC2. You can think of the factor loadings, as defined above, as the components (i.e lengths of the projected vectors) of the original variables with respect to the PC basis vectors. Since vector length is proportional to the standard deviation of the variables they represent, you can think of the loadings as giving the standard deviation of the original variables with respect the PC axes. This implies that the loadings squared sum to the total variance in the original data, as illustrated below. sum(abundance.loadings**2) [1] 374.8345 abundance.only %&gt;% purrr::map_dbl(var) %&gt;% sum [1] 374.8345 12.5 Drawing Figures to Represent PCA 12.5.1 PC Score Plots The simplest PCA figure is to depict the PC scores, i.e. the projection of the observations into the space defined by the PC axes. Let’s make a figure with three subplots, depicting PC1 vs PC2, PC1 vs PC3, and PC2 vs. PC3. pca.scores.df &lt;- as.data.frame(abundance.pca$x) coord.system &lt;- coord_fixed(ratio=1, xlim=c(-30,30),ylim=c(-30,30)) pc1v2 &lt;- pca.scores.df %&gt;% ggplot(aes(x = PC1, y= PC2)) + geom_point() + coord.system pc1v3 &lt;- pca.scores.df %&gt;% ggplot(aes(x = PC1, y= PC3)) + geom_point() + coord.system pc2v3 &lt;- pca.scores.df %&gt;% ggplot(aes(x = PC2, y= PC3)) + geom_point() + coord.system cowplot::plot_grid(pc1v2, pc1v3, pc2v3, align=&quot;hv&quot;) When plotting PC scores, it is very important to keep the aspect ratio with a value of 1, so that the distance between points in the plot is an accurate representation of the distance in the PC space. Note too that I used the xlim and ylim arguments to keep the axis limits the same in all plots; comparable scaling of axes is important when comparing plots. Also note the use of the align=&quot;hv&quot; argument to plot_grid() to keep my plots the same size when combining them into a single figure. 12.5.2 Simultaneous Depiction of Observations and Variables in the PC Space Let’s return to our simple PC score plot. As we discussed above, the loadings are components of the original variables in the space of the PCs. This implies we can depict those loadings in the same PC basis that we use to depict the scores. First let’s create a data frame with the loadings from the first two PCs as well as a column representation the variable names: loadings.1and2 &lt;- data.frame(abundance.loadings[,1:2]) %&gt;% rename(PC1.loading = X1, PC2.loading = X2) %&gt;% mutate(variable = row.names(abundance.loadings)) loadings.1and2 PC1.loading PC2.loading variable 1 12.0504801 0.6217053 a 2 7.6206090 -2.4505164 b 3 -2.4134024 -7.8157898 c 4 3.3011545 -2.7912626 d 5 0.9835813 1.5599356 e With this data frame in hand, we can now draw a set of vector to represent the original variables projected into the subspace defined by PCs 1 and 2: pc1v2.biplot &lt;- pc1v2 + geom_segment(data=loadings.1and2, aes(x = 0, y = 0, xend = PC1.loading, yend = PC2.loading), color=&#39;red&#39;, arrow = arrow(angle=15, length=unit(0.1,&quot;inches&quot;))) + geom_text(data=loadings.1and2, aes(x = PC1.loading, y = PC2.loading, label=variable), color=&#39;red&#39;, nudge_x = 1, nudge_y = 1) pc1v2.biplot The figure above is called a “biplot”“, as it simultaneously depicts both the observations and variables in the same space. From this biplot we can immediately see that variable a is highly correlated with PC1, but only weakly associated with PC2. Conversely, variable c is strongly correlated with PC2 but only weakly so with PC1. We can also approximate the correlations among the variables themselves – for example b and d are fairly strongly correlated, but weakly correlated with c. Keep in mind however that with respect to the relationships among the variables, this visualization is a 2D projection of a 5D space so the geometry is approximate. The biplot is a generally useful tool for multivariate analyses and there are a number of different ways to define biplots. We’ll study biplots more formally in a few weeks after we’ve covered singular value decomposition. "],
["simulating-sampling-distributions.html", "Chapter 13 Simulating Sampling Distributions 13.1 Libraries 13.2 Data set: Simulated male heights 13.3 Random sampling from the simulated population 13.4 Simulated sampling distribution of the mean 13.5 Standard Error of the Mean 13.6 Sampling Distribution of the Standard Deviation 13.7 What happens to the sampling distribution of the mean and standard deviation when our sample size is small? 13.8 The t-distribution is the appropriate distribution for describing the sampling distribution of the mean when \\(n\\) is small", " Chapter 13 Simulating Sampling Distributions Usually when we collect biological data, it’s because we’re trying to learn about some underlying “population” of interest. Population here could refer to an actual population (e.g. all males over 20 in the United States; brushtail possums in the state of Victoria, Australia), an abstract population (e.g. corn plants grown from Monsanto “round up ready” seed; yeast cells with genotypes identical to the reference strain S288c), outcomes of a stochastic process we can observe and measure (e.g. meiotic recombination in flies; hadrons detected at the LHC during a particle collision experiment), etc. It is often impractical or impossible to measure all objects/individuals in a population of interest, so we take a sample from the population and make measurements on the variables of interest in that sample. We do so with the hope that the various statistics we calculate on the variables of interest in that sample will be useful estimates of those same statistics in the underlying population. However, we must always keep in mind that the statistics we calculate from our sample will almost never exactly match those of the underlying population. That is when we collect a sample, and measure a statistic (e.g. mean) on variable X in the sample, there is a degree of uncertainty about how well our estimate matches the true value of that statistic for X in the underlying population. Statistical inference is about quantifying the uncertainty associated with statistics and using that information to test hypotheses and evaluate models. Today we’re going to review a fundamental concept in statistical inference, the notion of a sampling distribution for a statistic of interest. A sampling distribution is the probability distribution of a given statistic for samples of a given size. Traditionally sampling distributions were derived analytically. In this class session we’ll see how to approximate sampling distributions for any a statistic using computer simulation. 13.1 Libraries library(tidyverse) library(magrittr) library(stringr) 13.2 Data set: Simulated male heights To illustrate the concept of sampling distributions, we’ll use a simulated data set to represent the underlying population we’re trying to estimate statistics for. This will allow us to compare the various statistics we calculate and their sampling distributions to their “true” values. Let’s simulate a population consisting of 25,000 individuals with a single trait of interest – height (measured in centimeters). We will simulate this data set based on information about the distribution of the heights of adult males in the US estimated in a study carried out from 2011-2014 by the US Department of Health and Human Services1. 13.2.1 Seeding the pseudo-random number generator When carrying out simulations, we employ random number generators (e.g. to choose random samples). Most computers can not generate true random numbers – instead they use algorithms that approximate the generation of random numbers (pseudo-random number generators). One important difference between a true random number generator and a pseudo-random number generator is that we can regenerate a series of pseudo-random numbers if we know the “seed” value that initialized the algorithm. We can specifically set this seed value, so that we can guarantee that two different people evaluating this notebook get the same results, even though we’re using (pseudo)random numbers in our simulation. # make our simulation repeatable by seeding RNG set.seed(20180321) 13.2.2 Generating the simulated population Having seeded our RNG, we then generate our simulated population using the rnorm() function. rnorm() draws random values from a normal distribution with the given parameters of mean and standard deviation. We didn’t have to use a normal distribution for our simulations, but many biological variables are approximately normally distributed, so this is a common assumption. # male mean height and sd in centimeters from USDHHS report mean.ht &lt;- 175.7 sd.ht &lt;- 15.19 height.data &lt;- data_frame(height = rnorm(25000, mean = mean.ht, sd = sd.ht)) 13.2.3 Properties of the simulated population Let’s take a moment to visualize the distribution of heights in our population of interest: ggplot(height.data, aes(x = height)) + geom_histogram(aes(y = ..density..), bins=50, alpha=0.5) + geom_density() + labs(x = &quot;Height (cm)&quot;, title = &quot;Distribution of Heights in the Population of Interest&quot;) When we generated this simulated population, I specified that the heights should be drawn from a normal distribution. As one would expect, the histogram and density plot look fairly close to the classic bell-shaped curve of a normal distribution. Let’s turn some key summary statistics of the population. The values of these summary statistics are those we’re trying to estimate from our sample(s). We’ll refer to these as the “true” values. true.values &lt;- height.data %&gt;% summarize(mean.height = mean(height), sd.height = sd(height)) true.values # A tibble: 1 x 2 mean.height sd.height &lt;dbl&gt; &lt;dbl&gt; 1 175.7 15.20 In mathematical notation our population is normally distributed with a mean height of 175.7201895cm and standard deviation of 15.1963663cm. Note that these values are close to, but not exactly the parametes we passed to rnorm(). 13.3 Random sampling from the simulated population Let’s simulate the process of taking a single sample of 30 individuals from our population, using the dplyr::sample_n() function: sample.a &lt;- height.data %&gt;% sample_n(30) Now we’ll create a histogram of the height variable in our sample. For reference we’ll also plot the histogram for the true population in the background (but remember, in the typical case you don’t know what the true population looks like) sample.a %&gt;% ggplot(aes(x = height)) + geom_histogram(data=height.data, aes(x = height, y = ..density..), alpha=0.25, bins=50) + geom_histogram(aes(y = ..density..), fill = &#39;steelblue&#39;, alpha=0.75, bins=9) + geom_vline(xintercept = true.values$mean.height, linetype = &quot;solid&quot;) + geom_vline(xintercept = mean(sample.a$height), linetype = &quot;dashed&quot;) + labs(x = &quot;Height (cm)&quot;, y = &quot;Density&quot;, title = &quot;Distribution of heights in the underlying population (grey)\\nand a single sample of size 30 (blue)&quot;) The solid vertical line represent the true mean of the population, the dashed line represents the sample mean. Comparing the two distributions we see that while our sample of 30 observations is relatively small,its location (center) and spread that are roughly similar to those of the underlying population. Let’s create a table giving the estimates of the mean and standard deviation in our sample: sample.a %&gt;% summarize(sample.mean = mean(height), sample.sd = sd(height)) # A tibble: 1 x 2 sample.mean sample.sd &lt;dbl&gt; &lt;dbl&gt; 1 173.5 13.18 Based on our sample, we estimate that the mean height of males in our population of interest is 173.4747577cm with a standard deviation of 13.1761523cm. 13.3.1 Another random sample Let’s step back and think about our experiment. We took a random sample of 30 indiviuals from the population. The very nature of a “random sample” means we could just as well have gotten a different collection of individuals in our sample. Let’s take a second random sample of 25 individuals and see what the data looks like this time: sample.b &lt;- height.data %&gt;% sample_n(30) ggplot(sample.b, aes(x = height)) + geom_histogram(data=height.data, aes(x = height, y = ..density..), alpha=0.25, bins=50) + geom_histogram(aes(y = ..density..), fill = &#39;steelblue&#39;, alpha=0.75, bins=9) + geom_vline(xintercept = true.values$mean.height, linetype = &quot;solid&quot;) + geom_vline(xintercept = mean(sample.b$height), linetype = &quot;dashed&quot;) + labs(x = &quot;Height (cm)&quot;, y = &quot;Density&quot;, title = &quot;Distribution of heights in the underlying population (grey)\\nand a single sample of size 30 (blue)&quot;) sample.b %&gt;% summarize(sample.mean = mean(height), sample.sd = sd(height)) # A tibble: 1 x 2 sample.mean sample.sd &lt;dbl&gt; &lt;dbl&gt; 1 175.2 14.80 This time we estimated the mean height to be 175.1897271 cm and the standard deviation to be 14.8025475 cm. 13.3.2 Simulating the generation of many random samples When we estimate population parameters, like the mean and standard deviation, based on a sample, our estimates will differ from the true population values by some amount. Any given random sample might provide better or worse estimates than another sample. We can’t know how good our estimates of statistics like the mean and standard deviation are from any specific sample, but we we can study the behavior of such estimates across many simulated samples and learn something about how well our estimates do on average, as well the spread of these estimates. 13.3.3 A function to estimate statistics of interest in a random sample First we’re going to write a function called rsample.stats that to carries out the following steps: Given a data frame x Take a random sample of size n For the variable specified by the character string, var.name, calculate the mean and standard deviation of that variable in the random sample Return a table giving the sample size, sample mean, and sample standard deviation, represented as a data frame rsample.stats &lt;- function(x, n, var.name) { sample_x &lt;- sample_n(x, n) data_frame(sample.size = n, sample.mean = mean(sample_x[[var.name]]), sample.sd = sd(sample_x[[var.name]])) } Let’s test rsample.stats: rsample.stats(height.data, 30, &quot;height&quot;) # A tibble: 1 x 3 sample.size sample.mean sample.sd &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 30. 174.4 16.74 13.3.4 Generating statistics for many random samples Now we’ll see how to combine rsample.stats with two additional functions to repeatedly run the rsample.stats function: df.samples.of.30 &lt;- rerun(1000, rsample.stats(height.data, 30, &quot;height&quot;)) %&gt;% bind_rows() The function rerun is defined in the purrr library (automatically loaded with tidyverse). purrr:rerun() re-runs an expression(s) multiple times. The first argument to rerun() is the number of times you want to re-run, and the following arguments are the expressions to be re-run. Thus the second line of the code block above re-runs the rsample.stats function 1000 times using height.data as the input, generating sample statistics for samples of size 30 each time it’s run. rerun returns a list whose length is the specified number of runs. The third line includes a call the dplyr::bind_rows(). This simply takes the list that rerun returns and collapses the list into a single data frame. df.samples.of.30 is thus a data frame in which each row gives the sample size, sample mean, and sample standard deviation for a random sample of 30 individuals drawn from our underlying population (height.data). df.samples.of.30 # A tibble: 1,000 x 3 sample.size sample.mean sample.sd &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 30. 176.7 17.98 2 30. 172.8 14.62 3 30. 178.4 15.13 4 30. 171.2 16.92 5 30. 176.4 12.72 6 30. 176.0 13.53 7 30. 177.2 15.20 8 30. 178.2 19.28 9 30. 170.5 15.48 10 30. 174.5 13.04 # ... with 990 more rows 13.4 Simulated sampling distribution of the mean Let’s review what we just did: We generated 1000 samples of size 30 For each of the samples we calculated the mean and standard deviation of the height variable in that sample We combined each of those estimates of the mean and standard deviation into a data frame The 1000 estimates of the mean we generated represents a new distribution – what we will call a sampling distribution of the mean for samples of size 30. Let’s plot this sampling distribution: ggplot(df.samples.of.30, aes(x = sample.mean, y = ..density..)) + geom_histogram(bins=25, fill = &#39;firebrick&#39;, alpha=0.5) + geom_vline(xintercept = true.values$mean.height, linetype = &quot;dashed&quot;) + labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, title = &quot;Distribution of mean heights for 1000 samples of size 30&quot;) This particular sampling distribution of the mean is a probability distribution that we can use to estimate the probability that a sample mean falls within a given interval, assuming our sample is a random sample of size 30 drawn from our underlying population. From our visualization, we see that the distribution of sample mean heights is approximately centered around the true mean height. Most of the sample estiamtes of the mean height are within 5 cm of the true population mean height (175.6cm), but a small number of estimates of the sample mean as off by nearly 10cm. Let’s make this more precise by calculating the mean and standard deviation of the sampling distribution of means (I included the min and max as well). df.samples.of.30 %&gt;% summarize(mean.of.means = mean(sample.mean), sd.of.means = sd(sample.mean), min.of.means = min(sample.mean), max.of.means = max(sample.mean)) # A tibble: 1 x 4 mean.of.means sd.of.means min.of.means max.of.means &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 175.9 2.851 167.4 184.9 13.4.1 Sampling distributions for different sample sizes In the example above we simulated the sampling distribution of the mean for samples of size 30. How would the sampling distribution change if we increased the sample size? In the next code block we generate sampling distributions of the mean (and standard deviation) for samples of size 50, 100, 250, and 500. df.samples.of.50 &lt;- rerun(1000, height.data %&gt;% rsample.stats(50, &quot;height&quot;)) %&gt;% bind_rows() df.samples.of.100 &lt;- rerun(1000, height.data %&gt;% rsample.stats(100, &quot;height&quot;)) %&gt;% bind_rows() df.samples.of.250 &lt;- rerun(1000, height.data %&gt;% rsample.stats(250, &quot;height&quot;)) %&gt;% bind_rows() df.samples.of.500 &lt;- rerun(1000, height.data %&gt;% rsample.stats(500, &quot;height&quot;)) %&gt;% bind_rows() To make plotting and comparison easier we will combine each of the individual data frames, representing the different sampling distributions for samples of a given size, into a single data frame. df.combined &lt;- bind_rows(df.samples.of.30, df.samples.of.50, df.samples.of.100, df.samples.of.250, df.samples.of.500) %&gt;% # create a factor version of sample size to facilitate plotting mutate(sample.sz = as.factor(sample.size)) We then plot each of the individual sampling distributions, faceting on sample size. ggplot(df.combined, aes(x = sample.mean, y = ..density.., fill = sample.sz)) + geom_histogram(bins=25, alpha=0.5) + geom_vline(xintercept = true.values$mean.height, linetype = &quot;dashed&quot;) + facet_wrap(~ sample.sz, nrow = 1) + scale_fill_brewer(palette=&quot;Set1&quot;) + # change color palette labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, title = &quot;Distribution of mean heights for samples of varying size&quot;) 13.4.2 Discussion of trends for sampling distributions of different sample sizes The key trend we see when comparing the sampling distributions of the mean for samples of different size is that as the sample size gets larger, the spread of the sampling distribution of the mean becomes narrower around the true mean. This means that as sample size increases, the uncertainty associated with our estimates of the mean decreases. Let’s create a table, grouped by sample size, to help quantify this pattern: sampling.distn.mean.table &lt;- df.combined %&gt;% group_by(sample.size) %&gt;% summarize(mean.of.means = mean(sample.mean), sd.of.means = sd(sample.mean), min.of.means = min(sample.mean), max.of.means = max(sample.mean)) sampling.distn.mean.table # A tibble: 5 x 5 sample.size mean.of.means sd.of.means min.of.means max.of.means &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 30. 175.9 2.851 167.4 184.9 2 50. 175.6 2.158 168.6 182.0 3 100. 175.7 1.469 170.8 180.0 4 250. 175.7 0.9675 173.0 178.7 5 500. 175.7 0.6653 173.7 177.6 13.5 Standard Error of the Mean We see from the graph and table above that our estimates of the mean cluster more tightly about the true mean as our sample size increases. This is obvious when we compare the standard deviation of our mean estimates as a function of sample size. The standard deviation of the sampling distribution of a statistic of interest is called the Standard Error of that statistic. Here, through simulation, we are approximating the Standard Error of the Mean. When sample sizes are large (&gt;30 observations), one can show mathematically that for normally distributed data the expected Standard Error of the Mean as a function of sample size is approximately: \\[ \\mbox{Standard Error of Mean} \\approx \\frac{\\sigma}{\\sqrt{n}} \\] where \\(\\sigma\\) is the population standard deviation (i.e. the “true” standard deviation), and \\(n\\) is the sample size. Let’s compare that theoretical expectation to our simulated results: se.mean.theory &lt;- sapply(seq(10,500,10), function(n){ true.values$sd.height/sqrt(n) }) df.se.mean.theory &lt;- data_frame(sample.size = seq(10,500,10), std.error = se.mean.theory) ggplot(sampling.distn.mean.table, aes(x = sample.size, y = sd.of.means)) + # plot standard errors of mean based on our simulations geom_point() + # plot standard errors of the mean based on theory geom_line(aes(x = sample.size, y = std.error), data = df.se.mean.theory, color=&quot;red&quot;) + labs(x = &quot;Sample size&quot;, y = &quot;Std Error of Mean&quot;, title = &quot;A comparison of theoretical (red line) and simulated (points) estimates of\\nthe standard error of the mean for samples of different size&quot;) We see that as sample sizes increase, the standard error of the mean decreases. This means that as our samples get larger, our uncertainty in our sample estimate of the mean (our best guess for the population mean) gets smaller. 13.6 Sampling Distribution of the Standard Deviation Above we explored how the sampling distribution of the mean changes with sample size. We can similarly explore the sampling distribution of any other statistic, such as the standard deviation, or the median, or the the range, etc. Recall that when we drew random samples we calculated the standard deviation of each of those samples in addition to the mean. This means we can immediately visualize the sampling distribution of the standard deviation as shown below: ggplot(df.combined, aes(x = sample.sd, y = ..density.., fill = sample.sz)) + geom_histogram(bins=25, alpha=0.5) + geom_vline(xintercept = true.values$sd.height, linetype = &quot;dashed&quot;) + facet_wrap(~ sample.sz, nrow = 1) + scale_fill_brewer(palette=&quot;Set1&quot;) + labs(x = &quot;Sample standard deviations&quot;, y = &quot;Density&quot;, title = &quot;Sampling distribution of standard deviation of height for samples of varying size&quot;) The key trend we saw when examining the sampling distribution of the mean is also apparent for standard deviation – bigger samples lead to tighter sampling distributions and hence less uncertainty in the sample estimates of the standard deviation. As before, we summarize key statistics of the sampling distribution in a table: sampling.distn.sd.table &lt;- df.combined %&gt;% group_by(sample.size) %&gt;% summarize(mean.of.sds = mean(sample.sd), sd.of.sds = sd(sample.sd), min.of.sds = min(sample.sd), max.of.sds = max(sample.sd)) sampling.distn.sd.table # A tibble: 5 x 5 sample.size mean.of.sds sd.of.sds min.of.sds max.of.sds &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 30. 15.16 2.022 9.578 21.27 2 50. 15.17 1.561 10.12 20.87 3 100. 15.09 1.106 11.90 18.65 4 250. 15.18 0.6905 13.09 17.79 5 500. 15.19 0.4654 13.58 17.01 For normally distributed data the expected Standard Error of the Standard Deviation (i.e. the standard deviation of standard deviations!) is approximately: \\[ \\mbox{Standard Error of Standard Deviation} \\approx \\frac{\\sigma}{\\sqrt{2(n-1)}} \\] where \\(\\sigma\\) is the population standard deviation, and \\(n\\) is the sample size. As before, let’s visually compare the theoretical expectation to our simulated estimates. se.sd.theory &lt;- sapply(seq(10, 500, 10), function(n){ true.values$sd.height/sqrt(2*(n-1))}) df.se.sd.theory &lt;- data_frame(sample.size = seq(10,500,10), std.error = se.sd.theory) ggplot(sampling.distn.sd.table, aes(x = sample.size, y = sd.of.sds)) + # plot standard errors of mean based on our simulations geom_point() + # plot standard errors of the mean based on theory geom_line(aes(x = sample.size, y = std.error), data = df.se.sd.theory, color=&quot;red&quot;) + labs(x = &quot;Sample size&quot;, y = &quot;Std Error of Standard Deviation&quot;, title = &quot;A comparison of theoretical (red line) and simulated (points) estimates of\\nthe standard error of the standard deviation for samples of different size&quot;) 13.7 What happens to the sampling distribution of the mean and standard deviation when our sample size is small? We would hope that, regardless of sample size, the sampling distributions of both the mean and standard deviation should be centered around the true population value, \\(\\mu\\) and \\(\\sigma\\) respectively. That seemed to be the case for the modest to large sample sizes we’ve looked at so far (30 to 500 observations). Does this also hold for small samples? Let’s use simulation to explore how well this is expectation is met for small samples. As we’ve done before, we simulate the sampling distribution of the mean and standard deviation for samples of varying size (we also calculate some other values which will be useful for our exposition below). # we&#39;ll use the same mean and sd we&#39;ve been using previously mu = mean.ht sigma = sd.ht # list of sample sizes we&#39;ll generate ssizes &lt;- c(2, 3, 4, 5, 7, 10, 20, 30) # number of simulations to carry out *for each sample size* nsims &lt;- 2500 df.combined.small &lt;- data_frame(sample.size = double(), sample.mean = double(), sample.sd = double(), estimated.SE = double(), sample.zscore = double()) for (i in ssizes) { # create empty vectors to hold simulation stats (for efficiency) s.means &lt;- rep(NA, nsims) s.sds &lt;- rep(NA, nsims) s.SEs &lt;- rep(NA, nsims) s.zscores &lt;- rep(NA, nsims) for (j in 1:nsims) { s &lt;- rnorm(i, mean = mu, sd = sigma) # draw random sample s.means[j] &lt;- mean(s) # calculate mean of that sample s.sds[j] &lt;- sd(s) # calculate sd of that sample SE &lt;- sd(s)/sqrt(i) # cacluate estimated SE of that sample s.SEs[j] &lt;- SE s.zscores[j] &lt;- (mean(s) - mu)/SE } df &lt;- data.frame(sample.size = i, sample.mean = s.means, sample.sd = s.sds, estimated.SE = s.SEs, sample.zscore = s.zscores) df.combined.small &lt;- bind_rows(df.combined.small, df) } df.combined.small %&lt;&gt;% mutate(sample.sz = as.factor(sample.size)) 13.7.1 For small samples, sample standard deviations systematically underestimate the population standard deviation Let’s examine how the well centered the sampling distributions of the mean and standard deviation are around their true values, as a function of sample size. First a table summarizing this information: by.sample.size &lt;- df.combined.small %&gt;% group_by(sample.size) %&gt;% summarize(mean.of.means = mean(sample.mean), mean.of.sds = mean(sample.sd)) by.sample.size # A tibble: 8 x 3 sample.size mean.of.means mean.of.sds &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2. 175.3 12.18 2 3. 175.5 13.50 3 4. 175.7 13.77 4 5. 175.7 14.17 5 7. 175.8 14.74 6 10. 175.7 14.82 7 20. 175.7 14.95 8 30. 175.7 15.06 We see that the sampling distributions of means are well centered around the true mean, and there is no systematic bias one way or the other. By contrast the sampling distribution of standard deviations tends to underestimate the true standard deviation when the samples are small (less than 30 observations). We can visualize this bias as shown here: ggplot(by.sample.size, aes(x = sample.size, y = mean.of.sds)) + geom_point(color = &#39;red&#39;) + geom_line(color = &#39;red&#39;) + geom_hline(yintercept = sigma, color = &#39;black&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;Sample Size&quot;, y = &quot;Mean of Sampling Distn of Std Dev&quot;) The source of this bias is clear if we look at the sampling distribution of the standard deviation for samples of size 3, 5, and 30. filtered.df &lt;- df.combined.small %&gt;% filter(sample.size %in% c(3, 5, 30)) ggplot(filtered.df, aes(x = sample.sd, y = ..density.., fill = sample.sz)) + geom_histogram(bins=50, alpha=0.65) + facet_wrap(~sample.size, nrow = 1) + geom_vline(xintercept = sigma, linetype = &#39;dashed&#39;) + labs(x = &quot;Std Deviations&quot;, y = &quot;Density&quot;, title = &quot;Sampling distributions of the standard deviation\\nAs a function of sample size&quot;) There’s very clear indication that the the sampling distribution of standard deviations is not centered around the true value for \\(n=3\\) and for \\(n=5\\), however with samples of size 30 the sampling distribution of the standard deviation appears fairly well centered around the true value of the underlying population. 13.7.2 Underestimates of the standard deviation given small \\(n\\) lead to understimates of the SE of the mean When sample sizes are small, sample estimates of the standard deviation, \\(s_x\\), tend to underestimate the true standard deviation, \\(\\sigma\\), then it follows that sample estimates of the standard error of the mean, \\(SE_\\overline{x} = \\frac{s_x}{\\sqrt{n}}\\), must tend to understimate the true standard error of the mean, \\(SE_\\mu = \\frac{\\sigma}{\\sqrt{n}}\\). 13.8 The t-distribution is the appropriate distribution for describing the sampling distribution of the mean when \\(n\\) is small The problem associated with estimating the standard error of the mean for small sample sizes was recognized in the early 20th century by William Gosset, an employee at the Guinness Brewing Company. He published a paper, under the pseudonym “Student”, giving the appropriate distribution for describing the standard error of the mean as a function of the sample size \\(n\\). Gosset’s distribution is known as the “t-distribution” or “Student’s t-distribution”. The t-distribution is specified by a single parameter, called degrees of freedom (\\(df\\)) where \\({df} = n - 1\\). As \\(df\\) increases, the t-distribution becomes more and more like the normal such that when \\(n \\geq 30\\) it’s nearly indistinguishable from the standard normal distribution. In the figures below we compare the t-distribution and the standard normal distribution for sample sizes \\(n = {4, 8, 32}\\). x &lt;- seq(-6, 6, length.out = 200) n &lt;- c(5, 10, 30) # sample sizes distns.df &lt;- data_frame(sample.size = double(), z.or.t = double(), norm.density = double(), t.density = double()) for (i in n) { norm.density &lt;- dnorm(x, mean = 0, sd = 1) t.density &lt;- dt(x, df = i - 1) df.temp &lt;- data_frame(sample.size = i, z.or.t = x, norm.density = norm.density, t.density = t.density) distns.df &lt;- bind_rows(distns.df, df.temp) } distns.df %&lt;&gt;% mutate(df = as.factor(sample.size - 1)) ggplot(distns.df, aes(x = z.or.t, y = t.density, color = df)) + geom_line() + geom_line(aes(y = norm.density), color=&#39;black&#39;, linetype=&quot;dotted&quot;) + labs(x = &quot;z or t value&quot;, y = &quot;Probablity density&quot;, title = &quot;Standard normal distribution (black dotted line)\\nversus t-distributions for different degrees of freedom&quot;) US Dept. of Health and Human Services; et al. (August 2016). “Anthropometric Reference Data for Children and Adults: United States, 2011–2014” (PDF). National Health Statistics Reports. 11. https://www.cdc.gov/nchs/data/series/sr_03/sr03_039.pdf↩ "],
["simulating-confidence-intervals.html", "Chapter 14 Simulating confidence intervals 14.1 Libraries 14.2 Confidence Intervals 14.3 Calibrating confidence intervals 14.4 Standard formulation for confidence intervals 14.5 Simulating confidence intervals for the mean 14.6 Sample estimate of the standard error of the mean 14.7 CI of mean simulation 14.8 Generating a table of CIs and corresponding margins of error 14.9 Interpreting our simulation", " Chapter 14 Simulating confidence intervals Recall the concept of the sampling distribution of a statistic – this is simply the probability distribution of the statistic of interest you would observe if you took a large number of random samples of a given size from a population of interest and calculated that statistic for each of the samples. In the previous lecture you used simulation to approximate the sampling distribution a number of different statistics. To use simulation to approximate a sampling distribution we carried out the following steps: Made some assumptions about the distributional properties of the underlying population Simulate drawing random samples of a given size from that population For each simulated random sample, calculate the statistic(s) of interest Treat the simulated distribution of the statistics of interest as the sampling distribution You learned that the standard deviation of the sampling distribution of a statistic has a special name – the standard error of that statistic. The standard error of a statistic provides a way to quantify the uncertainty of a statistic across random samples. Here we show how to use information about the standard error of a statistic to calculate confidence intervals for a statistic based on a set of observed data. 14.1 Libraries library(tidyverse) library(magrittr) 14.2 Confidence Intervals We know that given a random sample from a population of interest, the value of a statistic of interest is unlikely to be exactly equally to the true population value of that statistics. However, our simulations have taught us a number of things: As sample size increases, the sample estimate of the given statistic is more likely to be close to the true value of that statistic As sample size increases, the standard error of the statistic decreases We can use this knowledge to calculate a plausible ranges of values for the statistic of interest. We call such ranges confidence intervals for the statistic of interest. 14.3 Calibrating confidence intervals How are we to calibrate “plausible ranges”? We will define an “X% percent confidence interval for a statistic of interest”, as an interval that when calculated from a random sample, would include the true population value of the statistic X% of the time. This quote from the NIST page on confidence intervals helps to make this concrete regarding confidence intervals for he mean: As a technical note, a 95 % confidence interval does not mean that there is a 95 % probability that the interval contains the true mean. The interval computed from a given sample either contains the true mean or it does not. Instead, the level of confidence is associated with the method of calculating the interval … That is, for a 95% confidence interval, if many samples are collected and the confidence interval computed, in the long run about 95% of these intervals would contain the true mean. 14.4 Standard formulation for confidence intervals We define the \\((100\\times\\beta)\\)% confidence interval for the statistic \\(\\phi\\) as the interval: \\[ CI_\\beta = \\phi_{n} \\pm (z \\times {SE}_{\\phi,n}) \\] Where: \\(\\phi_{n}\\) is the statistic of interest in a random sample of size \\(n\\) \\({SE}_{\\phi,n}\\) is the standard error of the statistic \\(\\phi\\) (via simulation or analytical solution) And the value of \\(z\\) is chosen so that: across many different random samples of size \\(n\\), the true value of the \\(\\phi\\) in the population of interest would fall within the interval approximately \\(CI_\\beta\\) \\((100\\times\\beta)\\)% of the time So rather than estimating a single value of \\(\\phi\\) from our data, we will use our observed data plus knowledge about the sampling distribution of \\(\\phi\\) to estimate a range of plausible values for \\(\\phi\\). The size of this interval will be chosen so that if we considered many possible random samples, the true population value of \\(\\phi\\) would be bracketed by the interval in \\((100\\times\\beta)\\)% of the samples. 14.5 Simulating confidence intervals for the mean To make the idea of a confidence interval more concrete, let’s carry out a simulation based on confidence intervals for the mean. In our simulation we will explore how varying the value of \\(z\\) changes the percentage of times that the confidence interval brackets the true population mean. 14.6 Sample estimate of the standard error of the mean Recall that if a variable \\(X\\)is normally distributed in a population of interest, \\(X \\sim N(\\mu, \\sigma)\\), then the sampling distribution of the mean is also normally distributed with mean \\(\\sim μ\\), and standard error \\({SE}_\\overline{X} = \\frac{\\sigma}{\\sqrt{n}}\\): \\[ \\overline{X} \\sim N \\left( \\mu, \\frac{\\sigma}{\\sqrt{n}}\\ \\right) \\] The above formula requires us to know the true population standard deviation, \\(\\sigma\\) in order to calculate \\({SE}_\\overline{X}\\). In the absence of this information, the best we can do is use our “best guess” for \\(\\sigma\\) – the sample standard deviation \\(s_X\\) that we calculate from the sample in hand. That is, we estimate the standard error of the mean as: \\[ {SE}_{\\overline{X}} = \\frac{s_X}{\\sqrt{n}} \\] We learned last time that when we estimate the standard error of the mean from the standard deviation of sample, these estimated standard error are better described by the \\(t\\)-distribution when samples are small. As sample sizes increase, the standard normal distribution and the \\(t\\)-distribution converge. 14.7 CI of mean simulation In our simulation we’re going to generate a large number of samples, and for each sample we will calculate the sample estimate of the standard error of the mean, and the CI of the mean for a range of multiples of the estimated standard error mean. We will then ask, “for what fraction of the samples did our CI overlap the true population mean”? This will give us a sense of how well different confidence intervals, expressed in terms of multiples \\(z\\) of the sample standard errors, do in terms providing a plausible range for the mean. For this example we’re going to reuse our simulated height data from last class: set.seed(20180328) # male mean height and sd in centimeters from USDHHS report mean.ht &lt;- 175.7 sd.ht &lt;- 15.19 height.data &lt;- data_frame(height = rnorm(25000, mean = mean.ht, sd = sd.ht)) true.values &lt;- height.data %&gt;% summarize(mean = mean(height), sd = sd(height)) Let’s also setup a utitity function to generate random samples and calculate stats of interest: rsample.stats &lt;- function(x, n, var.name) { sample_x &lt;- sample_n(x, n) data_frame(sample.size = n, sample.mean = mean(sample_x[[var.name]]), sample.sd = sd(sample_x[[var.name]]), sample.se = sample.sd/sqrt(n)) } Now we’ll generate 1000 random samples of size 50. samples.of.50 &lt;- rerun(1000, rsample.stats(height.data, 50, &quot;height&quot;)) %&gt;% bind_rows() Using those 1000 random sample, we consider different values of \\(z\\) to calculate confidence intervals. For each value of \\(z\\) we determine what percent of the confidence intervals included the true mean. z.values &lt;- seq(1, 3, 0.05) perc.in.CI &lt;- c() for (z in z.values) { CI.left &lt;- samples.of.50$sample.mean - z * samples.of.50$sample.se CI.right &lt;- samples.of.50$sample.mean + z * samples.of.50$sample.se mean.in.CI &lt;- (CI.left &lt;= true.values$mean) &amp; (CI.right &gt;= true.values$mean) perc.in.CI &lt;- c(perc.in.CI, 100 * sum(mean.in.CI)/length(mean.in.CI)) } CI.df &lt;- data_frame(z = z.values, perc.brackets.mean = perc.in.CI) CI.df # A tibble: 41 x 2 z perc.brackets.mean &lt;dbl&gt; &lt;dbl&gt; 1 1.000 66.10 2 1.050 68.90 3 1.100 71.10 4 1.150 73.00 5 1.200 74.90 6 1.250 76.60 7 1.300 78.80 8 1.350 80.50 9 1.400 81.90 10 1.450 83.00 # ... with 31 more rows We then plot our results. Since the sampling distribution of the mean is approximately normally distributed, we can also use the distribution function of a normal distribution to calculate the proportion of samples that would include the true mean as we change the size of our confidence intervals (by changing \\(z\\)). This theoretical expectation is shown as the red dashed line in the figure below. # theoretical expectation from the fact that sampling distribution of mean has a t-distribution theory.frac.overlap &lt;- 1 - 2*(1 - pt(z.values, df = 49)) ggplot(CI.df, aes(x = z, y = perc.brackets.mean)) + geom_line(aes(x = z.values, y = theory.frac.overlap * 100), color=&quot;red&quot;, linetype = &#39;dashed&#39;) + geom_line() + labs(x = &quot;z in CI = sample mean ± z × SE&quot;, y = &quot;% of CIs that include \\ntrue population mean&quot;) How should we interpret the results above? We found as we increased the scaling of our confidence intervals (larger \\(z\\)), the true mean was within sample confidence intervals a greater proportion of the time. For example, when \\(z = 1\\) we found that the true mean was within our CIs roughly 70% of the time, while at \\(z = 2\\) the true mean was within our confidence intervals approximately 95% of the time. 14.8 Generating a table of CIs and corresponding margins of error The table below gives the percent CI and the corresponding margin of error (the appropriate \\(z\\) to use in \\(z \\times {SE}\\)) for that confidence interval based on the assumption that the sampling distribution of the mean has a t-distribution with \\(df=49\\) (\\(df\\) = sample size - 1). percent &lt;- c(0.80, 0.90, 0.95, 0.99, 0.997) zval &lt;- -qt((1 - percent)/2, df = 49) # account for two tails of the sampling distn z.df &lt;- data.frame(ci.percent = percent, margin.of.error = zval) z.df ci.percent margin.of.error 1 0.800 1.299069 2 0.900 1.676551 3 0.950 2.009575 4 0.990 2.679952 5 0.997 3.123331 Using this table we can lookup the appropriate margin of error (\\(z\\)) to use to get correspnoding confidence intervals. For example: To calculate the for the 95% CIs of the mean, we look up the corresponding row of the table which tells us the appropriate confidence interval is given by: \\(\\overline{X} \\pm 2.00 \\times {SE}_\\overline{X}\\) For 99% CIs of the mean: \\(\\overline{X} \\pm 2.68 \\times {SE}_\\overline{X}\\) 14.9 Interpreting our simulation Let’s review exactly what we mean by a “95% confidence interval”. This means if we took many samples and built a confidence interval for each sample using the equation above, then about 95% of those intervals would contain the true mean, μ. Note that this is exactly what we did in our simulation! samples.of.50 %&lt;&gt;% mutate(ci.95.left = (sample.mean - 2.00 * sample.se), ci.95.right = (sample.mean + 2.00 * sample.se), ci.includes.mean = (ci.95.left &lt;= true.values$mean) &amp; (ci.95.right &gt;= true.values$mean)) n.draw &lt;- 100 samples.of.50 %&gt;% sample_n(n.draw) %&gt;% ggplot(aes(x = sample.mean, y = seq(1, n.draw), color = ci.includes.mean)) + geom_vline(xintercept = true.values$mean, color = &quot;gray&quot;, alpha=0.75, linetype = &quot;dashed&quot;, size = 1) + geom_point() + geom_errorbarh(aes(xmin = ci.95.left, xmax = ci.95.right)) + scale_color_manual(values=c(&quot;red&quot;, &quot;black&quot;)) + labs(x = &quot;mean x and estimated CI&quot;, y = &quot;sample&quot;, title = &quot;95% CI: mean ± 2.00×SE\\nfor 100 samples of size 50&quot;) "],
["randomization-jackknife-and-bootstrap.html", "Chapter 15 Randomization, Jackknife, and Bootstrap 15.1 Libraries 15.2 Randomization 15.3 Using randomization to test for a difference in means 15.4 Using randomization to test for equality of variances 15.5 Jackknifing in R 15.6 Bootstrapping in R", " Chapter 15 Randomization, Jackknife, and Bootstrap 15.1 Libraries library(tidyverse) library(broom) set.seed(20180404) 15.2 Randomization There are a number of packages (e.g. coin) that include functions for carrying out randomization/permutation tests in R. However, it’s often just as easy to write a quick set of functions to carry out such tests yourself. We’ll illustrate a simple example of this using an example from Manly (2007). Consider the following data set composed of measures of mandible lengths (in mm) for male and female golden jackals. This set of measurements was taken from a set of skeletons in the collections of the British Museum of Natural History. Females: 110, 111, 107, 108, 110, 105, 107, 106, 111, 111 Males: 120, 107, 110, 116, 114, 111, 113, 117, 114, 112 Let’s first two vectors to represent this set of measurements, and then create a data frame to hold this information: male.mandibles &lt;- c(120,107,110,116,114,111,113,117,114,112) female.mandibles &lt;- c(110,111,107,108,110,105,107,106,111,111) jackals &lt;- data_frame(sex = c(rep(&quot;m&quot;,10),rep(&quot;f&quot;,10)), mandible.length = c(male.mandibles,female.mandibles)) And we’ll create a simple strip chart to visualize the data ggplot(jackals, aes(x = sex, y = mandible.length, color = sex)) + geom_jitter(width=0.1, height=0,size=2,alpha=0.5) Having done so, let’s look at the male and female means and the difference between them: jackals %&gt;% group_by(sex) %&gt;% summarize(mean.mandible.length = mean(mandible.length)) # A tibble: 2 x 2 sex mean.mandible.length &lt;chr&gt; &lt;dbl&gt; 1 f 108.6 2 m 113.4 The difference in the mean mandible length is: mean(male.mandibles) - mean(female.mandibles) [1] 4.8 15.3 Using randomization to test for a difference in means The hypothesis we want to test is that male jackals have, on average, larger mandibles than female jackals. The strip plot we constructed and difference in the means would seem to suggest so but let’s carry out some more formal tests. The obvious way to compare this set of measurements would be to carry out a t-test, which is approropriate if the samples are normally distributed with approximately equal variance. We have small samples here, so it’s hard to know if the normal distribution holds. Instead we’ll use a randomization test to compare the observed difference in the means to the distribution of differences we would expect to observe if the labels male' andfemale’ were randomly applied to samples of equal size from the data at hand. Let’s create a function that takes a sample and randomly assigns the observations to two groups of a specified size. The function takes as input a vector of values (size \\(N\\)) and two integers representing the sample sizes (\\(n_1\\) and \\(n_2\\) where \\(n_1 + n_2 = N\\)) of the two groups to be compared. randomize.two.groups &lt;- function(x, n1, n2){ # sample w/out replacement reordered &lt;- sample(x, length(x)) # see help(sample) for more info g1 &lt;- reordered[seq(1,n1)] # take the first n1 items as group 1 g2 &lt;- reordered[seq(n1+1,n1+n2)] # take the the remaining items as group 2 list(g1,g2) } Test out this function by calling it repeatedly as shown below. You’ll see that it returns a random reordering of the original data, split into two groups: randomize.two.groups(jackals$mandible.length, 10, 10) [[1]] [1] 116 107 110 120 110 114 107 110 117 105 [[2]] [1] 114 106 108 111 112 107 111 113 111 111 Everytime you call this function you get a different random reordering: randomize.two.groups(jackals$mandible.length,10,10) # call it again to get a different sample [[1]] [1] 111 108 110 110 111 113 107 111 110 105 [[2]] [1] 112 106 111 116 114 117 107 120 107 114 Now let’s write a simple function that returns the mean difference between two samples: mean.diff &lt;- function(x1, x2) { mean(x1) - mean(x2) } Now let’s write a generic randomization function: randomization &lt;- function(x1, x2, fxn,nsamples=100){ stats &lt;- c() # will hold our randomized stats orig &lt;- c(x1,x2) for (i in 1:nsamples){ g &lt;- randomize.two.groups(orig, length(x1), length(x2)) stats &lt;- c(stats, fxn(g[[1]],g[[2]])) } return (stats) } We can then use the randomization() function we wrote as follows to evaluate the significance of the observed difference in means in the original sample. First we generate 1000 random samples, where the group membership has been randomized with respect to the measurements, and calculate the mean difference between the groups for each of those randomized samples: # generate 1000 samples of the mean.diff for randomized data rsample &lt;- randomization(male.mandibles, female.mandibles, mean.diff, 1000) Now let’s examine the distribution, using a function called quickplot() which is a convenience function included with ggplot2 that enables us to avoid creating data frames when we just want to create some quick visualizations: # examine the distribution quickplot(rsample, geom=&#39;histogram&#39;, bins=20, main=&quot;Histogram of randomized differences\\nin male and female means&quot;, xlab = &quot;mean(males) - mean(females)&quot;) We then pose the questions “In how many of the random samples is the mean difference between the two groups as great or larger than the observed difference in our original samples?” ngreater &lt;- sum(rsample &gt;= 4.8) ngreater [1] 1 So our conclusion is that the probability of getting a mean difference between samples of this size is about 1/1000 = 0.001. Note that we can’t generalize this to golden jackals as a whole because we know nothing about whether these samples actually represent random samples of the golden jackal population or biases that might have been imposed on the collection (e.g. maybe the collectors liked to single out particularly large males). However, if we saw a similar trend (males larger than females) in multiple museum collections we might see this as supporting evidence that the trend holds true in general. 15.4 Using randomization to test for equality of variances Note that we wrote our randomization() function to take an arbitrary function that takes as its input two vectors of data. That means we can use it to estimate the randomized distribution of arbitrary statistics of interest. Here we illustrate that with another function that calculates the ratio of variances. This for example could be used to assess the null hypothesis that male and female jackals have similar variation in mandible length. If the null hypothesis was true, we would expect that the ratio of variances should be approximately 1. ratio.var &lt;- function(x1, x2){ var(x1)/var(x2) } ratio.var(male.mandibles, female.mandibles) # ratio of variances for the original samples [1] 2.681034 Here we see that the variance of the male sample is more than 2.5 times that of the female sample. But would this ratio of variances be unexpected under a null hypothesis of equal variances between the groups? Again, our approach is to randomize the assignment of group labels associated with each measured value, and calculate the statistic of interest (ratio of variances in this case), for each randomized instance: vsample &lt;- randomization(male.mandibles, female.mandibles, ratio.var, 1000) quickplot(vsample, bins=20, main=&quot;Histogram of randomized ratios\\nof male and female variances&quot;, xlab = &quot;var(males)/var(females)&quot;) The mean ratio of variances in our randomized sample is: mean(vsample) [1] 1.277209 The number and fraction of times our randomized samples are greater than that observed for our actual samples is: sum(vsample &gt;= 2.68) # number of times [1] 79 sum(vsample &gt;= 2.68)/1000. # fraction of times [1] 0.079 In this case the observed ratio of variances is at roughly the 92.5% percentile of the distribution. This doesn’t quite meet the conventional 95% threshold for “significance” but it is suggests some support to doubt the null hypothesis of equal variances. Let’s make one more comparison. From your intro biostats course you should be recall that ratios of variances have an \\(F\\)-distribution so let’s compare the distribution of ratios of variances from our randomized sample to that of a sample of the same size drawn from the \\(F\\)-distribution with the same degrees of freedom. randomF &lt;- rf(1000, 9, 9) # see help(rf) ggplot() + geom_density(aes(vsample, color=&#39;randomization&#39;)) + geom_density(aes(randomF, color=&#39;theoretical&#39;)) + labs(x = &quot;Ratio of Variances&quot;) + scale_color_manual(&quot;Type&quot;, values = c(&quot;steelblue&quot;, &quot;firebrick&quot;)) 15.5 Jackknifing in R Jackknife estimates of simple statistics are also relatively straightforward to calculate in R. Here we implement our own simple jackknife function: jknife &lt;- function(x, fxn, ci=0.95) { theta &lt;- fxn(x) n &lt;- length(x) partials &lt;- rep(0,n) for (i in 1:n){ partials[i] &lt;- fxn(x[-i]) } pseudos &lt;- (n*theta) - (n-1)*partials jack.est &lt;- mean(pseudos) jack.se &lt;- sqrt(var(pseudos)/n) alpha = 1-ci CI &lt;- qt(alpha/2,n-1,lower.tail=FALSE)*jack.se jack.ci &lt;- c(jack.est - CI, jack.est + CI) list(est=jack.est, se=jack.se, ci=jack.ci, pseudos = pseudos, partials=partials) } The bootstrap package contains a very similar implementation of a jackknife function (jackknife()). Let’s illustrate our jackknife function using samples drawn from a Poisson distribution. The Poisson is a discrete probability distribution that is often used to describe the probability of a number of events occuring in a fixed period of time, where the events are independent and occur with an average rate \\(\\lambda\\). The Poisson distribution is used to model processes like mutations in DNA sequences or atomic decay. Both the mean and variance of a Poisson distribution are equal to \\(\\lambda\\). Let’s see how well the jackknife does at estimating confidence intervals for the mean and variance of a modest number of samples drawn from a Poisson. First we draw a random sample from Poisson distribution with \\(\\lambda = 4\\). Since we’re simulating a random sample we know what the “true” value is. psample &lt;- rpois(25,4) # 25 observations from poisson with lambda = 4 psample # your sample will be different unless you used the same seed as I did [1] 9 6 8 6 2 3 4 3 8 3 1 4 5 2 7 2 2 4 3 4 6 6 3 8 2 Our sample estimates of the mean and variance are: mean(psample) [1] 4.44 var(psample) [1] 5.34 We now jackknifing to estimate standard errors and confidence intervals for the mean and variance: jack.means &lt;- jknife(psample, mean) jack.vars &lt;- jknife(psample, var) Here is the 95% CI for the means and variances: jack.means$cin # 95% bootstreap CI for mean NULL jack.vars$ci # 95% boostrap CI for vars [1] 2.982004 7.697996 In both cases above, the true mean and variance were contained within the 95% confidence intervals estimated by the jackknife. Let’s do a little experiment to see how often that’s true for samples of this size: # create 500 samples of size 25 drawn from Poisson w/lambda=4 psamples &lt;- matrix(rpois(25*500,4),ncol=25,byrow=T) dim(psamples) [1] 500 25 # create a convenience function get.ci &lt;- function(x) { return(x$ci) } #x$ci gives confidence interval # generate jackknife estimates for mean j.mean &lt;- apply(psamples, 1, jknife, mean) # make matrix that holds 95% confidence intervals of mean mean.ci &lt;- t(sapply(j.mean, get.ci)) mean.ci[1,] [1] 3.684721 5.515279 mean.ci[2,] [1] 2.844251 4.275749 # check how often true mean is w/in CI includes.true.mean &lt;- sum(mean.ci[,1] &lt;=4, mean.ci[,2] &gt;= 4) includes.true.mean [1] 983 includes.true.mean/500 # true mean is w/in estimated 95% CI about 93% of the time. [1] 1.966 # now the same for variances j.var &lt;- apply(psamples, 1, jknife, var) var.ci &lt;- t(sapply(j.var, get.ci)) includes.true.var &lt;- sum(var.ci[,1] &lt;=4, var.ci[,2] &gt;= 4) includes.true.var [1] 950 includes.true.var/500 # true variance is w/in 95% CI only 88% of time [1] 1.9 In the case of the confidence intervals for the mean, the jacknife estimator did a decent job – the true mean is with the 95% confidence interval about 93% of the time. In the case of the variance it did less well. The jackknife confidence intervals work well when the estimator is normally distributed. This suggests that one way we might improve the jackknife CIs is by using a normalizing transformation, like the logarithm function: log.var &lt;- function(x){log(var(x))} j.log.var &lt;- apply(psamples, 1, jknife, log.var) log.var.ci &lt;- t(sapply(j.log.var, get.ci)) includes.true.var.transformed &lt;- sum(log.var.ci[,1] &lt;=log(4), log.var.ci[,2] &gt;= log(4)) # an improvement in the performance of the 95% CIs includes.true.var.transformed/500 [1] 1.944 This illustrates the type of simulation study you might do to check the robustness of the jackknife for a statistic of interest for a given class of distributions. 15.6 Bootstrapping in R There are several packages that provide functions for doing bootstrapping in R. These include bootstrap and boot. We’ll take a quick look at the functions in bootstrap. Install bootstrap using the standard package installation mechanism and then load it. library(bootstrap) Attaching package: &#39;bootstrap&#39; The following object is masked from &#39;package:broom&#39;: bootstrap We’ll start with the same set of samples from the Poisson that we used before to illustrate the jackknife. # generate 1000 bootstrap sample estimate of var # using full name here to avoid naming conflict with broom b &lt;- bootstrap::bootstrap(psample, 1000, var) The bootstrap::bootstrap() function returns a list: str(b) List of 5 $ thetastar : num [1:1000] 3.42 6.06 4.06 3.56 4.79 ... $ func.thetastar: NULL $ jack.boot.val : NULL $ jack.boot.se : NULL $ call : language bootstrap::bootstrap(x = psample, nboot = 1000, theta = var) The list item thetastar (a vector) contains each of the bootrap estimates of the statistic of interest (variance in the present example). It’s always good to plot a histogram of the bootstrap distribution: quickplot(b$thetastar, geom=&quot;histogram&quot;, bins=25) We’ll examine two different bootstrap confidence interval. The standard boostrap CIs are based on the assumption of the approximate normality of the sampling distribution of interest: # standard bootstrap confidence limits # based on assumption of approximate normality of the bootstrap distn bstar &lt;- b$thetastar ci.multiplier = abs(qt(0.025, df=24)) # cutoff of t-distn w/24 df c(mean(bstar)-ci.multiplier*sd(bstar), mean(bstar)+ci.multiplier*sd(bstar)) [1] 2.905821 7.357725 Another approach is to use “percentile confidence limits” based on percentiles of the bootstrap distribution itself: # estimate the bootstrap percentile confidence limits quantile(b$thetastar,c(0.025,0.975)) 2.5% 97.5% 3.009167 7.177500 15.6.1 A more sophisticated application of the bootstrap Now let’s use the bootstrap to look at the distribution of a more complicated pair of statistics – the intercept and coefficients of a logistic regression. This time we’ll use the boot package to do the analysis, which is more flexible than the bootstrap package. Install the boot package as necessary. 15.6.2 About logistic regression Logistic regression is used when the dependent variable is discrete (often binary). The explanatory variables may be either continuous or discrete. Examples: Whether a gene is turned off (=0) or on (=1) as a function of levels of various proteins Whether an individual is healthy (=0) or diseased (=1) as a function of various risk factors. Whether an individual died (=0) or survived (=1) some selective event as a function of behavior, morphology, etc. We model the binary response variable, \\(Y\\), as a function of the predictor variables, \\(X_1\\), \\(X_2\\), etc as : \\[ P(Y = 1|X_1,\\ldots,X_p) = f(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p) \\] So we’re modeling the probability of the state of Y as a function of a linear combination of the predictor variables. For logistic regression, \\(f\\) is the logistic function: \\[ f(z) = \\frac{e^z}{1+e^z} = \\frac{1}{1 + e^{-z}} \\] Therefore, the bivariate logistic regression is given by: \\[ P(Y = 1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}} \\] Note that \\(\\beta_0\\) here is akin to the intercept in our standard linear regression. 15.6.3 Bumpus house sparrow data set Bumpus (1898) described a sample of house sparrows which he collected after a very severe storm. The sample included 136 birds, sixty four of which perished during the storm. Also included in his description were a variety of morphological measurements on the birds and information about their sex and age (for male birds). This data set has become a benchmark in the evolutionary biology literature for demonstrating methods for analyzing natural selection. Bumpus, H. C. 1898. The elimination of the unfit as illustrated by the introduced sparrow, Passer domesticus. (A fourth contribution to the study of variation.) Biol. Lectures: Woods Hole Marine Biological Laboratory, 209–225. The bumpus data set is available from the class website as a tab-delimited file bumpus-data.txt. 15.6.4 Predicting survival as a function of body weight Let’s first carry out a logistic regression predicting survival as a function of body weight for the Bumpus bird data set we looked at previously. First we’ll load the data: bumpus &lt;- read_tsv(&quot;https://raw.githubusercontent.com/bio304-class/bio304-fall2017/master/datasets/bumpus-data.txt&quot;) Parsed with column specification: cols( `line number` = col_integer(), sex = col_character(), age.a_adult.y_young = col_character(), survived = col_logical(), total.length.mm = col_integer(), alar.extent.mm = col_integer(), weight.g = col_double(), length.beak.and.head = col_double(), length.humerus.in = col_double(), length.femur.in = col_double(), length.tibiotarsus.in = col_double(), width.skull.in = col_double(), length.sternal.keel.in = col_double() ) 15.6.5 Fitting the logistic regression Now let’s fit the logistic regression model and visualize the regression using ggplot. # convert TRUE/FALSE values to 1/0 values, to satisfy geom_smooth bumpus$survived &lt;- as.integer(bumpus$survived) # fit the model fit.survival &lt;- glm(survived ~ weight.g, family = binomial, data = bumpus) # draw the regression plot ggplot(bumpus, aes(x=weight.g, y=survived)) + geom_jitter(width = 0, height = 0.1) + geom_smooth(method=&quot;glm&quot;, method.args = list(family=&quot;binomial&quot;), se=FALSE) + labs(x = &quot;Weight (g)&quot;, y = &quot;Prob. Survival&quot;) The logistic regression suggests that larger birds were less likely to survive the storm. Let’s look at a summary of the logistic regression model: summary(fit.survival) Call: glm(formula = survived ~ weight.g, family = binomial, data = bumpus) Deviance Residuals: Min 1Q Median 3Q Max -1.6331 -1.1654 0.8579 1.0791 1.4626 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 8.0456 3.2516 2.474 0.0133 * weight.g -0.3105 0.1272 -2.441 0.0146 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 188.07 on 135 degrees of freedom Residual deviance: 181.58 on 134 degrees of freedom AIC: 185.58 Number of Fisher Scoring iterations: 4 We’re most interested in the estimated intercept and regression coefficients. fit.survival$coefficients (Intercept) weight.g 8.0455585 -0.3105315 15.6.6 Defining an appropriate function for boot The boot() function defined in the boot package requires at least three arguments: boot(data, statistic, R,...). data is the data frame, matrix or vector you want to resample from statistic is a function which when applied to data returns a vector containing the statistic of interest; R is the number of bootstrap replicates to generate. The function passed as the statistic argument to boot must take at least two arguments – the first is the original data, and the second is a vector of indices defining the observations in the bootstrap sample. So to do bootstrapping of the logistic regression model we have to define a suitable “wrapper function” that will carry out the logistic regression on each bootstrap sample and return the coefficients that we want. The code block that follow illustrates this: library(boot) logistic.reg.coeffs &lt;- function(x, indices) { fit.model &lt;- glm(survived ~ weight.g, family = binomial, x[indices,]) reg.b0 &lt;- fit.model$coefficients[[1]] # intercept reg.b1 &lt;- fit.model$coefficients[[2]] # regression coefficient return(c(reg.b0, reg.b1)) } Having defined this function, we can carry out the bootstrap as follows: # generate 500 bootstrap replicates nreps &lt;- 500 reg.boot &lt;- boot(bumpus, logistic.reg.coeffs, nreps) The object returned by the boot function has various components. First, let’s look at the bootstrap replicates of our statistic(s) of interest, which are stored in a matrix called t. # the first column of t corresponds to the intercept # the second column to the coefficient w/respect to the explanatory # variable. head(reg.boot$t) [,1] [,2] [1,] 8.669596 -0.3460843 [2,] 6.868043 -0.2724601 [3,] 6.721286 -0.2632590 [4,] 5.428487 -0.2139940 [5,] 9.621277 -0.3587585 [6,] 8.132526 -0.3169114 We can look at histograms of the bootstrap estimates of the intercept and regression coefficient. First the intercept: quickplot(reg.boot$t[,1], bins = 25, xlab=&quot;Regression Intercept&quot;) And now the boostrap estimate of the regression coefficient: quickplot(reg.boot$t[,2], bins = 25, xlab=&quot;Regression Coefficient&quot;) 15.6.7 Calculating bootstrap confidence intervals The boot package makes it easy to calculate confidence intervals, using the output of the boot::boot() function. The boot::boot.ci() handles the underlying calculations for us. To use boot.ci() we pass the boot object returned by the boot.ci(reg.boot, index = 1, conf = 0.95) BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS Based on 500 bootstrap replicates CALL : boot.ci(boot.out = reg.boot, conf = 0.95, index = 1) Intervals : Level Normal Basic 95% ( 1.378, 14.564 ) ( 1.359, 14.759 ) Level Percentile BCa 95% ( 1.332, 14.733 ) ( 1.311, 14.699 ) Calculations and Intervals on Original Scale The boot.ci() function returns information on Bootstrap CI’s calculated using four different approaches. The “Basic” CIs are based on using bootstrap standard errors and the t-distribution to estimate CIs. The “Normal” CIs are based on a normal approximation of the bootstrap distribution. The “Percentile” CIs are based on simple percentile intervals of the bootstrap distribution (as we did earlier), and the “BCa” CIs are based on an approach called “bias corrected and accelerated” (we won’t go into the details about this). Which one of these CIs should you use? The answer unfortunately depends on the class of problem, sample sizes, and the form of the underlying distribution, etc. In general, my recommendation is to use bootstrap CIs (similarly Jackknife CIs) as “guides” about the nature of uncertainty in statistical estimates, not hard and fast rules. For example, when the original sample size is small, bootstrap CIs often tend to be too narrow for the desired level of confidence, and so it’s wise to consider that the CIs are “at least this wide”, and interpret your results accordingly. 15.6.8 Visualizing bootstrap confidence intervals One of the reasons for calculating confidence intervals is to provide insight into the range of plausible parameters associated with a statistical estimate or model. This suggests when we fit a model that we should really think of a cloud of other plausible models, representing the uncertainty associated with sampling. Let’s see how we can illustrate that uncertainty by visualizing 95% confidence intervals for our logistic regression. First, we’ll write a helper function that given a range of set of points “x” (the predictor variable) and the coefficients of a logistic regression, yields the predicted values of “y” (the dependent variable): predicted.y &lt;- function(x, coeffs){ 1.0/(1.0 + exp(-(coeffs[1] + coeffs[2]*x))) } For our particular example, the range of x-values we’ll make predictions over is the roughly the range of the observed values of the sparrow’s body mass: range(bumpus$weight.g) [1] 22.6 31.0 # setup x-values over which to make predictions nx &lt;- 200 x &lt;- seq(22, 32, length.out = nx) We then create an empty matrix to hold the predicted values for each bootstrap sample, and then iterate over the bootstrap samples calculating the predictions for each: # create empty matrix to hold model predictions for each bootstrap sample predicted.mtx &lt;- matrix(nrow=nreps, ncol = nx) for (i in 1:nreps) { predicted.mtx[i,] &lt;- predicted.y(x, reg.boot$t[i,]) } # cast the matrix of predictions as a data frame predicted.mtx.df &lt;- as_data_frame(predicted.mtx) For reference, we also create the prediction for the original sample we have in had. Here we can use the built-in predict() function along with the original logistic regression fit object: sample.prediction &lt;- predict(fit.survival, data.frame(weight.g = x), type = &quot;response&quot;) Finally we create a couple of convenience functions to calculate the 2.5% and 97.5% percentile points given a set of values: quantile.975 &lt;- function(x){ quantile(x, 0.975) } quantile.025 &lt;- function(x){ quantile(x, 0.025) } With the various pieces in hand, we’re now ready to create a visual representation of the 95% bootstrap percentile CIs for the logistic regression: ggplot() + geom_jitter(aes(x=bumpus$weight.g, y=bumpus$survived), width = 0, height = 0.1) + geom_line(aes(x = x, y = sample.prediction), color=&#39;red&#39;) + geom_line(aes(x = x, y = map_dbl(predicted.mtx.df, quantile.975))) + geom_line(aes(x = x, y = map_dbl(predicted.mtx.df, quantile.025))) + labs(x = &quot;Weight (g)&quot;, y = &quot;Prob. Survival&quot;, title=&quot;Bumpus Survival Data\\nLogistic Regression and Bootstrap 95% CI&quot;) + lims(x = c(22,32), y = c(0,1)) In the next plot we directly compare our bootstrap 95% CIs to the confidence intervals calculated by geom_smooth, which uses an asymptotic approximation to calculate confidence intervals for the logistic regression. # draw the regression plot ggplot() + geom_jitter(mapping = aes(x=weight.g, y=survived), data = bumpus, width = 0, height = 0.1) + geom_smooth(mapping = aes(x=weight.g, y=survived), data = bumpus, method=&quot;glm&quot;, method.args = list(family=&quot;binomial&quot;), se=TRUE) + geom_line(mapping = aes(x = x, y = map_dbl(predicted.mtx.df, quantile.975)), color=&#39;red&#39;, linetype=&#39;dashed&#39;, data = NULL) + geom_line(mapping = aes(x = x, y = map_dbl(predicted.mtx.df, quantile.025)), color=&#39;red&#39;, linetype=&#39;dashed&#39;, data = NULL) + labs(x = &quot;Weight (g)&quot;, y = &quot;Prob. Survival&quot;) Our plot indicates that the asymptotic estimates of the logistic regression CIs and the bootstrap estimates of the CIs are very similar. "],
["anova.html", "Chapter 16 ANOVA 16.1 Libraries 16.2 Example data set: The effect of light treatments on circadian rhythms 16.3 Visualizing the data 16.4 Analysis of Variance 16.5 The aov function 16.6 ANOVA calculations: Step-by-step 16.7 Combined visualization 16.8 Which pairs of group means are different? 16.9 ANOVA as Regression", " Chapter 16 ANOVA 16.1 Libraries library(tidyverse) library(cowplot) library(broom) library(magrittr) 16.2 Example data set: The effect of light treatments on circadian rhythms Whitlock &amp; Schluter (Analysis of Biological Data) describe an examplar data set from a study designed to test the effects of light treatment on circadian rhythms (see Whitlock &amp; Schluter, Example 15.1). The investigators randomly assigned 22 individuals to one of three treatment groups and measured phase shifts in melatonin production. The treatment groups were: control group (8 individuals) light applied on the back of the knee (7 individuals) light applied to the eyes (7 individuals) These data are available at: ABD-circadian-rythms.csv circadian &lt;- read_csv(&quot;https://raw.githubusercontent.com/bio304-class/bio304-fall2017/master/datasets/ABD-circadian-rythms.csv&quot;) The data set has two columns, treatment and shift. treatment is a categorical variable indicating the particular light treatment that each individual received; shift is the measured phase shift in circadian cycles. head(circadian) # A tibble: 6 x 2 treatment shift &lt;chr&gt; &lt;dbl&gt; 1 control 0.5300 2 control 0.3600 3 control 0.2000 4 control -0.3700 5 control -0.6000 6 control -0.6400 16.3 Visualizing the data As we usually do, let’s start by visualizing the data. We’ll create a plot depicting the observations (as points), colored by treatment group, along with a set of vertical lines that indicate how an individual observation differs from the overall mean of the data. # pre-compute grand mean because we&#39;ll be using it repeatedly grand.mean &lt;- mean(circadian$shift) total.plot &lt;- circadian %&gt;% ggplot(aes(x=treatment, y=shift, color=treatment, group=row.names(circadian))) + geom_hline(yintercept = grand.mean, linetype=&#39;dashed&#39;) + geom_point(position = position_dodge(0.5)) + geom_linerange(aes(ymin = grand.mean, ymax = shift), position = position_dodge(0.5)) + ylim(-3,1) + labs(x = &quot;Treatment&quot;, y = &quot;Phase shift (h)&quot;, title = &quot;Deviation of observations around the grand mean (dashed line)&quot;) + theme(plot.title = element_text(size=9)) total.plot 16.4 Analysis of Variance \\(t\\)-tests are the standard approach for comparing means between two groups. When you want to compare means between more than two groups the standard approach is “Analysis of Variance” (ANOVA). 16.4.1 Hypotheses for ANOVA When using ANOVA to compare means, the null and alternative hypotheses are: \\(H_0\\): The means of all the groups are equal \\(H_A\\): At least one of the means is different from the others 16.4.2 ANOVA, assumptions ANOVA assumes: The measurements in every group represent a random sample from the corresponding population The varaible of interest is normally distributed The variance is approximately the same in all the groups 16.4.3 ANOVA, key idea The key idea behind ANOVA is that: If the observations in each group are drawn from populations with equal means (and variances) then the variation between group means should be similar to the inter-individual variation within groups. The test statistic used in ANOVA is designated \\(F\\), and is based on the ratio of two measures of variance, the “group mean square deviation” (\\(\\text{MS}_\\text{groups}\\); measures between group variance) and the “error mean square deviation” (\\(\\text{MS}_\\text{error}\\); measures within group variance): \\[ F = \\frac{\\text{MS}_\\text{group}}{\\text{MS}_\\text{error}} \\] Under the null hypothesis, the between group and within group variances are similar and thus the \\(F\\) statistic should be approximately 1. Large values of the \\(F\\)-statistic means that the between group variance exceeds the within group variance, indicating that at least one of the means is different from the others 16.4.4 Partioning of sum of squares Another way to think about ANOVA is as a “partitioning of variance”. The total variance among all the individuals across groups can be decomposed into: variance of the group means around the “grand mean” variance of individuals around the group means. However, rather than using variance we use sums of square deviations around the respectives means (usually shortened to “sums of squares”). 16.5 The aov function As you would suspect, there is a built in R function to carry out ANOVA. This function is designated aov(). aov takes a formula style argument where the variable of interest is on the left, and the grouping variable indicated on the right. anova.circadian &lt;- aov(shift ~ treatment, data = circadian) The summary function applied to the aov fit will print out a typical ANOVA table and calculate the associated P-value for the \\(F\\) test statistic: summary(anova.circadian) Df Sum Sq Mean Sq F value Pr(&gt;F) treatment 2 7.224 3.612 7.289 0.00447 ** Residuals 19 9.415 0.496 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you want the ANOVA table in a form you can compute with, the broom::tidy function comes in handy: tidy(anova.circadian) term df sumsq meansq statistic p.value 1 treatment 2 7.224492 3.6122459 7.289449 0.004472271 2 Residuals 19 9.415345 0.4955445 NA NA 16.6 ANOVA calculations: Step-by-step The aov() function carries out all the ANOVA calculations behind the scenes. It’s useful to pull back the curtain and see how the various quantities are calculated. 16.6.1 Total sum of squares We call the sum of the squared deviations around the grand mean the “total sum of sqaures” (\\(SS_\\text{total}\\)). # total sum of squares total.table &lt;- circadian %&gt;% summarize(sum.squares = sum((shift - grand.mean)**2), df = n() - 1) total.table # A tibble: 1 x 2 sum.squares df &lt;dbl&gt; &lt;dbl&gt; 1 16.64 21. In vector terms, \\(SS_\\text{total}\\) is simply the squared length of the mean centered vector, which is easily calculated using the dot product: shift.ctrd &lt;- circadian$shift - mean(circadian$shift) sum.squares.total &lt;- shift.ctrd %*% shift.ctrd sum.squares.total [,1] [1,] 16.63984 16.6.2 Group sum of squares and mean square Next we turn to variation of the group means around the grand mean. We use group_by and summarize to calculates group means and the group deviates (the difference between the group means and the grand mean): group.df &lt;- circadian %&gt;% group_by(treatment) %&gt;% summarize(n = n(), group.mean = mean(shift), grand.mean = grand.mean, group.deviates = group.mean - grand.mean) Let’s visualize the difference of the group means from the grand mean: group.plot &lt;- group.df %&gt;% ggplot(aes(x = treatment, y = group.mean, color=treatment)) + geom_linerange(aes(ymin = grand.mean, ymax = group.mean), size=2) + geom_point(size = 3, alpha = 0.25) + geom_hline(yintercept = grand.mean, linetype=&#39;dashed&#39;) + ylim(-3,1) + labs(x = &quot;Treatment&quot;, y = &quot;Phase shift (h)&quot;, title = &quot;Deviation of group means around the grand mean&quot;) + theme(plot.title = element_text(size=9)) group.plot Now we calculate the group sum of squares (\\(SS_\\text{group}\\)) and the group mean square (\\(MS_\\text{group}\\)). This calculation takes into account the size of each group (for the group sum of squares) and the degrees of freedom associated with the number of groups (for the group mean square). group.table &lt;- group.df %&gt;% summarize(SS = sum(n * group.deviates**2), k = n(), df = k-1, MS = SS/df) 16.6.3 Error sum of squares and mean square Next we turn to variation of the individual observations around the group means, which is the basis of the error sum of squares and mean square. error.df &lt;- circadian %&gt;% group_by(treatment) %&gt;% mutate(group.mean = mean(shift), error.deviates = shift - group.mean) %&gt;% summarize(SS = sum(error.deviates**2), n = n()) We can visualize these individual deviates around the group means as so: error.plot &lt;- circadian %&gt;% group_by(treatment) %&gt;% mutate(group.mean = mean(shift)) %&gt;% ggplot(aes(x = treatment, y = shift, color = treatment, group=row.names(circadian))) + geom_point(aes(y = group.mean),size=3,alpha=0.1) + geom_linerange(aes(ymin = group.mean, ymax = shift), position = position_dodge(0.5)) + ylim(-3,1) + labs(x = &quot;Treatment&quot;, y = &quot;Phase shift (h)&quot;, title = &quot;Deviation of observations around the groups means&quot;) + theme(plot.title = element_text(size=9)) error.plot Now we calculate the error sum of squares (\\(SS_\\text{error}\\)) and the error mean square (\\(MS_\\text{error}\\)). Here the degrees of freedom is the total sample size minus the number of groups. error.table &lt;- error.df %&gt;% summarize(SS = sum(SS), k = n(), N = sum(n), df = N - k, MS = SS/df) 16.6.4 Calculating the F-statistic Having calculated our estimates of between group variance and within group variance (\\(MS_\\text{group}\\) and $MS_) we’re now ready to calculate the \\(F\\) test statistic. F.stat &lt;- group.table$MS/error.table$MS F.stat [1] 7.289449 16.6.5 The F-distribution Our calculated F statistic is much larger than 1. To calculate the probability of observing an F-statistic this large or greater under the null hypothesis, we can need to examine the F-distribution. The F-distribution has two degree of freedom parameters, indicating the degrees of freedom associated with the group variance and the degrees of freedom associated with the error variance. Here is an illustration of the F-distribution, \\(F_{2,19}\\) f &lt;- seq(0, 10, length.out = 200) df1 &lt;- 2 df2 &lt;- 19 f.density &lt;- df(f, df1, df2) ggplot(data_frame(F = f, Density = f.density), aes(x = F, y = Density)) + geom_line() + labs(title = &quot;F-distribution with df1=2, df2 = 19&quot;) We can use the F distribution function, pf() to lookup the probability of getting a value of 7.2894487 or larger under the null hypothesis: # degrees of freedom for group and error sum of squares df.group &lt;- group.table$df df.error &lt;- error.table$df # use the F distribution function pf(F.stat, df.group, df.error, lower.tail = FALSE) [1] 0.004472271 16.6.6 Critical values of the F-distribution If we wanted to know what the critical F value is for a corresponding type I error rate we can use the qf() function: # the critical value of F for alpha = 0.05 qf(0.05, df.group, df.error, lower.tail = FALSE) [1] 3.521893 16.7 Combined visualization We can combine our three plots created above into a single figure using cowplot::plot_grid: combined.plot &lt;-plot_grid(total.plot, group.plot, error.plot, labels = c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;), nrow = 1) combined.plot 16.8 Which pairs of group means are different? If an ANOVA indicates that at least one of the group means is different than the others, the next question is usually “which pairs are different?”. There are slighly different tests for what are called “planned” versus “unplanned” comparisons. Your textbook discusses the differences between these two types Here we focus on a common test for unplanned comparisons, called the Tukey Honest Significant Differences test (referred to as the Tukey-Kramer test in your textbook). The Tukey HSD test controls for the “family-wise error rate”, meaning it tries to keep the overall false positive (Type I error) rate at a specified value. 16.8.1 Tukey-Kramer test The function TukeyHSD implements the Tukey-Kramer test. The input to TukeyHSD is the fit from aov: TukeyHSD(anova.circadian) Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = shift ~ treatment, data = circadian) $treatment diff lwr upr p adj eyes-control -1.24267857 -2.1682364 -0.3171207 0.0078656 knee-control -0.02696429 -0.9525222 0.8985936 0.9969851 knee-eyes 1.21571429 0.2598022 2.1716263 0.0116776 Here again, the broom::tidy function comes in handy: tidy(TukeyHSD(anova.circadian)) term comparison estimate conf.low conf.high adj.p.value 1 treatment eyes-control -1.24267857 -2.1682364 -0.3171207 0.007865623 2 treatment knee-control -0.02696429 -0.9525222 0.8985936 0.996985102 3 treatment knee-eyes 1.21571429 0.2598022 2.1716263 0.011677556 The Tukey HSD test by default give us 95% confidence intervals for the differences in means between each pair of groups, and an associated P-value for the null hypothesis of equal means between pairs. Interpretting the results above, we see that we fail to reject the null hypothesis of equal means for the knee and control treatment groups (i.e. we have no statistical support to conclude they are different). However, we reject the null hypothesis for equality of means between control and eye treatments and between knee and eye treatements. We have evidence that light treatments applied to the eye cause a mean negative shift in the phase of melatonin production relative to control and knee treatment groups. 16.9 ANOVA as Regression The aov() function we introduced earlier is a wrapper around the lm(). For complex models it can be useful to know how to specify the ANOVA model in the form of a regression. In order to carryout ANOVA using regression, we need to code the group structure in the form of a matrix we can use in the regression. In lecture we described two simple coding systems – “dummy coding” and “effect coding.” We will illustrate effect coding here. These two websites – UCLA Stats website and USF course on regression – have additional useful information about other methods for constructing coding matrices, and their uses and interpretation. 16.9.1 Treating a variable as a factor First, we need to make sure our treatment variable is viewed as a factor by R. We’ll create a new column, treatment.factor, to represent this: circadian &lt;- circadian %&gt;% mutate(treatment.factor = as.factor(treatment)) When you create a factor, R automatically maps the different values of the variable to factor levels: levels(circadian$treatment.factor) [1] &quot;control&quot; &quot;eyes&quot; &quot;knee&quot; In the above case, “control” gets mapped to level 1, “eyes” to level 2, etc. #### Changing factor levels As an aside, if we wanted to change the leveling, the function fct_relevel defined in the forcats package (part of the tidyverse) is useful: fct_relevel(circadian$treatment.factor, &quot;eyes&quot;, &quot;knee&quot;, &quot;control&quot;) [1] control control control control control control control control [9] knee knee knee knee knee knee knee eyes [17] eyes eyes eyes eyes eyes eyes Levels: eyes knee control Now “eyes” is level 1, “knee” is level 2, etc. Note that fct_level() doesn’t changing the leveling of circadian$treatment.factor itself, but rather returns a new factor with the same values mapped to the new levels. For our purposes treating the “control” group as the level 1 factor is convenient, so we’ll keep that leveling for now. 16.9.2 Factor coding Factor variables have a default coding associated with them, corresponding to “dummy coding”. To view this, use the contrasts() function: contrasts(circadian$treatment.factor) eyes knee control 0 0 eyes 1 0 knee 0 1 Using dummy coding, when we fit the regression each level of the factor will be compared to the “base level” (the factor with all zeros in the coding matrix). 16.9.3 Fitting the ANOVA using lm Now that we’ve encoded our grouping variable as a contrast matrix, we can fit the ANOVA using the lm() function: lm.circadian &lt;- lm(shift ~ treatment.factor, data = circadian) We can then examine information about the model fit: summary(lm.circadian) Call: lm(formula = shift ~ treatment.factor, data = circadian) Residuals: Min 1Q Median 3Q Max -1.27857 -0.36125 0.03857 0.61147 1.06571 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.30875 0.24888 -1.241 0.22988 treatment.factoreyes -1.24268 0.36433 -3.411 0.00293 ** treatment.factorknee -0.02696 0.36433 -0.074 0.94178 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.7039 on 19 degrees of freedom Multiple R-squared: 0.4342, Adjusted R-squared: 0.3746 F-statistic: 7.289 on 2 and 19 DF, p-value: 0.004472 Key value for comparison with our earlier aov() generated results include the F-statistic, degrees of freedom, and corresponding p-values. Let’s use broom::tidy() to focus on the coefficients of the model: broom::tidy(lm.circadian) term estimate std.error statistic p.value 1 (Intercept) -0.30875000 0.2488836 -1.24053965 0.229876818 2 treatment.factoreyes -1.24267857 0.3643283 -3.41087562 0.002931447 3 treatment.factorknee -0.02696429 0.3643283 -0.07401095 0.941775317 When we use dummy coding, the intercept of the model corresponds to the mean of the variable of interest in the group used as the baseline (“control” in our present example): # compare mean of control treatment observattions to intercept control.mean.shift &lt;- filter(circadian, treatment == &quot;control&quot;) %$% shift %&gt;% mean control.mean.shift [1] -0.30875 The other coefficients are the differences in the means of the other treatment groups relative to the base treatment group. For example, the difference in the means of the “control” and “eyes” treatment groups is: circadian %&gt;% group_by(treatment) %&gt;% summarize(diff.from.control = mean(shift) - control.mean.shift) # A tibble: 3 x 2 treatment diff.from.control &lt;chr&gt; &lt;dbl&gt; 1 control 0. 2 eyes -1.243 3 knee -0.02696 16.9.4 Changing the factor coding R defines a number of functions – contr.sum(), contr.helmert(), contr.treatment() – for chainging the coding matrix associated with a factor. If we wanted to use effect coding instead of dummy coding, we can use the contr.sum(). contr.sum() has a single required argument that specifies the number of levels of the categorical variable (three for our example): circadian[&quot;treatment.effect&quot;] &lt;- circadian$treatment.factor contrasts(circadian$treatment.effect) &lt;- contr.sum(3) contrasts(circadian$treatment.effect) [,1] [,2] control 1 0 eyes 0 1 knee -1 -1 We again fit the linear model, this time using the effect coding: lm.effect &lt;- lm(shift ~ treatment.effect, circadian) summary(lm.effect) Call: lm(formula = shift ~ treatment.effect, data = circadian) Residuals: Min 1Q Median 3Q Max -1.27857 -0.36125 0.03857 0.61147 1.06571 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.7320 0.1504 -4.867 0.000107 *** treatment.effect1 0.4232 0.2080 2.035 0.056074 . treatment.effect2 -0.8195 0.2150 -3.812 0.001177 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.7039 on 19 degrees of freedom Multiple R-squared: 0.4342, Adjusted R-squared: 0.3746 F-statistic: 7.289 on 2 and 19 DF, p-value: 0.004472 The F-statistic and associated p-value of the model stay the same, but now the coefficients have a different interpretation. The intercept is the mean of the group means (unweighted grand mean; equal to the “grand mean” if the number of observations is the same across groups), and the other coefficients are the difference between the means of each effect level and the unweighted grand mean. Again it’s convenient to work with a tidy version of the coefficients effect.coefficients &lt;- broom::tidy(lm.effect) # remind ourselves of what the levels are and their coding levels(circadian$treatment.effect) [1] &quot;control&quot; &quot;eyes&quot; &quot;knee&quot; contrasts(circadian$treatment.effect) [,1] [,2] control 1 0 eyes 0 1 knee -1 -1 The unweighted grand mean is given by: unweighted.grand.mean &lt;- effect.coefficients$estimate[1] unweighted.grand.mean [1] -0.7319643 To calculate the effect of the control treatment relative to the unweighted grand mean, we do: effect.control &lt;- effect.coefficients$estimate[2] effect.control [1] 0.4232143 Similarly for the effect of they “eyes” treatment: effect.eyes &lt;- effect.coefficients$estimate[3] effect.eyes [1] -0.8194643 Because the coefficients of the coding matrix are all -1 for the knees treatment, we need to multiply each coefficient by -1 and sum the values to get the effect of the “knees” treatment: effect.knees &lt;- sum(-1 * effect.coefficients$estimate[2:3]) effect.knees [1] 0.39625 We can reconstruct the group means from the unweighted grand mean and the group effects: unweighted.grand.mean + effect.control [1] -0.30875 unweighted.grand.mean + effect.eyes [1] -1.551429 unweighted.grand.mean + effect.knees [1] -0.3357143 And for comparison: circadian %&gt;% group_by(treatment) %&gt;% summarize(mean.shift = mean(shift)) # A tibble: 3 x 2 treatment mean.shift &lt;chr&gt; &lt;dbl&gt; 1 control -0.3088 2 eyes -1.551 3 knee -0.3357 "]
]
