[["index.html", "Biology 723: Statistical Computing for Biologists Chapter 1 Introduction 1.1 Accessing older versions of the course notes 1.2 How to use these lecture notes", " Biology 723: Statistical Computing for Biologists Paul M. Magwene 2021-02-17 Chapter 1 Introduction Bio 723 is a course I offer at Duke University. The focus of this course is statistical computing for the biological sciences with an emphasis on common multivariate statistical methods and techniques for exploratory data analysis. A major goal of the course is to help graduate students in the biological sciences develop practical insights into methods that they are likely to encounter in their own research, and the potential advantages and pitfalls that come with their use. In terms of mathematical perspectives, the course emphasize a geometric approach to understanding multivariate statistics. I try to help students develop an intuition for the geometry of vector spaces and discuss topics like correlation, regression, and principal components analysis in terms of angles between vectors, dot products, and projection. This course also provides an introduction to the R language/statistical computing environment. 1.1 Accessing older versions of the course notes The material covered in Bio 723 changes a bit from year to year. If you’d like to access older versions of the course notes, I will be making these available on the “Releases” page of the Github site for this book. 1.2 How to use these lecture notes In this and future materials to be posted on the course website you’ll encounter blocks of R code. Your natural intuition will be to cut and paste commands or code blocks into the R interpretter to save yourself the typing. DO NOT DO THIS!! In each of the examples below, I provide example input, but I don’t show you the output. It’s your job to type in these examples at the R console, evaluate what you typed, and to look at and think critically about the output. You will make mistakes and generate errors! Part of learning any new skill is making mistakes, figuring out where you went wrong, and correcting those mistakes. In the process of fixing those errors, you’ll learn more about how R works, and how to avoid such errors, or correct bugs in your own code in the future. If you cut and paste the examples into the R interpretter the code will run, but you will learn less than if you input the code yourself and you’ll be less capable of apply the concepts in new situations. The R interpretter, like all programming languages, is very exacting. A mispelled variable or function name, a stray period, or an unbalanced parenthesis will raise an error and finding the sources of such errors can sometimes be tedious and frustrating. Persist! If you read your code critically, think about what your doing, and seek help when needed (teaching team, R help, Google, etc) you’ll eventually get better at debugging your code. But keep in mind that like most new skills, learning to write and debug your code efficiently takes time and experience. "],["getting-started-with-r.html", "Chapter 2 Getting Started with R 2.1 What is R? 2.2 What is RStudio? 2.3 Entering commands in the console 2.4 Comments 2.5 Using R as a Calculator 2.6 Variable assignment 2.7 Data types 2.8 Packages 2.9 The R Help System", " Chapter 2 Getting Started with R 2.1 What is R? R is a statistical computing environment and programming language. It is free, open source, and has a large and active community of developers and users. There are many different R packages (libraries) available for conducting out a wide variety of different analyses, for everything from genome sequence data to geospatial information. 2.2 What is RStudio? RStudio (http://www.rstudio.com/) is an open source integrated development environment (IDE) that provides a nicer graphical interface to R than does the default R GUI. The figure below illustrates the RStudio interface, in it’s default configuration. For the exercises below you’ll be primarily entering commands in the “console” window. We’ll review key parts of the RStudio interface in greater detail in class. Figure 2.1: RStudio window with the panes labeled 2.3 Entering commands in the console You can type commands directly in the console. When you hit Return (Enter) on your keyboard the text you typed is evaluated by the R interpreter. This means that the R program reads your commands, makes sure there are no syntax errors, and then carries out any commands that were specified. Try evaluating the following arithmetic commands in the console: 10 + 5 10 - 5 10 / 5 10 * 5 If you type an incomplete command and then hit Return on your keyboard, the console will show a continuation line marked by a + symbol. For example enter the incomplete statement (10 + 5 and then hit Enter. You should see something like this. &gt; (10 + 5 + The continuation line tells you that R is waiting for additional input before it evaluates what you typed. Either complete your command (e.g. type the closing parenthesis) and hit Return, or hit the “Esc” key to exit the continuation line without evaluating what you typed. 2.4 Comments When working in the R console, or writing R code, the pound symbol (#) indicates the start of a comment. Anything after the #, up to the end of the current line, is ignored by the R interpretter. # This line will be ignored 5 + 4 # the first part of this line, up to the #, will be evaluated Throughout this course I will often include short explanatory comments in my code examples. When I want to display the output generated by an R statement typed at the console I will generally use a display convention in which I prepend the results with the symbols ##. 5 + 4 # same as above but with output displayed ## [1] 9 2.5 Using R as a Calculator The simplest way to use R is as a fancy calculator. Evaluate each of the following statements in the console. 10 + 2 # addition 10 - 2 # subtraction 10 * 2 # multiplication 10 / 2 # division 10 ^ 2 # exponentiation 10 ** 2 # alternate exponentiation pi * 2.5^2 # R knows about some constants such as Pi 10 %% 3 # modulus operator -- gives remainder after division 10 %/% 3 # integer division Be aware that certain operators have precedence over others. For example multiplication and division have higher precedence than addition and subtraction. Use parentheses to disambiguate potentially confusing statements. (10 + 2)/4-5 # was the output what you expected? (10 + 2)/(4-5) # compare the answer to the above Division by zero produces an object that represents infinite numbers. Infinite values can be either positive or negative 1/0 ## [1] Inf -1/0 ## [1] -Inf Invalid calculations produce a objected called NaN which is short for “Not a Number”: 0/0 # invalid calculation ## [1] NaN 2.5.1 Common mathematical functions Many commonly used mathematical functions are built into R. Here are some examples: abs(-3) # absolute value ## [1] 3 cos(pi/3) # cosine ## [1] 0.5 sin(pi/3) # sine ## [1] 0.8660254 log(10) # natural logarithm ## [1] 2.302585 log10(10) # log base 10 ## [1] 1 log2(10) # log base 2 ## [1] 3.321928 exp(1) # exponential function ## [1] 2.718282 sqrt(10) # square root ## [1] 3.162278 10^0.5 # same as square root ## [1] 3.162278 2.6 Variable assignment An important programming concept in all programming languages is that of “variable assignment.” Variable assignment is the act of creating labels that point to particular data values in a computers memory, which allows us to apply operations to the labels rather than directly to specific. Variable assignment is an important mechanism of abstracting and generalizing computational operations. Variable assignment in R is accomplished with the assignment operator, which is designated as &lt;- (left arrow, constructed from a left angular brack and the minus sign). This is illustrated below: x &lt;- 10 # assign the variable name &#39;x&#39; the value 10 sin(x) # apply the sin function to the value x points to ## [1] -0.5440211 x &lt;- pi # x now points to a different value sin(x) # the same function call now produces a different result ## [1] 1.224647e-16 # note that sin(pi) == 0, but R returns a floating point value very # very close to but not zero 2.6.1 Valid variable names As described in the R documentation, “A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number. Names such as ‘.2way’ are not valid, and neither are the reserved words.” Here are some examples of valid and invalid variable names. Mentally evaluate these based on the definition above, and then evaluate these in the R interpetter to confirm your understanding : x &lt;- 10 x.prime &lt;- 10 x_prime &lt;- 10 my.long.variable.name &lt;- 10 another_long_variable_name &lt;- 10 _x &lt;- 10 .x &lt;- 10 2.x &lt;- 2 * x 2.7 Data types The phrase “data types” refers to the representations of information that a programming language provides. In R, there are three core data types representing numbes, logical values, and strings. You can use the function typeof() to get information about an objects type in R. 2.7.1 Numeric data types There are three standard types of numbers in R. “double” – this is the default numeric data type, and is used to represent both real numbers and whole numbers (unless you explicitly ask for integers, see below). “double” is short for “double precision floating point value.” All of the previous computations you’ve seen up until this point used data of type double. typeof(10.0) # real number ## [1] &quot;double&quot; typeof(10) # whole numbers default to doubles ## [1] &quot;double&quot; “integer” – when your numeric data involves only whole numbers, you can get slighly better performance using the integer data type. You must explicitly ask for numbers to be treated as integers. typeof(as.integer(10)) # now treated as an integer ## [1] &quot;integer&quot; “complex” – R has a built-in data type to represent complex numbers – numbers with a “real” and “imaginary” component. We won’t encounter the use of complex numbers in this course, but they do have many important uses in mathematics and engineering and also have some interesting applications in biology. typeof(1 + 0i) ## [1] &quot;complex&quot; sqrt(-1) # sqrt of -1, using doubles ## [1] NaN sqrt(-1 + 0i) # sqrt of -1, using complex numbers ## [1] 0+1i 2.7.2 Logical values When we compare values to each other, our calculations no longer return “doubles” but rather TRUE and FALSE values. This is illustrated below: 10 &lt; 9 # is 10 less than 9? ## [1] FALSE 10 &gt; 9 # is 10 greater than 9? ## [1] TRUE 10 &lt;= (5 * 2) # less than or equal to? ## [1] TRUE 10 &gt;= pi # greater than or equal to? ## [1] TRUE 10 == 10 # equals? ## [1] TRUE 10 != 10 # does not equal? ## [1] FALSE TRUE and FALSE objects are of “logical” data type (known as “Booleans” in many other languages, after the mathematician George Boole). typeof(TRUE) typeof(FALSE) x &lt;- FALSE typeof(x) # x points to a logical x &lt;- 1 typeof(x) # the variable x no longer points to a logical When working with numerical data, tests of equality can be tricky. For example, consider the following two comparisons: 10 == (sqrt(10)^2) # Surprised by the result? See below. 4 == (sqrt(4)^2) # Even more confused? Mathematically we know that both \\((\\sqrt{10})^2 = 10\\) and \\((\\sqrt{4})^2 = 4\\) are true statements. Why does R tell us the first statement is false? What we’re running into here are the limits of computer precision. A computer can’t represent \\(\\sqrt 10\\) exactly, whereas \\(\\sqrt 4\\) can be exactly represented. Precision in numerical computing is a complex subject and a detailed discussion is beyond the scope of this course. However, it’s important to be aware of this limitation (this limitation is true of any programming language, not just R). To test “near equality” R provides a function called all.equal(). This function takes two inputs – the numerical values to be compared – and returns TRUE if their values are equal up to a certain level of tolerance (defined by the built-in numerical precision of your computer). all.equal(10, sqrt(10)^2) ## [1] TRUE Here’s another example where the simple equality operator returns an unexpected result, but all.equal() produces the comparison we’re likely after. sin(pi) == 0 ## [1] FALSE all.equal(sin(pi), 0) ## [1] TRUE 2.7.2.1 Logical operators Logical values support Boolean operations, like logical negation (“not”), “and,” “or,” “xor,” etc. This is illustrated below: !TRUE # logical negation -- reads as &quot;not x&quot; ## [1] FALSE TRUE &amp; FALSE # AND: are x and y both TRUE? ## [1] FALSE TRUE | FALSE # OR: are either x or y TRUE? ## [1] TRUE xor(TRUE,FALSE) # XOR: is either x or y TRUE, but not both? ## [1] TRUE The function isTRUE can be useful for evaluating the state of a variable: x &lt;- sample(1:10, 1) # sample a random number in the range 1 to 10 isTRUE(x &gt; 5) # was the random number picked greater than 5? ## [1] FALSE 2.7.3 Character strings Character strings (“character”) represent single textual characters or a longer sequence of characters. They are created by enclosing the characters in text either single our double quotes. typeof(&quot;abc&quot;) # double quotes ## [1] &quot;character&quot; typeof(&#39;abc&#39;) # single quotes ## [1] &quot;character&quot; Character strings have a length, which can be found using the nchar function: first.name &lt;- &quot;jasmine&quot; nchar(first.name) ## [1] 7 last.name &lt;- &#39;smith&#39; nchar(last.name) ## [1] 5 There are a number of built-in functions for manipulating character strings. Here are some of the most common ones. 2.7.3.1 Joining strings The paste() function joins two characters strings together: paste(first.name, last.name) # join two strings ## [1] &quot;jasmine smith&quot; paste(&quot;abc&quot;, &quot;def&quot;) ## [1] &quot;abc def&quot; Notice that paste() adds a space between the strings? If we didn’t want the space we can call the paste() function with an optional argument called sep (short for separator) which specifies the character(s) that are inserted between the joined strings. paste(&quot;abc&quot;, &quot;def&quot;, sep = &quot;&quot;) # join with no space; &quot;&quot; is an empty string ## [1] &quot;abcdef&quot; paste0(&quot;abc&quot;, &quot;def&quot;) # an equivalent function with no space in newer version of R ## [1] &quot;abcdef&quot; paste(&quot;abc&quot;, &quot;def&quot;, sep = &quot;|&quot;) # join with a vertical bar ## [1] &quot;abc|def&quot; 2.7.3.2 Splitting strings The strsplit() function allows us to split a character string into substrings according to matches to a specified split string (see ?strsplit for details). For example, we could break a sentence into it’s constituent words as follows: sentence &lt;- &quot;Call me Ishmael.&quot; words &lt;- strsplit(sentence, &quot; &quot;) # split on space words ## [[1]] ## [1] &quot;Call&quot; &quot;me&quot; &quot;Ishmael.&quot; Notice that strsplit() is the reverse of paste(). 2.7.3.3 Substrings The substr() function allows us to extract a substring from a character object by specifying the first and last positions (indices) to use in the extraction: substr(&quot;abcdef&quot;, 2, 5) # get substring from characters 2 to 5 ## [1] &quot;bcde&quot; substr(first.name, 1, 3) # get substring from characters 1 to ## [1] &quot;jas&quot; 2.8 Packages Packages are libraries of R functions and data that provide additional capabilities and tools beyond the standard library of functions included with R. Hundreds of people around the world have developed packages for R that provide functions and related data structures for conducting many different types of analyses. Throughout this course you’ll need to install a variety of packages. Here I show the basic procedure for installing new packages from the console as well as from the R Studio interface. 2.8.1 Installing packages from the console The function install.packages() provides a quick and conveniet way to install packages from the R console. 2.8.2 Install the tidyverse package To illustrate the use of install.packages(), we’ll install a collection of packages (a “meta-package”) called the tidyverse. Here’s how to install the tidyverse meta-package from the R console: install.packages(&quot;tidyverse&quot;, dependencies = TRUE) The first argument to install.packages gives the names of the package we want to install. The second argument, dependencies = TRUE, tells R to install any additional packages that tidyverse depends on. 2.8.3 Installing packages from the RStudio dialog You can also install packages using a graphical dialog provided by RStudio. To do so pick the Packages tab in RStudio, and then click the Install button. Figure 2.2: The Packages tab in RStudio In the packages entry box you can type the name of the package you wish to install. Let’s install another useful package called “stringr.” Type the package name in the “Packages” field, make sure the “Install dependencies” check box is checked, and then press the “Install” button. Figure 2.3: Package Install Dialog 2.8.4 Loading packages with the library() function Once a package is installed on your computer, the package can be loaded into your R session using the library function. To insure our previous install commands worked correctly, let’s load one of the packages we just installed. library(tidyverse) Since the tidyverse pacakge is a “meta-package” it provides some additional info about the sub-packages that got loaded. When you load tidyverse, you will also see a message about “Conflicts” as several of the functions provided in the dplyr package (a sub-package in tidyverse) conflict with names of functions provided by the “stats” package which usually gets automically loaded when you start R. The conflicting funcdtions are filter and lag. The conflicting functions in the stats package are lag and filter which are used in time series analysis. The dplyr functions are more generally useful. Furthermore, if you need these masked functions you can still access them by prefacing the function name with the name of the package (e.g. stats::filter). We will use the “tidyverse” package for almost every class session and assignment in this class. Get in the habit of including the library(tidyverse) statement in all of your R documents. 2.9 The R Help System R comes with fairly extensive documentation and a simple help system. You can access HTML versions of the R documentation under the Help tab in Rstudio. The HTML documentation also includes information on any packages you’ve installed. Take a few minutes to browse through the R HTML documentation. In addition to the HTML documentation there is also a search box where you can enter a term to search on (see red arrow in figure below). Figure 2.4: The RStudio Help tab 2.9.1 Getting help from the console In addition to getting help from the RStudio help tab, you can directly search for help from the console. The help system can be invoked using the help function or the ? operator. help(&quot;log&quot;) ?log If you are using RStudio, the help results will appear in the “Help” tab of the Files/Plots/Packages/Help/Viewer (lower right window by default). What if you don’t know the name of the function you want? You can use the help.search() function. help.search(&quot;log&quot;) In this case help.search(\"log\") returns all the functions with the string log in them. For more on help.search type ?help.search. Other useful help related functions include apropos() and example(), vignette(). apropos returns a list of all objects (including variable names and function names) in the current session that match the input string. apropos(&quot;log&quot;) ## [1] &quot;as.data.frame.logical&quot; &quot;as.logical&quot; &quot;as.logical.factor&quot; ## [4] &quot;dlogis&quot; &quot;is.logical&quot; &quot;log&quot; ## [7] &quot;log10&quot; &quot;log1p&quot; &quot;log2&quot; ## [10] &quot;logb&quot; &quot;Logic&quot; &quot;logical&quot; ## [13] &quot;logLik&quot; &quot;loglin&quot; &quot;plogis&quot; ## [16] &quot;qlogis&quot; &quot;rlogis&quot; &quot;SSlogis&quot; example() provides examples of how a function is used. example(log) ## ## log&gt; log(exp(3)) ## [1] 3 ## ## log&gt; log10(1e7) # = 7 ## [1] 7 ## ## log&gt; x &lt;- 10^-(1+2*1:9) ## ## log&gt; cbind(x, log(1+x), log1p(x), exp(x)-1, expm1(x)) ## x ## [1,] 1e-03 9.995003e-04 9.995003e-04 1.000500e-03 1.000500e-03 ## [2,] 1e-05 9.999950e-06 9.999950e-06 1.000005e-05 1.000005e-05 ## [3,] 1e-07 1.000000e-07 1.000000e-07 1.000000e-07 1.000000e-07 ## [4,] 1e-09 1.000000e-09 1.000000e-09 1.000000e-09 1.000000e-09 ## [5,] 1e-11 1.000000e-11 1.000000e-11 1.000000e-11 1.000000e-11 ## [6,] 1e-13 9.992007e-14 1.000000e-13 9.992007e-14 1.000000e-13 ## [7,] 1e-15 1.110223e-15 1.000000e-15 1.110223e-15 1.000000e-15 ## [8,] 1e-17 0.000000e+00 1.000000e-17 0.000000e+00 1.000000e-17 ## [9,] 1e-19 0.000000e+00 1.000000e-19 0.000000e+00 1.000000e-19 The vignette() function gives longer, more detailed documentation about libraries. Not all libraries include vignettes, but for those that do it’s usually a good place to get started. For example, the stringr package (which we installed above) includes a vignette. To read it’s vignette, type the following at the console vignette(&quot;stringr&quot;) "],["r-markdown-and-r-notebooks.html", "Chapter 3 R Markdown and R Notebooks 3.1 R Notebooks 3.2 Creating an R Notebook 3.3 The default R Notebook template 3.4 Code and Non-code blocks 3.5 Running a code chunk 3.6 Running all code chunks above 3.7 “Knitting” R Markdown to HTML 3.8 Sharing your reproducible R Notebook", " Chapter 3 R Markdown and R Notebooks RStudio comes with a useful set of tools, collectively called R Markdown, for generating “literate” statistical analyses. The idea behind literate statistical computing is that we should try to carry out our analyses in a manner that is transparent, self-explanatory, and reproducible. Literate statistical computing helps to ensure your research is reproducible because: The steps of your analyses are explicitly described, both as written text and the code and function calls used. Analyses can be more easily checked for correctness and reproduced from your literate code. Your literate code can serve as a template for future analyses, saving you time and the trouble of remembering all the gory details. As we’ll see, R Markdown will allow us to produce statistical documents that integrate prose, code, figures, and nicely formatted mathematics so that we can share and explain our analyses to others. Sometimes those “others” are advisors, supervisors, or collaborators; sometimes the “other” is you six months from now. For the purposes of this class, you will be asked to complete problem sets in the form of R Markdown documents. R Markdown documents are written in a light-weight markup language called Markdown. Markdown provides simple plain text “formatting” commands for specifying the structured elements of a document. Markdown was invented as a lightweight markup language for creating web pages and blogs, and has been adopted to a variety of different purposes. This chaptern provides a brief introduction to the capabilities of R Markdown. For more complete details, including lots of examples, see the R Markdown Website. 3.1 R Notebooks We’re going to create a type of R Markdown document called an “R Notebook.” The R Notebook Documentation describes R Notebooks as so: “An R Notebook is an R Markdown document with code chunks that can be executed independently and interactively, with output visible immediately beneath the input.” 3.2 Creating an R Notebook To create an R Notebook select File &gt; New File &gt; R Notebook from the files menu in RStudio. Figure 3.1: Using the File menu to create a new R Notebook. 3.3 The default R Notebook template The standard template that RStudio creates for you includes a header section like the following where you can specify document properties such as the title, author, and change the look and feel of the generated HTML document. --- title: &quot;R Notebook&quot; output: html_notebook --- The header is followed by several example sections that illustrate a few of the capabilities of R Markdown. Delete these and replace them with your own code as necessary. 3.4 Code and Non-code blocks R Markdown documents are divided into code blocks (also called “chunks”) and non-code blocks. Code blocks are sets of R commands that will be evalauted when the R Markdown document is run or “knitted” (see below). Non-code blocks include explanatory text, embedded images, etc. The default notebook template includes both code and non-code blocks. 3.4.1 Non-code blocks The first bit of text in the default notebook template is a non-code block that tells you how to use the notebook: This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. The text of non-code blocks can include lightweight markup information that can be used to format HTML or PDF output generated from the R Markdown document. Here are some examples: # Simple textual formatting This is a paragraph with plain text. Nothing fancy will happen here. This is a second paragraph with *italic*, **bold**, and `verbatim` text. # Lists ## Bullet points lists This is a list with bullet points: * Item a * Item b * Item c ## Numbered lists This is a numbered list: 1. Item 1 #. Item 2 #. Item 3 ## Mathematics R Markdown supports mathematical equations, formatted according to LaTeX conventions. Dollar signs ($) are used to offset mathematics like so: $x^2 + y^2 = z^2$. Notice from the example above that R Markdown supports LaTeX style formatting of mathematical equations. For example, $x^2 + y^2 = z^2$ appears as \\(x^2 + y^2 = z^2\\). 3.4.2 Code blocks Code blocks are delimited by matching sets of three backward ticks (```). Everything within a code block is interpretted as an R command and is evaluated by the R interpretter. Here’s the first code block in the default notebook template: ``` {r} plot(cars) ``` 3.5 Running a code chunk You can run a single code block by clicking the small green “Run” button in the upper right hand corner of the code block as shown in the image below. Figure 3.2: Click the Run button to execute a code chunk. If you click this button the commands within this code block are executed, and any generated output is shown below the code block. Try running the first code block in the default template now. After the code chunk is executed you should see a plot embedded in your R Notebook as shown below: Figure 3.3: An R Notebook showing an embedded plot after executing a code chunk. 3.6 Running all code chunks above Next to the “Run” button in each code chunk is a button for “Run all chunks above” (see figure below). This is useful when the code chunk you’re working on depends on calculations in earlier code chunks, and you want to evaluated those earlier code chunks prior to running the focal code chunk. Figure 3.4: Use the ‘Run all chunks above’ button to evaluate all previous code chunks. 3.7 “Knitting” R Markdown to HTML Save your R Notebook as first_rnotebook.Rmd (RStudio will automatically add the .Rmd extension so you don’t need to type it). You can generate an HTML version of your notebook by clicking the “Preview” menu on the Notebook taskbar and then choosing “Knit to HTML” (see image below). Figure 3.5: Use the ‘Knit to HTML’ menu to generate HTML output from your R Notebook When an RMarkdown document is “knit,” all of the code and non-code blocks are executed in a “clean” environment, in order from top to bottom. An output file is generated (HTML or one of the other available output types) that shows the results of executing the notebook. By default RStudio will pop-up a window showing you the HTML output you generated. Knitting a document is a good way to make sure your analysis is reproducible. If your code compiles correctly when the document is knit, and produces the expected output, there’s a good chance that someone else will be able to reproduce your analyses independently starting with your R Notebook document (after accounting for differences in file locations). 3.8 Sharing your reproducible R Notebook To share your R Notebook with someone else you just need to send them the source R Markdown file (i.e. the file with the .Rmd extension). Assuming they have access to the same source data, another user should be able to open the notebook file in RStudio and regenerate your analyses by evaluating the individual code chunks or knitting the document. In this course you will be submitting homework assignments in the form of R Notebook markdown files. "],["data-structures.html", "Chapter 4 Data structures 4.1 Vectors 4.2 Lists 4.3 Data frames", " Chapter 4 Data structures In computer science, the term “data structure” refers to the ways that data are stored, retrieved, and organized in a computer’s memory. Common examples include lists, hash tables (also called dictionaries), sets, queues, and trees. Different types of data structures are used to support different types of operations on data. In R, the three basic data structures are vectors, lists, and data frames. 4.1 Vectors Vectors are the core data structure in R. Vectors store an ordered lists of items, all of the same type (i.e. the data in a vector are “homogenous” with respect to their type). The simplest way to create a vector at the interactive prompt is to use the c() function, which is short hand for “combine” or “concatenate.” x &lt;- c(2,4,6,8) # create a vector, assignn it the variable name `x` x ## [1] 2 4 6 8 Vectors in R always have a type (accessed with the typeof() function) and a length (accessed with the length() function). length(x) ## [1] 4 typeof(x) ## [1] &quot;double&quot; Vectors don’t have to be numerical; logical and character vectors work just as well. y &lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE) y ## [1] TRUE TRUE FALSE TRUE FALSE FALSE typeof(y) ## [1] &quot;logical&quot; length(y) ## [1] 6 z &lt;- c(&quot;How&quot;, &quot;now&quot;, &quot;brown&quot;, &quot;cow&quot;) z ## [1] &quot;How&quot; &quot;now&quot; &quot;brown&quot; &quot;cow&quot; typeof(z) ## [1] &quot;character&quot; length(z) ## [1] 4 You can also use c() to concatenate two or more vectors together. x &lt;- c(2, 4, 6, 8) y &lt;- c(1, 3, 5, 7, 9) # create another vector, labeled y xy &lt;- c(x,y) # combine two vectors xy ## [1] 2 4 6 8 1 3 5 7 9 z &lt;- c(pi/4, pi/2, pi, 2*pi) xyz &lt;- c(x, y, z) # combine three vectors xyz ## [1] 2.0000000 4.0000000 6.0000000 8.0000000 1.0000000 3.0000000 5.0000000 ## [8] 7.0000000 9.0000000 0.7853982 1.5707963 3.1415927 6.2831853 4.1.1 Vector Arithmetic The basic R arithmetic operations work on numeric vectors as well as on single numbers (in fact, behind the scenes in R single numbers are vectors!). x &lt;- c(2, 4, 6, 8, 10) x * 2 # multiply each element of x by 2 ## [1] 4 8 12 16 20 x - pi # subtract pi from each element of x ## [1] -1.1415927 0.8584073 2.8584073 4.8584073 6.8584073 y &lt;- c(0, 1, 3, 5, 9) x + y # add together each matching element of x and y ## [1] 2 5 9 13 19 x * y # multiply each matching element of x and y ## [1] 0 4 18 40 90 x/y # divide each matching element of x and y ## [1] Inf 4.000000 2.000000 1.600000 1.111111 Basic numerical functions operate element-wise on numerical vectors: sin(x) ## [1] 0.9092974 -0.7568025 -0.2794155 0.9893582 -0.5440211 cos(x * pi) ## [1] 1 1 1 1 1 log(x) ## [1] 0.6931472 1.3862944 1.7917595 2.0794415 2.3025851 4.1.2 Vector recycling When vectors are not of the same length R “recycles” the elements of the shorter vector to make the lengths conform. x &lt;- c(2, 4, 6, 8, 10) length(x) ## [1] 5 z &lt;- c(1, 4, 7, 11) length(z) ## [1] 4 x + z ## [1] 3 8 13 19 11 In the example above z was treated as if it was the vector (1, 4, 7, 11, 1). Recycling can be useful but it can also be a subtle source of errors. Notice that R provides warning messages when recycling is being applied. Make sure to pay attention to such messages when debugging your code. 4.1.3 Simple statistical functions for numeric vectors Now that we’ve introduced vectors as the simplest data structure for holding collections of numerical values, we can introduce a few of the most common statistical functions that operate on such vectors. First let’s create a vector to hold our sample data of interest. Here I’ve taken a random sample of the lengths of the last names of students enrolled in Bio 723 during Spring 2018. len.name &lt;- c(7, 7, 6, 2, 9, 9, 7, 4, 10, 5) Some common statistics of interest include minimum, maximum, mean, median, variance, and standard deviation: sum(len.name) ## [1] 66 min(len.name) ## [1] 2 max(len.name) ## [1] 10 mean(len.name) ## [1] 6.6 median(len.name) ## [1] 7 var(len.name) # variance ## [1] 6.044444 sd(len.name) # standard deviation ## [1] 2.458545 The summary() function applied to a vector of doubles produce a useful table of some of these key statistics: summary(len.name) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 5.25 7.00 6.60 8.50 10.00 4.1.4 Indexing Vectors Accessing the element of a vector is called “indexing.” Indexing is the process of specifying the numerical positions (indices) that you want to take access from the vector. For a vector of length \\(n\\), we can access the elements by the indices \\(1 \\ldots n\\). We say that R vectors (and other data structures like lists) are “one-indexed.” Many other programming languages, such as Python, C, and Java, use zero-indexing where the elements of a data structure are accessed by the indices \\(0 \\ldots n-1\\). Indexing errors are a common source of bugs. Indexing a vector is done by specifying the index in square brackets as shown below: x &lt;- c(2, 4, 6, 8, 10) length(x) ## [1] 5 x[1] # return the 1st element of x ## [1] 2 x[4] # return the 4th element of x ## [1] 8 Negative indices are used to exclude particular elements. x[-1] returns all elements of x except the first. x[-1] ## [1] 4 6 8 10 You can get multiple elements of a vector by indexing by another vector. In the example below, x[c(3,5)] returns the third and fifth element of x`. x[c(3,5)] ## [1] 6 10 Besides numerical indexing, R allows logical indexing which takes a vector of Booleans and returns the positions with TRUE values. x[c(TRUE, FALSE, TRUE, FALSE, FALSE)] #return 1st and 3rd elements but ignore 2nd, 4th and 5th ## [1] 2 6 4.1.5 Comparison operators applied to vectors When the comparison operators, such as “greater than” (&gt;), “less than or equal to” (&lt;=), equality (==), etc, are applied to numeric vectors, they return logical vectors: x &lt;- c(2, 4, 6, 8, 10, 12) x &lt; 8 # returns TRUE for all elements lass than 8 ## [1] TRUE TRUE TRUE FALSE FALSE FALSE Here’s a fancier example: x &gt; 4 &amp; x &lt; 10 # greater than 4 AND less than 10 ## [1] FALSE FALSE TRUE TRUE FALSE FALSE 4.1.6 Combining Indexing and Comparison of Vectors A very powerful feature of R is the ability to combine the comparison operators (which return TRUE or FALSE values) with indexing. This facilitates data filtering and subsetting. Here’s an example: x &lt;- c(2, 4, 6, 8, 10) x[x &gt; 5] ## [1] 6 8 10 In the first example we retrieved all the elements of x that are larger than 5 (read as “x where x is greater than 5”). Notice how we got back all the elements where the statement in the brackets was TRUE. You can string together comparisons for more complex filtering. x[x &lt; 4 | x &gt; 8] # less than four OR greater than 8 ## [1] 2 10 In the second example we retrieved those elements of x that were smaller than four or greater than six. Combining indexing and comparison is a concept which we’ll use repeatedly in this course. 4.1.7 Vector manipulation You can combine indexing with assignment to change the elements of a vectors: x &lt;- c(2, 4, 6, 8, 10) x[2] &lt;- -4 x ## [1] 2 -4 6 8 10 You can also use indexing vectors to change multiple values at once: x &lt;- c(2, 4, 6, 8, 10) x[c(1, 3, 5)] &lt;- 6 x ## [1] 6 4 6 8 6 Using logical vectors to manipulate the elements of a vector also works: x &lt;- c(2, 4, 6, 8, 10) x[x &gt; 5] = 5 # truncate all values to have max value 5 x ## [1] 2 4 5 5 5 4.1.8 Vectors from regular sequences There are a variety of functions for creating regular sequences in the form of vectors. 1:10 # create a vector with the integer values from 1 to 10 ## [1] 1 2 3 4 5 6 7 8 9 10 20:11 # a vector with the integer values from 20 to 11 ## [1] 20 19 18 17 16 15 14 13 12 11 seq(1, 10) # like 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 seq(1, 10, by = 2) # 1:10, in steps of 2 ## [1] 1 3 5 7 9 seq(2, 4, by = 0.25) # 2 to 4, in steps of 0.25 ## [1] 2.00 2.25 2.50 2.75 3.00 3.25 3.50 3.75 4.00 4.1.9 Additional functions for working with vectors The function unique() returns the unique items in a vector: x &lt;- c(5, 2, 1, 4, 6, 9, 8, 5, 7, 9) unique(x) ## [1] 5 2 1 4 6 9 8 7 rev() returns the items in reverse order (without changing the input vector): y &lt;- rev(x) y ## [1] 9 7 5 8 9 6 4 1 2 5 x # x is still in original order ## [1] 5 2 1 4 6 9 8 5 7 9 There are a number of useful functions related to sorting. Plain sort() returns a new vector with the items in sorted order: sorted.x &lt;- sort(x) # returns items of x sorted sorted.x ## [1] 1 2 4 5 5 6 7 8 9 9 x # but x remains in its unsorted state ## [1] 5 2 1 4 6 9 8 5 7 9 The related function order() gives the indices which would rearrange the items into sorted order: order(x) ## [1] 3 2 4 1 8 5 9 7 6 10 order() can be useful when you want to sort one list by the values of another: students &lt;- c(&quot;fred&quot;, &quot;tabitha&quot;, &quot;beatriz&quot;, &quot;jose&quot;) class.ranking &lt;- c(4, 2, 1, 3) students[order(class.ranking)] # get the students sorted by their class.ranking ## [1] &quot;beatriz&quot; &quot;tabitha&quot; &quot;jose&quot; &quot;fred&quot; any() and all(), return single boolean values based on a specified comparison provided as an argument: y &lt;- c(2, 4, 5, 6, 8) any(y &gt; 5) # returns TRUE if any of the elements are TRUE ## [1] TRUE all(y &gt; 5) # returns TRUE if all of the elements are TRUE ## [1] FALSE which() returns the indices of the vector for which the input is true: which(y &gt; 5) ## [1] 4 5 4.2 Lists R lists are like vectors, but unlike a vector where all the elements are of the same type, the elements of a list can have arbitrary types (even other lists). Lists are a powerful data structure for organizing information, because there are few constraints on the shape or types of the data included in a list. Lists are easy to create: l &lt;- list(&#39;Bob&#39;, pi, 10) Note that lists can contain arbitrary data. Lists can even contain other lists: l &lt;- list(&#39;Bob&#39;, pi, 10, list(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;, &quot;qux&quot;)) Lists are displayed with a particular format, distinct from vectors: l ## [[1]] ## [1] &quot;Bob&quot; ## ## [[2]] ## [1] 3.141593 ## ## [[3]] ## [1] 10 ## ## [[4]] ## [[4]][[1]] ## [1] &quot;foo&quot; ## ## [[4]][[2]] ## [1] &quot;bar&quot; ## ## [[4]][[3]] ## [1] &quot;baz&quot; ## ## [[4]][[4]] ## [1] &quot;qux&quot; In the example above, the correspondence between the list and its display is obvious for the first three items. The fourth element may be a little confusing at first. Remember that the fourth item of l was another list. So what’s being shown in the output for the fourth item is the nested list. An alternative way to display a list is using the str() function (short for “structure”). str() provides a more compact representation that also tells us what type of data each element is: str(l) ## List of 4 ## $ : chr &quot;Bob&quot; ## $ : num 3.14 ## $ : num 10 ## $ :List of 4 ## ..$ : chr &quot;foo&quot; ## ..$ : chr &quot;bar&quot; ## ..$ : chr &quot;baz&quot; ## ..$ : chr &quot;qux&quot; 4.2.1 Length and type of lists Like vectors, lists have length: length(l) ## [1] 4 But the type of a list is simply “list,” not the type of the items within the list. This makes sense because lists are allowed to be heterogeneous (i.e. hold data of different types). typeof(l) ## [1] &quot;list&quot; 4.2.2 Indexing lists Lists have two indexing operators. Indexing a list with single brackets, like we did with vectors, returns a new list containing the element at index \\(i\\). Lists also support double bracket indexing (x[[i]]) which returns the bare element at index \\(i\\) (i.e. the element without the enclosing list). This is a subtle but important point so make sure you understand the difference between these two forms of indexing. 4.2.2.1 Single bracket list indexing First, let’s demonstrate single bracket indexing of the lists l we created above. l[1] # single brackets, returns list(&#39;Bob&#39;) ## [[1]] ## [1] &quot;Bob&quot; typeof(l[1]) # notice the list type ## [1] &quot;list&quot; When using single brackets, lists support indexing with ranges and numeric vectors: l[3:4] ## [[1]] ## [1] 10 ## ## [[2]] ## [[2]][[1]] ## [1] &quot;foo&quot; ## ## [[2]][[2]] ## [1] &quot;bar&quot; ## ## [[2]][[3]] ## [1] &quot;baz&quot; ## ## [[2]][[4]] ## [1] &quot;qux&quot; l[c(1, 3, 5)] ## [[1]] ## [1] &quot;Bob&quot; ## ## [[2]] ## [1] 10 ## ## [[3]] ## NULL 4.2.2.2 Double bracket list indexing If double bracket indexing is used, the object at the given index in a list is returned: l[[1]] # double brackets, return plain &#39;Bob&#39; ## [1] &quot;Bob&quot; typeof(l[[1]]) # notice the &#39;character&#39; type ## [1] &quot;character&quot; Double bracket indexing does not support multiple indices, but you can chain together double bracket operators to pull out the items of sublists. For example: # second item of the fourth item of the list l[[4]][[2]] ## [1] &quot;bar&quot; 4.2.3 Naming list elements The elements of a list can be given names when the list is created: p &lt;- list(first.name=&#39;Alice&#39;, last.name=&quot;Qux&quot;, age=27, years.in.school=10) You can retrieve the names associated with a list using the names function: names(p) ## [1] &quot;first.name&quot; &quot;last.name&quot; &quot;age&quot; &quot;years.in.school&quot; If a list has named elements, you can retrieve the corresponding elements by indexing with the quoted name in either single or double brackets. Consistent with previous usage, single brackets return a list with the corresponding named element, whereas double brackets return the bare element. For example, make sure you understand the difference in the output generated by these two indexing calls: p[&quot;first.name&quot;] ## $first.name ## [1] &quot;Alice&quot; p[[&quot;first.name&quot;]] ## [1] &quot;Alice&quot; 4.2.4 The $ operator Retrieving named elements of lists (and data frames as we’ll see), turns out to be a pretty common task (especially when doing interactive data analysis) so R has a special operator to make this more convenient. This is the $ operator, which is used as illustrated below: p$first.name # equivalent to p[[&quot;first.name&quot;]] ## [1] &quot;Alice&quot; p$age # equivalent to p[[&quot;age&quot;]] ## [1] 27 4.2.5 Changing and adding lists items Combining indexing and assignment allows you to change items in a list: suspect &lt;- list(first.name = &quot;unknown&quot;, last.name = &quot;unknown&quot;, aka = &quot;little&quot;) suspect$first.name &lt;- &quot;Bo&quot; suspect$last.name &lt;- &quot;Peep&quot; suspect[[3]] &lt;- &quot;LITTLE&quot; str(suspect) ## List of 3 ## $ first.name: chr &quot;Bo&quot; ## $ last.name : chr &quot;Peep&quot; ## $ aka : chr &quot;LITTLE&quot; By combining assignment with a new name or an index past the end of the list you can add items to a list: suspect$age &lt;- 17 # add a new item named age suspect[[5]] &lt;- &quot;shepardess&quot; # create an unnamed item at position 5 Be careful when adding an item using indexing, because if you skip an index an intervening NULL value is created: # there are only five items in the list, what happens if we # add a new item at position seven? suspect[[7]] &lt;- &quot;wanted for sheep stealing&quot; str(suspect) ## List of 7 ## $ first.name: chr &quot;Bo&quot; ## $ last.name : chr &quot;Peep&quot; ## $ aka : chr &quot;LITTLE&quot; ## $ age : num 17 ## $ : chr &quot;shepardess&quot; ## $ : NULL ## $ : chr &quot;wanted for sheep stealing&quot; 4.2.6 Combining lists The c (combine) function we introduced to create vectors can also be used to combine lists: list.a &lt;- list(&quot;little&quot;, &quot;bo&quot;, &quot;peep&quot;) list.b &lt;- list(&quot;has lost&quot;, &quot;her&quot;, &quot;sheep&quot;) list.c &lt;- c(list.a, list.b) list.c ## [[1]] ## [1] &quot;little&quot; ## ## [[2]] ## [1] &quot;bo&quot; ## ## [[3]] ## [1] &quot;peep&quot; ## ## [[4]] ## [1] &quot;has lost&quot; ## ## [[5]] ## [1] &quot;her&quot; ## ## [[6]] ## [1] &quot;sheep&quot; 4.2.7 Converting lists to vectors Sometimes it’s useful to convert a list to a vector. The unlist() function takes care of this for us. # a homogeneous list ex1 &lt;- list(2, 4, 6, 8) unlist(ex1) ## [1] 2 4 6 8 When you convert a list to a vector make sure you remember that vectors are homogeneous, so items within the new vector will be “coerced” to have the same type. # a heterogeneous list ex2 &lt;- list(2, 4, 6, c(&quot;bob&quot;, &quot;fred&quot;), list(1 + 0i, &#39;foo&#39;)) unlist(ex2) ## [1] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;bob&quot; &quot;fred&quot; &quot;1+0i&quot; &quot;foo&quot; Note that unlist() also unpacks nested vectors and lists as shown in the second example above. 4.3 Data frames Along with vectors and lists, data frames are one of the core data structures when working in R. A data frame is essentially a list which represents a data table, where each column in the table has the same number of rows and every item in the a column has to be of the same type. Unlike standard lists, the objects (columns) in a data frame must have names. We’ve seen data frames previously, for example when we loaded data sets using the read_csv function. 4.3.1 Creating a data frame While data frames will often be created by reading in a data set from a file, they can also be created directly in the console as illustrated below: age &lt;- c(30, 26, 21, 29, 25, 22, 28, 24, 23, 20) sex &lt;- rep(c(&quot;M&quot;,&quot;F&quot;), 5) wt.in.kg &lt;- c(88, 76, 67, 66, 56, 74, 71, 60, 52, 72) df &lt;- data.frame(age = age, sex = sex, wt = wt.in.kg) Here we created a data frame with three columns, each of length 10. 4.3.2 Type and class for data frames Data frames can be thought of as specialized lists, and in fact the type of a data frame is “list” as illustrated below: typeof(df) ## [1] &quot;list&quot; To distinguish a data frame from a generic list, we have to ask about it’s “class.” class(df) # the class of our data frame ## [1] &quot;data.frame&quot; class(l) # compare to the class of our generic list ## [1] &quot;list&quot; The term “class” comes from a style/approach to programming called “object oriented programming.” We won’t go into explicit detail about how object oriented programming works in this class, though we will exploit many of the features of objects that have a particular class. 4.3.3 Length and dimension for data frames Applying the length() function to a data frame returns the number of columns. This is consistent with the fact that data frames are specialized lists: length(df) ## [1] 3 To get the dimensions (number of rows and columns) of a data frame, we use the dim() function. dim() returns a vector, whose first value is the number of rows and whose second value is the number of columns: dim(df) ## [1] 10 3 We can get the number of rows and columns individually using the nrow() and ncol() functions: nrow(df) # number of rows ## [1] 10 ncol(df) # number of columsn ## [1] 3 4.3.4 Indexing and accessing data frames Data frames can be indexed by either column index, column name, row number, or a combination of row and column numbers. 4.3.4.1 Single bracket indexing of the columns of a data frame The single bracket operator with a single numeric index returns a data frame with the corresponding column. df[1] # get the first column (=age) of the data frame ## # A tibble: 10 x 1 ## age ## &lt;dbl&gt; ## 1 30 ## 2 26 ## 3 21 ## 4 29 ## 5 25 ## 6 22 ## 7 28 ## 8 24 ## 9 23 ## 10 20 The single bracket operator with multiple numeric indices returns a data frame with the corresponding columns. df[1:2] # first two columns ## # A tibble: 10 x 2 ## age sex ## &lt;dbl&gt; &lt;chr&gt; ## 1 30 M ## 2 26 F ## 3 21 M ## 4 29 F ## 5 25 M ## 6 22 F ## 7 28 M ## 8 24 F ## 9 23 M ## 10 20 F df[c(1, 3)] # columns 1 (=age) and 3 (=wt) ## # A tibble: 10 x 2 ## age wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 30 88 ## 2 26 76 ## 3 21 67 ## 4 29 66 ## 5 25 56 ## 6 22 74 ## 7 28 71 ## 8 24 60 ## 9 23 52 ## 10 20 72 Column names can be substituted for indices when using the single bracket operator: df[&quot;age&quot;] ## # A tibble: 10 x 1 ## age ## &lt;dbl&gt; ## 1 30 ## 2 26 ## 3 21 ## 4 29 ## 5 25 ## 6 22 ## 7 28 ## 8 24 ## 9 23 ## 10 20 df[c(&quot;age&quot;, &quot;wt&quot;)] ## # A tibble: 10 x 2 ## age wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 30 88 ## 2 26 76 ## 3 21 67 ## 4 29 66 ## 5 25 56 ## 6 22 74 ## 7 28 71 ## 8 24 60 ## 9 23 52 ## 10 20 72 4.3.4.2 Single bracket indexing of the rows of a data frame To get specific rows of a data frame, we use single bracket indexing with an additional comma following the index. For example to get the first row a data frame we would do: df[1,] # first row ## # A tibble: 1 x 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 30 M 88 This syntax extends to multiple rows: df[1:2,] # first two rows ## # A tibble: 2 x 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 30 M 88 ## 2 26 F 76 df[c(1, 3, 5),] # rows 1, 3 and 5 ## # A tibble: 3 x 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 30 M 88 ## 2 21 M 67 ## 3 25 M 56 4.3.4.3 Single bracket indexing of both the rows and columns of a data frame Single bracket indexing of data frames extends naturally to retrieve both rows and columns simultaneously: df[1, 2] # first row, second column ## [1] &quot;M&quot; df[1:3, 2:3] # first three rows, columns 2 and 3 ## # A tibble: 3 x 2 ## sex wt ## &lt;chr&gt; &lt;dbl&gt; ## 1 M 88 ## 2 F 76 ## 3 M 67 # you can even mix numerical indexing (rows) with named indexing of columns df[5:10, c(&quot;age&quot;, &quot;wt&quot;)] ## # A tibble: 6 x 2 ## age wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 25 56 ## 2 22 74 ## 3 28 71 ## 4 24 60 ## 5 23 52 ## 6 20 72 4.3.4.4 Double bracket and $ indexing of data frames Whereas single bracket indexing of a data frame always returns a new data frame, double bracket indexing and indexing using the $ operator, returns vectors. df[[&quot;age&quot;]] ## [1] 30 26 21 29 25 22 28 24 23 20 typeof(df[[&quot;age&quot;]]) ## [1] &quot;double&quot; df$wt ## [1] 88 76 67 66 56 74 71 60 52 72 typeof(df$wt) ## [1] &quot;double&quot; 4.3.5 Logical indexing of data frames Logical indexing using boolean values works on data frames in much the same way it works on vectors. Typically, logical indexing of a data frame is used to filter the rows of a data frame. For example, to get all the subject in our example data frame who are older than 25 we could do: # NOTE: the comma after 25 is important to insure we&#39;re indexing rows! df[df$age &gt; 25, ] ## # A tibble: 4 x 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 30 M 88 ## 2 26 F 76 ## 3 29 F 66 ## 4 28 M 71 Similarly, to get all the individuals whose weight is between 60 and 70 kgs we could do: df[(df$wt &gt;= 60 &amp; df$wt &lt;= 70),] ## # A tibble: 3 x 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 21 M 67 ## 2 29 F 66 ## 3 24 F 60 4.3.6 Adding columns to a data frame Adding columns to a data frame is similar to adding items to a list. The easiest way to do so is using named indexing. For example, to add a new column to our data frame that gives the individuals ages in number of days, we could do: df[[&quot;age.in.days&quot;]] &lt;- df$age * 365 dim(df) ## [1] 10 4 "],["functions-and-control-flow-statements.html", "Chapter 5 Functions and control flow statements 5.1 Writing your own functions 5.2 Control flow statements 5.3 map and related tools", " Chapter 5 Functions and control flow statements 5.1 Writing your own functions So far we’ve been using a variety of built in functions in R. However the real power of a programming language is the ability to write your own functions. Functions are a mechanism for organizing and abstracting a set of related computations. We usually write functions to represent sets of computations that we apply frequently, or to represent some conceptually coherent set of manipulations to data. function() {expr} defines a simplest form of R functions. function() packs the following expressions in {} as a function so they can be assigned to and run by a name. For example, #assgin a function called &quot;PrintSignature&quot; PrintSignature &lt;- function() { paste(&quot;Dr. Paul Magwene at Duke University&quot;, date()) } PrintSignature() #run the function by its name ## [1] &quot;Dr. Paul Magwene at Duke University Wed Jan 27 16:35:40 2021&quot; The more general form of an R function is as follows: funcname &lt;- function(arg1, arg2) { # one or more expressions that operate on the fxn arguments # last expression is the object returned # or you can explicitly return an object } Arg1 and arg2 are function arguments that allow you to pass different values to the expressions every time you run the function. Function arguments are given in the parentheses after function and seperated by ,. You can add as many arguments as you want. To make this concrete, here’s an example where we define a function to calculate the area of a circle: area.of.circle &lt;- function(r){ return(pi * r^2) } Since R returns the value of the last expression in the function, the return call is optional and we could have simply written: area.of.circle &lt;- function(r){ pi * r^2 } Very short and concise functions are often written as a single line. In practice I’d probably write the above function as: area.of.circle &lt;- function(r) {pi * r^2} The area.of.circle function takes one argument, r, and calculates the area of a circle with radius r. Having defined the function we can immediately put it to use: area.of.circle(3) ## [1] 28.27433 radius &lt;- 4 area.of.circle(radius) ## [1] 50.26548 If you type a function name without parentheses R shows you the function’s definition. This works for built-in functions as well (thought sometimes these functions are defined in C code in which case R will tell you that the function is a .Primitive). 5.1.1 Function arguments Function arguments can specify the data that a function operates on or parameters that the function use, i.e., the “input” or “independent varialbe” of the function. Any R objects can be taken as function arguments, including bare numbers, vectors, lists, data.frames, names of assigned variables, and even other functions. Each argument only takes a single R object, so if you have complicated input or uncertain length of input, it’s better to design some arguments that take vectors or lists. Function arguments can be either required or optional. In the case of optional arguments, a default value is assigned if the argument is not given. Take for example the log function. If you examine the help file for the log function (type ?log now) you’ll see that it takes two arguments, refered to as x and base. The argument x represents the numeric vector you pass to the function and is a required argument (see what happens when you type log() without giving an argument). The argument base is optional. By default the value of base is \\(e = 2.71828\\ldots\\). Therefore by default the log function returns natural logarithms. If you want logarithms to a different base you can change the base argument as in the following examples: log(2) # log of 2, base e ## [1] 0.6931472 log(2,2) # log of 2, base 2 ## [1] 1 log(2, 4) # log of 2, base 4 ## [1] 0.5 Because base 2 and base 10 logarithms are fairly commonly used, there are convenient aliases for calling log with these bases. log2(8) ## [1] 3 log10(100) ## [1] 2 5.1.2 Writing functions with optional arguments To write a function that has an optional argument, you can simply specify the optional argument and its default value in the function definition as so: # a function to substitute missing values in a vector sub.missing &lt;- function(x, sub.value = -99){ x[is.na(x)] &lt;- sub.value return(x) } You can then use this function as so: m &lt;- c(1, 2, NA, 4) sub.missing(m, -999) # explicitly define sub.value ## [1] 1 2 -999 4 sub.missing(m, sub.value = -333) # more explicit syntax ## [1] 1 2 -333 4 sub.missing(m) # use default sub.value ## [1] 1 2 -99 4 m # notice that m wasn&#39;t modified within the function ## [1] 1 2 NA 4 Notice that when we called sub.missing with our vector m, the vector did not get modified in the function body. Rather a new vector, x was created within the function and returned. However, if you did the missing value subsitute outside of a function call, then the vector would be modified: n &lt;- c(1, 2, NA, 4) n[is.na(n)] &lt;- -99 n ## [1] 1 2 -99 4 5.1.3 Putting R functions in Scripts When you define a function at the interactive prompt and then close the interpreter your function definition will be lost. The simple way around this is to define your R functions in a script that you can than access at any time. In RStudio choose File &gt; New File &gt; R Script. This will bring up a blank editor window. Type your function(s) into the editor. Everything in this file will be interpretted as R code, so you should not use the code block notation that is used in Markdown notebooks. Save the source file in your R working directory with a name like myfxns.R. # functions defined in myfxns.R area.of.circle &lt;- function(r) {pi * r^2} area.of.rectangle &lt;- function(l, w) {l * w} area.of.triangle &lt;- function(b, h) {0.5 * b * h } Once your functions are in a script file you can make them accesible by using the source function, which reads the named file as input and evaluates any definitions or statements in the input file (See also the Source button in the R Studio GUI): source(&quot;myfxns.R&quot;) Having sourced the file you can now use your functions like so: radius &lt;- 3 len &lt;- 4 width &lt;- 5 base &lt;- 6 height &lt;- 7 area.of.circle(radius) ## [1] 28.27433 area.of.rectangle(len, width) ## [1] 20 area.of.triangle(base, height) ## [1] 21 Note that if you change the source file, such as correcting a mistake or adding a new function, you need to call the source function again to make those changes available. 5.2 Control flow statements Control flow statements control the order of execution of different pieces of code. They can be used to do things like make sure code is only run when certain conditions are met, to iterate through data structures, to repeat something until a specified event happens, etc. Control flow statements are frequently used when writing functions or carrying out complex data transformation. 5.2.1 if and if-else statements if and if-else blocks allow you to structure the flow of execution so that certain expressions are executed only if particular conditions are met. The general form of an if expression is: if (Boolean expression) { Code to execute if Boolean expression is true } Here’s a simple if expression in which we check whether a number is less than 0.5, and if so assign a values to a variable. x &lt;- runif(1) # runif generates a random number between 0 and 1 face &lt;- NULL # set face to a NULL value if (x &lt; 0.5) { face &lt;- &quot;heads&quot; } face ## NULL The else clause specifies what to do in the event that the if statement is not true. The combined general for of an if-else expression is: if (Boolean expression) { Code to execute if Boolean expression is true } else { Code to execute if Boolean expression is false } Our previous example makes more sense if we include an else clause. x &lt;- runif(1) if (x &lt; 0.5) { face &lt;- &quot;heads&quot; } else { face &lt;- &quot;tails&quot; } face ## [1] &quot;heads&quot; With the addition of the else statement, this simple code block can be thought of as simulating the toss of a coin. 5.2.1.1 if-else in a function Let’s take our “if-else” example above and turn it into a function we’ll call coin.flip. A literal re-interpretation of our previous code in the context of a function is something like this: # coin.flip.literal takes no arguments coin.flip.literal &lt;- function() { x &lt;- runif(1) if (x &lt; 0.5) { face &lt;- &quot;heads&quot; } else { face &lt;- &quot;tails&quot; } face } coin.flip.literal is pretty long for what it does — we created a temporary variable x that is only used once, and we created the variable face to hold the results of our if-else statement, but then immediately returned the result. This is inefficient and decreases readability of our function. A much more compact implementation of this function is as follows: coin.flip &lt;- function() { if (runif(1) &lt; 0.5) { return(&quot;heads&quot;) } else { return(&quot;tails&quot;) } } Note that in our new version of coin.flip we don’t bother to create temporary the variables x and face and we immediately return the results within the if-else statement. 5.2.1.2 Multiple if-else statements When there are more than two possible outcomes of interest, multiple if-else statements can be chained together. Here is an example with three outcomes: x &lt;- sample(-5:5, 1) # sample a random integer between -5 and 5 if (x &lt; 0) { sign.x &lt;- &quot;Negative&quot; } else if (x &gt; 0) { sign.x &lt;- &quot;Positive&quot; } else { sign.x &lt;- &quot;Zero&quot; } sign.x ## [1] &quot;Negative&quot; 5.2.2 for loops A for statement iterates over the elements of a sequence (such as vectors or lists). A common use of for statements is to carry out a calculation on each element of a sequence (but see the discussion of map below) or to make a calculation that involves all the elements of a sequence. The general form of a for loop is: for (elem in sequence) { Do some calculations or Evaluate one or more expressions } As an example, say we wanted to call our coin.flip function multiple times. We could use a for loop to do so as follows: flips &lt;- c() # empty vector to hold outcomes of coin flips for (i in 1:20) { flips &lt;- c(flips, coin.flip()) # flip coin and add to our vector } flips ## [1] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; ## [10] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; ## [19] &quot;heads&quot; &quot;heads&quot; Let’s use a for loop to create a multi.coin.flip function thats accepts an optional argument n that specifies the number of coin flips to carry out: multi.coin.flip &lt;- function(n = 1) { # create an empty character vector of length n # it&#39;s more efficient to create an empty vector of the right # length than to &quot;grow&quot; a vector with each iteration flips &lt;- vector(mode=&quot;character&quot;, length=n) for (i in 1:n) { flips[i] &lt;- coin.flip() } flips } With this new definition, a single call of coin.flip returns a single outcome: multi.coin.flip() ## [1] &quot;tails&quot; And calling multi.coin.flip with a numeric argument returns multiple coin flips: multi.coin.flip(n=10) ## [1] &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; ## [10] &quot;tails&quot; 5.2.3 break statement A break statement allows you to exit a loop even if it hasn’t completed. This is useful for ending a control statement when some criteria has been satisfied. break statements are usually nested in if statements. In the following example we use a break statement inside a for loop. In this example, we pick random real numbers between 0 and 1, accumulating them in a vector (random.numbers). The for loop insures that we never pick more than 20 random numbers before the loop ends. However, the break statement allows the loop to end prematurely if the number picked is greater than 0.95. random.numbers &lt;- c() for (i in 1:20) { x &lt;- runif(1) random.numbers &lt;- c(random.numbers, x) if (x &gt; 0.95) { break } } random.numbers ## [1] 0.7993929 0.9054563 0.9547424 5.2.4 repeat loops A repeat loop will loop indefinitely until we explicitly break out of the loop with a break statement. For example, here’s an example of how we can use repeat and break to simulate flipping coins until we get a head: ct &lt;- 0 repeat { flip &lt;- coin.flip() ct &lt;- ct + 1 if (flip == &quot;heads&quot;){ break } } ct ## [1] 2 5.2.5 next statement A next satement allows you to halt the processing of the current iteration of a loop and immediately move to the next item of the loop. This is useful when you want to skip calculations for certain elements of a sequence: sum.not.div3 &lt;- 0 for (i in 1:20) { if (i %% 3 == 0) { # skip summing values that are evenly divisible by three next } sum.not.div3 &lt;- sum.not.div3 + i } sum.not.div3 ## [1] 147 5.2.6 while statements A while statement iterates as long as the condition statement it contains is true. In the following example, the while loop calls coin.flip until “heads” is the result, and keeps track of the number of flips. Note that this represents the same logic as the repeat-break example we saw earlier, but in a a more compact form. first.head &lt;- 1 while(coin.flip() == &quot;tails&quot;){ first.head &lt;- first.head + 1 } first.head ## [1] 1 5.2.7 ifelse The ifelse function is equivalent to a for-loop with a nested if-else statement. ifelse applies the specified test to each element of a vector, and returns different values depending on if the test is true or false. Here’s an example of using ifelse to replace NA elements in a vector with zeros. x &lt;- c(3, 1, 4, 5, 9, NA, 2, 6, 5, 4) newx &lt;- ifelse(is.na(x), 0, x) newx ## [1] 3 1 4 5 9 0 2 6 5 4 The equivalent for-loop could be written as: x &lt;- c(3, 1, 4, 5, 9, NA, 2, 6, 5, 4) newx &lt;- c() # create an empty vector for (elem in x) { if (is.na(elem)) { newx &lt;- c(newx, 0) # append zero to newx } else { newx &lt;- c(newx, elem) # append elem to newx } } newx ## [1] 3 1 4 5 9 0 2 6 5 4 The ifelse function is clearly a more compact and readable way to accomplish this. 5.3 map and related tools Another common situation is applying a function to every element of a list or vector. Again, we could use a for loop, but the map functions often are better alternatives. NOTE: map is a relative newcomer to R and must be loaded with the purrr package (purrr is loaded when we load tidyverse). Although base R has a complicated series of “apply” functions (apply, lapply, sapply, vapply, mapply), map provides similar functionality with a more consistent interface. We won’t use the apply functions in this class, but you may see them in older code. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.0.5 ✓ dplyr 1.0.3 ## ✓ tidyr 1.1.2 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.0 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 5.3.1 basic map Typically, map takes two arguments – a sequence (a vector, list, or data frame) and a function. It then applies the function to each element of the sequence, returning the results as a list. To illustrate map, let’s consider an example with a list of 2-vectors, where each vector gives the min and max values of some variable of interest for individuals in a sample (e.g. resting heart rate and maximum heart rate during exercise). We can use the map function to quickly generate the difference between the resting and maximum heart rates: heart.rates &lt;- list(bob = c(60, 120), fred = c(79, 150), jim = c(66, 110)) diff.fxn &lt;- function(x) {x[2] - x[1]} map(heart.rates, diff.fxn) ## $bob ## [1] 60 ## ## $fred ## [1] 71 ## ## $jim ## [1] 44 As a second example, here’s how we could use map to get the class of each object in a list: x &lt;- list(c(1,2,3), &quot;a&quot;, &quot;b&quot;, list(lead = &quot;Michael&quot;, keyboard = &quot;Jermaine&quot;)) map(x, class) ## [[1]] ## [1] &quot;numeric&quot; ## ## [[2]] ## [1] &quot;character&quot; ## ## [[3]] ## [1] &quot;character&quot; ## ## [[4]] ## [1] &quot;list&quot; 5.3.2 map_if and map_at map_if is a variant of map that takes a predicate function (a function that evaluates to TRUE or FALSE) to determine which elements of the input sequence are transformed by the map function. All elements of the sequence that do not meet the predicate are left un-transformed. Like map, map_if always returns a list. Here’s an example where we use map_if to apply the stringr::str_to_upper function to those columns of a data frame that are character vectors, and apply abs to obtain the absolute value of a numeric column: a &lt;- rnorm(6) b &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;) c &lt;- c(&quot;u&quot;, &quot;v&quot;, &quot;w&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;) df &lt;- data_frame(a, b, c) head(df) ## # A tibble: 6 x 3 ## a b c ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.874 a u ## 2 -1.13 b v ## 3 1.60 c w ## 4 0.597 d x ## 5 -1.50 e y ## 6 -0.512 f z df2 &lt;- map_if(df, is.character, str_to_upper) df2 &lt;- map_if(df2, is.numeric, abs) head(df2) ## $a ## [1] 0.8735154 1.1325669 1.5962971 0.5967004 1.4963274 0.5116802 ## ## $b ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; ## ## $c ## [1] &quot;U&quot; &quot;V&quot; &quot;W&quot; &quot;X&quot; &quot;Y&quot; &quot;Z&quot; Note that df2 is a list, not a data frame. We can convert df2 to a data frame df3, using the as_data_frame() function: # Next, create data frame df3 df3 &lt;- as_data_frame(df2) head(df3) ## # A tibble: 6 x 3 ## a b c ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.874 A U ## 2 1.13 B V ## 3 1.60 C W ## 4 0.597 D X ## 5 1.50 E Y ## 6 0.512 F Z Note that if our goal is to apply functions to the columns of a data frame, it may be easier with dplyr::mutate(): df4 &lt;- df %&gt;% as_tibble() %&gt;% mutate(a = abs(a), b = str_to_upper(b), c = str_to_upper(c)) head(df4) ## # A tibble: 6 x 3 ## a b c ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.874 A U ## 2 1.13 B V ## 3 1.60 C W ## 4 0.597 D X ## 5 1.50 E Y ## 6 0.512 F Z 5.3.3 mapping in parallel using map2 The map2 function applies a transformation function to two sequences in parallel. The following example illustrates this: first.names &lt;- c(&quot;John&quot;, &quot;Mary&quot;, &quot;Fred&quot;) last.names &lt;- c(&quot;Smith&quot;, &quot;Hernandez&quot;, &quot;Kidogo&quot;) map2(first.names, last.names, str_c, sep=&quot; &quot;) ## [[1]] ## [1] &quot;John Smith&quot; ## ## [[2]] ## [1] &quot;Mary Hernandez&quot; ## ## [[3]] ## [1] &quot;Fred Kidogo&quot; Note how we can specify arguments to the transformation function as additional arguments to map2 (i.e., the sep argument gets passed to str_c) 5.3.4 map variants that return vectors map, map_if, and map_at always return lists. The purrr library also has a series of map variants that return vectors: map_lgl (for logical vectors) map_chr (for character vectors) map_int (integer vectors) map_dbl (double vectors) # compare the outputs of map and map_chr a &lt;- map(letters[1:6], str_to_upper) str(a) ## List of 6 ## $ : chr &quot;A&quot; ## $ : chr &quot;B&quot; ## $ : chr &quot;C&quot; ## $ : chr &quot;D&quot; ## $ : chr &quot;E&quot; ## $ : chr &quot;F&quot; b &lt;- map_chr(letters[1:6], str_to_upper) str(b) # a vector ## chr [1:6] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; Here’s an example using map_dbl, where we create a data frame with three columns, and compute the median of each column: # Make data frame for analysis df &lt;- tibble(a = rnorm(100), b = rnorm(100),c = rnorm(100)) map_dbl(df, median) # median of each column of df ## a b c ## -0.10672923 -0.16857315 -0.03542569 "],["introduction-to-ggplot2.html", "Chapter 6 Introduction to ggplot2 6.1 Loading ggplot2 6.2 Example data set: Anderson’s Iris Data 6.3 Template for single layer plots in ggplot2 6.4 An aside about function arguments 6.5 Strip plots 6.6 Histograms 6.7 Faceting to depict categorical information 6.8 Density plots 6.9 Violin or Beanplot 6.10 Boxplots 6.11 Building complex visualizations with layers 6.12 Useful combination plots 6.13 ggplot layers can be assigned to variables 6.14 Adding titles and tweaking axis labels 6.15 ggplot2 themes 6.16 Other aspects of ggplots can be assigned to variables 6.17 Bivariate plots 6.18 Bivariate density plots 6.19 Combining Scatter Plots and Density Plots with Categorical Information 6.20 Density plots with fill 6.21 2D bin and hex plots 6.22 The cowplot package", " Chapter 6 Introduction to ggplot2 Pretty much any statistical plot can be thought of as a mapping between data and one or more visual representations. For example, in a scatter plot we map two ordered sets of numbers (the variables of interest) to points in the Cartesian plane (x,y-coordinates). The representation of data as points in a plane can be thought of as a type of geometric mapping. In a histogram, we divide the range of a variable of interest into bins, count the number of observations in each bin, and represent those counts as bars. The process of counting the data in bins is a type of statistical transformation (summing in this case), while the representation of the counts as bars is another example of a geometric mapping. Both types of plots can be further embellished with additional information, such as coloring the points or bars based on a categorical variable of interest, changing the shape of points, etc. These are examples of aesthetic mappings. An additional operation that is frequently useful is faceting (also called conditioning), in which a series of subplots are created to show particular subsets of the data. The package ggplot2 is based on a formalized approach for building statistical graphics as a combination of geometric mappings, aesthetic mappings, statistical transformations, and faceting (conditioning). In ggplot2, complex figures are built up by combining layers – where each layer includes a geometric mapping, an aesthetic mapping, and a statistical transformation – along with any desired faceting information. Many of the key ideas behind ggplot2 (and its predecessor,“ggplot”) are based on a book called “The Grammar of Graphics” (Leland Wilkinson, 1985). The “grammar of graphics” is the “gg” in the ggplot2 name. 6.1 Loading ggplot2 ggplot2 is one of the packages included in the tidyverse meta-package we installed during the previous class session (see the previous lecture notes for instruction if you have not installed tidyverse). If we load the tidyverse package, ggplot2 is automatically loaded as well. library(tidyverse) However if we wanted to we could load only ggplot2 as follows: library(ggplot2) # not necessary if we already loaded tidyverse 6.2 Example data set: Anderson’s Iris Data To illustrate ggplot2 we’ll use a dataset called iris. This data set was made famous by the statistician and geneticist R. A. Fisher who used it to illustrate many of the fundamental statistical methods he developed (Recall that Fisher was one of the key contributors to the modern synthesis in biology, reconciling evolution and genetics in the early 20th century). The data set consists of four morphometric measurements for specimens from three different iris species (Iris setosa, I. versicolor, and I. virginica). Use the R help to read about the iris data set (?iris). We’ll be using this data set repeatedly in future weeks so familiarize yourself with it. The iris data is included in a standard R package (datasets) that is made available automatically when you start up R. As a consequence we don’t need to explicitly load the iris data from a file. Let’s take a few minutes to explore this iris data set before we start generating plots: names(iris) # get the variable names in the dataset ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; dim(iris) # dimensions given as rows, columns ## [1] 150 5 head(iris) # can you figure out what the head function does? ## # A tibble: 6 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(iris) # what about the tail function? ## # A tibble: 6 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 6.7 3.3 5.7 2.5 virginica ## 2 6.7 3 5.2 2.3 virginica ## 3 6.3 2.5 5 1.9 virginica ## 4 6.5 3 5.2 2 virginica ## 5 6.2 3.4 5.4 2.3 virginica ## 6 5.9 3 5.1 1.8 virginica 6.3 Template for single layer plots in ggplot2 A basic template for building a single layer plot using ggplot2 is shown below. When creating a plot, you need to replace the text in brackets (e.g. &lt;DATA&gt;) with appropriate objects, functions, or arguments: # NOTE: this is pseudo-code. It will not run! ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) The base function ggplot() is responsible for creating the coordinate system in which the plot will be display. To this coordinate system we add a geometric mapping (called a “geom” for short) that specifies how data gets mapped into the coordinate system (e.g. points, bars, etc). Included as an input to the geom function is the aesthetic mapping function that specifies which variables to use in the geometric mapping (e.g. which variables to treat as the x- and y-coordinates), colors, etc. For example, using this template we can create a scatter plot that show the relationship between the variables Sepal.Width and Petal.Width. To do so we subsitute iris for &lt;DATA&gt;, geom_point for &lt;GEOM_FUNCTION&gt;, and x = Sepal.Width and y = Petal.Width for &lt;MAPPINGS&gt;. ggplot(data = iris) + geom_point(mapping = aes(x = Sepal.Width, y = Petal.Width)) If we were to translate this code block to English, we might write it as “Using the iris data frame as the source of data, create a point plot using each observeration’s Sepal.Width variable for the x-coordinate and the Petal.Width variable for the y-coordinate.” 6.4 An aside about function arguments The inputs to a function are also known as “arguments.” In R, when you call a function you can specify the arguments by keyword (i.e. using names specified in the function definition) or by position (i.e. the order of the inputs). In our bar plot above, we’re using using keyword arguments. For example, in the line ggplot(data = iris), iris is treated as the “data” argument. Similarly, in the second line, aes(x = Sepal.Width, y = Petal.Width) is the “mapping” argument to geom_bar. Note that aes is itself a function (see ?aes) that takes arguments that can be specified positionally or with keywords. If we wanted to, we could instead use position arguments when calling a function, by passing inputs to the function corresponding to the order they are specified in the function definition. For example, take a minute to read the documentation for the ggplot function (?ggplot). Near the top of the help page you’ll see a description of how the function is called under “Usage.” Reading the Usage section you’ll see that the the “data” argument is the first positional argument to ggplot. Similarly, if you read the docs for the geom_point function you’ll see that mapping is the first positional argument for that function. The equivalent of our previous example, but now using positional arguments is: ggplot(iris) + # note we dropped the &quot;data = &quot; part # note we dropped the &quot;mapping = &quot; part from the geom_point call geom_point(aes(x = Sepal.Width, y = Petal.Width)) The upside of using positional arguments is that it means less typing, which is useful when working interactively at the console (or in an R Notebok). The downside to using positional arguments is you need to remember or lookup the order of the arguments. Using positional arguments can also make your code less “self documenting” in the sense that it is less explicit about how the inputs are being treated. While the argument “x” is the first argument to the aes function, I chose to explicitly include the argument name to make it clear what variable I’m plotting on the x-axis. We will cover function arguments in greater detail a class session or two from now, when we learn how to write our own functions. 6.5 Strip plots One of the simplest visualizations of a continuous variable is to draw points along a number line, where each point represent the value of one of the observations. This is sometimes called a “strip plot.” First, we’ll use the geom_point function as shown below to generate a strip plot for the Sepal.Width variable in the iris data set. ggplot(data = iris) + geom_point(aes(x = Sepal.Width, y = 0)) 6.5.1 Jittering data There should have been 150 points plotted in the figure above (one for each of the iris plants in the data set), but visually it looks like only about 25 or 30 points are shown. What’s going on? If you examine the iris data, you’ll see that the all the measures are rounded to the nearest tenth of a centimer, so that there are a large number of observations with identical values of Sepal.Width. This is a limitation of the precision of measurements that was used when generating the data set. To provide a visual clue that there are multiple observations that share the same value, we can slightly “jitter” the values (randomly move points a small amount in either in the vertical or horizontal direction). Jittering is used solely to enhance visualization, and any statistical analyses you carry out would be based on the original data. When presenting your data to someone else, should note when you’ve used jittering so as not to misconvey the actual data. Jittering can be accomplished using geom_jitter, which is derived from geom_point: ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = 0), width = 0.05, height = 0, alpha = 0.25) The width and height arguments specify the maximum amount (as fractions of the data) to jitter the observed data points in the horizontal (width) and vertical (height) directions. Here we only jitter the data in the horizontal direction. The alpha argument controls the transparency of the points – the valid range of alpha values is 0 to 1, where 0 means completely transparent and 1 is completely opaque. Within a geom, arguments outside of the aes mapping apply uniformly across the visualization (i.e. they are fixed values). For example, setting `alpha = 0.25’ made all the points transparent. 6.5.2 Adding categorical information Recall that are three different species represented in the data: Iris setosa, I. versicolor, and I. virginica. Let’s see how to generate a strip plot that also includes a breakdown by species. ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species), width=0.05, height=0.1, alpha=0.5) That was easy! All we had to do was change the aesthetic mapping in geom_jitter, specifying “Species” as the y variable. I also added a little vertical jitter as well to better separate the points. Now we have a much better sense of the data. In particular it’s clear that the I. setosa specimens generally have wider sepals than samples from the other two species. Let’s tweak this a little by also adding color information, to further emphasize the distinct groupings. We can do this by adding another argument to the aesthetic mapping in geom_jitter. ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species, color=Species), width=0.05, height=0.1, alpha=0.5) 6.5.3 Rotating plot coordinates What if we wanted to rotate this plot 90 degrees, depicting species on the x-axis and sepal width on the y-axis. For this example, it would be easy to do this by simpling swapping the variables in the aes mapping argument. However an alternate way to do this is with a coordinate transformation function. Here we use coord_flip to flip the x- and y-axes: ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species, color=Species), width=0.05, height=0.1, alpha=0.5) + coord_flip() We’ll see other uses of coordinate transformations in later lectures. 6.6 Histograms Histograms are probably the most common way to depict univariate data. In a histogram rather than showing individual observations, we divide the range of the data into a set of bins, and use vertical bars to depict the number (frequency) of observations that fall into each bin. This gives a good sense of the intervals in which most of the observations are found. The geom, geom_histogram, takes care of both the geometric representation and the statistical transformations of the data necessary to calculate the counts in each binn. Here’s the simplest way to use geom_histogram: ggplot(iris) + geom_histogram(aes(x = Sepal.Width)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The default number of bins that geom_histogram uses is 30. For modest size data sets this is often too many bins, so it’s worth exploring how the histogram changes with different bin numbers: ggplot(iris) + geom_histogram(aes(x = Sepal.Width), bins = 10) ggplot(iris) + geom_histogram(aes(x = Sepal.Width), bins = 12) One important thing to note when looking at these histograms with different numbers of bins is that the number of bins used can change your perception of the data. For example, the number of peaks (modes) in the data can be very sensitive to the bin number as can the perception of gaps. 6.6.1 Variations on histograms when considering categorical data As before, we probably want to break the data down by species. Here we’re faced with some choices about how we depict that data. Do we generate a “stacked histogram” to where the colors indicate the number of observations in each bin that belong to each species? Do we generate side-by-side bars for each species? Or Do we generate separate histograms for each species, and show them overlapping? Stacked histograms are the default if we associate a categorical variable with the bar fill color: ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12) To get side-by-side bars, specify “dodge” as the position argument to geom_histogram. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12, position = &quot;dodge&quot;) If you want overlapping histograms, use position = \"identity\" instead. When generating overlapping histograms like this, you probably want to make the bars semi-transparent so you can can distinguish the overlapping data. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12, position = &quot;identity&quot;, alpha = 0.4) 6.7 Faceting to depict categorical information Yet another way to represent the histograms for the three species is to using faceting, the create subplots for each species. Faceting is the operation of subsetting the data with respect to a discrete or categorical variable of interest, and generating the same plot type for each subset. Here we use the “ncol” argument to the facet_wrap function to specify that the subplots should be drawn in a single vertical column to facilitate comparison of the distributions. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12) + facet_wrap(~Species, ncol = 1) 6.8 Density plots One shortcoming of histograms is that they are sensitive to the choice of bin margins and the number of bins. An alternative is a “density plot,” which you can think of as a smoothed version of a histogram. ggplot(iris) + geom_density(aes(x = Sepal.Width, fill = Species), alpha=0.25) Density plots still make some assumptions that affect the visualization, in particular a “smoothing bandwidth” (specified by the argument bw) which determines how course or granular the density estimation is. Note that the vertical scale on a density plot is no longer counts (frequency) but probability density. In a density plot, the total area under the plot adds up to one. Intervals in a density plot therefore have a probabilistic intepretation. 6.9 Violin or Beanplot A violin plot (sometimes called a bean plot) is closely related to a density plot. In fact you can think of a violin plot as a density plot rotated 90 degress and mirrored left/right. ggplot(iris) + geom_violin(aes(x = Species, y = Sepal.Width, color = Species, fill=Species), alpha = 0.25) 6.10 Boxplots Boxplots are another frequently used univariate visualization. Boxplots provide a compact summary of single variables, and are most often used for comparing distributions between groups. A standard box plot depicts five useful features of a set of observations: 1) the median (center most line); 2 and 3) the first and third quartiles (top and bottom of the box); 4) the whiskers of a boxplot extend from the first/third quartile to the highest value that is within 1.5 * IQR, where IQR is the inter-quartile range (distance between the first and third quartiles); 5) points outside of the whiskers are usually consider extremal points or outliers. There are many variants on box plots, particularly with respect to the “whiskers.” It’s always a good idea to be explicit about what a box plot you’ve created shows. ggplot(iris) + geom_boxplot(aes(x = Species, y = Sepal.Width, color = Species)) Boxplots are most commonly drawn with the cateogorical variable on the x-axis. 6.11 Building complex visualizations with layers All of our ggplot2 examples up to now have involved a single geom. We can think of geoms as “layers” of information in a plot. One of the powerful features of plotting useing ggplot2 is that it is trivial to combine layers to make more complex plots. The template for multi-layered plots is a simple extension of the single layer: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION1&gt;(mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION2&gt;(mapping = aes(&lt;MAPPINGS&gt;)) 6.12 Useful combination plots Boxplot or violin plots represent visual summaries/simplifications of the underlying data. This is useful but sometimes key information is lost in the process of summarizing. Combining these plots with a strip plot give you both the “birds eye view” as well as granular information. 6.12.1 Boxplot plus strip plot Here’s an example of combining box plots and strip plots: ggplot(iris) + # outlier.shape = NA suppresses the depiction of outlier points in the boxplot geom_boxplot(aes(x = Species, y = Sepal.Width), outlier.shape = NA) + # size sets the point size for the jitter plot geom_jitter(aes(x = Species, y = Sepal.Width), width=0.2, height=0.05, alpha=0.35, size=0.75) Note that I suppressed the plotting of outliers in geom_boxplot so as not to draw the same points twice (the individual data are drawn by geom_jitter). 6.12.2 Setting shared aesthetics The example above works well, but you might have noticed that there’s some repetition of code. In particular, we set the same aesthetic mapping in both geom_boxplot and geom_jitter. It turns out that creating layers that share some of the same aesthetic values is a common case. To deal with such cases, you can specify shared aesthetic mappings as an argument to the ggplot function and then set additional aesthetics specific to each layer in the individual geoms. Using this approach, our previous example can be written more compactly as follow. ggplot(iris, mapping = aes(x = Species, y = Sepal.Width)) + geom_boxplot(outlier.shape = NA) + # note how we specify a layer specific aesthetic in geom_jitter geom_jitter(aes(color = Species), width=0.2, height=0.05, alpha=0.5, size=0.75) 6.13 ggplot layers can be assigned to variables The function ggplot() returns a “plot object” that we can assign to a variable. The following example illustrates this: # create base plot object and assign to variable p # this does NOT draw the plot p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width)) In the code above we created a plot object and assigned it to the variable p. However, the plot wasn’t drawn. To draw the plot object we evaluate it as so: p # try to draw the plot object The code block above didn’t generate an image, because we haven’t added a geom to the plot to determine how our data should be drawn. We can add a geom to our pre-created plot object as so: # add a point geom to our base layer and draw the plot p + geom_boxplot() If we wanted to we could have assigned the geom to a variable as well: box.layer &lt;- geom_boxplot() p + box.layer In this case we don’t really gain anything by creating an intermediate variable, but for more complex plots or when considering different versions of a plot this can be very useful. 6.13.1 Violin plot plus strip plot Here is the principle of combining layers, applied to a combined violin plot + strip plot. Again, we set shared aesthetic mappings in ggplot function call and this time we assign individual layers of the plot to variables. p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width, color = Species)) violin.layer &lt;- geom_violin() jitter.layer &lt;- geom_jitter(width=0.15, height=0.05, alpha=0.5, size=0.75) p + violin.layer + jitter.layer # combined layers of plot and draw 6.14 Adding titles and tweaking axis labels ggplot2 automatically adds axis labels based on the variable names in the data frame passed to ggplot. Sometimes these are appropriate, but more presentable figures you’ll usually want to tweak the axis labs (e.g. adding units). The labs (short for labels) function allows you to do so, and also let’s you set a title for your plot. We’ll illustrate this by modifying our previous figure. Note that we save considerable amounts of re-typing since we had already assigned three of the plot layers to variables in the previous code block: p + violin.layer + jitter.layer + labs(x = &quot;Species&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Sepal Width Distributions for Three Iris Species&quot;) 6.15 ggplot2 themes By now you’re probably familiar with the default “look” of plots generated by ggplot2, in particular the ubiquitous gray background with a white grid. This default works fairly well in the context of RStudio notebooks and HTML output, but might not work as well for a published figure or a slide presentation. Almost every individual aspect of a plot can be tweaked, but ggplot2 provides an easier way to make consistent changes to a plot using “themes.” You can think of a theme as adding another layer to your plot. Themes should generally be applied after all the other graphical layers are created (geoms, facets, labels) so the changes they create affect all the prior layers. There are eight default themes included with ggplot2, which can be invoked by calling the corresponding theme functions: theme_gray, theme_bw, theme_linedraw, theme_light, theme_dark, theme_minimal, theme_classic, and theme_void (See http://ggplot2.tidyverse.org/reference/ggtheme.html for a visual tour of all the default themes) For example, let’s generate a boxplot using theme_bw which get’s rid of the gray background: # create another variable to hold combination of three previous # ggplot layers. I&#39;m doing this because I&#39;m going to keep re-using # the same plot in the following code blocks violin.plus.jitter &lt;- p + violin.layer + jitter.layer violin.plus.jitter + theme_bw() Another theme, theme_classic, remove the grid lines completely, and also gets rid of the top-most and right-most axis lines. violin.plus.jitter + theme_classic() 6.15.1 Further customization with ggplot2::theme In addition to the eight complete themes, there is a theme function in ggplot2 that allows you to tweak particular elements of a theme (see ?theme for all the possible options). For example, to tweak just the aspect ratio of a plot (the ratio of width to height), you can set the aspect.ratio argument in theme: violin.plus.jitter + theme_classic() + theme(aspect.ratio = 1) Theme related function calls can be combined to generate new themes. For example, let’s create a theme called my.theme by combining theme_classic with a call to theme: my.theme &lt;- theme_classic() + theme(aspect.ratio = 1) We can then apply this theme as so: violin.plus.jitter + my.theme 6.16 Other aspects of ggplots can be assigned to variables Plot objects, geoms and themes are not the only aspects of a figure that can be assigned to variables for later use. For example, we can create a label object: my.labels &lt;- labs(x = &quot;Species&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Sepal Width Distributions for Three Iris Species&quot;) Combining all of our variables as so, we generate our new plot: violin.plus.jitter + my.labels + my.theme 6.17 Bivariate plots Now we turn our attention to some useful representations of bivariate distributions. For the purposes of these illustrations I’m initially going to restrict my attention to just one of the three species represented in the iris data set – the I. setosa specimens. This allows us to introduce a vary useful base function called subset(). subset() will return subsets of a vector or data frames that meets the specified conditions. This can also be accomplished with conditional indexing but subset() is usually less verbose. # create a new data frame composed only of the I. setosa samples setosa.only &lt;- subset(iris, Species == &quot;setosa&quot;) In the examples that follow, I’m going to illustrate different ways of representing the same bivariate distribution – the joint distribution of Sepal Length and Sepal Width – over and over again. To avoid repitition, let’s assign the base ggplot layer to a variable as we did in our previous examples. We’ll also pre-create a label layer. setosa.sepals &lt;- ggplot(setosa.only, mapping = aes(x = Sepal.Length, y = Sepal.Width)) sepal.labels &lt;- labs(x = &quot;Sepal Length (cm)&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Relationship between Sepal Length and Width&quot;, caption = &quot;data from Anderson (1935)&quot;) 6.17.1 Scatter plots A scatter plot is one of the simplest representations of a bivariate distribution. Scatter plots are simple to create in ggplot2 by specifying the appropriate X and Y variables in the aesthetic mapping and using geom_point for the geometric mapping. setosa.sepals + geom_point() + sepal.labels 6.17.2 Adding a trend line to a scatter plot ggplot2 makes it easy to add trend lines to plots. I use “trend lines” here to refer to representations like regression lines, smoothing splines, or other representations mean to help visualize the relationship between pairs of variables. We’ll spend a fair amount of time exploring the mathematics and interpetation of regression lines and related techniques in later lectures, but for now just think about trends lines as summary representations for bivariate relationships. Trend lines can be created using geom_smooth. Let’s add a default trend line to our I. setosa scatter plot of the Sepal Width vs Sepal Length: setosa.sepals + geom_jitter() + # using geom_jitter to avoid overplotting of points geom_smooth() + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The defaul trend line that geom_smooth fits is generated by a technique called “LOESS regression.” LOESS regression is a non-linear curve fitting method, hence the squiggly trend line we see above. The smoothness of the LOESS regression is controlled by a parameter called span which is related to the proportion of points used. We’ll discuss LOESS in detail in a later lecture, but here’s an illustration how changing the span affects the smoothness of the fit curve: setosa.sepals + geom_jitter() + # using geom_jitter to avoid overplotting of points geom_smooth(span = 0.95) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 6.17.2.1 Linear trend lines If instead we want a straight trend line, as would typically be depicted for a linear regression model we can specify a different statistical method: setosa.sepals + geom_jitter() + # using geom_jitter to avoid overplotting of points geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # using linear model (&quot;lm&quot;) sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme ## `geom_smooth()` using formula &#39;y ~ x&#39; 6.18 Bivariate density plots The density plot, which we introduced as a visualization for univariate data, can be extended to two-dimensional data. In a one dimensional density plot, the height of the curve was related to the relatively density of points in the surrounding region. In a 2D density plot, nested contours (or contours plus colors) indicate regions of higher local density. Let’s illustrate this with an example: setosa.sepals + geom_density2d() + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme The relationship between the 2D density plot and a scatter plot can be made clearer if we combine the two: setosa.sepals + geom_density_2d() + geom_jitter(alpha=0.35) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme 6.19 Combining Scatter Plots and Density Plots with Categorical Information As with many of the univariate visualizations we explored, it is often useful to depict bivariate relationships as we change a categorical variable. To illustrate this, we’ll go back to using the full iris data set. all.sepals &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) all.sepals + geom_point(aes(color = Species, shape = Species), size = 2, alpha = 0.6) + sepal.labels + labs(subtitle = &quot;All species&quot;) + my.theme Notice how in our aesthetic mapping we specified that both color and shape should be used to represent the species categories. The same thing can be accomplished with a 2D density plot. all.sepals + geom_density_2d(aes(color = Species)) + sepal.labels + labs(subtitle = &quot;All species&quot;) + my.theme As you can see, in the density plots above, when you have multiple categorical variables and there is significant overlap in the range of each sub-distribution, figures can become quite busy. As we’ve seen previously, faceting (conditioning) can be a good way to deal with this. Below a combination of scatter plots and 2D density plots, combined with faceting on the species variable. all.sepals + geom_density_2d(aes(color = Species), alpha = 0.5) + geom_point(aes(color = Species), alpha=0.5, size=1) + facet_wrap(~ Species) + sepal.labels + labs(subtitle = &quot;All species&quot;) + theme_bw() + theme(aspect.ratio = 1, legend.position = &quot;none&quot;) # get rid of legend In this example I went back to using a theme that includes grid lines to facilitate more accurate comparisons of the distributions across the facets. I also got rid of the legend, because the information there was redundant. 6.20 Density plots with fill Let’s revisit our earlier single species 2D density plot. Instead of simply drawing contour lines, let’s use color information to help guide the eye to areas of higher density. To draw filled contours, we use a sister function to geom_density_2d called stat_density_2d: setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme Using the default color scale, areas of low density are drawn in dark blue, whereas areas of high density are drawn in light blue. I personally find this dark -to-light color scale non-intuitive for density data, and would prefer that darker regions indicate area of higher density. If we want to change the color scale, we can use the a scale function (in this case scale_fill_continuous) to set the color values used for the low and high values (this function we’ll interpolate the intervening values for us). NOTE: when specifying color names, R accepts standard HTML color names (see the Wikipedia page on web colors for a list). We’ll also see other ways to set color values in a later class session. setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + # lavenderblush is the HTML standard name for a light purplish-pink color scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme The two contour plots we generated looked a little funny because the contours are cutoff due to the contour regions being outside the limits of the plot. To fix this, we can change the plot limits using the lims function as shown in the following code block. We’ll also add the scatter (jittered) to the emphasize the relationship between the levels, and we’ll change the title for the color legend on the right by specifying a text label associated with the fill arguments in the labs function. setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + geom_jitter(alpha=0.5, size = 1.1) + # customize labels, including legend label for fill labs(x = &quot;Sepal Length(cm)&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Relationship between sepal length and width&quot;, subtitle = &quot;I. setosa specimens only&quot;, fill = &quot;Density&quot;) + # Set plot limits lims(x = c(4,6), y = c(2.5, 4.5)) + my.theme 6.21 2D bin and hex plots Two dimensional bin and hex plots are alterative ways to represent the joint density of points in the Cartesian plane. Here are examples of to generate these plot types. Compare them to our previous examples. A 2D bin plot can be tought of as a 2D histogram: setosa.sepals + geom_bin2d(binwidth = 0.2) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme A hex plot is similar to a 2D bin plot but uses hexagonal regions instead of squares. Hexagonal bins are useful because they can avoid visual artefacts sometimes apparent with square bins: setosa.sepals + geom_hex(binwidth = 0.2) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme 6.22 The cowplot package A common task when preparing visualizations for scientific presentations and manuscripts is combining different plots as subfigures of a larger figure. To accomplish this we’ll use a package called cowplot that compliments the power of ggplot2. Install cowplot either via the command line or the R Studio GUI (see Section 2.8). library(cowplot) # assumes package has been installed cowplot allows us to create individual plots using ggplot, and then arrange them in a grid-like fashion with labels for each plot of interest, as you would typically see in publications. The core function of cowplot is plot_grid(), which allows the user to layout the sub-plots in an organized fashion and add labels as necesary. To illustrate plot_grid() let’s create three different representations of the distribution of sepal width in the irisu data set, and combine them into a single figure: p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width, color = Species)) # for the histogram we&#39;re going to override the mapping because # geom_histogram only takes an x argument plot.1 &lt;- p + geom_histogram(bins=12, mapping = aes(x = Sepal.Width), inherit.aes = FALSE) plot.2 &lt;- p + geom_boxplot() plot.3 &lt;- p + geom_violin() plot_grid(plot.1, plot.2, plot.3) If instead, we wanted to layout the plots in a single row we could change the call to plot_grid as so: plot_grid(plot.1, plot.2, plot.3, nrow = 1, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) Notice we also added labels to our sub-plots. "],["introduction-to-dplyr.html", "Chapter 7 Introduction to dplyr 7.1 Libraries 7.2 Reading data with the readr package 7.3 A note on “tibbles” 7.4 Data filtering and transformation with dplyr 7.5 dplyr’s “verbs” 7.6 Pipes", " Chapter 7 Introduction to dplyr In today’s class we introduce a new package, dplyr, which, along with ggplot2 will be used in almost every class session. We will also introduce the readr package, for reading tabular data. 7.1 Libraries Both readr and dplyr are members of the tidyverse, so a single invocation of library() makes the functions defined in these two packages available for our use: library(tidyverse) 7.2 Reading data with the readr package The readr package defines a number of functions for reading data tables from common file formats like Comma-Separated-Value (CSV) and Tab-Separated-Value (TSV) files. The two most frequently used readr functions we’ll use in this class are read_csv() and read_tsv() for reading CSV and TSV files respectively. There are some variants of these basic function, which you can read about by invoking the help system (?read_csv). 7.2.1 Reading Excel files The tidyverse also includes a package called readxl which can be used to read Excel spreadsheets (recent versions with .xls and .xlsx extensions). Excel files are somewhat more complicated to deal with because they can include separate “sheets.” We won’t use readxl in this class, but documentation and examples of how readxl is used can be found at the page linked above. 7.2.2 Example data: NC Births For today’s hands on session we’ll use a data set that contains information on 150 cases of mothers and their newborns in North Carolina in 2004. This data set is available at the following URL: https://github.com/Bio723-class/example-datasets/raw/master/nc-births.txt The births data is a TSV file, so we’ll use the read_tsv() function to read it: births &lt;- read_tsv(&quot;https://github.com/Bio723-class/example-datasets/raw/master/nc-births.txt&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## fAge = col_double(), ## mAge = col_double(), ## weeks = col_double(), ## premature = col_character(), ## visits = col_double(), ## gained = col_double(), ## weight = col_double(), ## sexBaby = col_character(), ## smoke = col_character() ## ) Notice that when you used read_tsv() the function printed information about how it “parsed” the data (i.e. the types it assigned to each of the columns). The variables in the data set are: father’s age (fAge), mother’s age (mAge), weeks of gestation (weeks) whether the birth was premature or full term (premature) number of OB/GYN visits (visits) mother’s weight gained in pounds (gained) babies birth weight (weight) sex of the baby (sexBaby) whether the mother was a smoker (smoke). Notice too that we read the TSV file directly from a remote location via a URL. If instead, you wanted to load a local file on your computer you would specify the “path” – i.e. the location on your hard drive where you stored the file. For example, here is how I would load the same file if it was stored in the Downloads directory on my Mac laptop: # load the data from a local file births &lt;- read_tsv(&quot;/Users/pmagwene/Downloads/nc-births.txt&quot;) 7.3 A note on “tibbles” You may have noticed that most of the functions defined in tidyverse related packages return not data frames, but rather something called a “tibble.” You can think about tibbles as light-weight data frames. In fact if you ask about the “class” of a tibble you’ll see that it includes data.frame as one of it’s classes as well as tbl and tbl_df. class(births) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; There are some minor differences between data frame and tibbles. For example, tibbles print differently in the console and don’t automatically change variable names and types in the same way that standard data frames do. Usually tibbles can be used wherever a standard data frame is expected, but you may occasionally find a function that only works with a standard data frame. It’s easy to convert a tibble to a standard data frame using the as.data.frame function: births.std.df &lt;- as.data.frame(births) For more details about tibbles, see the Tibbles chapter in R for Data Analysis. 7.4 Data filtering and transformation with dplyr dplyr is powerful tool for data filter and transformation. In the same way that ggplot2 attempts to provide a “grammar of graphics,” dplyr aims to provide a “grammar of data manipulation.” In today’s material we will see how dplyr complements and simplifies standard data frame indexing and subsetting operations. However, dplyr is focused only on data frames and doesn’t completely replace the basic subsetting operations, and so being adept with both dplyr and the indexing approaches we’ve seen previously is important. If you’re curious about the name “dplyr,” the package’s originator Hadley Wickham says it’s supposed to invoke the idea of pliers for data frames (Github: Meaning of dplyrs name) 7.5 dplyr’s “verbs” The primary functions in the dplyr package can be thought of as a set of “verbs,” each verb corresponding to a common data manipulation task. Some of the most frequently used verbs/functions in dplyr include: select – select columns filter – filter rows mutate – create new columns arrange– reorder rows summarize – summarize values group_by – split data frame on some grouping variable. Can be powerfully combined with summarize All of these functions return new data frames rather than modifying the existing data frame (though some of the functions support in place modification of data frames via optional arguments).We illustrate these below by example using the NC births data. 7.5.1 select The select function subsets the columns (variables) of a data frame. For example, to select just the weeks and weight columns from the births data set we could do: # note I&#39;m prefixing select with the package name (dplyr) # to avoid name clashes with built-in select function wks.weight &lt;- dplyr::select(births, weeks, weight) dim(wks.weight) # dim should be 50 x 2 ## [1] 150 2 head(wks.weight) ## # A tibble: 6 x 2 ## weeks weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 39 6.88 ## 2 39 7.69 ## 3 40 8.88 ## 4 40 9 ## 5 40 7.94 ## 6 40 8.25 The equivalent using standard indexing would be: wks.wt.alt &lt;- births[c(&quot;weeks&quot;, &quot;weight&quot;)] dim(wks.wt.alt) ## [1] 150 2 head(wks.wt.alt) ## # A tibble: 6 x 2 ## weeks weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 39 6.88 ## 2 39 7.69 ## 3 40 8.88 ## 4 40 9 ## 5 40 7.94 ## 6 40 8.25 Notes: * The first argument to all of the dplyr functions is the data frame you’re operating on When using functions defined in dplyr and ggplot2 variable names are (usually) not quoted or used with the $ operator. This is a design feature of these libraries and makes it easier to carry out interactive analyes because it saves a fair amount of typing. 7.5.2 filter The filter function returns those rows of the data set that meet the given logical criterion. For example, to get all the premature babies in the data set we could use filter as so: premies &lt;- filter(births, premature == &quot;premie&quot;) dim(premies) ## [1] 21 9 The equivalent using standard indexing would be: premies.alt &lt;- births[births$premature == &quot;premie&quot;,] The filter function will work with more than one logical argument, and these are joined together using Boolean AND logic (i.e. intersection). For example, to find those babies that were premature and whose mothers were smokers we could do: smoking.premies &lt;- filter(births, premature == &quot;premie&quot;, smoke == &quot;smoker&quot;) The equivalent call using standard indexing is: # don&#39;t forget the trailing comma to indicate rows! smoking.premies.alt &lt;- births[(births$premature == &quot;premie&quot;) &amp; (births$smoke == &quot;smoker&quot;),] filter also accepts logical statements chained together using the standard Boolean operators. For example, to find babies who were premature or whose moms were older than 35 you could use the OR operator |: premies.or.oldmom &lt;- filter(births, premature == &quot;premie&quot; | fAge &gt; 35) 7.5.3 mutate The mutate function creates a new data frame that is the same as input data frame but with additional variables (columns) as specified by the function arguments. In the example below, I create two new variables, weight.in.kg and a mom.smoked: # to make code more readable it&#39;s sometime useful to spread out # function arguments over multiple lines like I&#39;ve done here births.plus &lt;- mutate(births, weight.in.kg = weight / 2.2, mom.smoked = (smoke == &quot;smoker&quot;)) head(births.plus) ## # A tibble: 6 x 11 ## fAge mAge weeks premature visits gained weight sexBaby smoke weight.in.kg ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 31 30 39 full term 13 1 6.88 male smok… 3.13 ## 2 34 36 39 full term 5 35 7.69 male nons… 3.50 ## 3 36 35 40 full term 12 29 8.88 male nons… 4.04 ## 4 41 40 40 full term 13 30 9 female nons… 4.09 ## 5 42 37 40 full term NA 10 7.94 male nons… 3.61 ## 6 37 28 40 full term 12 35 8.25 male smok… 3.75 ## # … with 1 more variable: mom.smoked &lt;lgl&gt; The equivalent using standard indexing would be to create a new data frame from births, appending the new variables to the end as so: births.plus.alt &lt;- data.frame(births, weight.in.kg = births$weight / 2.2, mom.smoked = (births$smoke == &quot;smoker&quot;)) 7.5.4 arrange Arrange creates a new data frame where the rows are sorted according to their values for one or more variables. For example, to sort by mothers age we could do: young.moms.first &lt;- arrange(births, mAge) head(young.moms.first) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18 15 37 full term 12 76 8.44 male nonsmoker ## 2 NA 16 40 full term 4 12 6 female nonsmoker ## 3 21 16 38 full term 15 75 7.56 female smoker ## 4 26 17 38 full term 11 30 9.5 female nonsmoker ## 5 17 17 29 premie 4 10 2.63 female nonsmoker ## 6 20 17 40 full term 17 38 7.19 male nonsmoker The equivalent to arrange using standard indexing would be to use the information returned by the order function: young.moms.first.alt &lt;- births[order(births$mAge),] head(young.moms.first.alt) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18 15 37 full term 12 76 8.44 male nonsmoker ## 2 NA 16 40 full term 4 12 6 female nonsmoker ## 3 21 16 38 full term 15 75 7.56 female smoker ## 4 26 17 38 full term 11 30 9.5 female nonsmoker ## 5 17 17 29 premie 4 10 2.63 female nonsmoker ## 6 20 17 40 full term 17 38 7.19 male nonsmoker When using arrange, multiple sorting variables can be specified: sorted.by.moms.and.dads &lt;- arrange(births, mAge, fAge) head(sorted.by.moms.and.dads) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18 15 37 full term 12 76 8.44 male nonsmoker ## 2 21 16 38 full term 15 75 7.56 female smoker ## 3 NA 16 40 full term 4 12 6 female nonsmoker ## 4 17 17 29 premie 4 10 2.63 female nonsmoker ## 5 20 17 40 full term 17 38 7.19 male nonsmoker ## 6 26 17 38 full term 11 30 9.5 female nonsmoker If you want to sort in descending order, you can combing arrange with the desc (=descend) function, also defined in dplyr: old.moms.first &lt;- arrange(births, desc(mAge)) head(old.moms.first) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 NA 41 33 premie 13 0 5.69 female nonsmoker ## 2 41 40 40 full term 13 30 9 female nonsmoker ## 3 33 40 36 premie 13 23 7.81 female nonsmoker ## 4 40 40 38 full term 13 38 7.31 male nonsmoker ## 5 46 39 38 full term 10 35 6.75 male smoker ## 6 NA 38 32 premie 10 16 2.19 female smoker 7.5.5 summarize summarize applies a function of interest to one or more variables in a data frame, reducing a vector of values to a single value and returning the results in a data frame. This is most often used to calculate statistics like means, medians, count, etc. As we’ll see below, this is powerful when combined with the group_by function. summarize(births, mean.wt = mean(weight), median.wks = median(weeks)) ## # A tibble: 1 x 2 ## mean.wt median.wks ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7.05 39 You’ll need to be diligent if your data has missing values (NAs). For example, by default the mean function returns NA if any of the input values are NA: summarize(births, mean.gained = mean(gained)) ## # A tibble: 1 x 1 ## mean.gained ## &lt;dbl&gt; ## 1 NA However, if you read the mean docs (?mean) you’ll see that there is an na.rm argument that indicates whether NA values should be removed before computing the mean. This is what we want so we instead call summarize as follows: summarize(births, mean.gained = mean(gained, na.rm = TRUE)) ## # A tibble: 1 x 1 ## mean.gained ## &lt;dbl&gt; ## 1 32.5 7.5.6 group_by The group_by function implicitly adds grouping information to a data frame. # group the births by whether mom smoked or not by_smoking &lt;- group_by(births, smoke) The object returned by group_by is a “grouped data frame”: class(by_smoking) ## [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Some functions, like count() and summarize() (see below) know how to use the grouping information. For example, to count the number of births conditional on mother smoking status we could do: count(by_smoking) ## # A tibble: 2 x 2 ## smoke n ## &lt;chr&gt; &lt;int&gt; ## 1 nonsmoker 100 ## 2 smoker 50 group_by also works with multiple grouping variables, with each added grouping variable specified as an additional argument: by_smoking.and.mAge &lt;- group_by(births, smoke, mAge &gt; 35) 7.5.7 Combining grouping and summarizing Grouped data frames can be combined with the summarize function we saw above. For example, if we wanted to calculate mean birth weight, broken down by whether the baby’s mother smoked or not we could call summarize with our by_smoking grouped data frame: summarize(by_smoking, mean.wt = mean(weight)) ## # A tibble: 2 x 2 ## smoke mean.wt ## &lt;chr&gt; &lt;dbl&gt; ## 1 nonsmoker 7.18 ## 2 smoker 6.78 Similarly to get the mean birth weight of children conditioned on mothers smoking status and age: summarize(by_smoking.and.mAge, mean(weight)) ## `summarise()` has grouped output by &#39;smoke&#39;. You can override using the `.groups` argument. ## # A tibble: 4 x 3 ## smoke `mAge &gt; 35` `mean(weight)` ## &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 nonsmoker FALSE 7.17 ## 2 nonsmoker TRUE 7.26 ## 3 smoker FALSE 6.83 ## 4 smoker TRUE 6.30 7.5.8 Scoped variants of mutate and summarize Both the mutate() and summarize() functions provide “scoped” alternatives, that allow us to apply the operation on a selection of variables. These variants are often used in combination with grouping. We’ll look at the summarize versions – summarize_all(), summarize_at(), and summarize_if(). See the documentation (?mutate_all) for descriptions of the mutate versions. 7.5.8.1 summarize_all() summarize_all() applies a one or more functions to all columns in a data frame. Here we illustrate a simple version of this with the iris data: # group by species by_species &lt;- group_by(iris, Species) # calculate the mean of every variable, grouped by species summarize_all(by_species, mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Note that if we try and apply summarize_all() in the same way to the grouped data frame by_smoking we’ll get a bunch of warning messages: summarize_all(by_smoking, mean) ## # A tibble: 2 x 9 ## smoke fAge mAge weeks premature visits gained weight sexBaby ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker NA 26.9 38.6 NA NA NA 7.18 NA ## 2 smoker NA 26 38.5 NA 10.8 NA 6.78 NA Here’s an example of one of these warnings: Warning messages: 1: In mean.default(premature) : argument is not numeric or logical: returning NA This message is telling us that we can’t apply the mean() function to the data frame column premature because this is not a numerical or logical vector. Despite this and the other similar warnings, summarize_all() does return a result, but the means for any non-numeric values are replaced with NAs, as shown below: # A tibble: 2 x 9 smoke fAge mAge weeks premature visits gained weight sexBaby &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker NA 26.9 38.6 NA NA NA 7.18 NA 2 smoker NA 26.0 38.5 NA 10.8 NA 6.78 NA If you examine the output above, you’ll see that there are several variables that are numeric, however we still got NAs when we calculated the grouped means. This is because those variables contain NA values. The mean function has an optional argument, na.rm, which tells the function to remove any missing data before calculating the mean. Thus we can modify our call to summarize_all as follows: # calculate mean of all variables, grouped by smoking status summarize_all(by_smoking, mean, na.rm = TRUE) ## # A tibble: 2 x 9 ## smoke fAge mAge weeks premature visits gained weight sexBaby ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 29.8 26.9 38.6 NA 11.9 32.5 7.18 NA ## 2 smoker 29.7 26 38.5 NA 10.8 32.3 6.78 NA Note that the non-numeric data columns still lead to NA values. 7.5.8.2 summarize_if() summarize_if() is similar to summarize_all(), except it only applies the function of interest to those variables that match a particular predicate (i.e. are TRUE for a particular TRUE/FALSE test). Here we use summarize_if() to apply the mean() function to only those variables (columns) that are numeric. # calculate mean of all numeric variables, grouped by smoking status summarize_if(by_smoking, is.numeric, mean, na.rm = TRUE) ## # A tibble: 2 x 7 ## smoke fAge mAge weeks visits gained weight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 29.8 26.9 38.6 11.9 32.5 7.18 ## 2 smoker 29.7 26 38.5 10.8 32.3 6.78 7.5.8.3 summarize_at() summarize_at() allows us to apply functions of interest only to specific variables. # calculate mean of gained and weight variables, grouped by smoking status summarize_at(by_smoking, c(&quot;gained&quot;, &quot;weight&quot;), mean, na.rm = TRUE) ## # A tibble: 2 x 3 ## smoke gained weight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 32.5 7.18 ## 2 smoker 32.3 6.78 All three of the scoped summarize functions can also be used to apply multiple functions, by wrapping the function names in a call to dplyr::funs(): # calculate mean and std deviation of # gained and weight variables, grouped by smoking status summarize_at(by_smoking, c(&quot;gained&quot;, &quot;weight&quot;), funs(mean, sd), na.rm = TRUE) ## # A tibble: 2 x 5 ## smoke gained_mean weight_mean gained_sd weight_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 32.5 7.18 15.2 1.43 ## 2 smoker 32.3 6.78 16.6 1.60 summarize_at() accepts as the the argument for variables a character vector of column names, a numeric vector of column positions, or a list of columns generated by the dplyr::vars() function, which can be be used as so: # reformatted to promote readability of arguments summarize_at(by_smoking, vars(gained, weight), funs(mean, sd), na.rm = TRUE) ## # A tibble: 2 x 5 ## smoke gained_mean weight_mean gained_sd weight_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 32.5 7.18 15.2 1.43 ## 2 smoker 32.3 6.78 16.6 1.60 7.5.9 Combining summarize with grouping aesthetics in ggplot2 We’ve already seen an instance of grouping (conditioning) when we used aesthetics like color or fill to distinguish subgroups in different types of statistical graphics. Below is an example where we integrate information from a group_by/summarize operation into a plot: # calculate mean weights, conditioned on smoking status wt.by.smoking &lt;- summarize(by_smoking, mean_weight = mean(weight, na.rm = TRUE)) # create density plot for all the data # and then use geom_vline to draw vertical lines at the means for # each group ggplot(births) + geom_density(aes(x = weight, color = smoke)) + # data drawn from births geom_vline(data = wt.by.smoking, # note use of different data frame! mapping = aes(xintercept = mean_weight, color = smoke), linetype = &#39;dashed&#39;) 7.6 Pipes dplyr includes a very useful operator available called a pipe available to us. Pipes are powerful because they allow us to chain together sets of operations in a very intuitive fashion while minimizing nested function calls. We can think of pipes as taking the output of one function and feeding it as the first argument to another function call, where we’ve already specified the subsequent arguments. Pipes are actually defined in another packaged called magrittr. We’ll look at the basic pipe operator and then look at a few additional “special” pipes that magrittr provides. 7.6.1 Install and load magrittr In magrittr in not already installed, install it via the command line or the RStudio GUI. Having done so, you will need to load magrittr via the library() function: library(magrittr) 7.6.2 The basic pipe operator The pipe operator is designated by %&gt;%. Using pipes, the expression x %&gt;% f() is equivalent to f(x) and the expression x %&gt;% f(y) is equivalent to f(x,y). The documentation on pipes (see ?magrittr) uses the notation lhs %&gt;% rhs where lhs and rhs are short for “left-hand side” and “right-hand side” respectively. I’ll use this same notation in some of the explanations that follow. births %&gt;% head() # same as head(births) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker births %&gt;% head # you can even leave the parentheses out ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker births %&gt;% head(10) # same as head(births, 10) ## # A tibble: 10 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker ## 7 35 35 28 premie 6 29 1.63 female nonsmoker ## 8 28 21 35 premie 9 15 5.5 female smoker ## 9 22 20 32 premie 5 40 2.69 male smoker ## 10 36 25 40 full term 13 34 8.75 female nonsmoker Multiple pipes can be chained together, such that x %&gt;% f() %&gt;% g() %&gt;% h() is equivalent to h(g(f(x))). # equivalent to: head(arrange(births, weight), 10) births %&gt;% arrange(weight) %&gt;% head(10) ## # A tibble: 10 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 35 35 28 premie 6 29 1.63 female nonsmoker ## 2 NA 18 33 premie 7 40 1.69 male smoker ## 3 NA 38 32 premie 10 16 2.19 female smoker ## 4 17 17 29 premie 4 10 2.63 female nonsmoker ## 5 22 20 32 premie 5 40 2.69 male smoker ## 6 38 37 26 premie 5 25 3.63 male nonsmoker ## 7 25 22 34 premie 10 20 3.75 male nonsmoker ## 8 NA 24 38 full term 16 50 3.75 female nonsmoker ## 9 30 25 35 premie 15 40 4.5 male smoker ## 10 19 20 34 premie 13 6 4.5 male nonsmoker When there are multiple piping operations, I like to arrange the statements vertically to help emphasize the flow of processing and to facilitate debugging and/or modification. I would usually rearrange the above code block as follows: births %&gt;% arrange(weight) %&gt;% head(10) ## # A tibble: 10 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 35 35 28 premie 6 29 1.63 female nonsmoker ## 2 NA 18 33 premie 7 40 1.69 male smoker ## 3 NA 38 32 premie 10 16 2.19 female smoker ## 4 17 17 29 premie 4 10 2.63 female nonsmoker ## 5 22 20 32 premie 5 40 2.69 male smoker ## 6 38 37 26 premie 5 25 3.63 male nonsmoker ## 7 25 22 34 premie 10 20 3.75 male nonsmoker ## 8 NA 24 38 full term 16 50 3.75 female nonsmoker ## 9 30 25 35 premie 15 40 4.5 male smoker ## 10 19 20 34 premie 13 6 4.5 male nonsmoker 7.6.3 An example without pipes To illustrate how pipes help us, first let’s look at an example set of analysis steps without using pipes. Let’s say we wanted to explore the relationship between father’s age and baby’s birth weight. We’ll start this process of exploration by generating a bivariate scatter plot. Being good scientists we want to express our data in SI units, so we’ll need to converts pounds to kilograms. You’ll also recall that a number of the cases have missing data on father’s age, so we’ll want to remove those before we plot them. Here’s how we might accomplish these steps: # add a new column for weight in kg births.kg &lt;- mutate(births, weight.kg = weight / 2.2) # filter out the NA fathers filtered.births &lt;- filter(births.kg, !is.na(fAge)) # create our plot ggplot(filtered.births, aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) Notice that we created two “temporary” data frames along the way – births.kg and filtered.births. These probably aren’t of particular interest to us, but we needed to generate them to build the plot we wanted. If you were particularly masochistic you could avoid these temporary data frames by using nested functions call like this: # You SHOULD NOT write nested code like this. # Code like this is hard to debug and understand! ggplot(filter(mutate(births, weight.kg = weight / 2.2), !is.na(fAge)), aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) 7.6.4 The same example using pipes The pipe operator makes the output of one statement (lhs) as the first input of a following function (rhs). This simplifies the above example to: births %&gt;% mutate(weight.kg = weight / 2.2) %&gt;% filter(!is.na(fAge)) %&gt;% ggplot(aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) In the example above, we feed the data frame into the mutate function. mutate expects a data frame as a first argument, and subsequent arguments specify the new variables to be created. births %&gt;% mutate(weight.kg = weight / 2.2) is thus equivalent to mutate(births, weight.kg = weight / 2.2)). We then pipe the output to filter, removing NA fathers, and then pipe that output as the input to ggplot. As mentioned previously, it’s good coding style to write each discrete step as its own line when using piping. This make it easier to understand what the steps of the analysis are as well as facilitating changes to the code (commenting out lines, adding lines, etc) 7.6.5 Assigning the output of a statement involving pipes to a variable It’s important to recognize that pipes are simply a convenient way to chain together a series of expression. Just like any other compound expression, the output of a series of pipe statements can be assigned to a variable, like so: stats.old.moms &lt;- births %&gt;% filter(mAge &gt; 35) %&gt;% summarize(median.gestation = median(weeks), mean.weight = mean(weight)) stats.old.moms ## # A tibble: 1 x 2 ## median.gestation mean.weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 38 6.94 Note that our summary table, stats.old.moms, is itself a data frame. 7.6.6 Compound assignment pipe operator A fairly common operation when working interactively in R is to update an existing data frame. magrittr defines another pipe operator – %&lt;&gt;% – called the “compound assignment” pipe operator, to facilitate this. The compound assignment pipe operator has the basic usage lhs %&lt;&gt;% rhs. This operator evaluates the function on the rhs using the lhs as the first argument, and then updates the lhs with the resulting value. This is simply shorthand for writing lhs &lt;- lhs %&gt;% rhs. stats.old.moms %&lt;&gt;% # note compound pipe operator! mutate(mean.weight.kg = mean.weight / 2.2) 7.6.7 The dot operator with pipes When working with pipes, sometimes you’ll want to use the lhs in multiple places on the rhs, or as something other than the first argument to the rhs. magrittr provides for this situation by using the dot (.) operator as a placeholder. Using the dot operator, the expression y %&gt;% f(x, .) is equivalent to f(x,y). c(&quot;dog&quot;, &quot;cakes&quot;, &quot;sauce&quot;, &quot;house&quot;) %&gt;% # create a vector sample(1) %&gt;% # pick a random single element of that vector str_c(&quot;hot&quot;, .) # string concatenate the pick with the word &quot;hot&quot; ## [1] &quot;hotsauce&quot; 7.6.8 The exposition pipe operator magrittr defines another operator called the “exposition pipe operator,” designed %$%. This operator exposes the names in the lhs to the expression on the rhs. Here is an example of using the exposition pipe operator to simply return the vector of weights: births %&gt;% filter(premature == &quot;premie&quot;) %$% # note the different pipe operator! weight ## [1] 1.63 5.50 2.69 6.50 7.81 4.75 3.75 2.19 6.81 4.69 6.75 4.50 5.94 4.50 5.06 ## [16] 5.69 1.69 6.31 2.63 5.88 3.63 If we wanted to calculate the minimum and maximum weight of premature babies in the data set we could do the following (though I’d usually prefer summarize() unless I needed the results in the form of a vector): births %&gt;% filter(mAge &gt; 35) %$% # note the different pipe operator! c(min(weight), max(weight)) ## [1] 2.19 10.13 "],["data-wrangling.html", "Chapter 8 Data wrangling 8.1 Libraries 8.2 Data 8.3 Renaming data frame columms 8.4 Dropping unneeded columns 8.5 Merging data frames 8.6 Reshaping data with tidyr 8.7 Using your tidy data 8.8 Long-to-wide conversion using tidyr::spread 8.9 Exploring bivariate relationships using “wide” data", " Chapter 8 Data wrangling In the real world you’ll often create a data set (or be given one) in a format that is less than ideal for analysis. This can happen for a number of reasons. For example, the data may have been recorded in a manner convenient for collection and visual inspection, but which does not work well for analysis and plotting. Or the data may be an amalgamation of multiple experiments, in which each of the experimenters used slightly different naming conventions. Or the data may have been produced by an instrument that produces output with a fixed format. Sometimes important experimental information is included in the column headers of a spreadsheet. Whatever the case, we often find ourselves in the situation where we need to “wrangle” our data into a “tidy” format before we can proceed with visualization and analysis. The “R for Data Science” text discusses some desirable rules for “tidy” data in order to facilitate downstream analyses. These are: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In this lecture we’re going to walk through an extended example of wrangling some data into a “tidy” format. 8.1 Libraries library(magrittr) library(stringr) library(tidyverse) library(cowplot) 8.2 Data To illustrate a data wrangling pipeline, we’re going to use a gene expression microarray data set, based on the following paper: Spellman PT, et al. 1998. Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization. Mol Biol Cell 9(12): 3273-97. In this paper, Spellman and colleagues tried to identify all the genes in the yeast genome (&gt;6000 genes) that exhibited oscillatory behaviors suggestive of cell cycle regulation. To do so, they combined gene expression measurements from six different types of cell cycle synchronization experiments. Download the Spellman data to your filesystem from this link (right-click the “Download” button and save to your Downloads folder or similar). I suggest that once you download the data, you open it in a spreadsheet program (e.g. Excel) or use the RStudio Data Viewer to get a sense of what the data looks like. Let’s load it into R, using the read_tsv() function, using the appropriate file path. # the filepath may differ on your computer spellman &lt;- read_tsv(&quot;~/Downloads/spellman-combined.txt&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## X1 = col_character(), ## clb = col_logical(), ## alpha = col_logical(), ## cdc15 = col_logical(), ## cdc28 = col_logical(), ## elu = col_logical() ## ) ## ℹ Use `spec()` for the full column specifications. The initial dimenions of the data frame are: dim(spellman) ## [1] 6178 83 The six types of cell cycle synchronization experiments included in this data set are: synchronization by alpha-factor = “alpha” synchronization by cdc15 temperature sensitive mutants = “cdc15” synchronization by cdc28 temperature sensitive mutants = “cdc28” synchronization by elutration = “elu” synchronization by cln3 mutatant strains = “cln3” synchronization by clb2 mutant strains = “clb2” 8.3 Renaming data frame columms Notice that when we imported the data we got a warning message: Missing column names filled in: 'X1' [1]. In a data frame, every column must have a name. The first column of our data set did not have a name in the header, so read_tsv automatically gave it the name X1. Our first task is to give the first column a more meaningful name. This column gives “systematic gene names” – a standardized naming scheme for genes in the yeast genome. We’ll use dplyr::rename to rename X1 to gene. Note that rename can take multiple arguments if you need to rename multiple columns simultaneously. spellman.clean &lt;- spellman %&gt;% rename(gene = X1) 8.4 Dropping unneeded columns Take a look at the Spellman data again in your spreadsheet program (or the RStudio data viewer). You’ll notice there are some blank columns. For example there is a column with the header “alpha” that has no entries. These are simply visual organizing elements that the creator of the spreadsheet added to separate the different experiments that are included in the data set. We can use dplyr::select() to drop columns by prepending column names with the negative sign: # drop the alpha column keeping all others spellman.clean %&lt;&gt;% dplyr::select(-alpha) Note that usually select() keeps only the variables you specify. However if the first expression is negative, select will instead automatically keep all variables, dropping only those you specify. 8.4.1 Finding all empty columns In the example above, we looked at the data and saw that the “alpha” column was empty, and thus dropped it. This worked because there are only a modest number of columns in the data frame in it’s initial form. However, if our data frame contained thousands of columns, this “look and see” procedure would not be efficient. Can we come up with a general solution for removing empty columns from a data frame? When you load a data frame from a spreadsheet, empty cells are given the value NA. In previous class sessions we were introduced to the function is.na() which tests each value in a vector or data frame for whether it’s NA or not. We can count NA values in a vector by summing the output of is.na(). Conversely we can count the number of “not NA” items by using the negation operator (!): # count number of NA values in the alpha0 column sum(is.na(spellman$alpha0)) ## [1] 165 # count number of values that are NOT NA in alpha0 sum(!is.na(spellman$alpha0)) ## [1] 6013 This seems like it should get us close to a solution but sum(is.na(..)) when applied to a data frame counts NAs across the entire data frame, not column-by-column. # doesn&#39;t do what we hoped! sum(is.na(spellman)) ## [1] 59017 If we want sums of NAs by column, we instead use the colSums() function: # get number of NAs by column colSums(is.na(spellman)) ## X1 cln3-1 cln3-2 clb clb2-2 clb2-1 alpha alpha0 ## 0 193 365 6178 454 142 6178 165 ## alpha7 alpha14 alpha21 alpha28 alpha35 alpha42 alpha49 alpha56 ## 525 191 312 267 207 123 257 147 ## alpha63 alpha70 alpha77 alpha84 alpha91 alpha98 alpha105 alpha112 ## 186 185 178 155 329 209 174 222 ## alpha119 cdc15 cdc15_10 cdc15_30 cdc15_50 cdc15_70 cdc15_80 cdc15_90 ## 251 6178 677 477 501 608 573 562 ## cdc15_100 cdc15_110 cdc15_120 cdc15_130 cdc15_140 cdc15_150 cdc15_160 cdc15_170 ## 606 570 611 495 574 811 583 571 ## cdc15_180 cdc15_190 cdc15_200 cdc15_210 cdc15_220 cdc15_230 cdc15_240 cdc15_250 ## 803 613 1014 573 741 596 847 379 ## cdc15_270 cdc15_290 cdc28 cdc28_0 cdc28_10 cdc28_20 cdc28_30 cdc28_40 ## 537 426 6178 122 72 67 55 66 ## cdc28_50 cdc28_60 cdc28_70 cdc28_80 cdc28_90 cdc28_100 cdc28_110 cdc28_120 ## 56 82 84 75 237 165 319 312 ## cdc28_130 cdc28_140 cdc28_150 cdc28_160 elu elu0 elu30 elu60 ## 1439 2159 521 543 6178 122 153 175 ## elu90 elu120 elu150 elu180 elu210 elu240 elu270 elu300 ## 132 103 119 111 118 131 110 112 ## elu330 elu360 elu390 ## 112 156 114 Columns with all missing values can be more conveniently found by asking for those columns where the number of “not missing” values is zero: # get names of all columns for which all rows are NA # useing standard indexing names(spellman)[colSums(!is.na(spellman)) == 0] ## [1] &quot;clb&quot; &quot;alpha&quot; &quot;cdc15&quot; &quot;cdc28&quot; &quot;elu&quot; We can combine the colSums(!is.na()) idiom with the dplyr::select_if function to quickly remove all empty columns as so: spellman.clean %&lt;&gt;% # note use of magritrr $&lt;&gt;% operator here #keep ONLY the non-empty columns select_if(colSums(!is.na(.)) &gt; 0) 8.4.2 Dropping columns by matching names Only two time points from the cln3 and clb2 experiments were reported in the original publication. Since complete time series are unavailable for these two experimental conditions we will drop them from further consideration. select() can be called be called with a number of “helper function” (?select_helpers). Here we’ll illustrate the matches() helper function which matches column names to a “regular expression.” Regular expressions (also referred to as “regex” or “regexp”) are a way of specifying patterns in strings. For the purposes of this document we’ll illustrate regexs by example; for a more detailed explanation of regular expressions see the the regex help(?regex) and the Chapter on Strings in “R for Data Analysis”: Let’s see how to drop all the “cln3” and “clb2” columns from the data frame using matches(): spellman.clean %&lt;&gt;% dplyr::select(-matches(&quot;cln3&quot;)) %&gt;% dplyr::select(-matches(&quot;clb2&quot;)) If we wanted we could have collapsed our two match statements into one as follows: spellman.clean %&lt;&gt;% dplyr::select(-matches(&quot;cln3|clb2&quot;)) In this second example, the character “|” is specifing an OR match within the regular expression, so this regular expression matches column names that contain “cln3” OR “clb2.” 8.5 Merging data frames Often you’ll find yourself in the situation where you want to combine information from multiple data sources. The usual requirement is that the data sources have one or more shared columns, that allow you to relate the entities or observations (rows) between the data sets. dplyr provides a variety of join functions to handle different data merging operators. To illustrating merging or joining data sources, we’ll add information about each genes “common name” and a description of the gene functions to our Spellman data set. I’ve prepared a file with this info based on info I downloaded from the Saccharomyces Genome Database. gene.info &lt;- read_csv(&quot;https://github.com/bio304-class/bio304-course-notes/raw/master/datasets/yeast-ORF-info.csv&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## ftr.name = col_character(), ## std.name = col_character(), ## description = col_character() ## ) Having loaded the data, let’s get a quick overview of it’s structure: names(gene.info) ## [1] &quot;ftr.name&quot; &quot;std.name&quot; &quot;description&quot; dim(gene.info) ## [1] 6610 3 head(gene.info) ## # A tibble: 6 x 3 ## ftr.name std.name description ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 YAL069W &lt;NA&gt; Dubious open reading frame; unlikely to encode a functiona… ## 2 YAL068W-A &lt;NA&gt; Dubious open reading frame; unlikely to encode a functiona… ## 3 YAL068C PAU8 Protein of unknown function; member of the seripauperin mu… ## 4 YAL067W-A &lt;NA&gt; Putative protein of unknown function; identified by gene-t… ## 5 YAL067C SEO1 Putative permease; member of the allantoate transporter su… ## 6 YAL066W &lt;NA&gt; Dubious open reading frame; unlikely to encode a functiona… In gene.info, the ftr.name column corresponds to the gene column in our Spellman data set. The std.name column gives the “common” gene name (not every gene has a common name so there are lots of NAs). The description column gives a brief textual description of what the gene product does. To combine spellmean.clean with gene.info we use the left_join function defined in dplyr. As noted in the description of the function, left_join(x, y) returns “all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns.” In addition, we have to specify the column to join by using the by argument to left_join. spellman.merged &lt;- left_join(spellman.clean, gene.info, by = c(&quot;gene&quot; = &quot;ftr.name&quot;)) By default, the joined columns are merged at the end of the data frame, so we’ll reorder variables to bring the std.name and description to the second and thirds columns, preserving the order of all the other colums. spellman.merged %&lt;&gt;% dplyr::select(gene, std.name, description, everything()) spellman.merged ## # A tibble: 6,178 x 76 ## gene std.name description alpha0 alpha7 alpha14 alpha21 alpha28 alpha35 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 YAL0… TFC3 Subunit of… -0.15 -0.15 -0.21 0.17 -0.42 -0.44 ## 2 YAL0… VPS8 Membrane-b… -0.11 0.1 0.01 0.06 0.04 -0.26 ## 3 YAL0… EFB1 Translatio… -0.14 -0.71 0.1 -0.32 -0.4 -0.580 ## 4 YAL0… &lt;NA&gt; Dubious op… -0.02 -0.48 -0.11 0.12 -0.03 0.19 ## 5 YAL0… SSA1 ATPase inv… -0.05 -0.53 -0.47 -0.06 0.11 -0.07 ## 6 YAL0… ERP2 Member of … -0.6 -0.45 -0.13 0.35 -0.01 0.49 ## 7 YAL0… FUN14 Integral m… -0.28 -0.22 -0.06 0.22 0.25 0.13 ## 8 YAL0… SPO7 Putative r… -0.03 -0.27 0.17 -0.12 -0.27 0.06 ## 9 YAL0… MDM10 Subunit of… -0.05 0.13 0.13 -0.21 -0.45 -0.21 ## 10 YAL0… SWC3 Protein of… -0.31 -0.43 -0.3 -0.23 -0.13 -0.07 ## # … with 6,168 more rows, and 67 more variables: alpha42 &lt;dbl&gt;, alpha49 &lt;dbl&gt;, ## # alpha56 &lt;dbl&gt;, alpha63 &lt;dbl&gt;, alpha70 &lt;dbl&gt;, alpha77 &lt;dbl&gt;, alpha84 &lt;dbl&gt;, ## # alpha91 &lt;dbl&gt;, alpha98 &lt;dbl&gt;, alpha105 &lt;dbl&gt;, alpha112 &lt;dbl&gt;, ## # alpha119 &lt;dbl&gt;, cdc15_10 &lt;dbl&gt;, cdc15_30 &lt;dbl&gt;, cdc15_50 &lt;dbl&gt;, ## # cdc15_70 &lt;dbl&gt;, cdc15_80 &lt;dbl&gt;, cdc15_90 &lt;dbl&gt;, cdc15_100 &lt;dbl&gt;, ## # cdc15_110 &lt;dbl&gt;, cdc15_120 &lt;dbl&gt;, cdc15_130 &lt;dbl&gt;, cdc15_140 &lt;dbl&gt;, ## # cdc15_150 &lt;dbl&gt;, cdc15_160 &lt;dbl&gt;, cdc15_170 &lt;dbl&gt;, cdc15_180 &lt;dbl&gt;, ## # cdc15_190 &lt;dbl&gt;, cdc15_200 &lt;dbl&gt;, cdc15_210 &lt;dbl&gt;, cdc15_220 &lt;dbl&gt;, ## # cdc15_230 &lt;dbl&gt;, cdc15_240 &lt;dbl&gt;, cdc15_250 &lt;dbl&gt;, cdc15_270 &lt;dbl&gt;, ## # cdc15_290 &lt;dbl&gt;, cdc28_0 &lt;dbl&gt;, cdc28_10 &lt;dbl&gt;, cdc28_20 &lt;dbl&gt;, ## # cdc28_30 &lt;dbl&gt;, cdc28_40 &lt;dbl&gt;, cdc28_50 &lt;dbl&gt;, cdc28_60 &lt;dbl&gt;, ## # cdc28_70 &lt;dbl&gt;, cdc28_80 &lt;dbl&gt;, cdc28_90 &lt;dbl&gt;, cdc28_100 &lt;dbl&gt;, ## # cdc28_110 &lt;dbl&gt;, cdc28_120 &lt;dbl&gt;, cdc28_130 &lt;dbl&gt;, cdc28_140 &lt;dbl&gt;, ## # cdc28_150 &lt;dbl&gt;, cdc28_160 &lt;dbl&gt;, elu0 &lt;dbl&gt;, elu30 &lt;dbl&gt;, elu60 &lt;dbl&gt;, ## # elu90 &lt;dbl&gt;, elu120 &lt;dbl&gt;, elu150 &lt;dbl&gt;, elu180 &lt;dbl&gt;, elu210 &lt;dbl&gt;, ## # elu240 &lt;dbl&gt;, elu270 &lt;dbl&gt;, elu300 &lt;dbl&gt;, elu330 &lt;dbl&gt;, elu360 &lt;dbl&gt;, ## # elu390 &lt;dbl&gt; 8.6 Reshaping data with tidyr The tidyr package provides functions for reshaping or tidying data frames. tidyr is yet another component of the tidyverse, and thus was loaded by the library(tidyverse). We’re going to look at two functions tidyr::gather() and tidyr::extract(), and how they can be combined with now familiar dplyr functions we’ve seen previously. The reading assignment for today’s class session covers a variety of other functions defined in tidyr. The Spellman data, as I provided it to you, is in what we would call “wide” format. Each column (besides the gene column) corresponds to an experimental condition and time point. For example, “alpha0” is the alpha-factor experiment at time point 0 mins; “alpha7” is the alpha-factor experiment at time point 7 mins, etc. The cells within each column correspond to the expression of a corresponding gene (given by the first column which we renamed gene) in that particular experiment at that particular time point. In every column (except “gene”), the cells represents the same abstract property of interest – the expression of a gene of interest in a particular experiment/time point. Our first task will be to rearrange our “wide” data frame that consists of many different columns representing gene expression into a “long” data frame with just a single column representing expression. We’ll also create a new column to keep track of which experiment and time point the measurement came from. 8.6.1 Wide to long conversions using tidyr::gather tidyr::gather() takes multiple columns, and collapses them together into a smaller number of new columns. When using gather() you give the names of the new columns to create, as well as the names of any existing columns gather() should not collect together. Here we want to collapse all 73 or the expression columns – “alpha0” to “elu390” – into two columns: 1) a column to represent the expt/time point of the measurement, and 2) a column to represent the corresponding expression value. The column we don’t want to touch are the gene, std.name, and description. # convert &quot;wide&quot; data to &quot;long&quot; spellman.long &lt;- spellman.merged %&gt;% gather(expt.and.time, expression, -gene, -std.name, -description) Take a moment to look at the data in the “long format”: head(spellman.long) ## # A tibble: 6 x 5 ## gene std.name description expt.and.time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III transc… alpha0 -0.15 ## 2 YAL00… VPS8 Membrane-binding component of the CO… alpha0 -0.11 ## 3 YAL00… EFB1 Translation elongation factor 1 beta… alpha0 -0.14 ## 4 YAL00… &lt;NA&gt; Dubious open reading frame; unlikely… alpha0 -0.02 ## 5 YAL00… SSA1 ATPase involved in protein folding a… alpha0 -0.05 ## 6 YAL00… ERP2 Member of the p24 family involved in… alpha0 -0.6 And compare the dimensions of the wide data to the new data: dim(spellman.merged) # for comparison ## [1] 6178 76 dim(spellman.long) ## [1] 450994 5 As you see, we’ve gone from a data frame with 6178 rows and 76 columns (wide format), to a new data frame with 450994 rows and 5 columns (long format). 8.6.2 Extracting information from combined variables using tidyr::extract The column expt.and.time violates one of our principles of tidy data: “Each variable must have its own column.” This column conflates two different types of information – the experiment type and the time point of the measurement. Our next task is to split this information up into two new variables, which will help to facilitate downstream plotting and analysis. One complicating factor is that the different experiments/time combinations have different naming conventions: The “alpha” and “elu” experiments are of the form “alpha0,” “alpha7,” “elu0,” “elu30,” etc. In this case, the first part of the string gives the experiment type (either alpha or elu) and the following digits give the time point. In the “cdc15” and “cdc28” experiments the convention is slightly different; they are of the form “cdc15_0,” “cdc15_10,” “cdc28_0,” “cdc28_10,” etc. Here the part of the string before the underscore gives the experiment type, and the digits after the underscore give the time point. Because of the differences in naming conventions, we will find it easiest to break up spellman.long into a series of sub-data sets corresponding to each experiment type in order to extract out the experiment and time information. After processing each data subset separately, we will join the modified sub-data frames back together. 8.6.3 Subsetting rows Let’s start by getting just the rows corresponding to the “alpha” experiment/times. Here we use dplyr::filter in combination with stringr::str_detect to get all those rows in which the expt.and.time variable contains the string “alpha.” alpha.long &lt;- spellman.long %&gt;% filter(str_detect(expt.and.time, &quot;alpha&quot;)) # look at the new data frame dim(alpha.long) ## [1] 111204 5 head(alpha.long, n = 10) ## # A tibble: 10 x 5 ## gene std.name description expt.and.time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III trans… alpha0 -0.15 ## 2 YAL00… VPS8 Membrane-binding component of the C… alpha0 -0.11 ## 3 YAL00… EFB1 Translation elongation factor 1 bet… alpha0 -0.14 ## 4 YAL00… &lt;NA&gt; Dubious open reading frame; unlikel… alpha0 -0.02 ## 5 YAL00… SSA1 ATPase involved in protein folding … alpha0 -0.05 ## 6 YAL00… ERP2 Member of the p24 family involved i… alpha0 -0.6 ## 7 YAL00… FUN14 Integral mitochondrial outer membra… alpha0 -0.28 ## 8 YAL00… SPO7 Putative regulatory subunit of Nem1… alpha0 -0.03 ## 9 YAL01… MDM10 Subunit of both the ERMES and the S… alpha0 -0.05 ## 10 YAL01… SWC3 Protein of unknown function; compon… alpha0 -0.31 8.6.4 Splitting columns Having subsetted the data, we can now split expt.and.time into two new variables – expt and time. To do this we use tidyr::extract. alpha.long %&lt;&gt;% tidyr::extract(expt.and.time, # column we&#39;re extracting from c(&quot;expt&quot;, &quot;time&quot;), # new columns we&#39;re creating regex=&quot;(alpha)([[:digit:]]+)&quot;, # regexp (see below) convert=TRUE) # automatically convert column types # NOTE: I&#39;m being explict about saying tidyr::extract because the # magrittr package defines a different extract function Let’s take a moment to look at the regex argument to extract – regex=\"(alpha)([[:digit:]]+)\". The regex is specified as a character string. Each part we want to match and extract is surround by parentheses. In this case we have two sets of parentheses corresponding to the two matches we want to make. The first part of the regex is (alpha); here we’re looking to make an exact match to the string “alpha.” The second part of the regex reads ([[:digit:]]+). [[:digit:]] indicates we’re looking for a numeric digit. The + after [[:digit:]] indicates that we want to match one or more digits (i.e. to get a match we need to find at least one digit, but more than one digit should also be a match). Let’s take a look at the new version of alpha.long following application of extract: head(alpha.long, n = 10) ## # A tibble: 10 x 6 ## gene std.name description expt time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 0 -0.15 ## 2 YAL00… VPS8 Membrane-binding component of the COR… alpha 0 -0.11 ## 3 YAL00… EFB1 Translation elongation factor 1 beta;… alpha 0 -0.14 ## 4 YAL00… &lt;NA&gt; Dubious open reading frame; unlikely … alpha 0 -0.02 ## 5 YAL00… SSA1 ATPase involved in protein folding an… alpha 0 -0.05 ## 6 YAL00… ERP2 Member of the p24 family involved in … alpha 0 -0.6 ## 7 YAL00… FUN14 Integral mitochondrial outer membrane… alpha 0 -0.28 ## 8 YAL00… SPO7 Putative regulatory subunit of Nem1p-… alpha 0 -0.03 ## 9 YAL01… MDM10 Subunit of both the ERMES and the SAM… alpha 0 -0.05 ## 10 YAL01… SWC3 Protein of unknown function; componen… alpha 0 -0.31 Notice our two new variables, both of which have appropriate types! A data frame for the elutriation data can be created similarly: elu.long &lt;- spellman.long %&gt;% filter(str_detect(expt.and.time, &quot;elu&quot;)) %&gt;% tidyr::extract(expt.and.time, # column we&#39;re extracting from c(&quot;expt&quot;, &quot;time&quot;), # new columns we&#39;re creating regex=&quot;(elu)([[:digit:]]+)&quot;, # regexp (see below) convert=TRUE) # automatically convert column types 8.6.4.1 A fancier regex for the cdc experiments Now let’s process the cdc experiments (cdc15 and cdc28). As before we extract the corresponding rows of the data frame using filter and str_detect. We then split expt.and.time using tidyr::extract. In this case we carry out the two steps in a single code block using pipes: cdc.long &lt;- spellman.long %&gt;% # both cdc15 and cdc28 contain &quot;cdc&quot; as a sub-string filter(str_detect(expt.and.time, &quot;cdc&quot;)) %&gt;% tidyr::extract(expt.and.time, c(&quot;expt&quot;, &quot;time&quot;), regex=&quot;(cdc15|cdc28)_([[:digit:]]+)&quot;, # note the fancier regex convert=TRUE) The regex – \"(cdc15|cdc28)_([[:digit:]]+)\" – is slightly fancier in this example. As before there are two parts we’re extracting: (cdc15|cdc28) and ([[:digit:]]+). The first parenthesized regexp is an “OR” – i.e. match “cdc15” or “cdc28.” The second parenthesized regexp is the same as we saw previously. Separating the two parenthesized regexps is an underscore (_). The underscore isn’t parenthesized because we only want to use it to make a match not to extract the corresponding match. 8.6.5 Combining data frames If you have two or more data frames with identical columns, the rows of the data frames can be combined into a single data frame using rbind (defined in the base package). For example, to reassemble the alpha.long, elu.long, and cdc.long data frames into a single data frame we do: spellman.final &lt;- rbind(alpha.long, elu.long, cdc.long) # check the dimensions of the new data frame dim(spellman.final) ## [1] 450994 6 8.6.6 Sorting data frame rows Currently the spellman.final data frame is sorted by time point and experiment. head(spellman.final, n = 10) ## # A tibble: 10 x 6 ## gene std.name description expt time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 0 -0.15 ## 2 YAL00… VPS8 Membrane-binding component of the COR… alpha 0 -0.11 ## 3 YAL00… EFB1 Translation elongation factor 1 beta;… alpha 0 -0.14 ## 4 YAL00… &lt;NA&gt; Dubious open reading frame; unlikely … alpha 0 -0.02 ## 5 YAL00… SSA1 ATPase involved in protein folding an… alpha 0 -0.05 ## 6 YAL00… ERP2 Member of the p24 family involved in … alpha 0 -0.6 ## 7 YAL00… FUN14 Integral mitochondrial outer membrane… alpha 0 -0.28 ## 8 YAL00… SPO7 Putative regulatory subunit of Nem1p-… alpha 0 -0.03 ## 9 YAL01… MDM10 Subunit of both the ERMES and the SAM… alpha 0 -0.05 ## 10 YAL01… SWC3 Protein of unknown function; componen… alpha 0 -0.31 It might be useful instead to sort by gene and experiment. To do this we can use dplyr::arrange: spellman.final %&lt;&gt;% arrange(gene, expt) # look again at the rearranged data head(spellman.final, n = 10) ## # A tibble: 10 x 6 ## gene std.name description expt time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 0 -0.15 ## 2 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 7 -0.15 ## 3 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 14 -0.21 ## 4 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 21 0.17 ## 5 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 28 -0.42 ## 6 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 35 -0.44 ## 7 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 42 -0.15 ## 8 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 49 0.24 ## 9 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 56 -0.1 ## 10 YAL00… TFC3 Subunit of RNA polymerase III transcr… alpha 63 NA 8.7 Using your tidy data Whew – that was a fair amount of work to tidy our data! But having done so we can now carry out a wide variety of very powerful analyses. 8.7.1 Visualizing gene expression time series Let’s start by walking through a series of visualizations of gene expression time series. Each plot will show the expression of one or more genes, at different time points, in one or more experimental conditions. Our initial visualizations exploit the “long” versions of the tidy data. First a single gene in a single experimental condition: spellman.final %&gt;% filter(expt == &quot;alpha&quot;, gene == &quot;YAL022C&quot;) %&gt;% ggplot(aes(x = time, y = expression)) + geom_line() + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression of YAL022C&quot;) We can easily modify the above code block to visualize the expression of multiple genes of interest: genes.of.interest &lt;- c(&quot;YAL022C&quot;, &quot;YAR018C&quot;, &quot;YGR188C&quot;) spellman.final %&gt;% filter(gene %in% genes.of.interest, expt == &quot;alpha&quot;) %&gt;% ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of multiple genes\\nfollowing synchronization by alpha factor&quot;) By employing facet_wrap() we can visualize the relationship between this set of genes in each of the experiment types: spellman.final %&gt;% filter(gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + facet_wrap(~ expt) + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of Multiple Genes\\nAcross experiments&quot;) The different experimental treatments were carried out for varying lengths of time due to the differences in their physiological effects. Plotting them all on the same time scale can obscure that patterns of oscillation we might be interested in, so let’s modify our code block so that plots that share the same y-axis, but have differently scaled x-axes. spellman.final %&gt;% filter(gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + facet_wrap(~ expt, scales = &quot;free_x&quot;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of Multiple Genes\\nAcross experiments&quot;) 8.7.2 Finding the most variable genes When dealing with vary large data sets, one ad hoc filtering criteria that is often employed is to focus on those variables that exhibit that greatest variation (variation is measure of the spread of data; we will give a precise definition in a later lecture). To do this, we first need to order our variables (genes) by their variation. Let’s see how we can accomplish this using our long data frame: by.variance &lt;- spellman.final %&gt;% group_by(gene) %&gt;% summarize(expression.var = var(expression, na.rm = TRUE)) %&gt;% arrange(desc(expression.var)) head(by.variance) ## # A tibble: 6 x 2 ## gene expression.var ## &lt;chr&gt; &lt;dbl&gt; ## 1 YLR286C 2.16 ## 2 YNR067C 1.73 ## 3 YNL327W 1.65 ## 4 YGL028C 1.57 ## 5 YHL028W 1.52 ## 6 YKL164C 1.52 The code above calculates the variance of each gene but ignores the fact that we have different experimental conditions. To take into account the experimental design of the data at hand, let’s calculate the average variance across the experimental conditions: by.avg.variance &lt;- spellman.final %&gt;% group_by(gene, expt) %&gt;% summarize(expression.var = var(expression, na.rm = TRUE)) %&gt;% group_by(gene) %&gt;% summarize(avg.expression.var = mean(expression.var)) %&gt;% arrange(desc(avg.expression.var)) ## `summarise()` has grouped output by &#39;gene&#39;. You can override using the `.groups` argument. head(by.avg.variance) ## # A tibble: 6 x 2 ## gene avg.expression.var ## &lt;chr&gt; &lt;dbl&gt; ## 1 YFR014C 3.58 ## 2 YFR053C 2.38 ## 3 YBL032W 2.30 ## 4 YDR274C 2.17 ## 5 YLR286C 2.13 ## 6 YMR206W 1.94 Based on the average experession variance across experimental conditions, let’s get the names of the 1000 most variable genes: top.genes.1k &lt;- by.avg.variance[1:1000,]$gene head(top.genes.1k) ## [1] &quot;YFR014C&quot; &quot;YFR053C&quot; &quot;YBL032W&quot; &quot;YDR274C&quot; &quot;YLR286C&quot; &quot;YMR206W&quot; 8.7.3 Heat maps In our prior visualizations we’ve used line plots to depict how gene expression changes over time. For example here are line plots for 15 genes in the data set, in the cdc28 experimental conditions: genes.of.interest &lt;- c(&quot;YHR084W&quot;, &quot;YBR083W&quot;, &quot;YPL049C&quot;, &quot;YDR480W&quot;, &quot;YGR040W&quot;, &quot;YLR229C&quot;, &quot;YDL159W&quot;, &quot;YBL016W&quot;, &quot;YDR103W&quot;, &quot;YJL157C&quot;, &quot;YNL271C&quot;, &quot;YDR461W&quot;, &quot;YHL007C&quot;, &quot;YHR005C&quot;, &quot;YJR086W&quot;) spellman.final %&gt;% filter(expt == &quot;cdc28&quot;, gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = expression, color=gene)) + geom_line() + labs(x = &quot;Time (min)&quot;, y = &quot;Expression&quot;) Even with just 10 overlapping line plots, this figure is quite busy and it’s hard to make out the individual behavior of each gene. An alternative approach to depicting such data is a “heat map” which depicts the same information in a grid like form, with the expression values indicated by color. Heat maps are good for depicting large amounts of data and providing a coarse “10,000 foot view.” We can create a heat map using geom_tile as follows: spellman.final %&gt;% filter(expt == &quot;cdc28&quot;, gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + xlab(&quot;Time (mins)&quot;) This figure represents the same information as our line plot, but now there is row for each gene, and the expression of that gene at a given time point is represented by color (scale given on the right). Missing data is shown as gray boxes. Unfortunately, the default color scale used by ggplot is a very subtle gradient from light to dark blue. This make it hard to distinguish patterns of change. Let’s now see how we can improve that. 8.7.3.1 Better color schemes with RColorBrewer The RColorBrewer packages provides nice color schemes that are useful for creating heat maps. RColorBrewer defines a set of color palettes that have been optimized for color discrimination, many of which are color blind friendly, etc. Install the RColorBrewer package using the command line or the RStudio GUI. Once you’ve installed the RColorBrewer package you can see the available color palettes as so: library(RColorBrewer) # show representations of the palettes par(cex = 0.5) # reduce size of text in the following plot display.brewer.all() We’ll use the Red-to-Blue (“RdBu”) color scheme defined in RColorBrewer, however we’ll reverse the scheme so blues represent low expression and reds represent high expression. We’ll divide the range of color values into 9 discrete bins. # displays the RdBu color scheme divided into a palette of 9 colors display.brewer.pal(9, &quot;RdBu&quot;) # assign the reversed (blue to red) RdBu palette color.scheme &lt;- rev(brewer.pal(9,&quot;RdBu&quot;)) Now let’s regenerate the heat map we created previously with this new color scheme. To do this we specify a gradient color scale using the scale_fill_gradientn() function from ggplot. In addition to specifying the color scale, we also constrain the limits of the scale to insure it’s symmetric about zero. spellman.final %&gt;% filter(expt == &quot;cdc28&quot;, gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme, limits = c(-2.5, 2.5)) + xlab(&quot;Time (mins)&quot;) 8.7.3.2 Looking for patterns using sorted data and heat maps The real power of heat maps becomes apparent when you you rearrange the rows of the heat map to emphasize patterns of interest. For example, let’s create a heat map in which we sort genes by the time of their maximal expression. This is one way to identify genes that reach their peak expression at similar times, which is one criteria one might use to identify genes acting in concert. For simplicities sake we will restrict our attention to the cdc28 experiment, and only consider the 1000 most variables genes with no more than one missing observation in this experimental condition. cdc28 &lt;- spellman.final %&gt;% filter(expt == &quot;cdc28&quot;) %&gt;% group_by(gene) %&gt;% filter(sum(is.na(expression)) &lt;= 1) %&gt;% ungroup # removes grouping information from data frame top1k.genes &lt;- cdc28 %&gt;% group_by(gene) %&gt;% summarize(expression.var = var(expression, na.rm = TRUE)) %&gt;% arrange(desc(expression.var)) %$% gene[1:1000] top1k.cdc28 &lt;- cdc28 %&gt;% filter(gene %in% top1k.genes) To find the time of maximum expression we’ll employ the function which.max (which.min), which finds the index of the maximum (minimum) element of a vector. For example to find the index of the maximum expression measurement for YAR018C we could do: top1k.cdc28 %&gt;% filter(gene == &quot;YAR018C&quot;) %$% # note the exposition pipe operator! which.max(expression) ## [1] 8 From the code above we find that the index of the observation at which YAR018C is maximal at 8. To get the corresponding time point we can do something like this: top1k.cdc28 %&gt;% filter(gene == &quot;YAR018C&quot;) %$% # again note the exposition pipe operator! time[which.max(expression)] ## [1] 70 Thus YAR018C expression peaks at 70 minutes in the cdc28 experiment. To find the index of maximal expression of all genes we can apply the dplyr::group_by() and dplyr::summarize() functions peak.expression.cdc28 &lt;- top1k.cdc28 %&gt;% group_by(gene) %&gt;% summarise(peak = which.max(expression)) head(peak.expression.cdc28) ## # A tibble: 6 x 2 ## gene peak ## &lt;chr&gt; &lt;int&gt; ## 1 YAL003W 10 ## 2 YAL005C 2 ## 3 YAL022C 17 ## 4 YAL028W 5 ## 5 YAL035C-A 12 ## 6 YAL038W 15 Let’s sort the order of genes by their peak expression: peak.expression.cdc28 %&lt;&gt;% arrange(peak) We can then generate a heatmap where we sort the rows (genes) of the heatmap by their time of peak expression. We introduce a new geom – geom_raster – which is like geom_tile but better suited for large data (hundreds to thousands of rows) The explicit sorting of the data by peak expression is carried out in the call to scale_y_discrete() where the limits (and order) of this axis are set with the limits argument (see scale_y_discrete and discrete_scale in the ggplot2 docs). # we reverse the ordering because geom_raster (and geom_tile) # draw from the bottom row up, whereas we want to depict the # earliest peaking genes at the top of the figure gene.ordering &lt;- rev(peak.expression.cdc28$gene) top1k.cdc28 %&gt;% ggplot(aes(x = time, y = gene)) + geom_raster(aes(fill = expression)) + # scale_fill_gradientn(limits=c(-2.5, 2.5), colors=color.scheme) + scale_y_discrete(limits=gene.ordering) + labs(x = &quot;Time (mins)&quot;, y = &quot;Genes&quot;, title = &quot;1000 most variable genes&quot;, subtitle = &quot;Sorted by time of peak expression&quot;) + # the following line suppresses tick and labels on y-axis theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) Figure 8.1: A Heatmap showing genes in the cdc28 experiment, sorted by peak expression The brightest red regions in each row of the heat map correspond to the times of peak expression, and the sorting of the rows helps to highlight those gene whose peak expression times are similar. 8.8 Long-to-wide conversion using tidyr::spread Our long data frame consists of four variables – gene, expt, time, and expression. This made it easy to create visualizations and summaries where time and expression were the primaries variables of interest, and gene and experiment type were categories we could condition on. To facilitate analyses that emphasize comparison between genes, we want to create a new data frame in which each gene is itself treated as a variable of interest along with time, and experiment type remains a categorical variable. In this new data frame rather than just four columns in our data frame, we’ll have several thousand columns – one for each gene. To accomplish this reshaping of data, we’ll use the function tidyr::spread(). tidyr::spread() is the inverse of tidyr::gather(). gather() took multiple columns and collapsed them together into a smaller number of new columns. The tidyr documentation calls this “collapsing into key-value pairs.” By contrast, spread() creates new columns by spreading “key-value pairs” (a column representing the “keys” and a column reprsenting the “values”) into multiple columns. Here let’s use spread() to use the gene names (the “key”) and expression measures (the “values”) to create a new data frame where the genes are the primary variables (columns) of the data. spellman.wide &lt;- spellman.final %&gt;% dplyr::select(-std.name, -description) %&gt;% # drop unneeded columns spread(gene, expression) Now let’s examine the dimensions of this wide version of the data: dim(spellman.wide) ## [1] 73 6180 And here’s a visual view of the first few rows and columns of the wide data: spellman.wide[1:5, 1:8] ## # A tibble: 5 x 8 ## expt time YAL001C YAL002W YAL003W YAL004W YAL005C YAL007C ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alpha 0 -0.15 -0.11 -0.14 -0.02 -0.05 -0.6 ## 2 alpha 7 -0.15 0.1 -0.71 -0.48 -0.53 -0.45 ## 3 alpha 14 -0.21 0.01 0.1 -0.11 -0.47 -0.13 ## 4 alpha 21 0.17 0.06 -0.32 0.12 -0.06 0.35 ## 5 alpha 28 -0.42 0.04 -0.4 -0.03 0.11 -0.01 From this view we infer that the rows of the data set represent the various combination of experimental condition and time points, and the columns represents the 6178 genes in the data set plus the two columns for expt and time. 8.9 Exploring bivariate relationships using “wide” data The “long” version of our data frame proved useful for exploring how gene expression changed over time. By contrast, our “wide” data frame is more convient for exploring how pairs of genes covary together. For example, we can generate bivariate scatter plots depicting the relationship between two genes of interest: two.gene.plot &lt;- spellman.wide %&gt;% filter(!is.na(YAR018C) &amp; !is.na(YAL022C)) %&gt;% # remove NAs ggplot(aes(x = YAR018C, y = YAL022C)) + geom_point() + theme(aspect.ratio = 1) two.gene.plot From the scatter plot we infer that the two genes are “positively correlated” with each other, meaning that high values of one tend to be associated with high values of the other (and the same for low values). We can easily extend this visualization to facet the plot based on the experimental conditions: two.gene.plot + facet_wrap(~expt, nrow = 2, ncol = 2) A statistic we use to measure the degree of association between pairs of continuous variables is called the “correlation coefficient.” Briefly, correlation is a measure of linear association between a pair of variables, and ranges from -1 to 1. A value near zero indicates the variables are uncorrelated (no linear association), while values approaching +1 indicate a strong positive association (the variables tend to get bigger or smaller together) while values near -1 indicate strong negative association (when one variable is larger, the other tends to be small). Let’s calculate the correlation between YAR018C and YAL022C: spellman.wide %&gt;% filter(!is.na(YAR018C) &amp; !is.na(YAL022C)) %&gt;% summarize(cor = cor(YAR018C, YAL022C)) ## # A tibble: 1 x 1 ## cor ## &lt;dbl&gt; ## 1 0.692 The value of the correlation coefficient for YAR018C and YAL022C, ~0.69, indicates a fairly strong association between the two genes. As we did for our visualization, we can also calculate the correlation coefficients for the two genes under each experimental condition: spellman.wide %&gt;% filter(!is.na(YAR018C) &amp; !is.na(YAL022C)) %&gt;% group_by(expt) %&gt;% summarize(cor = cor(YAR018C, YAL022C)) ## # A tibble: 4 x 2 ## expt cor ## &lt;chr&gt; &lt;dbl&gt; ## 1 alpha 0.634 ## 2 cdc15 0.575 ## 3 cdc28 0.847 ## 4 elu 0.787 This table suggests that the the strength of correlation between YAR018C and YAL022C may depend on the experimental conditions, with the highest correlations evident in the cdc28 and elu experiments. 8.9.1 Large scale patterns of correlations Now we’ll move from considering the correlation between two specific genes to looking at the correlation between many pairs of genes. As we did in the previous section, we’ll focus specifically onthe 1000 most variable genes in the cdc28 experiment. First we filter our wide data set to only consider the cdc28 experiment and those genes in the top 1000 most variable genes in cdc28: top1k.cdc28.wide &lt;- top1k.cdc28 %&gt;% dplyr::select(-std.name, -description) %&gt;% spread(gene, expression) With this restricted data set, we can then calculate the correlations between every pair of genes as follows: cdc28.correlations &lt;- top1k.cdc28.wide %&gt;% dplyr::select(-expt, -time) %&gt;% # drop expt and time cor(use = &quot;pairwise.complete.obs&quot;) The argument use = \"pairwise.complete.obs\" tells the correlation function that for each pair of genes to use only the pariwse where there is a value for both genes (i.e. neither one can be NA). Given \\(n\\) genes, there are \\(n \\times n\\) pairs of correlations, as seen by the dimensions of the correlation matrix. dim(cdc28.correlations) ## [1] 1000 1000 To get the correlations with a gene of interest, we can index with the gene name on the rows of the correlation matrix. For example, to get the correlations between the gene YAR018C and the first 10 genes in the top 1000 set: cdc28.correlations[&quot;YAR018C&quot;,1:10] ## YAL003W YAL005C YAL022C YAL028W YAL035C-A YAL038W ## 0.07100626 -0.53315493 0.84741624 0.33379901 -0.22316755 -0.03984599 ## YAL044C YAL048C YAL060W YAL062W ## 0.32253692 0.12220221 0.49445700 -0.60972118 In the next statement we extract the names of the genes that have correlations with YAR018C greater than 0.6. First we test genes to see if they have a correlation with YAR018C greater than 0.6, which returns a vector of TRUE or FALSE values. This vector of Boolean values is than used to index into the rownames of the correlation matrix, pulling out the gene names where the statement was true. pos.corr.YAR018C &lt;- rownames(cdc28.correlations)[cdc28.correlations[&quot;YAR018C&quot;,] &gt; 0.6] length(pos.corr.YAR018C) ## [1] 65 We then return to our long data to show this set of genes that are strongly positively correlated with YAR018C. top1k.cdc28 %&gt;% filter(gene %in% pos.corr.YAR018C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33) + theme(legend.position = &quot;none&quot;) As is expected, genes with strong positive correlations with YAR018C. show similar temporal patterns with YAR018C. We can similarly filter for genes that have negative correlations with YAR018C. neg.corr.YAR018C &lt;- colnames(cdc28.correlations)[cdc28.correlations[&quot;YAR018C&quot;,] &lt;= -0.6] As before we generate a line plot showing these genes: top1k.cdc28 %&gt;% filter(gene %in% neg.corr.YAR018C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33) + theme(legend.position = &quot;none&quot;) 8.9.2 Adding new columns and combining filtered data frames Now let’s create a new data frame by: 1) filtering on our list of genes that have strong positive and negative correlations with YAR018C; and 2) creating a new variable, “corr.with.YAR018C,” which indicates the sign of the correlation. We’ll use this new variable to group genes when we create the plot. pos.corr.df &lt;- top1k.cdc28 %&gt;% filter(gene %in% pos.corr.YAR018C) %&gt;% mutate(corr.with.YAR018C = &quot;positive&quot;) neg.corr.df &lt;- top1k.cdc28 %&gt;% filter(gene %in% neg.corr.YAR018C) %&gt;% mutate(corr.with.YAR018C = &quot;negative&quot;) combined.pos.neg &lt;- rbind(pos.corr.df, neg.corr.df) Finally, we plot the data, colored according to the correlation with YAR018C: ggplot(combined.pos.neg, aes(x = time, y = expression, group = gene, color = corr.with.YAR018C)) + geom_line(alpha=0.25) + geom_line(aes(x = time, y = expression), data = filter(top1k.cdc28, gene == &quot;YAR018C&quot;), color = &quot;DarkRed&quot;, size = 2,alpha=0.5) + # changes legend title and values for color sclae scale_color_manual(name = &quot;Correlation with YAR018C&quot;, values = c(&quot;blue&quot;, &quot;red&quot;)) + labs(title = &quot;Genes strongly positively and negatively correlated with YAR018C&quot;, subtitle = &quot;YAR018C shown in dark red&quot;, x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;) 8.9.3 A heat mapped sorted by correlations In our previous heat map example figure, we sorted genes according to peak expression. Now let’s generate a heat map for the genes that are strongly correlated (both positive and negative) with YAR018C. We will sort the genes according to the sign of their correlation. # re-factor gene names so positive and negative genes are spatially distinct in plot combined.pos.neg$gene &lt;- factor(combined.pos.neg$gene, levels = c(pos.corr.YAR018C, neg.corr.YAR018C)) combined.pos.neg %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme) + xlab(&quot;Time (mins)&quot;) The breakpoint between the positively and negatively correlated sets of genes is quite obvious in this figure. 8.9.4 A “fancy” figure Recall that we introduced the cowplot library in Chapter 6, as a way to combine different ggplot outputs into subfigures such as you might find in a published paper. Here we’ll make further use cowplot to combine our heat map and line plot visualizations of genes that covary with YAR018C. library(cowplot) cowplot’s draw_plot() function allows us to place plots at arbitrary locations and with arbitrary sizes onto the canvas. The coordinates of the canvas run from 0 to 1, and the point (0, 0) is in the lower left corner of the canvas. We’ll use draw_plot to draw a complex figure with a heatmap on the left, and two smaller line plots on the right. pos.corr.lineplot &lt;- combined.pos.neg %&gt;% filter(gene %in% pos.corr.YAR018C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33, color = &#39;red&#39;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;, title = &quot;Genes Positively correlated\\nwith YAR018C&quot;) neg.corr.lineplot &lt;- combined.pos.neg %&gt;% filter(gene %in% neg.corr.YAR018C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33, color = &#39;blue&#39;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;, title = &quot;Genes negatively correlated\\nwith YAR018C&quot;) heat.map &lt;- ggplot(combined.pos.neg, aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme) + labs(x = &quot;Time (mins)&quot;, y = &quot;Gene&quot;) + theme(legend.position = &quot;none&quot;) The coordinates of the canvas run from 0 to 1, and the point (0, 0) is in the lower left corner of the canvas. We’ll use draw_plot to draw a complex figure with a heatmap on the left, and two smaller line plots on the right. I determined the coordinates below by experimentation to create a visually pleasing layout. fancy.plot &lt;- ggdraw() + draw_plot(heat.map, 0, 0, width = 0.6) + draw_plot(neg.corr.lineplot, 0.6, 0.5, width = 0.4, height = 0.5) + draw_plot(pos.corr.lineplot, 0.6, 0, width = 0.4, height = 0.5) + draw_plot_label(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c(0, 0.6, 0.6), c(1, 1, 0.5), size = 15) fancy.plot "],["vector-algebra.html", "Chapter 9 Vector algebra 9.1 Libraries 9.2 Vector Mathematics in R 9.3 Simple statistics in vector form", " Chapter 9 Vector algebra 9.1 Libraries library(tidyverse) library(magrittr) 9.2 Vector Mathematics in R R vectors support basic arithmetic operations that correspond to the same operations on geometric vectors. For example: &gt; x &lt;- 1:15 &gt; y &lt;- 10:24 &gt; x ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 &gt; y ## [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 &gt; x + y # vector addition ## [1] 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 &gt; x - y # vector subtraction ## [1] -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 &gt; x * 3 # multiplication by a scalar ## [1] 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 R also has an operator for the dot product, denoted %*%. This operator also designates matrix multiplication, which we will discuss in the next chapter. By default this operator returns an object of the R matrix class. If you want a scalar (or the R equivalent of a scalar, i.e. a vector of length 1) you need to use the drop() function. &gt; z &lt;- x %*% x &gt; class(z) # note use of class() function ## [1] &quot;matrix&quot; &quot;array&quot; &gt; z ## [,1] ## [1,] 1240 &gt; drop(z) ## [1] 1240 In lecture we saw that many useful geometric properties of vectors could be expressed in the form of dot products. Let’s start with some two-dimensional vectors where the geometry is easy to visualize: &gt; a &lt;- c(2, 0) # the point (2,0) &gt; b &lt;- c(1, 3) # the point (1,3) To draw our vectors using ggplot, we’ll need to create a data frame with columns representing the x,y coordinates of the end-points of our vectors: df &lt;- data.frame(x.end = c(a[1], b[1]), y.end = c(a[2], b[2]), label = c(&#39;a&#39;, &#39;b&#39;)) ggplot(df) + geom_segment(aes(x=0, y = 0, xend = x.end, yend = y.end, color=label), arrow = arrow()) + labs(x = &quot;x-coordinate&quot;, y = &quot;y-coordinate&quot;) + coord_fixed(ratio = 1) + # insures x and y axis scale are same theme_bw() Let’s see what the dot product can tell us about these vectors. First recall that we can calculate the length of a vector as the square-root of the dot product of the vector with itself (\\(\\vert\\vec{a}\\vert^2 = \\vec{a} \\cdot \\vec{a}\\)) &gt; len.a &lt;- drop(sqrt(a %*% a)) &gt; len.a ## [1] 2 &gt; len.b &lt;- drop(sqrt(b %*% b)) &gt; len.b ## [1] 3.162278 How about the angle between \\(a\\) and \\(b\\)? First we can use the dot product and the previously calculated lengths to calculate the cosine of the angle between the vectors: &gt; cos.ab &lt;- (a %*% b)/(len.a * len.b) &gt; cos.ab ## [,1] ## [1,] 0.3162278 To go from the cosine of the angle to the angle (in radians) we need the arc-cosine function, acos(): &gt; acos(cos.ab) # given angle in radians ## [,1] ## [1,] 1.249046 9.3 Simple statistics in vector form Now let’s turn our attention to seeing how to calculate a variety of simple statistics such as the mean, variance, etc. in terms of vector operations. To illustrate these oeprations we’ll use the I. setosa data from the iris examplar data set. setosa &lt;- filter(iris, Species == &quot;setosa&quot;) 9.3.1 Mean First let’s calculate the mean for the Sepal.Length variable. Referring back to the slides for today’s lecture, we see we can calculate the mean as: \\[ \\bar{x} = \\frac{\\vec{1} \\cdot \\vec{x}}{\\vec{1} \\cdot \\vec{1}} \\] Applying this formula in R: &gt; sepal.length &lt;- setosa$Sepal.Length &gt; ones &lt;- rep(1, length(sepal.length)) # 1-vector of length n &gt; mean.sepal.length &lt;- (ones %*% sepal.length)/(ones %*% ones) &gt; mean.sepal.length %&lt;&gt;% drop # use drop to convert back to scalar &gt; mean.sepal.length ## [1] 5.006 Let’s compare our calculation against the built-in mean function: &gt; mean(sepal.length) ## [1] 5.006 9.3.2 Mean centering Mean centering a vector, means subtracting the mean from each element of that vector: \\[ \\vec{x}_c = \\vec{x} - \\bar{x}\\vec{1} \\] Now let’s create a mean centered vector from sepal.length, which we’ll refer to as the vector of deviates about the mean: &gt; sepal.length.deviates &lt;- sepal.length - mean.sepal.length Note that we didn’t have to explicitly multiply the a one vector by the mean, as R will automatically make the lengths of the sepal.length (a vector of length 150) and mean.sepal.length (a vector of length 1) match by vector recycling. 9.3.3 Variance and standard deviation Using the vector of deviates we can easily calculate the variance and standard deviation of a variable. The variance of a variable, in vector algebraic terms, is: \\[ S_x^2 = \\frac{\\vec{x}_c \\cdot \\vec{x}_c}{n-1} \\] The standard deviation is simply the square root of the variance \\[ S_x = \\sqrt{S_x^2} \\] These calculations for the Sepal.Length variable: &gt; n &lt;- length(sepal.length.deviates) &gt; var.sepal.length &lt;- (sepal.length.deviates %*% sepal.length.deviates)/(n-1) &gt; var.sepal.length ## [,1] ## [1,] 0.124249 &gt; sd.sepal.length &lt;- sqrt(var.sepal.length) &gt; sd.sepal.length ## [,1] ## [1,] 0.3524897 Again, we can compare our calculations to the built-in var() and sd() functions: &gt; var(sepal.length) ## [1] 0.124249 &gt; sd(sepal.length) ## [1] 0.3524897 9.3.4 Covariance and correlation Now let’s consider the common measures of bivariate association, covariance and correlation. Covariance is: \\[ S_{XY} = \\frac{\\vec{x} \\cdot \\vec{y}}{n-1} \\] Correlation is: \\[ r_{XY} = \\frac{\\vec{x} \\cdot \\vec{y}}{|\\vec{x}||\\vec{y}|} = \\frac{S_{XY}}{S_x S_Y} \\] We’ll examine the relationship between sepal length and width: sepal.width &lt;- setosa$Sepal.Width mean.sepal.width &lt;- (ones %*% sepal.width)/(ones %*% ones) sepal.width.deviates &lt;- sepal.width - mean.sepal.width var.sepal.width &lt;- drop((sepal.width.deviates %*% sepal.width.deviates)/(n-1)) sd.sepal.width &lt;- sqrt(var.sepal.width) With the vector of sepal width deviates in hand we can now calculate covariances: &gt; cov.swidth.slength &lt;- (sepal.length.deviates %*% sepal.width.deviates)/(n-1) &gt; cov.swidth.slength ## [,1] ## [1,] 0.09921633 &gt; cov(sepal.length, sepal.width) # and compare to built-in covariance ## [1] 0.09921633 And correlations: &gt; len.sepal.length &lt;- sqrt(sepal.length.deviates %*% sepal.length.deviates) &gt; len.sepal.width &lt;- sqrt(sepal.width.deviates %*% sepal.width.deviates) &gt; &gt; corr.swidth.slength &lt;- + (sepal.length.deviates %*% sepal.width.deviates) / (len.sepal.length * len.sepal.width) &gt; corr.swidth.slength ## [,1] ## [1,] 0.7425467 &gt; cor(sepal.length, sepal.width) # and compare to built-in correlation ## [1] 0.7425467 Alternately, we could have calculated the correlation more simply as follows: &gt; cov.swidth.slength/(sd.sepal.length * sd.sepal.width) ## [,1] ## [1,] 0.7425467 "],["matrices-in-r.html", "Chapter 10 Matrices in R 10.1 Creating matrices in R 10.2 Matrix arithmetic operations in R 10.3 Descriptive statistics as matrix functions 10.4 Matrix Inverse 10.5 Solving sets of simultaneous equations", " Chapter 10 Matrices in R In R matrices are two-dimensional collections of elements all of which have the same mode or type. This is different than a data frame in which the columns of the frame can hold elements of different type (but all of the same length), or from a list which can hold objects of arbitrary type and length. Matrices are more efficient for carrying out most numerical operations, so if you’re working with a very large data set that is amenable to representation by a matrix you should consider using this data structure. library(tidyverse) 10.1 Creating matrices in R There are a number of different ways to create matrices in R. For creating small matrices at the command line you can use the matrix() function. &gt; x &lt;- matrix(1:5) # creates a column vector &gt; x ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 ## [5,] 5 &gt; X &lt;- matrix(1:12, nrow=4) # creates a matrix &gt; X ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 &gt; dim(X) # give the shape of the matrix ## [1] 4 3 matrix() takes a data vector as input and the shape of the matrix to be created is specified by using the nrow and ncol arguments. If the number of elements in the input data vector is less than nrows \\(\\times\\) ncols the elements will be ‘recycled’ as discussed in previous chapters. Without any shape arguments the matrix() function will create a column vector as shown above. By default the matrix() function fills in the matrix in a column-wise fashion. To fill in the matrix in a row-wise fashion use the argument byrow=T. If you have a pre-existing data set in a list or data frame you can use the as.matrix() function to convert it to a matrix. &gt; iris.mtx &lt;- as.matrix(iris) &gt; head(iris.mtx) # NOTE: the elements were all converted to character ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## [1,] &quot;5.1&quot; &quot;3.5&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; ## [2,] &quot;4.9&quot; &quot;3.0&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; ## [3,] &quot;4.7&quot; &quot;3.2&quot; &quot;1.3&quot; &quot;0.2&quot; &quot;setosa&quot; ## [4,] &quot;4.6&quot; &quot;3.1&quot; &quot;1.5&quot; &quot;0.2&quot; &quot;setosa&quot; ## [5,] &quot;5.0&quot; &quot;3.6&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; ## [6,] &quot;5.4&quot; &quot;3.9&quot; &quot;1.7&quot; &quot;0.4&quot; &quot;setosa&quot; Since all elements of an R matrix must be of the same type, when we passed the iris data frame to as.matrix(), everything was converted to a character due to the presence of the Species column in the data frame. &gt; # This is probably more along the lines of what you want &gt; iris.mtx &lt;- iris %&gt;% dplyr::select(-Species) %&gt;% as.matrix &gt; head(iris.mtx) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## [1,] 5.1 3.5 1.4 0.2 ## [2,] 4.9 3.0 1.4 0.2 ## [3,] 4.7 3.2 1.3 0.2 ## [4,] 4.6 3.1 1.5 0.2 ## [5,] 5.0 3.6 1.4 0.2 ## [6,] 5.4 3.9 1.7 0.4 You can use the various indexing operations to get particular rows, columns, or elements. Here are some examples: &gt; X &lt;- matrix(1:12, nrow=4) &gt; X ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 &gt; X[1,] # get the first row ## [1] 1 5 9 &gt; X[,1] # get the first column ## [1] 1 2 3 4 &gt; X[1:2,] # get the first two rows ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 &gt; X[,2:3] # get the second and third columns ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10 ## [3,] 7 11 ## [4,] 8 12 &gt; Y &lt;- matrix(1:12, byrow=T, nrow=4) &gt; Y ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 &gt; Y[4] # see explanation below ## [1] 10 &gt; Y[5] ## [1] 2 &gt; dim(Y) &lt;- c(2,6) # reshape Y &gt; Y ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 7 2 8 3 9 ## [2,] 4 10 5 11 6 12 &gt; Y[5] ## [1] 2 The example above where we create a matrix Y is meant to show that matrices are stored internally in a column wise fashion (think of the columns stacked one atop the other), regardless of whether we use the byrow=T argument. Therefore using single indices returns the elements with respect to this arrangement. Note also the use of assignment operator in conjuction with the dim() function to reshape the matrix. Despite the reshaping, the internal representation in memory hasn’t changed so Y[5] still gives the same element. You can use the diag() function to get the diagonal of a matrix or to create a diagonal matrix as show below: &gt; Z &lt;- matrix(rnorm(16), ncol=4) &gt; Z ## [,1] [,2] [,3] [,4] ## [1,] 2.0251885 2.0174258 -0.6052043 1.4415494 ## [2,] -0.3728585 0.3460140 -1.3688444 -0.1282668 ## [3,] -0.3033103 0.2451794 -0.4126611 -0.4608993 ## [4,] 1.7492453 1.1036075 0.4314619 0.4923353 &gt; diag(Z) ## [1] 2.0251885 0.3460140 -0.4126611 0.4923353 &gt; diag(5) # create the 5 x 5 identity matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 &gt; s &lt;- sqrt(10:13) &gt; diag(s) ## [,1] [,2] [,3] [,4] ## [1,] 3.162278 0.000000 0.000000 0.000000 ## [2,] 0.000000 3.316625 0.000000 0.000000 ## [3,] 0.000000 0.000000 3.464102 0.000000 ## [4,] 0.000000 0.000000 0.000000 3.605551 10.2 Matrix arithmetic operations in R The standard mathematical operations of addition and subtraction and scalar multiplication work element-wise for matrices in the same way as they did for vectors. Matrix multiplication uses the operator %*% which you saw last week for the dot product. To get the transpose of a matrix use the function t(). &gt; A &lt;- matrix(1:12, nrow=4) &gt; A ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 &gt; &gt; B &lt;- matrix(rnorm(12), nrow=4) &gt; B ## [,1] [,2] [,3] ## [1,] 0.2685323 0.7264978 1.16234345 ## [2,] -1.1877063 0.5425726 0.02106534 ## [3,] 0.1142667 -1.4874033 -0.98231868 ## [4,] -0.4246680 0.8533398 -0.09887401 &gt; &gt; t(A) # transpose ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 &gt; A + B # matrix addition ## [,1] [,2] [,3] ## [1,] 1.2685323 5.726498 10.16234 ## [2,] 0.8122937 6.542573 10.02107 ## [3,] 3.1142667 5.512597 10.01768 ## [4,] 3.5753320 8.853340 11.90113 &gt; A - B # matrix subtraction ## [,1] [,2] [,3] ## [1,] 0.7314677 4.273502 7.837657 ## [2,] 3.1877063 5.457427 9.978935 ## [3,] 2.8857333 8.487403 11.982319 ## [4,] 4.4246680 7.146660 12.098874 &gt; 5 * A # multiplication by a scalar ## [,1] [,2] [,3] ## [1,] 5 25 45 ## [2,] 10 30 50 ## [3,] 15 35 55 ## [4,] 20 40 60 When applying matrix multiplication, the dimensions of the matrices involved must be conformable. For example, you can’t do this: A %*% B # do you understand why this generates an error? But this works: &gt; A %*% t(B) ## [,1] [,2] [,3] [,4] ## [1,] 14.36211 1.7147450 -16.16362 2.952165 ## [2,] 16.51949 1.0906767 -18.51907 3.281963 ## [3,] 18.67686 0.4666084 -20.87453 3.611761 ## [4,] 20.83423 -0.1574599 -23.22998 3.941558 10.3 Descriptive statistics as matrix functions Assume you have a data set represented as a \\(n \\times p\\) matrix, \\(X\\), with observations in rows and variables in columns. Below I give formulae for calculating some descriptive statistics as matrix functions. 10.3.1 Mean vector and matrix You can calculate a row vector of means, \\(\\mathbf{m}\\), as: \\[ \\mathbf{m} = \\frac{1}{n} \\mathbf{1}^T X \\] where \\(1\\) is a \\(n \\times 1\\) vector of ones. A \\(n \\times p\\) matrix \\(M\\) where each column is filled with the mean value for that column is: \\[ M = \\mathbf{1}\\mathbf{m} \\] 10.3.2 Deviation matrix To re-express each value as the deviation from the variable means (i.e.~each columns is a mean centered vector) we calculate a deviation matrix: \\[ D = X - M \\] 10.3.3 Covariance matrix The \\(p \\times p\\) covariance matrix can be expressed as a matrix product of the deviation matrix: \\[ S = \\frac{1}{n-1} D^T D \\] 10.3.4 Correlation matrix The correlation matrix, \\(R\\), can be calculated from the covariance matrix by: \\[ R = V S V \\] where \\(V\\) is a \\(p \\times p\\) diagonal matrix where \\(V_{ii} = 1/\\sqrt{S_{ii}}\\). 10.4 Matrix Inverse The function solve() can be used to find matrix inverses in R. A &lt;- matrix(1:4, nrow=2) A ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Ainv &lt;- solve(A) Ainv ## [,1] [,2] ## [1,] -2 1.5 ## [2,] 1 -0.5 A %*% Ainv # should give identity matrix ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 Ainv %*% A # should also result in identity matrix ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 Keep in mind that not all square matrices are invertible: C &lt;- matrix(1:16, nrow=4) C ## [,1] [,2] [,3] [,4] ## [1,] 1 5 9 13 ## [2,] 2 6 10 14 ## [3,] 3 7 11 15 ## [4,] 4 8 12 16 Cinv &lt;- solve(C) ## Error in solve.default(C): Lapack routine dgesv: system is exactly singular: U[3,3] = 0 10.5 Solving sets of simultaneous equations The solve() function introduced above can also be used to solve sets of simultaneous equations. For example, given the set of equations below: \\[ \\begin{eqnarray*} x_1 + 3x_2 + 2x_3 &amp; = &amp; 3\\\\ -x_1 + x_2 + 2x_3 &amp; = &amp; -2\\\\ 2x_1 + 4x_2 -x_3 &amp; = &amp; 10 \\end{eqnarray*} \\] We can rewrite this in vector form as: \\[x_1 \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 4 \\end{bmatrix} + x_3 \\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 10 \\end{bmatrix} \\] which is equivalent to the matrix form: \\[ \\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\ -1 &amp; 1 &amp; 2\\\\ 2 &amp; 4 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 10 \\end{bmatrix} \\] solve() takes two arguments: - a – a square numeric matrix containing the coefficients of the linear equations (the left-most matrix above) - b – a vector (or matrix) giving the right hand side of the linear equations First let’s create the matrix of coefficients on the left and right sides: A = matrix( c(c(1, 3 , 2), c(-1, 1, 2), c(2, 4, 1)), nrow=3, ncol=3, byrow=TRUE) b = c(3, -2, 10) Now we solve the equations: x &lt;- solve(A,b) x ## [1] -2.25 4.75 -4.50 Let’s confirm the solution works by multiplying A by x: A %*% x ## [,1] ## [1,] 3 ## [2,] -2 ## [3,] 10 Voila! "]]
