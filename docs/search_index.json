[["index.html", "Biology 723: Statistical Computing for Biologists Chapter 1 Introduction 1.1 Accessing older versions of the course notes 1.2 How to use these lecture notes", " Biology 723: Statistical Computing for Biologists Paul M. Magwene 2023-03-08 Chapter 1 Introduction Bio 723 is a course I offer at Duke University. The focus of this course is statistical computing for the biological sciences with an emphasis on common multivariate statistical methods and techniques for exploratory data analysis. A major goal of the course is to help graduate students in the biological sciences develop practical insights into methods that they are likely to encounter in their own research, and the potential advantages and pitfalls that come with their use. In terms of mathematical perspectives, the course emphasize a geometric approach to understanding multivariate statistics. I try to help students develop an intuition for the geometry of vector spaces and discuss topics like correlation, regression, and principal components analysis in terms of angles between vectors, dot products, and projection. This course also provides an introduction to the R language/statistical computing environment. 1.1 Accessing older versions of the course notes The material covered in Bio 723 changes a bit from year to year. If you’d like to access older versions of the course notes, I will be making these available on the “Releases” page of the Github site for this book. 1.2 How to use these lecture notes In this and future materials to be posted on the course website you’ll encounter blocks of R code. Your natural intuition will be to cut and paste commands or code blocks into the R interpretter to save yourself the typing. DO NOT DO THIS!! In each of the examples below, I provide example input, but I don’t show you the output. It’s your job to type in these examples at the R console, evaluate what you typed, and to look at and think critically about the output. You will make mistakes and generate errors! Part of learning any new skill is making mistakes, figuring out where you went wrong, and correcting those mistakes. In the process of fixing those errors, you’ll learn more about how R works, and how to avoid such errors, or correct bugs in your own code in the future. If you cut and paste the examples into the R interpretter the code will run, but you will learn less than if you input the code yourself and you’ll be less capable of apply the concepts in new situations. The R interpretter, like all programming languages, is very exacting. A mispelled variable or function name, a stray period, or an unbalanced parenthesis will raise an error and finding the sources of such errors can sometimes be tedious and frustrating. Persist! If you read your code critically, think about what your doing, and seek help when needed (teaching team, R help, Google, etc) you’ll eventually get better at debugging your code. But keep in mind that like most new skills, learning to write and debug your code efficiently takes time and experience. "],["getting-started-with-r.html", "Chapter 2 Getting Started with R 2.1 What is R? 2.2 What is RStudio? 2.3 Entering commands in the console 2.4 Comments 2.5 Using R as a Calculator 2.6 Variable assignment 2.7 Data types 2.8 Packages 2.9 The R Help System", " Chapter 2 Getting Started with R 2.1 What is R? R is a statistical computing environment and programming language. It is free, open source, and has a large and active community of developers and users. There are many different R packages (libraries) available for conducting out a wide variety of different analyses, for everything from genome sequence data to geospatial information. 2.2 What is RStudio? RStudio (http://www.rstudio.com/) is an open source integrated development environment (IDE) that provides a nicer graphical interface to R than does the default R GUI. The figure below illustrates the RStudio interface, in it’s default configuration. For the exercises below you’ll be primarily entering commands in the “console” window. We’ll review key parts of the RStudio interface in greater detail in class. Figure 2.1: RStudio window with the panes labeled 2.3 Entering commands in the console You can type commands directly in the console. When you hit Return (Enter) on your keyboard the text you typed is evaluated by the R interpreter. This means that the R program reads your commands, makes sure there are no syntax errors, and then carries out any commands that were specified. Try evaluating the following arithmetic commands in the console: 10 + 5 10 - 5 10 / 5 10 * 5 If you type an incomplete command and then hit Return on your keyboard, the console will show a continuation line marked by a + symbol. For example enter the incomplete statement (10 + 5 and then hit Enter. You should see something like this. &gt; (10 + 5 + The continuation line tells you that R is waiting for additional input before it evaluates what you typed. Either complete your command (e.g. type the closing parenthesis) and hit Return, or hit the “Esc” key to exit the continuation line without evaluating what you typed. 2.4 Comments When working in the R console, or writing R code, the pound symbol (#) indicates the start of a comment. Anything after the #, up to the end of the current line, is ignored by the R interpretter. # This line will be ignored 5 + 4 # the first part of this line, up to the #, will be evaluated Throughout this course I will often include short explanatory comments in my code examples. When I want to display the output generated by an R statement typed at the console I will generally use a display convention in which I prepend the results with the symbols ##. 5 + 4 # same as above but with output displayed ## [1] 9 2.5 Using R as a Calculator The simplest way to use R is as a fancy calculator. Evaluate each of the following statements in the console. 10 + 2 # addition 10 - 2 # subtraction 10 * 2 # multiplication 10 / 2 # division 10 ^ 2 # exponentiation 10 ** 2 # alternate exponentiation pi * 2.5^2 # R knows about some constants such as Pi 10 %% 3 # modulus operator -- gives remainder after division 10 %/% 3 # integer division Be aware that certain operators have precedence over others. For example multiplication and division have higher precedence than addition and subtraction. Use parentheses to disambiguate potentially confusing statements. (10 + 2)/4-5 # was the output what you expected? (10 + 2)/(4-5) # compare the answer to the above Division by zero produces an object that represents infinite numbers. Infinite values can be either positive or negative 1/0 ## [1] Inf -1/0 ## [1] -Inf Invalid calculations produce a objected called NaN which is short for “Not a Number”: 0/0 # invalid calculation ## [1] NaN 2.5.1 Common mathematical functions Many commonly used mathematical functions are built into R. Here are some examples: abs(-3) # absolute value ## [1] 3 cos(pi/3) # cosine ## [1] 0.5 sin(pi/3) # sine ## [1] 0.8660254 log(10) # natural logarithm ## [1] 2.302585 log10(10) # log base 10 ## [1] 1 log2(10) # log base 2 ## [1] 3.321928 exp(1) # exponential function ## [1] 2.718282 sqrt(10) # square root ## [1] 3.162278 10^0.5 # same as square root ## [1] 3.162278 2.6 Variable assignment An important programming concept in all programming languages is that of “variable assignment”. Variable assignment is the act of creating labels that point to particular data values in a computers memory, which allows us to apply operations to the labels rather than directly to specific. Variable assignment is an important mechanism of abstracting and generalizing computational operations. Variable assignment in R is accomplished with the assignment operator, which is designated as &lt;- (left arrow, constructed from a left angular brack and the minus sign). This is illustrated below: x &lt;- 10 # assign the variable name &#39;x&#39; the value 10 sin(x) # apply the sin function to the value x points to ## [1] -0.5440211 x &lt;- pi # x now points to a different value sin(x) # the same function call now produces a different result ## [1] 1.224647e-16 # note that sin(pi) == 0, but R returns a floating point value very # very close to but not zero 2.6.1 Valid variable names As described in the R documentation, “A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number. Names such as ‘.2way’ are not valid, and neither are the reserved words.” Here are some examples of valid and invalid variable names. Mentally evaluate these based on the definition above, and then evaluate these in the R interpetter to confirm your understanding : x &lt;- 10 x.prime &lt;- 10 x_prime &lt;- 10 my.long.variable.name &lt;- 10 another_long_variable_name &lt;- 10 _x &lt;- 10 .x &lt;- 10 2.x &lt;- 2 * x 2.7 Data types The phrase “data types” refers to the representations of information that a programming language provides. In R, there are three core data types representing numbes, logical values, and strings. You can use the function typeof() to get information about an objects type in R. 2.7.1 Numeric data types There are three standard types of numbers in R. “double” – this is the default numeric data type, and is used to represent both real numbers and whole numbers (unless you explicitly ask for integers, see below). “double” is short for “double precision floating point value”. All of the previous computations you’ve seen up until this point used data of type double. typeof(10.0) # real number ## [1] &quot;double&quot; typeof(10) # whole numbers default to doubles ## [1] &quot;double&quot; “integer” – when your numeric data involves only whole numbers, you can get slighly better performance using the integer data type. You must explicitly ask for numbers to be treated as integers. typeof(as.integer(10)) # now treated as an integer ## [1] &quot;integer&quot; “complex” – R has a built-in data type to represent complex numbers – numbers with a “real” and “imaginary” component. We won’t encounter the use of complex numbers in this course, but they do have many important uses in mathematics and engineering and also have some interesting applications in biology. typeof(1 + 0i) ## [1] &quot;complex&quot; sqrt(-1) # sqrt of -1, using doubles ## [1] NaN sqrt(-1 + 0i) # sqrt of -1, using complex numbers ## [1] 0+1i 2.7.2 Logical values When we compare values to each other, our calculations no longer return “doubles” but rather TRUE and FALSE values. This is illustrated below: 10 &lt; 9 # is 10 less than 9? ## [1] FALSE 10 &gt; 9 # is 10 greater than 9? ## [1] TRUE 10 &lt;= (5 * 2) # less than or equal to? ## [1] TRUE 10 &gt;= pi # greater than or equal to? ## [1] TRUE 10 == 10 # equals? ## [1] TRUE 10 != 10 # does not equal? ## [1] FALSE TRUE and FALSE objects are of “logical” data type (known as “Booleans” in many other languages, after the mathematician George Boole). typeof(TRUE) typeof(FALSE) x &lt;- FALSE typeof(x) # x points to a logical x &lt;- 1 typeof(x) # the variable x no longer points to a logical When working with numerical data, tests of equality can be tricky. For example, consider the following two comparisons: 10 == (sqrt(10)^2) # Surprised by the result? See below. 4 == (sqrt(4)^2) # Even more confused? Mathematically we know that both \\((\\sqrt{10})^2 = 10\\) and \\((\\sqrt{4})^2 = 4\\) are true statements. Why does R tell us the first statement is false? What we’re running into here are the limits of computer precision. A computer can’t represent \\(\\sqrt 10\\) exactly, whereas \\(\\sqrt 4\\) can be exactly represented. Precision in numerical computing is a complex subject and a detailed discussion is beyond the scope of this course. However, it’s important to be aware of this limitation (this limitation is true of any programming language, not just R). To test “near equality” R provides a function called all.equal(). This function takes two inputs – the numerical values to be compared – and returns TRUE if their values are equal up to a certain level of tolerance (defined by the built-in numerical precision of your computer). all.equal(10, sqrt(10)^2) ## [1] TRUE Here’s another example where the simple equality operator returns an unexpected result, but all.equal() produces the comparison we’re likely after. sin(pi) == 0 ## [1] FALSE all.equal(sin(pi), 0) ## [1] TRUE 2.7.2.1 Logical operators Logical values support Boolean operations, like logical negation (“not”), “and”, “or”, “xor”, etc. This is illustrated below: !TRUE # logical negation -- reads as &quot;not x&quot; ## [1] FALSE TRUE &amp; FALSE # AND: are x and y both TRUE? ## [1] FALSE TRUE | FALSE # OR: are either x or y TRUE? ## [1] TRUE xor(TRUE,FALSE) # XOR: is either x or y TRUE, but not both? ## [1] TRUE The function isTRUE can be useful for evaluating the state of a variable: x &lt;- sample(1:10, 1) # sample a random number in the range 1 to 10 isTRUE(x &gt; 5) # was the random number picked greater than 5? ## [1] FALSE 2.7.3 Character strings Character strings (“character”) represent single textual characters or a longer sequence of characters. They are created by enclosing the characters in text either single our double quotes. typeof(&quot;abc&quot;) # double quotes ## [1] &quot;character&quot; typeof(&#39;abc&#39;) # single quotes ## [1] &quot;character&quot; Character strings have a length, which can be found using the nchar function: first.name &lt;- &quot;jasmine&quot; nchar(first.name) ## [1] 7 last.name &lt;- &#39;smith&#39; nchar(last.name) ## [1] 5 There are a number of built-in functions for manipulating character strings. Here are some of the most common ones. 2.7.3.1 Joining strings The paste() function joins two characters strings together: paste(first.name, last.name) # join two strings ## [1] &quot;jasmine smith&quot; paste(&quot;abc&quot;, &quot;def&quot;) ## [1] &quot;abc def&quot; Notice that paste() adds a space between the strings? If we didn’t want the space we can call the paste() function with an optional argument called sep (short for separator) which specifies the character(s) that are inserted between the joined strings. paste(&quot;abc&quot;, &quot;def&quot;, sep = &quot;&quot;) # join with no space; &quot;&quot; is an empty string ## [1] &quot;abcdef&quot; paste0(&quot;abc&quot;, &quot;def&quot;) # an equivalent function with no space in newer version of R ## [1] &quot;abcdef&quot; paste(&quot;abc&quot;, &quot;def&quot;, sep = &quot;|&quot;) # join with a vertical bar ## [1] &quot;abc|def&quot; 2.7.3.2 Splitting strings The strsplit() function allows us to split a character string into substrings according to matches to a specified split string (see ?strsplit for details). For example, we could break a sentence into it’s constituent words as follows: sentence &lt;- &quot;Call me Ishmael.&quot; words &lt;- strsplit(sentence, &quot; &quot;) # split on space words ## [[1]] ## [1] &quot;Call&quot; &quot;me&quot; &quot;Ishmael.&quot; Notice that strsplit() is the reverse of paste(). 2.7.3.3 Substrings The substr() function allows us to extract a substring from a character object by specifying the first and last positions (indices) to use in the extraction: substr(&quot;abcdef&quot;, 2, 5) # get substring from characters 2 to 5 ## [1] &quot;bcde&quot; substr(first.name, 1, 3) # get substring from characters 1 to ## [1] &quot;jas&quot; 2.8 Packages Packages are libraries of R functions and data that provide additional capabilities and tools beyond the standard library of functions included with R. Hundreds of people around the world have developed packages for R that provide functions and related data structures for conducting many different types of analyses. Throughout this course you’ll need to install a variety of packages. Here I show the basic procedure for installing new packages from the console as well as from the R Studio interface. 2.8.1 Installing packages from the console The function install.packages() provides a quick and conveniet way to install packages from the R console. 2.8.2 Install the tidyverse package To illustrate the use of install.packages(), we’ll install a collection of packages (a “meta-package”) called the tidyverse. Here’s how to install the tidyverse meta-package from the R console: install.packages(&quot;tidyverse&quot;, dependencies = TRUE) The first argument to install.packages gives the names of the package we want to install. The second argument, dependencies = TRUE, tells R to install any additional packages that tidyverse depends on. 2.8.3 Installing packages from the RStudio dialog You can also install packages using a graphical dialog provided by RStudio. To do so pick the Packages tab in RStudio, and then click the Install button. Figure 2.2: The Packages tab in RStudio In the packages entry box you can type the name of the package you wish to install. Let’s install another useful package called “stringr”. Type the package name in the “Packages” field, make sure the “Install dependencies” check box is checked, and then press the “Install” button. Figure 2.3: Package Install Dialog 2.8.4 Loading packages with the library() function Once a package is installed on your computer, the package can be loaded into your R session using the library function. To insure our previous install commands worked correctly, let’s load one of the packages we just installed. library(tidyverse) Since the tidyverse pacakge is a “meta-package” it provides some additional info about the sub-packages that got loaded. When you load tidyverse, you will also see a message about “Conflicts” as several of the functions provided in the dplyr package (a sub-package in tidyverse) conflict with names of functions provided by the “stats” package which usually gets automically loaded when you start R. The conflicting funcdtions are filter and lag. The conflicting functions in the stats package are lag and filter which are used in time series analysis. The dplyr functions are more generally useful. Furthermore, if you need these masked functions you can still access them by prefacing the function name with the name of the package (e.g. stats::filter). We will use the “tidyverse” package for almost every class session and assignment in this class. Get in the habit of including the library(tidyverse) statement in all of your R documents. 2.9 The R Help System R comes with fairly extensive documentation and a simple help system. You can access HTML versions of the R documentation under the Help tab in Rstudio. The HTML documentation also includes information on any packages you’ve installed. Take a few minutes to browse through the R HTML documentation. In addition to the HTML documentation there is also a search box where you can enter a term to search on (see red arrow in figure below). Figure 2.4: The RStudio Help tab 2.9.1 Getting help from the console In addition to getting help from the RStudio help tab, you can directly search for help from the console. The help system can be invoked using the help function or the ? operator. help(&quot;log&quot;) ?log If you are using RStudio, the help results will appear in the “Help” tab of the Files/Plots/Packages/Help/Viewer (lower right window by default). What if you don’t know the name of the function you want? You can use the help.search() function. help.search(&quot;log&quot;) In this case help.search(\"log\") returns all the functions with the string log in them. For more on help.search type ?help.search. Other useful help related functions include apropos() and example(), vignette(). apropos returns a list of all objects (including variable names and function names) in the current session that match the input string. apropos(&quot;log&quot;) ## [1] &quot;as.data.frame.logical&quot; &quot;as.logical&quot; &quot;as.logical.factor&quot; ## [4] &quot;dlogis&quot; &quot;is.logical&quot; &quot;log&quot; ## [7] &quot;log10&quot; &quot;log1p&quot; &quot;log2&quot; ## [10] &quot;logb&quot; &quot;Logic&quot; &quot;logical&quot; ## [13] &quot;logLik&quot; &quot;loglin&quot; &quot;plogis&quot; ## [16] &quot;qlogis&quot; &quot;rlogis&quot; &quot;SSlogis&quot; example() provides examples of how a function is used. example(log) ## ## log&gt; log(exp(3)) ## [1] 3 ## ## log&gt; log10(1e7) # = 7 ## [1] 7 ## ## log&gt; x &lt;- 10^-(1+2*1:9) ## ## log&gt; cbind(x, log(1+x), log1p(x), exp(x)-1, expm1(x)) ## x ## [1,] 1e-03 9.995003e-04 9.995003e-04 1.000500e-03 1.000500e-03 ## [2,] 1e-05 9.999950e-06 9.999950e-06 1.000005e-05 1.000005e-05 ## [3,] 1e-07 1.000000e-07 1.000000e-07 1.000000e-07 1.000000e-07 ## [4,] 1e-09 1.000000e-09 1.000000e-09 1.000000e-09 1.000000e-09 ## [5,] 1e-11 1.000000e-11 1.000000e-11 1.000000e-11 1.000000e-11 ## [6,] 1e-13 9.992007e-14 1.000000e-13 9.992007e-14 1.000000e-13 ## [7,] 1e-15 1.110223e-15 1.000000e-15 1.110223e-15 1.000000e-15 ## [8,] 1e-17 0.000000e+00 1.000000e-17 0.000000e+00 1.000000e-17 ## [9,] 1e-19 0.000000e+00 1.000000e-19 0.000000e+00 1.000000e-19 The vignette() function gives longer, more detailed documentation about libraries. Not all libraries include vignettes, but for those that do it’s usually a good place to get started. For example, the stringr package (which we installed above) includes a vignette. To read it’s vignette, type the following at the console vignette(&quot;stringr&quot;) "],["r-markdown-and-r-notebooks.html", "Chapter 3 R Markdown and R Notebooks 3.1 R Notebooks 3.2 Creating an R Notebook 3.3 The default R Notebook template 3.4 Code and Non-code blocks 3.5 Running a code chunk 3.6 Running all code chunks above 3.7 “Knitting” R Markdown to HTML 3.8 Sharing your reproducible R Notebook", " Chapter 3 R Markdown and R Notebooks RStudio comes with a useful set of tools, collectively called R Markdown, for generating “literate” statistical analyses. The idea behind literate statistical computing is that we should try to carry out our analyses in a manner that is transparent, self-explanatory, and reproducible. Literate statistical computing helps to ensure your research is reproducible because: The steps of your analyses are explicitly described, both as written text and the code and function calls used. Analyses can be more easily checked for correctness and reproduced from your literate code. Your literate code can serve as a template for future analyses, saving you time and the trouble of remembering all the gory details. As we’ll see, R Markdown will allow us to produce statistical documents that integrate prose, code, figures, and nicely formatted mathematics so that we can share and explain our analyses to others. Sometimes those “others” are advisors, supervisors, or collaborators; sometimes the “other” is you six months from now. For the purposes of this class, you will be asked to complete problem sets in the form of R Markdown documents. R Markdown documents are written in a light-weight markup language called Markdown. Markdown provides simple plain text “formatting” commands for specifying the structured elements of a document. Markdown was invented as a lightweight markup language for creating web pages and blogs, and has been adopted to a variety of different purposes. This chaptern provides a brief introduction to the capabilities of R Markdown. For more complete details, including lots of examples, see the R Markdown Website. 3.1 R Notebooks We’re going to create a type of R Markdown document called an “R Notebook”. The R Notebook Documentation describes R Notebooks as so: “An R Notebook is an R Markdown document with code chunks that can be executed independently and interactively, with output visible immediately beneath the input.” 3.2 Creating an R Notebook To create an R Notebook select File &gt; New File &gt; R Notebook from the files menu in RStudio. Figure 3.1: Using the File menu to create a new R Notebook. 3.3 The default R Notebook template The standard template that RStudio creates for you includes a header section like the following where you can specify document properties such as the title, author, and change the look and feel of the generated HTML document. --- title: &quot;R Notebook&quot; output: html_notebook --- The header is followed by several example sections that illustrate a few of the capabilities of R Markdown. Delete these and replace them with your own code as necessary. 3.4 Code and Non-code blocks R Markdown documents are divided into code blocks (also called “chunks”) and non-code blocks. Code blocks are sets of R commands that will be evalauted when the R Markdown document is run or “knitted” (see below). Non-code blocks include explanatory text, embedded images, etc. The default notebook template includes both code and non-code blocks. 3.4.1 Non-code blocks The first bit of text in the default notebook template is a non-code block that tells you how to use the notebook: This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. The text of non-code blocks can include lightweight markup information that can be used to format HTML or PDF output generated from the R Markdown document. Here are some examples: # Simple textual formatting This is a paragraph with plain text. Nothing fancy will happen here. This is a second paragraph with *italic*, **bold**, and `verbatim` text. # Lists ## Bullet points lists This is a list with bullet points: * Item a * Item b * Item c ## Numbered lists This is a numbered list: 1. Item 1 #. Item 2 #. Item 3 ## Mathematics R Markdown supports mathematical equations, formatted according to LaTeX conventions. Dollar signs ($) are used to offset mathematics like so: $x^2 + y^2 = z^2$. Notice from the example above that R Markdown supports LaTeX style formatting of mathematical equations. For example, $x^2 + y^2 = z^2$ appears as \\(x^2 + y^2 = z^2\\). 3.4.2 Code blocks Code blocks are delimited by matching sets of three backward ticks (```). Everything within a code block is interpretted as an R command and is evaluated by the R interpretter. Here’s the first code block in the default notebook template: ``` {r} plot(cars) ``` 3.5 Running a code chunk You can run a single code block by clicking the small green “Run” button in the upper right hand corner of the code block as shown in the image below. Figure 3.2: Click the Run button to execute a code chunk. If you click this button the commands within this code block are executed, and any generated output is shown below the code block. Try running the first code block in the default template now. After the code chunk is executed you should see a plot embedded in your R Notebook as shown below: Figure 3.3: An R Notebook showing an embedded plot after executing a code chunk. 3.6 Running all code chunks above Next to the “Run” button in each code chunk is a button for “Run all chunks above” (see figure below). This is useful when the code chunk you’re working on depends on calculations in earlier code chunks, and you want to evaluated those earlier code chunks prior to running the focal code chunk. Figure 3.4: Use the ‘Run all chunks above’ button to evaluate all previous code chunks. 3.7 “Knitting” R Markdown to HTML Save your R Notebook as first_rnotebook.Rmd (RStudio will automatically add the .Rmd extension so you don’t need to type it). You can generate an HTML version of your notebook by clicking the “Preview” menu on the Notebook taskbar and then choosing “Knit to HTML” (see image below). Figure 3.5: Use the ‘Knit to HTML’ menu to generate HTML output from your R Notebook When an RMarkdown document is “knit”, all of the code and non-code blocks are executed in a “clean” environment, in order from top to bottom. An output file is generated (HTML or one of the other available output types) that shows the results of executing the notebook. By default RStudio will pop-up a window showing you the HTML output you generated. Knitting a document is a good way to make sure your analysis is reproducible. If your code compiles correctly when the document is knit, and produces the expected output, there’s a good chance that someone else will be able to reproduce your analyses independently starting with your R Notebook document (after accounting for differences in file locations). 3.8 Sharing your reproducible R Notebook To share your R Notebook with someone else you just need to send them the source R Markdown file (i.e. the file with the .Rmd extension). Assuming they have access to the same source data, another user should be able to open the notebook file in RStudio and regenerate your analyses by evaluating the individual code chunks or knitting the document. In this course you will be submitting homework assignments in the form of R Notebook markdown files. "],["data-structures.html", "Chapter 4 Data structures 4.1 Vectors 4.2 Lists 4.3 Data frames", " Chapter 4 Data structures In computer science, the term “data structure” refers to the ways that data are stored, retrieved, and organized in a computer’s memory. Common examples include lists, hash tables (also called dictionaries), sets, queues, and trees. Different types of data structures are used to support different types of operations on data. In R, the three basic data structures are vectors, lists, and data frames. 4.1 Vectors Vectors are the core data structure in R. Vectors store an ordered lists of items, all of the same type (i.e. the data in a vector are “homogenous” with respect to their type). The simplest way to create a vector at the interactive prompt is to use the c() function, which is short hand for “combine” or “concatenate”. x &lt;- c(2,4,6,8) # create a vector, assignn it the variable name `x` x ## [1] 2 4 6 8 Vectors in R always have a type (accessed with the typeof() function) and a length (accessed with the length() function). length(x) ## [1] 4 typeof(x) ## [1] &quot;double&quot; Vectors don’t have to be numerical; logical and character vectors work just as well. y &lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE) y ## [1] TRUE TRUE FALSE TRUE FALSE FALSE typeof(y) ## [1] &quot;logical&quot; length(y) ## [1] 6 z &lt;- c(&quot;How&quot;, &quot;now&quot;, &quot;brown&quot;, &quot;cow&quot;) z ## [1] &quot;How&quot; &quot;now&quot; &quot;brown&quot; &quot;cow&quot; typeof(z) ## [1] &quot;character&quot; length(z) ## [1] 4 You can also use c() to concatenate two or more vectors together. x &lt;- c(2, 4, 6, 8) y &lt;- c(1, 3, 5, 7, 9) # create another vector, labeled y xy &lt;- c(x,y) # combine two vectors xy ## [1] 2 4 6 8 1 3 5 7 9 z &lt;- c(pi/4, pi/2, pi, 2*pi) xyz &lt;- c(x, y, z) # combine three vectors xyz ## [1] 2.0000000 4.0000000 6.0000000 8.0000000 1.0000000 3.0000000 5.0000000 ## [8] 7.0000000 9.0000000 0.7853982 1.5707963 3.1415927 6.2831853 4.1.1 Vector Arithmetic The basic R arithmetic operations work on numeric vectors as well as on single numbers (in fact, behind the scenes in R single numbers are vectors!). x &lt;- c(2, 4, 6, 8, 10) x * 2 # multiply each element of x by 2 ## [1] 4 8 12 16 20 x - pi # subtract pi from each element of x ## [1] -1.1415927 0.8584073 2.8584073 4.8584073 6.8584073 y &lt;- c(0, 1, 3, 5, 9) x + y # add together each matching element of x and y ## [1] 2 5 9 13 19 x * y # multiply each matching element of x and y ## [1] 0 4 18 40 90 x/y # divide each matching element of x and y ## [1] Inf 4.000000 2.000000 1.600000 1.111111 Basic numerical functions operate element-wise on numerical vectors: sin(x) ## [1] 0.9092974 -0.7568025 -0.2794155 0.9893582 -0.5440211 cos(x * pi) ## [1] 1 1 1 1 1 log(x) ## [1] 0.6931472 1.3862944 1.7917595 2.0794415 2.3025851 4.1.2 Vector recycling When vectors are not of the same length R “recycles” the elements of the shorter vector to make the lengths conform. x &lt;- c(2, 4, 6, 8, 10) length(x) ## [1] 5 z &lt;- c(1, 4, 7, 11) length(z) ## [1] 4 x + z ## [1] 3 8 13 19 11 In the example above z was treated as if it was the vector (1, 4, 7, 11, 1). Recycling can be useful but it can also be a subtle source of errors. Notice that R provides warning messages when recycling is being applied. Make sure to pay attention to such messages when debugging your code. 4.1.3 Simple statistical functions for numeric vectors Now that we’ve introduced vectors as the simplest data structure for holding collections of numerical values, we can introduce a few of the most common statistical functions that operate on such vectors. First let’s create a vector to hold our sample data of interest. Here I’ve taken a random sample of the lengths of the last names of students enrolled in Bio 723 during Spring 2018. len.name &lt;- c(7, 7, 6, 2, 9, 9, 7, 4, 10, 5) Some common statistics of interest include minimum, maximum, mean, median, variance, and standard deviation: sum(len.name) ## [1] 66 min(len.name) ## [1] 2 max(len.name) ## [1] 10 mean(len.name) ## [1] 6.6 median(len.name) ## [1] 7 var(len.name) # variance ## [1] 6.044444 sd(len.name) # standard deviation ## [1] 2.458545 The summary() function applied to a vector of doubles produce a useful table of some of these key statistics: summary(len.name) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 5.25 7.00 6.60 8.50 10.00 4.1.4 Indexing Vectors Accessing the element of a vector is called “indexing”. Indexing is the process of specifying the numerical positions (indices) that you want to take access from the vector. For a vector of length \\(n\\), we can access the elements by the indices \\(1 \\ldots n\\). We say that R vectors (and other data structures like lists) are “one-indexed”. Many other programming languages, such as Python, C, and Java, use zero-indexing where the elements of a data structure are accessed by the indices \\(0 \\ldots n-1\\). Indexing errors are a common source of bugs. Indexing a vector is done by specifying the index in square brackets as shown below: x &lt;- c(2, 4, 6, 8, 10) length(x) ## [1] 5 x[1] # return the 1st element of x ## [1] 2 x[4] # return the 4th element of x ## [1] 8 Negative indices are used to exclude particular elements. x[-1] returns all elements of x except the first. x[-1] ## [1] 4 6 8 10 You can get multiple elements of a vector by indexing by another vector. In the example below, x[c(3,5)] returns the third and fifth element of x`. x[c(3,5)] ## [1] 6 10 Besides numerical indexing, R allows logical indexing which takes a vector of Booleans and returns the positions with TRUE values. x[c(TRUE, FALSE, TRUE, FALSE, FALSE)] #return 1st and 3rd elements but ignore 2nd, 4th and 5th ## [1] 2 6 4.1.5 Comparison operators applied to vectors When the comparison operators, such as “greater than” (&gt;), “less than or equal to” (&lt;=), equality (==), etc, are applied to numeric vectors, they return logical vectors: x &lt;- c(2, 4, 6, 8, 10, 12) x &lt; 8 # returns TRUE for all elements lass than 8 ## [1] TRUE TRUE TRUE FALSE FALSE FALSE Here’s a fancier example: x &gt; 4 &amp; x &lt; 10 # greater than 4 AND less than 10 ## [1] FALSE FALSE TRUE TRUE FALSE FALSE 4.1.6 Combining Indexing and Comparison of Vectors A very powerful feature of R is the ability to combine the comparison operators (which return TRUE or FALSE values) with indexing. This facilitates data filtering and subsetting. Here’s an example: x &lt;- c(2, 4, 6, 8, 10) x[x &gt; 5] ## [1] 6 8 10 In the first example we retrieved all the elements of x that are larger than 5 (read as “x where x is greater than 5”). Notice how we got back all the elements where the statement in the brackets was TRUE. You can string together comparisons for more complex filtering. x[x &lt; 4 | x &gt; 8] # less than four OR greater than 8 ## [1] 2 10 In the second example we retrieved those elements of x that were smaller than four or greater than six. Combining indexing and comparison is a concept which we’ll use repeatedly in this course. 4.1.7 Vector manipulation You can combine indexing with assignment to change the elements of a vectors: x &lt;- c(2, 4, 6, 8, 10) x[2] &lt;- -4 x ## [1] 2 -4 6 8 10 You can also use indexing vectors to change multiple values at once: x &lt;- c(2, 4, 6, 8, 10) x[c(1, 3, 5)] &lt;- 6 x ## [1] 6 4 6 8 6 Using logical vectors to manipulate the elements of a vector also works: x &lt;- c(2, 4, 6, 8, 10) x[x &gt; 5] = 5 # truncate all values to have max value 5 x ## [1] 2 4 5 5 5 4.1.8 Vectors from regular sequences There are a variety of functions for creating regular sequences in the form of vectors. 1:10 # create a vector with the integer values from 1 to 10 ## [1] 1 2 3 4 5 6 7 8 9 10 20:11 # a vector with the integer values from 20 to 11 ## [1] 20 19 18 17 16 15 14 13 12 11 seq(1, 10) # like 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 seq(1, 10, by = 2) # 1:10, in steps of 2 ## [1] 1 3 5 7 9 seq(2, 4, by = 0.25) # 2 to 4, in steps of 0.25 ## [1] 2.00 2.25 2.50 2.75 3.00 3.25 3.50 3.75 4.00 4.1.9 Additional functions for working with vectors The function unique() returns the unique items in a vector: x &lt;- c(5, 2, 1, 4, 6, 9, 8, 5, 7, 9) unique(x) ## [1] 5 2 1 4 6 9 8 7 rev() returns the items in reverse order (without changing the input vector): y &lt;- rev(x) y ## [1] 9 7 5 8 9 6 4 1 2 5 x # x is still in original order ## [1] 5 2 1 4 6 9 8 5 7 9 There are a number of useful functions related to sorting. Plain sort() returns a new vector with the items in sorted order: sorted.x &lt;- sort(x) # returns items of x sorted sorted.x ## [1] 1 2 4 5 5 6 7 8 9 9 x # but x remains in its unsorted state ## [1] 5 2 1 4 6 9 8 5 7 9 The related function order() gives the indices which would rearrange the items into sorted order: order(x) ## [1] 3 2 4 1 8 5 9 7 6 10 order() can be useful when you want to sort one list by the values of another: students &lt;- c(&quot;fred&quot;, &quot;tabitha&quot;, &quot;beatriz&quot;, &quot;jose&quot;) class.ranking &lt;- c(4, 2, 1, 3) students[order(class.ranking)] # get the students sorted by their class.ranking ## [1] &quot;beatriz&quot; &quot;tabitha&quot; &quot;jose&quot; &quot;fred&quot; any() and all(), return single boolean values based on a specified comparison provided as an argument: y &lt;- c(2, 4, 5, 6, 8) any(y &gt; 5) # returns TRUE if any of the elements are TRUE ## [1] TRUE all(y &gt; 5) # returns TRUE if all of the elements are TRUE ## [1] FALSE which() returns the indices of the vector for which the input is true: which(y &gt; 5) ## [1] 4 5 4.2 Lists R lists are like vectors, but unlike a vector where all the elements are of the same type, the elements of a list can have arbitrary types (even other lists). Lists are a powerful data structure for organizing information, because there are few constraints on the shape or types of the data included in a list. Lists are easy to create: l &lt;- list(&#39;Bob&#39;, pi, 10) Note that lists can contain arbitrary data. Lists can even contain other lists: l &lt;- list(&#39;Bob&#39;, pi, 10, list(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;, &quot;qux&quot;)) Lists are displayed with a particular format, distinct from vectors: l ## [[1]] ## [1] &quot;Bob&quot; ## ## [[2]] ## [1] 3.141593 ## ## [[3]] ## [1] 10 ## ## [[4]] ## [[4]][[1]] ## [1] &quot;foo&quot; ## ## [[4]][[2]] ## [1] &quot;bar&quot; ## ## [[4]][[3]] ## [1] &quot;baz&quot; ## ## [[4]][[4]] ## [1] &quot;qux&quot; In the example above, the correspondence between the list and its display is obvious for the first three items. The fourth element may be a little confusing at first. Remember that the fourth item of l was another list. So what’s being shown in the output for the fourth item is the nested list. An alternative way to display a list is using the str() function (short for “structure”). str() provides a more compact representation that also tells us what type of data each element is: str(l) ## List of 4 ## $ : chr &quot;Bob&quot; ## $ : num 3.14 ## $ : num 10 ## $ :List of 4 ## ..$ : chr &quot;foo&quot; ## ..$ : chr &quot;bar&quot; ## ..$ : chr &quot;baz&quot; ## ..$ : chr &quot;qux&quot; 4.2.1 Length and type of lists Like vectors, lists have length: length(l) ## [1] 4 But the type of a list is simply “list”, not the type of the items within the list. This makes sense because lists are allowed to be heterogeneous (i.e. hold data of different types). typeof(l) ## [1] &quot;list&quot; 4.2.2 Indexing lists Lists have two indexing operators. Indexing a list with single brackets, like we did with vectors, returns a new list containing the element at index \\(i\\). Lists also support double bracket indexing (x[[i]]) which returns the bare element at index \\(i\\) (i.e. the element without the enclosing list). This is a subtle but important point so make sure you understand the difference between these two forms of indexing. 4.2.2.1 Single bracket list indexing First, let’s demonstrate single bracket indexing of the lists l we created above. l[1] # single brackets, returns list(&#39;Bob&#39;) ## [[1]] ## [1] &quot;Bob&quot; typeof(l[1]) # notice the list type ## [1] &quot;list&quot; When using single brackets, lists support indexing with ranges and numeric vectors: l[3:4] ## [[1]] ## [1] 10 ## ## [[2]] ## [[2]][[1]] ## [1] &quot;foo&quot; ## ## [[2]][[2]] ## [1] &quot;bar&quot; ## ## [[2]][[3]] ## [1] &quot;baz&quot; ## ## [[2]][[4]] ## [1] &quot;qux&quot; l[c(1, 3, 5)] ## [[1]] ## [1] &quot;Bob&quot; ## ## [[2]] ## [1] 10 ## ## [[3]] ## NULL 4.2.2.2 Double bracket list indexing If double bracket indexing is used, the object at the given index in a list is returned: l[[1]] # double brackets, return plain &#39;Bob&#39; ## [1] &quot;Bob&quot; typeof(l[[1]]) # notice the &#39;character&#39; type ## [1] &quot;character&quot; Double bracket indexing does not support multiple indices, but you can chain together double bracket operators to pull out the items of sublists. For example: # second item of the fourth item of the list l[[4]][[2]] ## [1] &quot;bar&quot; 4.2.3 Naming list elements The elements of a list can be given names when the list is created: p &lt;- list(first.name=&#39;Alice&#39;, last.name=&quot;Qux&quot;, age=27, years.in.school=10) You can retrieve the names associated with a list using the names function: names(p) ## [1] &quot;first.name&quot; &quot;last.name&quot; &quot;age&quot; &quot;years.in.school&quot; If a list has named elements, you can retrieve the corresponding elements by indexing with the quoted name in either single or double brackets. Consistent with previous usage, single brackets return a list with the corresponding named element, whereas double brackets return the bare element. For example, make sure you understand the difference in the output generated by these two indexing calls: p[&quot;first.name&quot;] ## $first.name ## [1] &quot;Alice&quot; p[[&quot;first.name&quot;]] ## [1] &quot;Alice&quot; 4.2.4 The $ operator Retrieving named elements of lists (and data frames as we’ll see), turns out to be a pretty common task (especially when doing interactive data analysis) so R has a special operator to make this more convenient. This is the $ operator, which is used as illustrated below: p$first.name # equivalent to p[[&quot;first.name&quot;]] ## [1] &quot;Alice&quot; p$age # equivalent to p[[&quot;age&quot;]] ## [1] 27 4.2.5 Changing and adding lists items Combining indexing and assignment allows you to change items in a list: suspect &lt;- list(first.name = &quot;unknown&quot;, last.name = &quot;unknown&quot;, aka = &quot;little&quot;) suspect$first.name &lt;- &quot;Bo&quot; suspect$last.name &lt;- &quot;Peep&quot; suspect[[3]] &lt;- &quot;LITTLE&quot; str(suspect) ## List of 3 ## $ first.name: chr &quot;Bo&quot; ## $ last.name : chr &quot;Peep&quot; ## $ aka : chr &quot;LITTLE&quot; By combining assignment with a new name or an index past the end of the list you can add items to a list: suspect$age &lt;- 17 # add a new item named age suspect[[5]] &lt;- &quot;shepardess&quot; # create an unnamed item at position 5 Be careful when adding an item using indexing, because if you skip an index an intervening NULL value is created: # there are only five items in the list, what happens if we # add a new item at position seven? suspect[[7]] &lt;- &quot;wanted for sheep stealing&quot; str(suspect) ## List of 7 ## $ first.name: chr &quot;Bo&quot; ## $ last.name : chr &quot;Peep&quot; ## $ aka : chr &quot;LITTLE&quot; ## $ age : num 17 ## $ : chr &quot;shepardess&quot; ## $ : NULL ## $ : chr &quot;wanted for sheep stealing&quot; 4.2.6 Combining lists The c (combine) function we introduced to create vectors can also be used to combine lists: list.a &lt;- list(&quot;little&quot;, &quot;bo&quot;, &quot;peep&quot;) list.b &lt;- list(&quot;has lost&quot;, &quot;her&quot;, &quot;sheep&quot;) list.c &lt;- c(list.a, list.b) list.c ## [[1]] ## [1] &quot;little&quot; ## ## [[2]] ## [1] &quot;bo&quot; ## ## [[3]] ## [1] &quot;peep&quot; ## ## [[4]] ## [1] &quot;has lost&quot; ## ## [[5]] ## [1] &quot;her&quot; ## ## [[6]] ## [1] &quot;sheep&quot; 4.2.7 Converting lists to vectors Sometimes it’s useful to convert a list to a vector. The unlist() function takes care of this for us. # a homogeneous list ex1 &lt;- list(2, 4, 6, 8) unlist(ex1) ## [1] 2 4 6 8 When you convert a list to a vector make sure you remember that vectors are homogeneous, so items within the new vector will be “coerced” to have the same type. # a heterogeneous list ex2 &lt;- list(2, 4, 6, c(&quot;bob&quot;, &quot;fred&quot;), list(1 + 0i, &#39;foo&#39;)) unlist(ex2) ## [1] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;bob&quot; &quot;fred&quot; &quot;1+0i&quot; &quot;foo&quot; Note that unlist() also unpacks nested vectors and lists as shown in the second example above. 4.3 Data frames Along with vectors and lists, data frames are one of the core data structures when working in R. A data frame is essentially a list which represents a data table, where each column in the table has the same number of rows and every item in the a column has to be of the same type. Unlike standard lists, the objects (columns) in a data frame must have names. We’ve seen data frames previously, for example when we loaded data sets using the read_csv function. 4.3.1 Creating a data frame While data frames will often be created by reading in a data set from a file, they can also be created directly in the console as illustrated below: age &lt;- c(30, 26, 21, 29, 25, 22, 28, 24, 23, 20) sex &lt;- rep(c(&quot;M&quot;,&quot;F&quot;), 5) wt.in.kg &lt;- c(88, 76, 67, 66, 56, 74, 71, 60, 52, 72) df &lt;- data.frame(age = age, sex = sex, wt = wt.in.kg) Here we created a data frame with three columns, each of length 10. 4.3.2 Type and class for data frames Data frames can be thought of as specialized lists, and in fact the type of a data frame is “list” as illustrated below: typeof(df) ## [1] &quot;list&quot; To distinguish a data frame from a generic list, we have to ask about it’s “class”. class(df) # the class of our data frame ## [1] &quot;data.frame&quot; class(l) # compare to the class of our generic list ## [1] &quot;list&quot; The term “class” comes from a style/approach to programming called “object oriented programming”. We won’t go into explicit detail about how object oriented programming works in this class, though we will exploit many of the features of objects that have a particular class. 4.3.3 Length and dimension for data frames Applying the length() function to a data frame returns the number of columns. This is consistent with the fact that data frames are specialized lists: length(df) ## [1] 3 To get the dimensions (number of rows and columns) of a data frame, we use the dim() function. dim() returns a vector, whose first value is the number of rows and whose second value is the number of columns: dim(df) ## [1] 10 3 We can get the number of rows and columns individually using the nrow() and ncol() functions: nrow(df) # number of rows ## [1] 10 ncol(df) # number of columsn ## [1] 3 4.3.4 Indexing and accessing data frames Data frames can be indexed by either column index, column name, row number, or a combination of row and column numbers. 4.3.4.1 Single bracket indexing of the columns of a data frame The single bracket operator with a single numeric index returns a data frame with the corresponding column. df[1] # get the first column (=age) of the data frame ## # A tibble: 10 × 1 ## age ## &lt;dbl&gt; ## 1 30 ## 2 26 ## 3 21 ## 4 29 ## 5 25 ## 6 22 ## 7 28 ## 8 24 ## 9 23 ## 10 20 The single bracket operator with multiple numeric indices returns a data frame with the corresponding columns. df[1:2] # first two columns ## # A tibble: 10 × 2 ## age sex ## &lt;dbl&gt; &lt;chr&gt; ## 1 30 M ## 2 26 F ## 3 21 M ## 4 29 F ## 5 25 M ## 6 22 F ## 7 28 M ## 8 24 F ## 9 23 M ## 10 20 F df[c(1, 3)] # columns 1 (=age) and 3 (=wt) ## # A tibble: 10 × 2 ## age wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 30 88 ## 2 26 76 ## 3 21 67 ## 4 29 66 ## 5 25 56 ## 6 22 74 ## 7 28 71 ## 8 24 60 ## 9 23 52 ## 10 20 72 Column names can be substituted for indices when using the single bracket operator: df[&quot;age&quot;] ## # A tibble: 10 × 1 ## age ## &lt;dbl&gt; ## 1 30 ## 2 26 ## 3 21 ## 4 29 ## 5 25 ## 6 22 ## 7 28 ## 8 24 ## 9 23 ## 10 20 df[c(&quot;age&quot;, &quot;wt&quot;)] ## # A tibble: 10 × 2 ## age wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 30 88 ## 2 26 76 ## 3 21 67 ## 4 29 66 ## 5 25 56 ## 6 22 74 ## 7 28 71 ## 8 24 60 ## 9 23 52 ## 10 20 72 4.3.4.2 Single bracket indexing of the rows of a data frame To get specific rows of a data frame, we use single bracket indexing with an additional comma following the index. For example to get the first row a data frame we would do: df[1,] # first row ## # A tibble: 1 × 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 30 M 88 This syntax extends to multiple rows: df[1:2,] # first two rows ## # A tibble: 2 × 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 30 M 88 ## 2 26 F 76 df[c(1, 3, 5),] # rows 1, 3 and 5 ## # A tibble: 3 × 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 30 M 88 ## 2 21 M 67 ## 3 25 M 56 4.3.4.3 Single bracket indexing of both the rows and columns of a data frame Single bracket indexing of data frames extends naturally to retrieve both rows and columns simultaneously: df[1, 2] # first row, second column ## [1] &quot;M&quot; df[1:3, 2:3] # first three rows, columns 2 and 3 ## # A tibble: 3 × 2 ## sex wt ## &lt;chr&gt; &lt;dbl&gt; ## 1 M 88 ## 2 F 76 ## 3 M 67 # you can even mix numerical indexing (rows) with named indexing of columns df[5:10, c(&quot;age&quot;, &quot;wt&quot;)] ## # A tibble: 6 × 2 ## age wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 25 56 ## 2 22 74 ## 3 28 71 ## 4 24 60 ## 5 23 52 ## 6 20 72 4.3.4.4 Double bracket and $ indexing of data frames Whereas single bracket indexing of a data frame always returns a new data frame, double bracket indexing and indexing using the $ operator, returns vectors. df[[&quot;age&quot;]] ## [1] 30 26 21 29 25 22 28 24 23 20 typeof(df[[&quot;age&quot;]]) ## [1] &quot;double&quot; df$wt ## [1] 88 76 67 66 56 74 71 60 52 72 typeof(df$wt) ## [1] &quot;double&quot; 4.3.5 Logical indexing of data frames Logical indexing using boolean values works on data frames in much the same way it works on vectors. Typically, logical indexing of a data frame is used to filter the rows of a data frame. For example, to get all the subject in our example data frame who are older than 25 we could do: # NOTE: the comma after 25 is important to insure we&#39;re indexing rows! df[df$age &gt; 25, ] ## # A tibble: 4 × 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 30 M 88 ## 2 26 F 76 ## 3 29 F 66 ## 4 28 M 71 Similarly, to get all the individuals whose weight is between 60 and 70 kgs we could do: df[(df$wt &gt;= 60 &amp; df$wt &lt;= 70),] ## # A tibble: 3 × 3 ## age sex wt ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 21 M 67 ## 2 29 F 66 ## 3 24 F 60 4.3.6 Adding columns to a data frame Adding columns to a data frame is similar to adding items to a list. The easiest way to do so is using named indexing. For example, to add a new column to our data frame that gives the individuals ages in number of days, we could do: df[[&quot;age.in.days&quot;]] &lt;- df$age * 365 dim(df) ## [1] 10 4 "],["functions-and-control-flow-statements.html", "Chapter 5 Functions and control flow statements 5.1 Writing your own functions 5.2 Control flow statements 5.3 map and related tools", " Chapter 5 Functions and control flow statements 5.1 Writing your own functions So far we’ve been using a variety of built in functions in R. However the real power of a programming language is the ability to write your own functions. Functions are a mechanism for organizing and abstracting a set of related computations. We usually write functions to represent sets of computations that we apply frequently, or to represent some conceptually coherent set of manipulations to data. function() {expr} defines a simplest form of R functions. function() packs the following expressions in {} as a function so they can be assigned to and run by a name. For example, #assgin a function called &quot;PrintSignature&quot; PrintSignature &lt;- function() { paste(&quot;Dr. Paul Magwene at Duke University&quot;, date()) } PrintSignature() #run the function by its name ## [1] &quot;Dr. Paul Magwene at Duke University Fri Feb 3 16:56:47 2023&quot; The more general form of an R function is as follows: funcname &lt;- function(arg1, arg2) { # one or more expressions that operate on the fxn arguments # last expression is the object returned # or you can explicitly return an object } Arg1 and arg2 are function arguments that allow you to pass different values to the expressions every time you run the function. Function arguments are given in the parentheses after function and seperated by ,. You can add as many arguments as you want. To make this concrete, here’s an example where we define a function to calculate the area of a circle: area.of.circle &lt;- function(r){ return(pi * r^2) } Since R returns the value of the last expression in the function, the return call is optional and we could have simply written: area.of.circle &lt;- function(r){ pi * r^2 } Very short and concise functions are often written as a single line. In practice I’d probably write the above function as: area.of.circle &lt;- function(r) {pi * r^2} The area.of.circle function takes one argument, r, and calculates the area of a circle with radius r. Having defined the function we can immediately put it to use: area.of.circle(3) ## [1] 28.27433 radius &lt;- 4 area.of.circle(radius) ## [1] 50.26548 If you type a function name without parentheses R shows you the function’s definition. This works for built-in functions as well (thought sometimes these functions are defined in C code in which case R will tell you that the function is a .Primitive). 5.1.1 Function arguments Function arguments can specify the data that a function operates on or parameters that the function use, i.e., the “input” or “independent varialbe” of the function. Any R objects can be taken as function arguments, including bare numbers, vectors, lists, data.frames, names of assigned variables, and even other functions. Each argument only takes a single R object, so if you have complicated input or uncertain length of input, it’s better to design some arguments that take vectors or lists. Function arguments can be either required or optional. In the case of optional arguments, a default value is assigned if the argument is not given. Take for example the log function. If you examine the help file for the log function (type ?log now) you’ll see that it takes two arguments, refered to as x and base. The argument x represents the numeric vector you pass to the function and is a required argument (see what happens when you type log() without giving an argument). The argument base is optional. By default the value of base is \\(e = 2.71828\\ldots\\). Therefore by default the log function returns natural logarithms. If you want logarithms to a different base you can change the base argument as in the following examples: log(2) # log of 2, base e ## [1] 0.6931472 log(2,2) # log of 2, base 2 ## [1] 1 log(2, 4) # log of 2, base 4 ## [1] 0.5 Because base 2 and base 10 logarithms are fairly commonly used, there are convenient aliases for calling log with these bases. log2(8) ## [1] 3 log10(100) ## [1] 2 5.1.2 Writing functions with optional arguments To write a function that has an optional argument, you can simply specify the optional argument and its default value in the function definition as so: # a function to substitute missing values in a vector sub.missing &lt;- function(x, sub.value = -99){ x[is.na(x)] &lt;- sub.value return(x) } You can then use this function as so: m &lt;- c(1, 2, NA, 4) sub.missing(m, -999) # explicitly define sub.value ## [1] 1 2 -999 4 sub.missing(m, sub.value = -333) # more explicit syntax ## [1] 1 2 -333 4 sub.missing(m) # use default sub.value ## [1] 1 2 -99 4 m # notice that m wasn&#39;t modified within the function ## [1] 1 2 NA 4 Notice that when we called sub.missing with our vector m, the vector did not get modified in the function body. Rather a new vector, x was created within the function and returned. However, if you did the missing value subsitute outside of a function call, then the vector would be modified: n &lt;- c(1, 2, NA, 4) n[is.na(n)] &lt;- -99 n ## [1] 1 2 -99 4 5.1.3 Putting R functions in Scripts When you define a function at the interactive prompt and then close the interpreter your function definition will be lost. The simple way around this is to define your R functions in a script that you can than access at any time. In RStudio choose File &gt; New File &gt; R Script. This will bring up a blank editor window. Type your function(s) into the editor. Everything in this file will be interpretted as R code, so you should not use the code block notation that is used in Markdown notebooks. Save the source file in your R working directory with a name like myfxns.R. # functions defined in myfxns.R area.of.circle &lt;- function(r) {pi * r^2} area.of.rectangle &lt;- function(l, w) {l * w} area.of.triangle &lt;- function(b, h) {0.5 * b * h } Once your functions are in a script file you can make them accesible by using the source function, which reads the named file as input and evaluates any definitions or statements in the input file (See also the Source button in the R Studio GUI): source(&quot;myfxns.R&quot;) Having sourced the file you can now use your functions like so: radius &lt;- 3 len &lt;- 4 width &lt;- 5 base &lt;- 6 height &lt;- 7 area.of.circle(radius) ## [1] 28.27433 area.of.rectangle(len, width) ## [1] 20 area.of.triangle(base, height) ## [1] 21 Note that if you change the source file, such as correcting a mistake or adding a new function, you need to call the source function again to make those changes available. 5.2 Control flow statements Control flow statements control the order of execution of different pieces of code. They can be used to do things like make sure code is only run when certain conditions are met, to iterate through data structures, to repeat something until a specified event happens, etc. Control flow statements are frequently used when writing functions or carrying out complex data transformation. 5.2.1 if and if-else statements if and if-else blocks allow you to structure the flow of execution so that certain expressions are executed only if particular conditions are met. The general form of an if expression is: if (Boolean expression) { Code to execute if Boolean expression is true } Here’s a simple if expression in which we check whether a number is less than 0.5, and if so assign a values to a variable. x &lt;- runif(1) # runif generates a random number between 0 and 1 face &lt;- NULL # set face to a NULL value if (x &lt; 0.5) { face &lt;- &quot;heads&quot; } face ## NULL The else clause specifies what to do in the event that the if statement is not true. The combined general for of an if-else expression is: if (Boolean expression) { Code to execute if Boolean expression is true } else { Code to execute if Boolean expression is false } Our previous example makes more sense if we include an else clause. x &lt;- runif(1) if (x &lt; 0.5) { face &lt;- &quot;heads&quot; } else { face &lt;- &quot;tails&quot; } face ## [1] &quot;heads&quot; With the addition of the else statement, this simple code block can be thought of as simulating the toss of a coin. 5.2.1.1 if-else in a function Let’s take our “if-else” example above and turn it into a function we’ll call coin.flip. A literal re-interpretation of our previous code in the context of a function is something like this: # coin.flip.literal takes no arguments coin.flip.literal &lt;- function() { x &lt;- runif(1) if (x &lt; 0.5) { face &lt;- &quot;heads&quot; } else { face &lt;- &quot;tails&quot; } face } coin.flip.literal is pretty long for what it does — we created a temporary variable x that is only used once, and we created the variable face to hold the results of our if-else statement, but then immediately returned the result. This is inefficient and decreases readability of our function. A much more compact implementation of this function is as follows: coin.flip &lt;- function() { if (runif(1) &lt; 0.5) { return(&quot;heads&quot;) } else { return(&quot;tails&quot;) } } Note that in our new version of coin.flip we don’t bother to create temporary the variables x and face and we immediately return the results within the if-else statement. 5.2.1.2 Multiple if-else statements When there are more than two possible outcomes of interest, multiple if-else statements can be chained together. Here is an example with three outcomes: x &lt;- sample(-5:5, 1) # sample a random integer between -5 and 5 if (x &lt; 0) { sign.x &lt;- &quot;Negative&quot; } else if (x &gt; 0) { sign.x &lt;- &quot;Positive&quot; } else { sign.x &lt;- &quot;Zero&quot; } sign.x ## [1] &quot;Positive&quot; 5.2.2 for loops A for statement iterates over the elements of a sequence (such as vectors or lists). A common use of for statements is to carry out a calculation on each element of a sequence (but see the discussion of map below) or to make a calculation that involves all the elements of a sequence. The general form of a for loop is: for (elem in sequence) { Do some calculations or Evaluate one or more expressions } As an example, say we wanted to call our coin.flip function multiple times. We could use a for loop to do so as follows: flips &lt;- c() # empty vector to hold outcomes of coin flips for (i in 1:20) { flips &lt;- c(flips, coin.flip()) # flip coin and add to our vector } flips ## [1] &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; ## [10] &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; ## [19] &quot;heads&quot; &quot;heads&quot; Let’s use a for loop to create a multi.coin.flip function thats accepts an optional argument n that specifies the number of coin flips to carry out: multi.coin.flip &lt;- function(n = 1) { # create an empty character vector of length n # it&#39;s more efficient to create an empty vector of the right # length than to &quot;grow&quot; a vector with each iteration flips &lt;- vector(mode=&quot;character&quot;, length=n) for (i in 1:n) { flips[i] &lt;- coin.flip() } flips } With this new definition, a single call of coin.flip returns a single outcome: multi.coin.flip() ## [1] &quot;tails&quot; And calling multi.coin.flip with a numeric argument returns multiple coin flips: multi.coin.flip(n=10) ## [1] &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; ## [10] &quot;tails&quot; 5.2.3 break statement A break statement allows you to exit a loop even if it hasn’t completed. This is useful for ending a control statement when some criteria has been satisfied. break statements are usually nested in if statements. In the following example we use a break statement inside a for loop. In this example, we pick random real numbers between 0 and 1, accumulating them in a vector (random.numbers). The for loop insures that we never pick more than 20 random numbers before the loop ends. However, the break statement allows the loop to end prematurely if the number picked is greater than 0.95. random.numbers &lt;- c() for (i in 1:20) { x &lt;- runif(1) random.numbers &lt;- c(random.numbers, x) if (x &gt; 0.95) { break } } random.numbers ## [1] 0.33824003 0.27150893 0.80147861 0.82639203 0.72485470 0.45984222 ## [7] 0.33742675 0.13972448 0.70618396 0.30628448 0.83748920 0.38625625 ## [13] 0.72688588 0.83052833 0.26359268 0.81387537 0.65306014 0.84585978 ## [19] 0.01309625 0.49304270 5.2.4 repeat loops A repeat loop will loop indefinitely until we explicitly break out of the loop with a break statement. For example, here’s an example of how we can use repeat and break to simulate flipping coins until we get a head: ct &lt;- 0 repeat { flip &lt;- coin.flip() ct &lt;- ct + 1 if (flip == &quot;heads&quot;){ break } } ct ## [1] 1 5.2.5 next statement A next satement allows you to halt the processing of the current iteration of a loop and immediately move to the next item of the loop. This is useful when you want to skip calculations for certain elements of a sequence: sum.not.div3 &lt;- 0 for (i in 1:20) { if (i %% 3 == 0) { # skip summing values that are evenly divisible by three next } sum.not.div3 &lt;- sum.not.div3 + i } sum.not.div3 ## [1] 147 5.2.6 while statements A while statement iterates as long as the condition statement it contains is true. In the following example, the while loop calls coin.flip until “heads” is the result, and keeps track of the number of flips. Note that this represents the same logic as the repeat-break example we saw earlier, but in a a more compact form. first.head &lt;- 1 while(coin.flip() == &quot;tails&quot;){ first.head &lt;- first.head + 1 } first.head ## [1] 1 5.2.7 ifelse The ifelse function is equivalent to a for-loop with a nested if-else statement. ifelse applies the specified test to each element of a vector, and returns different values depending on if the test is true or false. Here’s an example of using ifelse to replace NA elements in a vector with zeros. x &lt;- c(3, 1, 4, 5, 9, NA, 2, 6, 5, 4) newx &lt;- ifelse(is.na(x), 0, x) newx ## [1] 3 1 4 5 9 0 2 6 5 4 The equivalent for-loop could be written as: x &lt;- c(3, 1, 4, 5, 9, NA, 2, 6, 5, 4) newx &lt;- c() # create an empty vector for (elem in x) { if (is.na(elem)) { newx &lt;- c(newx, 0) # append zero to newx } else { newx &lt;- c(newx, elem) # append elem to newx } } newx ## [1] 3 1 4 5 9 0 2 6 5 4 The ifelse function is clearly a more compact and readable way to accomplish this. 5.3 map and related tools Another common situation is applying a function to every element of a list or vector. Again, we could use a for loop, but the map functions often are better alternatives. NOTE: map is a relative newcomer to R and must be loaded with the purrr package (purrr is loaded when we load tidyverse). Although base R has a complicated series of “apply” functions (apply, lapply, sapply, vapply, mapply), map provides similar functionality with a more consistent interface. We won’t use the apply functions in this class, but you may see them in older code. library(tidyverse) ## ── Attaching packages ──────────────────── ## ✔ ggplot2 3.4.0 ✔ purrr 1.0.1 ## ✔ tibble 3.1.8 ✔ dplyr 1.1.0 ## ✔ tidyr 1.3.0 ✔ stringr 1.5.0 ## ✔ readr 2.1.3 ✔ forcats 1.0.0 ## ── Conflicts ──── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() 5.3.1 basic map Typically, map takes two arguments – a sequence (a vector, list, or data frame) and a function. It then applies the function to each element of the sequence, returning the results as a list. To illustrate map, let’s consider an example with a list of 2-vectors, where each vector gives the min and max values of some variable of interest for individuals in a sample (e.g. resting heart rate and maximum heart rate during exercise). We can use the map function to quickly generate the difference between the resting and maximum heart rates: heart.rates &lt;- list(bob = c(60, 120), fred = c(79, 150), jim = c(66, 110)) diff.fxn &lt;- function(x) {x[2] - x[1]} map(heart.rates, diff.fxn) ## $bob ## [1] 60 ## ## $fred ## [1] 71 ## ## $jim ## [1] 44 As a second example, here’s how we could use map to get the class of each object in a list: x &lt;- list(c(1,2,3), &quot;a&quot;, &quot;b&quot;, list(lead = &quot;Michael&quot;, keyboard = &quot;Jermaine&quot;)) map(x, class) ## [[1]] ## [1] &quot;numeric&quot; ## ## [[2]] ## [1] &quot;character&quot; ## ## [[3]] ## [1] &quot;character&quot; ## ## [[4]] ## [1] &quot;list&quot; 5.3.2 map_if and map_at map_if is a variant of map that takes a predicate function (a function that evaluates to TRUE or FALSE) to determine which elements of the input sequence are transformed by the map function. All elements of the sequence that do not meet the predicate are left un-transformed. Like map, map_if always returns a list. Here’s an example where we use map_if to apply the stringr::str_to_upper function to those columns of a data frame that are character vectors, and apply abs to obtain the absolute value of a numeric column: a &lt;- rnorm(6) b &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;) c &lt;- c(&quot;u&quot;, &quot;v&quot;, &quot;w&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;) df &lt;- data_frame(a, b, c) head(df) ## # A tibble: 6 × 3 ## a b c ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 -0.498 a u ## 2 0.428 b v ## 3 -0.371 c w ## 4 0.0106 d x ## 5 0.591 e y ## 6 0.807 f z df2 &lt;- map_if(df, is.character, str_to_upper) df2 &lt;- map_if(df2, is.numeric, abs) head(df2) ## $a ## [1] 0.49798655 0.42817641 0.37114033 0.01056529 0.59110949 0.80657685 ## ## $b ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; ## ## $c ## [1] &quot;U&quot; &quot;V&quot; &quot;W&quot; &quot;X&quot; &quot;Y&quot; &quot;Z&quot; Note that df2 is a list, not a data frame. We can convert df2 to a data frame df3, using the as_data_frame() function: # Next, create data frame df3 df3 &lt;- as_data_frame(df2) head(df3) ## # A tibble: 6 × 3 ## a b c ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.498 A U ## 2 0.428 B V ## 3 0.371 C W ## 4 0.0106 D X ## 5 0.591 E Y ## 6 0.807 F Z Note that if our goal is to apply functions to the columns of a data frame, it may be easier with dplyr::mutate(): df4 &lt;- df %&gt;% as_tibble() %&gt;% mutate(a = abs(a), b = str_to_upper(b), c = str_to_upper(c)) head(df4) ## # A tibble: 6 × 3 ## a b c ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.498 A U ## 2 0.428 B V ## 3 0.371 C W ## 4 0.0106 D X ## 5 0.591 E Y ## 6 0.807 F Z 5.3.3 mapping in parallel using map2 The map2 function applies a transformation function to two sequences in parallel. The following example illustrates this: first.names &lt;- c(&quot;John&quot;, &quot;Mary&quot;, &quot;Fred&quot;) last.names &lt;- c(&quot;Smith&quot;, &quot;Hernandez&quot;, &quot;Kidogo&quot;) map2(first.names, last.names, str_c, sep=&quot; &quot;) ## [[1]] ## [1] &quot;John Smith&quot; ## ## [[2]] ## [1] &quot;Mary Hernandez&quot; ## ## [[3]] ## [1] &quot;Fred Kidogo&quot; Note how we can specify arguments to the transformation function as additional arguments to map2 (i.e., the sep argument gets passed to str_c) 5.3.4 map variants that return vectors map, map_if, and map_at always return lists. The purrr library also has a series of map variants that return vectors: map_lgl (for logical vectors) map_chr (for character vectors) map_int (integer vectors) map_dbl (double vectors) # compare the outputs of map and map_chr a &lt;- map(letters[1:6], str_to_upper) str(a) ## List of 6 ## $ : chr &quot;A&quot; ## $ : chr &quot;B&quot; ## $ : chr &quot;C&quot; ## $ : chr &quot;D&quot; ## $ : chr &quot;E&quot; ## $ : chr &quot;F&quot; b &lt;- map_chr(letters[1:6], str_to_upper) str(b) # a vector ## chr [1:6] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; Here’s an example using map_dbl, where we create a data frame with three columns, and compute the median of each column: # Make data frame for analysis df &lt;- tibble(a = rnorm(100), b = rnorm(100),c = rnorm(100)) map_dbl(df, median) # median of each column of df ## a b c ## 0.14931120 0.13070397 -0.09626582 "],["introduction-to-ggplot2.html", "Chapter 6 Introduction to ggplot2 6.1 Loading ggplot2 6.2 Example data set: Anderson’s Iris Data 6.3 Template for single layer plots in ggplot2 6.4 An aside about function arguments 6.5 Strip plots 6.6 Histograms 6.7 Faceting to depict categorical information 6.8 Density plots 6.9 Violin or Beanplot 6.10 Boxplots 6.11 Building complex visualizations with layers 6.12 Useful combination plots 6.13 ggplot layers can be assigned to variables 6.14 Adding titles and tweaking axis labels 6.15 ggplot2 themes 6.16 Other aspects of ggplots can be assigned to variables 6.17 Bivariate plots 6.18 Bivariate density plots 6.19 Combining Scatter Plots and Density Plots with Categorical Information 6.20 Density plots with fill 6.21 2D bin and hex plots 6.22 The cowplot package", " Chapter 6 Introduction to ggplot2 Pretty much any statistical plot can be thought of as a mapping between data and one or more visual representations. For example, in a scatter plot we map two ordered sets of numbers (the variables of interest) to points in the Cartesian plane (x,y-coordinates). The representation of data as points in a plane can be thought of as a type of geometric mapping. In a histogram, we divide the range of a variable of interest into bins, count the number of observations in each bin, and represent those counts as bars. The process of counting the data in bins is a type of statistical transformation (summing in this case), while the representation of the counts as bars is another example of a geometric mapping. Both types of plots can be further embellished with additional information, such as coloring the points or bars based on a categorical variable of interest, changing the shape of points, etc. These are examples of aesthetic mappings. An additional operation that is frequently useful is faceting (also called conditioning), in which a series of subplots are created to show particular subsets of the data. The package ggplot2 is based on a formalized approach for building statistical graphics as a combination of geometric mappings, aesthetic mappings, statistical transformations, and faceting (conditioning). In ggplot2, complex figures are built up by combining layers – where each layer includes a geometric mapping, an aesthetic mapping, and a statistical transformation – along with any desired faceting information. Many of the key ideas behind ggplot2 (and its predecessor,“ggplot”) are based on a book called “The Grammar of Graphics” (Leland Wilkinson, 1985). The “grammar of graphics” is the “gg” in the ggplot2 name. 6.1 Loading ggplot2 ggplot2 is one of the packages included in the tidyverse meta-package we installed during the previous class session (see the previous lecture notes for instruction if you have not installed tidyverse). If we load the tidyverse package, ggplot2 is automatically loaded as well. library(tidyverse) However if we wanted to we could load only ggplot2 as follows: library(ggplot2) # not necessary if we already loaded tidyverse 6.2 Example data set: Anderson’s Iris Data To illustrate ggplot2 we’ll use a dataset called iris. This data set was made famous by the statistician and geneticist R. A. Fisher who used it to illustrate many of the fundamental statistical methods he developed (Recall that Fisher was one of the key contributors to the modern synthesis in biology, reconciling evolution and genetics in the early 20th century). The data set consists of four morphometric measurements for specimens from three different iris species (Iris setosa, I. versicolor, and I. virginica). Use the R help to read about the iris data set (?iris). We’ll be using this data set repeatedly in future weeks so familiarize yourself with it. The iris data is included in a standard R package (datasets) that is made available automatically when you start up R. As a consequence we don’t need to explicitly load the iris data from a file. Let’s take a few minutes to explore this iris data set before we start generating plots: names(iris) # get the variable names in the dataset ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; dim(iris) # dimensions given as rows, columns ## [1] 150 5 head(iris) # can you figure out what the head function does? ## # A tibble: 6 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(iris) # what about the tail function? ## # A tibble: 6 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 6.7 3.3 5.7 2.5 virginica ## 2 6.7 3 5.2 2.3 virginica ## 3 6.3 2.5 5 1.9 virginica ## 4 6.5 3 5.2 2 virginica ## 5 6.2 3.4 5.4 2.3 virginica ## 6 5.9 3 5.1 1.8 virginica 6.3 Template for single layer plots in ggplot2 A basic template for building a single layer plot using ggplot2 is shown below. When creating a plot, you need to replace the text in brackets (e.g. &lt;DATA&gt;) with appropriate objects, functions, or arguments: # NOTE: this is pseudo-code. It will not run! ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) The base function ggplot() is responsible for creating the coordinate system in which the plot will be display. To this coordinate system we add a geometric mapping (called a “geom” for short) that specifies how data gets mapped into the coordinate system (e.g. points, bars, etc). Included as an input to the geom function is the aesthetic mapping function that specifies which variables to use in the geometric mapping (e.g. which variables to treat as the x- and y-coordinates), colors, etc. For example, using this template we can create a scatter plot that show the relationship between the variables Sepal.Width and Petal.Width. To do so we subsitute iris for &lt;DATA&gt;, geom_point for &lt;GEOM_FUNCTION&gt;, and x = Sepal.Width and y = Petal.Width for &lt;MAPPINGS&gt;. ggplot(data = iris) + geom_point(mapping = aes(x = Sepal.Width, y = Petal.Width)) If we were to translate this code block to English, we might write it as “Using the iris data frame as the source of data, create a point plot using each observeration’s Sepal.Width variable for the x-coordinate and the Petal.Width variable for the y-coordinate.” 6.4 An aside about function arguments The inputs to a function are also known as “arguments”. In R, when you call a function you can specify the arguments by keyword (i.e. using names specified in the function definition) or by position (i.e. the order of the inputs). In our bar plot above, we’re using using keyword arguments. For example, in the line ggplot(data = iris), iris is treated as the “data” argument. Similarly, in the second line, aes(x = Sepal.Width, y = Petal.Width) is the “mapping” argument to geom_bar. Note that aes is itself a function (see ?aes) that takes arguments that can be specified positionally or with keywords. If we wanted to, we could instead use position arguments when calling a function, by passing inputs to the function corresponding to the order they are specified in the function definition. For example, take a minute to read the documentation for the ggplot function (?ggplot). Near the top of the help page you’ll see a description of how the function is called under “Usage”. Reading the Usage section you’ll see that the the “data” argument is the first positional argument to ggplot. Similarly, if you read the docs for the geom_point function you’ll see that mapping is the first positional argument for that function. The equivalent of our previous example, but now using positional arguments is: ggplot(iris) + # note we dropped the &quot;data = &quot; part # note we dropped the &quot;mapping = &quot; part from the geom_point call geom_point(aes(x = Sepal.Width, y = Petal.Width)) The upside of using positional arguments is that it means less typing, which is useful when working interactively at the console (or in an R Notebok). The downside to using positional arguments is you need to remember or lookup the order of the arguments. Using positional arguments can also make your code less “self documenting” in the sense that it is less explicit about how the inputs are being treated. While the argument “x” is the first argument to the aes function, I chose to explicitly include the argument name to make it clear what variable I’m plotting on the x-axis. We will cover function arguments in greater detail a class session or two from now, when we learn how to write our own functions. 6.5 Strip plots One of the simplest visualizations of a continuous variable is to draw points along a number line, where each point represent the value of one of the observations. This is sometimes called a “strip plot”. First, we’ll use the geom_point function as shown below to generate a strip plot for the Sepal.Width variable in the iris data set. ggplot(data = iris) + geom_point(aes(x = Sepal.Width, y = 0)) 6.5.1 Jittering data There should have been 150 points plotted in the figure above (one for each of the iris plants in the data set), but visually it looks like only about 25 or 30 points are shown. What’s going on? If you examine the iris data, you’ll see that the all the measures are rounded to the nearest tenth of a centimer, so that there are a large number of observations with identical values of Sepal.Width. This is a limitation of the precision of measurements that was used when generating the data set. To provide a visual clue that there are multiple observations that share the same value, we can slightly “jitter” the values (randomly move points a small amount in either in the vertical or horizontal direction). Jittering is used solely to enhance visualization, and any statistical analyses you carry out would be based on the original data. When presenting your data to someone else, should note when you’ve used jittering so as not to misconvey the actual data. Jittering can be accomplished using geom_jitter, which is derived from geom_point: ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = 0), width = 0.05, height = 0, alpha = 0.25) The width and height arguments specify the maximum amount (as fractions of the data) to jitter the observed data points in the horizontal (width) and vertical (height) directions. Here we only jitter the data in the horizontal direction. The alpha argument controls the transparency of the points – the valid range of alpha values is 0 to 1, where 0 means completely transparent and 1 is completely opaque. Within a geom, arguments outside of the aes mapping apply uniformly across the visualization (i.e. they are fixed values). For example, setting `alpha = 0.25’ made all the points transparent. 6.5.2 Adding categorical information Recall that are three different species represented in the data: Iris setosa, I. versicolor, and I. virginica. Let’s see how to generate a strip plot that also includes a breakdown by species. ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species), width=0.05, height=0.1, alpha=0.5) That was easy! All we had to do was change the aesthetic mapping in geom_jitter, specifying “Species” as the y variable. I also added a little vertical jitter as well to better separate the points. Now we have a much better sense of the data. In particular it’s clear that the I. setosa specimens generally have wider sepals than samples from the other two species. Let’s tweak this a little by also adding color information, to further emphasize the distinct groupings. We can do this by adding another argument to the aesthetic mapping in geom_jitter. ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species, color=Species), width=0.05, height=0.1, alpha=0.5) 6.5.3 Rotating plot coordinates What if we wanted to rotate this plot 90 degrees, depicting species on the x-axis and sepal width on the y-axis. For this example, it would be easy to do this by simpling swapping the variables in the aes mapping argument. However an alternate way to do this is with a coordinate transformation function. Here we use coord_flip to flip the x- and y-axes: ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species, color=Species), width=0.05, height=0.1, alpha=0.5) + coord_flip() We’ll see other uses of coordinate transformations in later lectures. 6.6 Histograms Histograms are probably the most common way to depict univariate data. In a histogram rather than showing individual observations, we divide the range of the data into a set of bins, and use vertical bars to depict the number (frequency) of observations that fall into each bin. This gives a good sense of the intervals in which most of the observations are found. The geom, geom_histogram, takes care of both the geometric representation and the statistical transformations of the data necessary to calculate the counts in each binn. Here’s the simplest way to use geom_histogram: ggplot(iris) + geom_histogram(aes(x = Sepal.Width)) ## `stat_bin()` using `bins = 30`. Pick ## better value with `binwidth`. The default number of bins that geom_histogram uses is 30. For modest size data sets this is often too many bins, so it’s worth exploring how the histogram changes with different bin numbers: ggplot(iris) + geom_histogram(aes(x = Sepal.Width), bins = 10) ggplot(iris) + geom_histogram(aes(x = Sepal.Width), bins = 12) One important thing to note when looking at these histograms with different numbers of bins is that the number of bins used can change your perception of the data. For example, the number of peaks (modes) in the data can be very sensitive to the bin number as can the perception of gaps. 6.6.1 Variations on histograms when considering categorical data As before, we probably want to break the data down by species. Here we’re faced with some choices about how we depict that data. Do we generate a “stacked histogram” to where the colors indicate the number of observations in each bin that belong to each species? Do we generate side-by-side bars for each species? Or Do we generate separate histograms for each species, and show them overlapping? Stacked histograms are the default if we associate a categorical variable with the bar fill color: ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12) To get side-by-side bars, specify “dodge” as the position argument to geom_histogram. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12, position = &quot;dodge&quot;) If you want overlapping histograms, use position = \"identity\" instead. When generating overlapping histograms like this, you probably want to make the bars semi-transparent so you can can distinguish the overlapping data. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12, position = &quot;identity&quot;, alpha = 0.4) 6.7 Faceting to depict categorical information Yet another way to represent the histograms for the three species is to using faceting, the create subplots for each species. Faceting is the operation of subsetting the data with respect to a discrete or categorical variable of interest, and generating the same plot type for each subset. Here we use the “ncol” argument to the facet_wrap function to specify that the subplots should be drawn in a single vertical column to facilitate comparison of the distributions. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12) + facet_wrap(~Species, ncol = 1) 6.8 Density plots One shortcoming of histograms is that they are sensitive to the choice of bin margins and the number of bins. An alternative is a “density plot”, which you can think of as a smoothed version of a histogram. ggplot(iris) + geom_density(aes(x = Sepal.Width, fill = Species), alpha=0.25) Density plots still make some assumptions that affect the visualization, in particular a “smoothing bandwidth” (specified by the argument bw) which determines how course or granular the density estimation is. Note that the vertical scale on a density plot is no longer counts (frequency) but probability density. In a density plot, the total area under the plot adds up to one. Intervals in a density plot therefore have a probabilistic intepretation. 6.9 Violin or Beanplot A violin plot (sometimes called a bean plot) is closely related to a density plot. In fact you can think of a violin plot as a density plot rotated 90 degress and mirrored left/right. ggplot(iris) + geom_violin(aes(x = Species, y = Sepal.Width, color = Species, fill=Species), alpha = 0.25) 6.10 Boxplots Boxplots are another frequently used univariate visualization. Boxplots provide a compact summary of single variables, and are most often used for comparing distributions between groups. A standard box plot depicts five useful features of a set of observations: 1) the median (center most line); 2 and 3) the first and third quartiles (top and bottom of the box); 4) the whiskers of a boxplot extend from the first/third quartile to the highest value that is within 1.5 * IQR, where IQR is the inter-quartile range (distance between the first and third quartiles); 5) points outside of the whiskers are usually consider extremal points or outliers. There are many variants on box plots, particularly with respect to the “whiskers”. It’s always a good idea to be explicit about what a box plot you’ve created shows. ggplot(iris) + geom_boxplot(aes(x = Species, y = Sepal.Width, color = Species)) Boxplots are most commonly drawn with the cateogorical variable on the x-axis. 6.11 Building complex visualizations with layers All of our ggplot2 examples up to now have involved a single geom. We can think of geoms as “layers” of information in a plot. One of the powerful features of plotting useing ggplot2 is that it is trivial to combine layers to make more complex plots. The template for multi-layered plots is a simple extension of the single layer: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION1&gt;(mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION2&gt;(mapping = aes(&lt;MAPPINGS&gt;)) 6.12 Useful combination plots Boxplot or violin plots represent visual summaries/simplifications of the underlying data. This is useful but sometimes key information is lost in the process of summarizing. Combining these plots with a strip plot give you both the “birds eye view” as well as granular information. 6.12.1 Boxplot plus strip plot Here’s an example of combining box plots and strip plots: ggplot(iris) + # outlier.shape = NA suppresses the depiction of outlier points in the boxplot geom_boxplot(aes(x = Species, y = Sepal.Width), outlier.shape = NA) + # size sets the point size for the jitter plot geom_jitter(aes(x = Species, y = Sepal.Width), width=0.2, height=0.05, alpha=0.35, size=0.75) Note that I suppressed the plotting of outliers in geom_boxplot so as not to draw the same points twice (the individual data are drawn by geom_jitter). 6.12.2 Setting shared aesthetics The example above works well, but you might have noticed that there’s some repetition of code. In particular, we set the same aesthetic mapping in both geom_boxplot and geom_jitter. It turns out that creating layers that share some of the same aesthetic values is a common case. To deal with such cases, you can specify shared aesthetic mappings as an argument to the ggplot function and then set additional aesthetics specific to each layer in the individual geoms. Using this approach, our previous example can be written more compactly as follow. ggplot(iris, mapping = aes(x = Species, y = Sepal.Width)) + geom_boxplot(outlier.shape = NA) + # note how we specify a layer specific aesthetic in geom_jitter geom_jitter(aes(color = Species), width=0.2, height=0.05, alpha=0.5, size=0.75) 6.13 ggplot layers can be assigned to variables The function ggplot() returns a “plot object” that we can assign to a variable. The following example illustrates this: # create base plot object and assign to variable p # this does NOT draw the plot p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width)) In the code above we created a plot object and assigned it to the variable p. However, the plot wasn’t drawn. To draw the plot object we evaluate it as so: p # try to draw the plot object The code block above didn’t generate an image, because we haven’t added a geom to the plot to determine how our data should be drawn. We can add a geom to our pre-created plot object as so: # add a point geom to our base layer and draw the plot p + geom_boxplot() If we wanted to we could have assigned the geom to a variable as well: box.layer &lt;- geom_boxplot() p + box.layer In this case we don’t really gain anything by creating an intermediate variable, but for more complex plots or when considering different versions of a plot this can be very useful. 6.13.1 Violin plot plus strip plot Here is the principle of combining layers, applied to a combined violin plot + strip plot. Again, we set shared aesthetic mappings in ggplot function call and this time we assign individual layers of the plot to variables. p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width, color = Species)) violin.layer &lt;- geom_violin() jitter.layer &lt;- geom_jitter(width=0.15, height=0.05, alpha=0.5, size=0.75) p + violin.layer + jitter.layer # combined layers of plot and draw 6.14 Adding titles and tweaking axis labels ggplot2 automatically adds axis labels based on the variable names in the data frame passed to ggplot. Sometimes these are appropriate, but more presentable figures you’ll usually want to tweak the axis labs (e.g. adding units). The labs (short for labels) function allows you to do so, and also let’s you set a title for your plot. We’ll illustrate this by modifying our previous figure. Note that we save considerable amounts of re-typing since we had already assigned three of the plot layers to variables in the previous code block: p + violin.layer + jitter.layer + labs(x = &quot;Species&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Sepal Width Distributions for Three Iris Species&quot;) 6.15 ggplot2 themes By now you’re probably familiar with the default “look” of plots generated by ggplot2, in particular the ubiquitous gray background with a white grid. This default works fairly well in the context of RStudio notebooks and HTML output, but might not work as well for a published figure or a slide presentation. Almost every individual aspect of a plot can be tweaked, but ggplot2 provides an easier way to make consistent changes to a plot using “themes”. You can think of a theme as adding another layer to your plot. Themes should generally be applied after all the other graphical layers are created (geoms, facets, labels) so the changes they create affect all the prior layers. There are eight default themes included with ggplot2, which can be invoked by calling the corresponding theme functions: theme_gray, theme_bw, theme_linedraw, theme_light, theme_dark, theme_minimal, theme_classic, and theme_void (See http://ggplot2.tidyverse.org/reference/ggtheme.html for a visual tour of all the default themes) For example, let’s generate a boxplot using theme_bw which get’s rid of the gray background: # create another variable to hold combination of three previous # ggplot layers. I&#39;m doing this because I&#39;m going to keep re-using # the same plot in the following code blocks violin.plus.jitter &lt;- p + violin.layer + jitter.layer violin.plus.jitter + theme_bw() Another theme, theme_classic, remove the grid lines completely, and also gets rid of the top-most and right-most axis lines. violin.plus.jitter + theme_classic() 6.15.1 Further customization with ggplot2::theme In addition to the eight complete themes, there is a theme function in ggplot2 that allows you to tweak particular elements of a theme (see ?theme for all the possible options). For example, to tweak just the aspect ratio of a plot (the ratio of width to height), you can set the aspect.ratio argument in theme: violin.plus.jitter + theme_classic() + theme(aspect.ratio = 1) Theme related function calls can be combined to generate new themes. For example, let’s create a theme called my.theme by combining theme_classic with a call to theme: my.theme &lt;- theme_classic() + theme(aspect.ratio = 1) We can then apply this theme as so: violin.plus.jitter + my.theme 6.16 Other aspects of ggplots can be assigned to variables Plot objects, geoms and themes are not the only aspects of a figure that can be assigned to variables for later use. For example, we can create a label object: my.labels &lt;- labs(x = &quot;Species&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Sepal Width Distributions for Three Iris Species&quot;) Combining all of our variables as so, we generate our new plot: violin.plus.jitter + my.labels + my.theme 6.17 Bivariate plots Now we turn our attention to some useful representations of bivariate distributions. For the purposes of these illustrations I’m initially going to restrict my attention to just one of the three species represented in the iris data set – the I. setosa specimens. This allows us to introduce a vary useful base function called subset(). subset() will return subsets of a vector or data frames that meets the specified conditions. This can also be accomplished with conditional indexing but subset() is usually less verbose. # create a new data frame composed only of the I. setosa samples setosa.only &lt;- subset(iris, Species == &quot;setosa&quot;) In the examples that follow, I’m going to illustrate different ways of representing the same bivariate distribution – the joint distribution of Sepal Length and Sepal Width – over and over again. To avoid repitition, let’s assign the base ggplot layer to a variable as we did in our previous examples. We’ll also pre-create a label layer. setosa.sepals &lt;- ggplot(setosa.only, mapping = aes(x = Sepal.Length, y = Sepal.Width)) sepal.labels &lt;- labs(x = &quot;Sepal Length (cm)&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Relationship between Sepal Length and Width&quot;, caption = &quot;data from Anderson (1935)&quot;) 6.17.1 Scatter plots A scatter plot is one of the simplest representations of a bivariate distribution. Scatter plots are simple to create in ggplot2 by specifying the appropriate X and Y variables in the aesthetic mapping and using geom_point for the geometric mapping. setosa.sepals + geom_point() + sepal.labels 6.17.2 Adding a trend line to a scatter plot ggplot2 makes it easy to add trend lines to plots. I use “trend lines” here to refer to representations like regression lines, smoothing splines, or other representations mean to help visualize the relationship between pairs of variables. We’ll spend a fair amount of time exploring the mathematics and interpetation of regression lines and related techniques in later lectures, but for now just think about trends lines as summary representations for bivariate relationships. Trend lines can be created using geom_smooth. Let’s add a default trend line to our I. setosa scatter plot of the Sepal Width vs Sepal Length: setosa.sepals + geom_jitter() + # using geom_jitter to avoid overplotting of points geom_smooth() + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme ## `geom_smooth()` using method = &#39;loess&#39; ## and formula = &#39;y ~ x&#39; The defaul trend line that geom_smooth fits is generated by a technique called “LOESS regression”. LOESS regression is a non-linear curve fitting method, hence the squiggly trend line we see above. The smoothness of the LOESS regression is controlled by a parameter called span which is related to the proportion of points used. We’ll discuss LOESS in detail in a later lecture, but here’s an illustration how changing the span affects the smoothness of the fit curve: setosa.sepals + geom_jitter() + # using geom_jitter to avoid overplotting of points geom_smooth(span = 0.95) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme ## `geom_smooth()` using method = &#39;loess&#39; ## and formula = &#39;y ~ x&#39; 6.17.2.1 Linear trend lines If instead we want a straight trend line, as would typically be depicted for a linear regression model we can specify a different statistical method: setosa.sepals + geom_jitter() + # using geom_jitter to avoid overplotting of points geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # using linear model (&quot;lm&quot;) sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme ## `geom_smooth()` using formula = &#39;y ~ x&#39; 6.18 Bivariate density plots The density plot, which we introduced as a visualization for univariate data, can be extended to two-dimensional data. In a one dimensional density plot, the height of the curve was related to the relatively density of points in the surrounding region. In a 2D density plot, nested contours (or contours plus colors) indicate regions of higher local density. Let’s illustrate this with an example: setosa.sepals + geom_density2d() + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme The relationship between the 2D density plot and a scatter plot can be made clearer if we combine the two: setosa.sepals + geom_density_2d() + geom_jitter(alpha=0.35) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme 6.19 Combining Scatter Plots and Density Plots with Categorical Information As with many of the univariate visualizations we explored, it is often useful to depict bivariate relationships as we change a categorical variable. To illustrate this, we’ll go back to using the full iris data set. all.sepals &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) all.sepals + geom_point(aes(color = Species, shape = Species), size = 2, alpha = 0.6) + sepal.labels + labs(subtitle = &quot;All species&quot;) + my.theme Notice how in our aesthetic mapping we specified that both color and shape should be used to represent the species categories. The same thing can be accomplished with a 2D density plot. all.sepals + geom_density_2d(aes(color = Species)) + sepal.labels + labs(subtitle = &quot;All species&quot;) + my.theme As you can see, in the density plots above, when you have multiple categorical variables and there is significant overlap in the range of each sub-distribution, figures can become quite busy. As we’ve seen previously, faceting (conditioning) can be a good way to deal with this. Below a combination of scatter plots and 2D density plots, combined with faceting on the species variable. all.sepals + geom_density_2d(aes(color = Species), alpha = 0.5) + geom_point(aes(color = Species), alpha=0.5, size=1) + facet_wrap(~ Species) + sepal.labels + labs(subtitle = &quot;All species&quot;) + theme_bw() + theme(aspect.ratio = 1, legend.position = &quot;none&quot;) # get rid of legend In this example I went back to using a theme that includes grid lines to facilitate more accurate comparisons of the distributions across the facets. I also got rid of the legend, because the information there was redundant. 6.20 Density plots with fill Let’s revisit our earlier single species 2D density plot. Instead of simply drawing contour lines, let’s use color information to help guide the eye to areas of higher density. To draw filled contours, we use a sister function to geom_density_2d called stat_density_2d: setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme Using the default color scale, areas of low density are drawn in dark blue, whereas areas of high density are drawn in light blue. I personally find this dark -to-light color scale non-intuitive for density data, and would prefer that darker regions indicate area of higher density. If we want to change the color scale, we can use the a scale function (in this case scale_fill_continuous) to set the color values used for the low and high values (this function we’ll interpolate the intervening values for us). NOTE: when specifying color names, R accepts standard HTML color names (see the Wikipedia page on web colors for a list). We’ll also see other ways to set color values in a later class session. setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + # lavenderblush is the HTML standard name for a light purplish-pink color scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme The two contour plots we generated looked a little funny because the contours are cutoff due to the contour regions being outside the limits of the plot. To fix this, we can change the plot limits using the lims function as shown in the following code block. We’ll also add the scatter (jittered) to the emphasize the relationship between the levels, and we’ll change the title for the color legend on the right by specifying a text label associated with the fill arguments in the labs function. setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + geom_jitter(alpha=0.5, size = 1.1) + # customize labels, including legend label for fill labs(x = &quot;Sepal Length(cm)&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Relationship between sepal length and width&quot;, subtitle = &quot;I. setosa specimens only&quot;, fill = &quot;Density&quot;) + # Set plot limits lims(x = c(4,6), y = c(2.5, 4.5)) + my.theme 6.21 2D bin and hex plots Two dimensional bin and hex plots are alterative ways to represent the joint density of points in the Cartesian plane. Here are examples of to generate these plot types. Compare them to our previous examples. A 2D bin plot can be tought of as a 2D histogram: setosa.sepals + geom_bin2d(binwidth = 0.2) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme A hex plot is similar to a 2D bin plot but uses hexagonal regions instead of squares. Hexagonal bins are useful because they can avoid visual artefacts sometimes apparent with square bins: setosa.sepals + geom_hex(binwidth = 0.2) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme 6.22 The cowplot package A common task when preparing visualizations for scientific presentations and manuscripts is combining different plots as subfigures of a larger figure. To accomplish this we’ll use a package called cowplot that compliments the power of ggplot2. Install cowplot either via the command line or the R Studio GUI (see Section 2.8). library(cowplot) # assumes package has been installed cowplot allows us to create individual plots using ggplot, and then arrange them in a grid-like fashion with labels for each plot of interest, as you would typically see in publications. The core function of cowplot is plot_grid(), which allows the user to layout the sub-plots in an organized fashion and add labels as necesary. To illustrate plot_grid() let’s create three different representations of the distribution of sepal width in the irisu data set, and combine them into a single figure: p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width, color = Species)) # for the histogram we&#39;re going to override the mapping because # geom_histogram only takes an x argument plot.1 &lt;- p + geom_histogram(bins=12, mapping = aes(x = Sepal.Width), inherit.aes = FALSE) plot.2 &lt;- p + geom_boxplot() plot.3 &lt;- p + geom_violin() plot_grid(plot.1, plot.2, plot.3) If instead, we wanted to layout the plots in a single row we could change the call to plot_grid as so: plot_grid(plot.1, plot.2, plot.3, nrow = 1, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) Notice we also added labels to our sub-plots. "],["introduction-to-dplyr.html", "Chapter 7 Introduction to dplyr 7.1 Libraries 7.2 Reading data with the readr package 7.3 A note on “tibbles” 7.4 Data filtering and transformation with dplyr 7.5 dplyr’s “verbs” 7.6 Pipes", " Chapter 7 Introduction to dplyr In today’s class we introduce a new package, dplyr, which, along with ggplot2 will be used in almost every class session. We will also introduce the readr package, for reading tabular data. 7.1 Libraries Both readr and dplyr are members of the tidyverse, so a single invocation of library() makes the functions defined in these two packages available for our use: library(tidyverse) 7.2 Reading data with the readr package The readr package defines a number of functions for reading data tables from common file formats like Comma-Separated-Value (CSV) and Tab-Separated-Value (TSV) files. The two most frequently used readr functions we’ll use in this class are read_csv() and read_tsv() for reading CSV and TSV files respectively. There are some variants of these basic function, which you can read about by invoking the help system (?read_csv). 7.2.1 Reading Excel files The tidyverse also includes a package called readxl which can be used to read Excel spreadsheets (recent versions with .xls and .xlsx extensions). Excel files are somewhat more complicated to deal with because they can include separate “sheets”. We won’t use readxl in this class, but documentation and examples of how readxl is used can be found at the page linked above. 7.2.2 Example data: NC Births For today’s hands on session we’ll use a data set that contains information on 150 cases of mothers and their newborns in North Carolina in 2004. This data set is available at the following URL: https://github.com/Bio723-class/example-datasets/raw/master/nc-births.txt The births data is a TSV file, so we’ll use the read_tsv() function to read it: births &lt;- read_tsv(&quot;https://github.com/Bio723-class/example-datasets/raw/master/nc-births.txt&quot;) ## Rows: 150 Columns: 9 ## ── Column specification ────────────────── ## Delimiter: &quot;\\t&quot; ## chr (3): premature, sexBaby, smoke ## dbl (6): fAge, mAge, weeks, visits, gained, weight ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Notice that when you used read_tsv() the function printed information about how it “parsed” the data (i.e. the types it assigned to each of the columns). The variables in the data set are: father’s age (fAge), mother’s age (mAge), weeks of gestation (weeks) whether the birth was premature or full term (premature) number of OB/GYN visits (visits) mother’s weight gained in pounds (gained) babies birth weight (weight) sex of the baby (sexBaby) whether the mother was a smoker (smoke). Notice too that we read the TSV file directly from a remote location via a URL. If instead, you wanted to load a local file on your computer you would specify the “path” – i.e. the location on your hard drive where you stored the file. For example, here is how I would load the same file if it was stored in the Downloads directory on my Mac laptop: # load the data from a local file births &lt;- read_tsv(&quot;/Users/pmagwene/Downloads/nc-births.txt&quot;) 7.3 A note on “tibbles” You may have noticed that most of the functions defined in tidyverse related packages return not data frames, but rather something called a “tibble”. You can think about tibbles as light-weight data frames. In fact if you ask about the “class” of a tibble you’ll see that it includes data.frame as one of it’s classes as well as tbl and tbl_df. class(births) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; There are some minor differences between data frame and tibbles. For example, tibbles print differently in the console and don’t automatically change variable names and types in the same way that standard data frames do. Usually tibbles can be used wherever a standard data frame is expected, but you may occasionally find a function that only works with a standard data frame. It’s easy to convert a tibble to a standard data frame using the as.data.frame function: births.std.df &lt;- as.data.frame(births) For more details about tibbles, see the Tibbles chapter in R for Data Analysis. 7.4 Data filtering and transformation with dplyr dplyr is powerful tool for data filter and transformation. In the same way that ggplot2 attempts to provide a “grammar of graphics”, dplyr aims to provide a “grammar of data manipulation”. In today’s material we will see how dplyr complements and simplifies standard data frame indexing and subsetting operations. However, dplyr is focused only on data frames and doesn’t completely replace the basic subsetting operations, and so being adept with both dplyr and the indexing approaches we’ve seen previously is important. If you’re curious about the name “dplyr”, the package’s originator Hadley Wickham says it’s supposed to invoke the idea of pliers for data frames (Github: Meaning of dplyrs name) 7.5 dplyr’s “verbs” The primary functions in the dplyr package can be thought of as a set of “verbs”, each verb corresponding to a common data manipulation task. Some of the most frequently used verbs/functions in dplyr include: select – select columns filter – filter rows mutate – create new columns arrange– reorder rows summarize – summarize values group_by – split data frame on some grouping variable. Can be powerfully combined with summarize All of these functions return new data frames rather than modifying the existing data frame (though some of the functions support in place modification of data frames via optional arguments).We illustrate these below by example using the NC births data. 7.5.1 select The select function subsets the columns (variables) of a data frame. For example, to select just the weeks and weight columns from the births data set we could do: # note I&#39;m prefixing select with the package name (dplyr) # to avoid name clashes with built-in select function wks.weight &lt;- dplyr::select(births, weeks, weight) dim(wks.weight) # dim should be 50 x 2 ## [1] 150 2 head(wks.weight) ## # A tibble: 6 × 2 ## weeks weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 39 6.88 ## 2 39 7.69 ## 3 40 8.88 ## 4 40 9 ## 5 40 7.94 ## 6 40 8.25 The equivalent using standard indexing would be: wks.wt.alt &lt;- births[c(&quot;weeks&quot;, &quot;weight&quot;)] dim(wks.wt.alt) ## [1] 150 2 head(wks.wt.alt) ## # A tibble: 6 × 2 ## weeks weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 39 6.88 ## 2 39 7.69 ## 3 40 8.88 ## 4 40 9 ## 5 40 7.94 ## 6 40 8.25 Notes: * The first argument to all of the dplyr functions is the data frame you’re operating on When using functions defined in dplyr and ggplot2 variable names are (usually) not quoted or used with the $ operator. This is a design feature of these libraries and makes it easier to carry out interactive analyes because it saves a fair amount of typing. 7.5.2 filter The filter function returns those rows of the data set that meet the given logical criterion. For example, to get all the premature babies in the data set we could use filter as so: premies &lt;- filter(births, premature == &quot;premie&quot;) dim(premies) ## [1] 21 9 The equivalent using standard indexing would be: premies.alt &lt;- births[births$premature == &quot;premie&quot;,] The filter function will work with more than one logical argument, and these are joined together using Boolean AND logic (i.e. intersection). For example, to find those babies that were premature and whose mothers were smokers we could do: smoking.premies &lt;- filter(births, premature == &quot;premie&quot;, smoke == &quot;smoker&quot;) The equivalent call using standard indexing is: # don&#39;t forget the trailing comma to indicate rows! smoking.premies.alt &lt;- births[(births$premature == &quot;premie&quot;) &amp; (births$smoke == &quot;smoker&quot;),] filter also accepts logical statements chained together using the standard Boolean operators. For example, to find babies who were premature or whose moms were older than 35 you could use the OR operator |: premies.or.oldmom &lt;- filter(births, premature == &quot;premie&quot; | fAge &gt; 35) 7.5.3 mutate The mutate function creates a new data frame that is the same as input data frame but with additional variables (columns) as specified by the function arguments. In the example below, I create two new variables, weight.in.kg and a mom.smoked: # to make code more readable it&#39;s sometime useful to spread out # function arguments over multiple lines like I&#39;ve done here births.plus &lt;- mutate(births, weight.in.kg = weight / 2.2, mom.smoked = (smoke == &quot;smoker&quot;)) head(births.plus) ## # A tibble: 6 × 11 ## fAge mAge weeks premature visits gained weight sexBaby smoke weigh…¹ mom.s…² ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 31 30 39 full term 13 1 6.88 male smok… 3.13 TRUE ## 2 34 36 39 full term 5 35 7.69 male nons… 3.50 FALSE ## 3 36 35 40 full term 12 29 8.88 male nons… 4.04 FALSE ## 4 41 40 40 full term 13 30 9 female nons… 4.09 FALSE ## 5 42 37 40 full term NA 10 7.94 male nons… 3.61 FALSE ## 6 37 28 40 full term 12 35 8.25 male smok… 3.75 TRUE ## # … with abbreviated variable names ¹​weight.in.kg, ²​mom.smoked The equivalent using standard indexing would be to create a new data frame from births, appending the new variables to the end as so: births.plus.alt &lt;- data.frame(births, weight.in.kg = births$weight / 2.2, mom.smoked = (births$smoke == &quot;smoker&quot;)) 7.5.4 arrange Arrange creates a new data frame where the rows are sorted according to their values for one or more variables. For example, to sort by mothers age we could do: young.moms.first &lt;- arrange(births, mAge) head(young.moms.first) ## # A tibble: 6 × 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18 15 37 full term 12 76 8.44 male nonsmoker ## 2 NA 16 40 full term 4 12 6 female nonsmoker ## 3 21 16 38 full term 15 75 7.56 female smoker ## 4 26 17 38 full term 11 30 9.5 female nonsmoker ## 5 17 17 29 premie 4 10 2.63 female nonsmoker ## 6 20 17 40 full term 17 38 7.19 male nonsmoker The equivalent to arrange using standard indexing would be to use the information returned by the order function: young.moms.first.alt &lt;- births[order(births$mAge),] head(young.moms.first.alt) ## # A tibble: 6 × 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18 15 37 full term 12 76 8.44 male nonsmoker ## 2 NA 16 40 full term 4 12 6 female nonsmoker ## 3 21 16 38 full term 15 75 7.56 female smoker ## 4 26 17 38 full term 11 30 9.5 female nonsmoker ## 5 17 17 29 premie 4 10 2.63 female nonsmoker ## 6 20 17 40 full term 17 38 7.19 male nonsmoker When using arrange, multiple sorting variables can be specified: sorted.by.moms.and.dads &lt;- arrange(births, mAge, fAge) head(sorted.by.moms.and.dads) ## # A tibble: 6 × 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18 15 37 full term 12 76 8.44 male nonsmoker ## 2 21 16 38 full term 15 75 7.56 female smoker ## 3 NA 16 40 full term 4 12 6 female nonsmoker ## 4 17 17 29 premie 4 10 2.63 female nonsmoker ## 5 20 17 40 full term 17 38 7.19 male nonsmoker ## 6 26 17 38 full term 11 30 9.5 female nonsmoker If you want to sort in descending order, you can combing arrange with the desc (=descend) function, also defined in dplyr: old.moms.first &lt;- arrange(births, desc(mAge)) head(old.moms.first) ## # A tibble: 6 × 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 NA 41 33 premie 13 0 5.69 female nonsmoker ## 2 41 40 40 full term 13 30 9 female nonsmoker ## 3 33 40 36 premie 13 23 7.81 female nonsmoker ## 4 40 40 38 full term 13 38 7.31 male nonsmoker ## 5 46 39 38 full term 10 35 6.75 male smoker ## 6 NA 38 32 premie 10 16 2.19 female smoker 7.5.5 summarize summarize applies a function of interest to one or more variables in a data frame, reducing a vector of values to a single value and returning the results in a data frame. This is most often used to calculate statistics like means, medians, count, etc. As we’ll see below, this is powerful when combined with the group_by function. summarize(births, mean.wt = mean(weight), median.wks = median(weeks)) ## # A tibble: 1 × 2 ## mean.wt median.wks ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7.05 39 You’ll need to be diligent if your data has missing values (NAs). For example, by default the mean function returns NA if any of the input values are NA: summarize(births, mean.gained = mean(gained)) ## # A tibble: 1 × 1 ## mean.gained ## &lt;dbl&gt; ## 1 NA However, if you read the mean docs (?mean) you’ll see that there is an na.rm argument that indicates whether NA values should be removed before computing the mean. This is what we want so we instead call summarize as follows: summarize(births, mean.gained = mean(gained, na.rm = TRUE)) ## # A tibble: 1 × 1 ## mean.gained ## &lt;dbl&gt; ## 1 32.5 7.5.6 group_by The group_by function implicitly adds grouping information to a data frame. # group the births by whether mom smoked or not by_smoking &lt;- group_by(births, smoke) The object returned by group_by is a “grouped data frame”: class(by_smoking) ## [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Some functions, like count() and summarize() (see below) know how to use the grouping information. For example, to count the number of births conditional on mother smoking status we could do: count(by_smoking) ## # A tibble: 2 × 2 ## smoke n ## &lt;chr&gt; &lt;int&gt; ## 1 nonsmoker 100 ## 2 smoker 50 group_by also works with multiple grouping variables, with each added grouping variable specified as an additional argument: by_smoking.and.mAge &lt;- group_by(births, smoke, mAge &gt; 35) 7.5.7 Combining grouping and summarizing Grouped data frames can be combined with the summarize function we saw above. For example, if we wanted to calculate mean birth weight, broken down by whether the baby’s mother smoked or not we could call summarize with our by_smoking grouped data frame: summarize(by_smoking, mean.wt = mean(weight)) ## # A tibble: 2 × 2 ## smoke mean.wt ## &lt;chr&gt; &lt;dbl&gt; ## 1 nonsmoker 7.18 ## 2 smoker 6.78 Similarly to get the mean birth weight of children conditioned on mothers smoking status and age: summarize(by_smoking.and.mAge, mean(weight)) ## `summarise()` has grouped output by ## &#39;smoke&#39;. You can override using the ## `.groups` argument. ## # A tibble: 4 × 3 ## smoke `mAge &gt; 35` `mean(weight)` ## &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 nonsmoker FALSE 7.17 ## 2 nonsmoker TRUE 7.26 ## 3 smoker FALSE 6.83 ## 4 smoker TRUE 6.30 7.5.8 Scoped variants of mutate and summarize Both the mutate() and summarize() functions provide “scoped” alternatives, that allow us to apply the operation on a selection of variables. These variants are often used in combination with grouping. We’ll look at the summarize versions – summarize_all(), summarize_at(), and summarize_if(). See the documentation (?mutate_all) for descriptions of the mutate versions. 7.5.8.1 summarize_all() summarize_all() applies a one or more functions to all columns in a data frame. Here we illustrate a simple version of this with the iris data: # group by species by_species &lt;- group_by(iris, Species) # calculate the mean of every variable, grouped by species summarize_all(by_species, mean) ## # A tibble: 3 × 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Note that if we try and apply summarize_all() in the same way to the grouped data frame by_smoking we’ll get a bunch of warning messages: summarize_all(by_smoking, mean) ## # A tibble: 2 × 9 ## smoke fAge mAge weeks premature visits gained weight sexBaby ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker NA 26.9 38.6 NA NA NA 7.18 NA ## 2 smoker NA 26 38.5 NA 10.8 NA 6.78 NA Here’s an example of one of these warnings: Warning messages: 1: In mean.default(premature) : argument is not numeric or logical: returning NA This message is telling us that we can’t apply the mean() function to the data frame column premature because this is not a numerical or logical vector. Despite this and the other similar warnings, summarize_all() does return a result, but the means for any non-numeric values are replaced with NAs, as shown below: # A tibble: 2 x 9 smoke fAge mAge weeks premature visits gained weight sexBaby &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker NA 26.9 38.6 NA NA NA 7.18 NA 2 smoker NA 26.0 38.5 NA 10.8 NA 6.78 NA If you examine the output above, you’ll see that there are several variables that are numeric, however we still got NAs when we calculated the grouped means. This is because those variables contain NA values. The mean function has an optional argument, na.rm, which tells the function to remove any missing data before calculating the mean. Thus we can modify our call to summarize_all as follows: # calculate mean of all variables, grouped by smoking status summarize_all(by_smoking, mean, na.rm = TRUE) ## # A tibble: 2 × 9 ## smoke fAge mAge weeks premature visits gained weight sexBaby ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 29.8 26.9 38.6 NA 11.9 32.5 7.18 NA ## 2 smoker 29.7 26 38.5 NA 10.8 32.3 6.78 NA Note that the non-numeric data columns still lead to NA values. 7.5.8.2 summarize_if() summarize_if() is similar to summarize_all(), except it only applies the function of interest to those variables that match a particular predicate (i.e. are TRUE for a particular TRUE/FALSE test). Here we use summarize_if() to apply the mean() function to only those variables (columns) that are numeric. # calculate mean of all numeric variables, grouped by smoking status summarize_if(by_smoking, is.numeric, mean, na.rm = TRUE) ## # A tibble: 2 × 7 ## smoke fAge mAge weeks visits gained weight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 29.8 26.9 38.6 11.9 32.5 7.18 ## 2 smoker 29.7 26 38.5 10.8 32.3 6.78 7.5.8.3 summarize_at() summarize_at() allows us to apply functions of interest only to specific variables. # calculate mean of gained and weight variables, grouped by smoking status summarize_at(by_smoking, c(&quot;gained&quot;, &quot;weight&quot;), mean, na.rm = TRUE) ## # A tibble: 2 × 3 ## smoke gained weight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 32.5 7.18 ## 2 smoker 32.3 6.78 All three of the scoped summarize functions can also be used to apply multiple functions, by wrapping the function names in a call to dplyr::funs(): # calculate mean and std deviation of # gained and weight variables, grouped by smoking status summarize_at(by_smoking, c(&quot;gained&quot;, &quot;weight&quot;), funs(mean, sd), na.rm = TRUE) ## # A tibble: 2 × 5 ## smoke gained_mean weight_mean gained_sd weight_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 32.5 7.18 15.2 1.43 ## 2 smoker 32.3 6.78 16.6 1.60 summarize_at() accepts as the the argument for variables a character vector of column names, a numeric vector of column positions, or a list of columns generated by the dplyr::vars() function, which can be be used as so: # reformatted to promote readability of arguments summarize_at(by_smoking, vars(gained, weight), funs(mean, sd), na.rm = TRUE) ## # A tibble: 2 × 5 ## smoke gained_mean weight_mean gained_sd weight_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 32.5 7.18 15.2 1.43 ## 2 smoker 32.3 6.78 16.6 1.60 7.5.9 Combining summarize with grouping aesthetics in ggplot2 We’ve already seen an instance of grouping (conditioning) when we used aesthetics like color or fill to distinguish subgroups in different types of statistical graphics. Below is an example where we integrate information from a group_by/summarize operation into a plot: # calculate mean weights, conditioned on smoking status wt.by.smoking &lt;- summarize(by_smoking, mean_weight = mean(weight, na.rm = TRUE)) # create density plot for all the data # and then use geom_vline to draw vertical lines at the means for # each group ggplot(births) + geom_density(aes(x = weight, color = smoke)) + # data drawn from births geom_vline(data = wt.by.smoking, # note use of different data frame! mapping = aes(xintercept = mean_weight, color = smoke), linetype = &#39;dashed&#39;) 7.6 Pipes dplyr includes a very useful operator available called a pipe available to us. Pipes are powerful because they allow us to chain together sets of operations in a very intuitive fashion while minimizing nested function calls. We can think of pipes as taking the output of one function and feeding it as the first argument to another function call, where we’ve already specified the subsequent arguments. Pipes are actually defined in another packaged called magrittr. We’ll look at the basic pipe operator and then look at a few additional “special” pipes that magrittr provides. 7.6.1 Install and load magrittr In magrittr in not already installed, install it via the command line or the RStudio GUI. Having done so, you will need to load magrittr via the library() function: library(magrittr) 7.6.2 The basic pipe operator The pipe operator is designated by %&gt;%. Using pipes, the expression x %&gt;% f() is equivalent to f(x) and the expression x %&gt;% f(y) is equivalent to f(x,y). The documentation on pipes (see ?magrittr) uses the notation lhs %&gt;% rhs where lhs and rhs are short for “left-hand side” and “right-hand side” respectively. I’ll use this same notation in some of the explanations that follow. births %&gt;% head() # same as head(births) ## # A tibble: 6 × 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker births %&gt;% head # you can even leave the parentheses out ## # A tibble: 6 × 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker births %&gt;% head(10) # same as head(births, 10) ## # A tibble: 10 × 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker ## 7 35 35 28 premie 6 29 1.63 female nonsmoker ## 8 28 21 35 premie 9 15 5.5 female smoker ## 9 22 20 32 premie 5 40 2.69 male smoker ## 10 36 25 40 full term 13 34 8.75 female nonsmoker Multiple pipes can be chained together, such that x %&gt;% f() %&gt;% g() %&gt;% h() is equivalent to h(g(f(x))). # equivalent to: head(arrange(births, weight), 10) births %&gt;% arrange(weight) %&gt;% head(10) ## # A tibble: 10 × 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 35 35 28 premie 6 29 1.63 female nonsmoker ## 2 NA 18 33 premie 7 40 1.69 male smoker ## 3 NA 38 32 premie 10 16 2.19 female smoker ## 4 17 17 29 premie 4 10 2.63 female nonsmoker ## 5 22 20 32 premie 5 40 2.69 male smoker ## 6 38 37 26 premie 5 25 3.63 male nonsmoker ## 7 25 22 34 premie 10 20 3.75 male nonsmoker ## 8 NA 24 38 full term 16 50 3.75 female nonsmoker ## 9 30 25 35 premie 15 40 4.5 male smoker ## 10 19 20 34 premie 13 6 4.5 male nonsmoker When there are multiple piping operations, I like to arrange the statements vertically to help emphasize the flow of processing and to facilitate debugging and/or modification. I would usually rearrange the above code block as follows: births %&gt;% arrange(weight) %&gt;% head(10) ## # A tibble: 10 × 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 35 35 28 premie 6 29 1.63 female nonsmoker ## 2 NA 18 33 premie 7 40 1.69 male smoker ## 3 NA 38 32 premie 10 16 2.19 female smoker ## 4 17 17 29 premie 4 10 2.63 female nonsmoker ## 5 22 20 32 premie 5 40 2.69 male smoker ## 6 38 37 26 premie 5 25 3.63 male nonsmoker ## 7 25 22 34 premie 10 20 3.75 male nonsmoker ## 8 NA 24 38 full term 16 50 3.75 female nonsmoker ## 9 30 25 35 premie 15 40 4.5 male smoker ## 10 19 20 34 premie 13 6 4.5 male nonsmoker 7.6.3 An example without pipes To illustrate how pipes help us, first let’s look at an example set of analysis steps without using pipes. Let’s say we wanted to explore the relationship between father’s age and baby’s birth weight. We’ll start this process of exploration by generating a bivariate scatter plot. Being good scientists we want to express our data in SI units, so we’ll need to converts pounds to kilograms. You’ll also recall that a number of the cases have missing data on father’s age, so we’ll want to remove those before we plot them. Here’s how we might accomplish these steps: # add a new column for weight in kg births.kg &lt;- mutate(births, weight.kg = weight / 2.2) # filter out the NA fathers filtered.births &lt;- filter(births.kg, !is.na(fAge)) # create our plot ggplot(filtered.births, aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) Notice that we created two “temporary” data frames along the way – births.kg and filtered.births. These probably aren’t of particular interest to us, but we needed to generate them to build the plot we wanted. If you were particularly masochistic you could avoid these temporary data frames by using nested functions call like this: # You SHOULD NOT write nested code like this. # Code like this is hard to debug and understand! ggplot(filter(mutate(births, weight.kg = weight / 2.2), !is.na(fAge)), aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) 7.6.4 The same example using pipes The pipe operator makes the output of one statement (lhs) as the first input of a following function (rhs). This simplifies the above example to: births %&gt;% mutate(weight.kg = weight / 2.2) %&gt;% filter(!is.na(fAge)) %&gt;% ggplot(aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) In the example above, we feed the data frame into the mutate function. mutate expects a data frame as a first argument, and subsequent arguments specify the new variables to be created. births %&gt;% mutate(weight.kg = weight / 2.2) is thus equivalent to mutate(births, weight.kg = weight / 2.2)). We then pipe the output to filter, removing NA fathers, and then pipe that output as the input to ggplot. As mentioned previously, it’s good coding style to write each discrete step as its own line when using piping. This make it easier to understand what the steps of the analysis are as well as facilitating changes to the code (commenting out lines, adding lines, etc) 7.6.5 Assigning the output of a statement involving pipes to a variable It’s important to recognize that pipes are simply a convenient way to chain together a series of expression. Just like any other compound expression, the output of a series of pipe statements can be assigned to a variable, like so: stats.old.moms &lt;- births %&gt;% filter(mAge &gt; 35) %&gt;% summarize(median.gestation = median(weeks), mean.weight = mean(weight)) stats.old.moms ## # A tibble: 1 × 2 ## median.gestation mean.weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 38 6.94 Note that our summary table, stats.old.moms, is itself a data frame. 7.6.6 Compound assignment pipe operator A fairly common operation when working interactively in R is to update an existing data frame. magrittr defines another pipe operator – %&lt;&gt;% – called the “compound assignment” pipe operator, to facilitate this. The compound assignment pipe operator has the basic usage lhs %&lt;&gt;% rhs. This operator evaluates the function on the rhs using the lhs as the first argument, and then updates the lhs with the resulting value. This is simply shorthand for writing lhs &lt;- lhs %&gt;% rhs. stats.old.moms %&lt;&gt;% # note compound pipe operator! mutate(mean.weight.kg = mean.weight / 2.2) 7.6.7 The dot operator with pipes When working with pipes, sometimes you’ll want to use the lhs in multiple places on the rhs, or as something other than the first argument to the rhs. magrittr provides for this situation by using the dot (.) operator as a placeholder. Using the dot operator, the expression y %&gt;% f(x, .) is equivalent to f(x,y). c(&quot;dog&quot;, &quot;cakes&quot;, &quot;sauce&quot;, &quot;house&quot;) %&gt;% # create a vector sample(1) %&gt;% # pick a random single element of that vector str_c(&quot;hot&quot;, .) # string concatenate the pick with the word &quot;hot&quot; ## [1] &quot;hothouse&quot; 7.6.8 The exposition pipe operator magrittr defines another operator called the “exposition pipe operator”, designed %$%. This operator exposes the names in the lhs to the expression on the rhs. Here is an example of using the exposition pipe operator to simply return the vector of weights: births %&gt;% filter(premature == &quot;premie&quot;) %$% # note the different pipe operator! weight ## [1] 1.63 5.50 2.69 6.50 7.81 4.75 3.75 2.19 6.81 4.69 6.75 4.50 5.94 4.50 5.06 ## [16] 5.69 1.69 6.31 2.63 5.88 3.63 If we wanted to calculate the minimum and maximum weight of premature babies in the data set we could do the following (though I’d usually prefer summarize() unless I needed the results in the form of a vector): births %&gt;% filter(mAge &gt; 35) %$% # note the different pipe operator! c(min(weight), max(weight)) ## [1] 2.19 10.13 "],["data-wrangling.html", "Chapter 8 Data wrangling 8.1 Libraries 8.2 Data 8.3 Loading the data 8.4 Check your data! 8.5 Renaming data frame columms 8.6 Dropping unneeded columns 8.7 Merging data frames 8.8 Reshaping data with tidyr 8.9 Using your tidy data 8.10 Long-to-wide conversion using pivot_wider() 8.11 Exploring bivariate relationships using “wide” data", " Chapter 8 Data wrangling In the real world you’ll often create a data set (or be given one) in a format that is less than ideal for analysis. This can happen for a number of reasons. For example, the data may have been recorded in a manner convenient for collection and visual inspection, but which does not work well for analysis and plotting. Or the data may be an amalgamation of multiple experiments, in which each of the experimenters used slightly different naming conventions. Or the data may have been produced by an instrument that produces output with a fixed format. Sometimes important experimental information is included in the column headers of a spreadsheet. Whatever the case, we often find ourselves in the situation where we need to “wrangle” our data into a “tidy” format before we can proceed with visualization and analysis. The “R for Data Science” text discusses some desirable rules for “tidy” data in order to facilitate downstream analyses. These are: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In this lecture we’re going to walk through an extended example of wrangling some data into a “tidy” format. 8.1 Libraries library(tidyverse) library(cowplot) 8.2 Data To illustrate a data wrangling pipeline, we’re going to use a gene expression microarray data set, based on the following paper: Spellman PT, et al. 1998. Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization. Mol Biol Cell 9(12): 3273-97. In this paper, Spellman and colleagues tried to identify all the genes in the yeast genome (&gt;6000 genes) that exhibited oscillatory behaviors suggestive of cell cycle regulation. To do so, they combined gene expression measurements from six different types of cell cycle synchronization experiments. The six types of cell cycle synchronization experiments included in this data set are: synchronization by alpha-factor = “alpha” synchronization by cdc15 temperature sensitive mutants = “cdc15” synchronization by cdc28 temperature sensitive mutants = “cdc28” synchronization by elutration = “elu” synchronization by cln3 mutatant strains = “cln3” synchronization by clb2 mutant strains = “clb2” 8.3 Loading the data Download the Spellman data to your filesystem from this link (right-click the “Download” button and save to your Downloads folder or similar). I suggest that once you download the data, you open it in a spreadsheet program (e.g. Excel) or use the RStudio Data Viewer to get a sense of what the data looks like. Let’s load it into R, using the read_tsv() function, using the appropriate file path. # the file path may differ on your computer spellman &lt;- read_tsv(&quot;~/Downloads/spellman-combined.txt&quot;) ## New names: ## Rows: 6178 Columns: 83 ## ── Column specification ## ────────────────── Delimiter: &quot;\\t&quot; chr ## (1): ...1 dbl (77): cln3-1, cln3-2, ## clb2-2, clb2-1, alpha0, alpha7, alpha14, ## alpha21, ... lgl (5): clb, alpha, cdc15, ## cdc28, elu ## ℹ Use `spec()` to retrieve the full ## column specification for this data. ℹ ## Specify the column types or set ## `show_col_types = FALSE` to quiet this ## message. ## • `` -&gt; `...1` The initial dimenions of the data frame are: dim(spellman) ## [1] 6178 83 8.4 Check your data! It’s a good habit when working with any new data set to check your data after import to make sure it got read as expected. The View() function provides a spreadsheet like graphical view of the data: View(spellman) The names() function will show you the column names assigned to the data frame: names(spellman) The dplyr::glimpse() function will provide a summary view of each column, showing you the type assigned to each column and the first few values in each. 8.5 Renaming data frame columms NOTE: Naming rules for un-named columns has changed as of readr 2.0. Previously, missing column names were given names like X1, X2, etc. Now the default is ...1, ...2, etc. If you look at the Spellman data set in your spreadsheet program you might have noticed that the first column of data did not have a name in the header. This is a problem during import because in a R data frame, every column must have a name. To resolve this issues, the read_tsv() function (and other functions in the readr pckage) assigned “dummy names” in cases where none was found. There was a single missing name corresponding to the first column, which was given the dummy name ...1. Our first task is to give the first column a more meaningful name. This column gives “systematic gene names” – a standardized naming scheme for genes in the yeast genome. We’ll use dplyr::rename() to rename ...1 to gene. Note that rename can take multiple arguments if you need to rename multiple columns simultaneously. spellman_clean &lt;- spellman |&gt; rename(gene = &quot;...1&quot;) 8.6 Dropping unneeded columns Take a look at the Spellman data again in your spreadsheet program (or using View()). You’ll notice there are some blank columns. For example there is a column with the header “alpha” that has no entries. These are simply visual organizing elements that the creator of the spreadsheet added to separate the different experiments that are included in the data set. We can use dplyr::select() to drop columns by prepending column names with the negative sign: # drop the alpha column keeping all others spellman_clean &lt;- spellman_clean |&gt; dplyr::select(-alpha) Note that usually select() keeps only the variables you specify. However if the first expression is negative, select will instead automatically keep all variables, dropping only those you specify. 8.6.1 Finding all empty columns In the example above, we looked at the data and saw that the “alpha” column was empty, and thus dropped it. This worked because there are only a modest number of columns in the data frame in its initial form. However, if our data frame contained thousands of columns, this “look and see” procedure would be inefficient. Can we come up with a general solution for removing empty columns from a data frame? When you load a data frame from a spreadsheet, empty cells are given the value NA. I’m going to illustrate two ways to work with NA values to remove empty columns from our data frame. 8.6.1.1 Solution using colSums() and standard indexing In previous class sessions we were introduced to the function is.na() which tests each value in a vector or data frame for whether it’s NA or not. We can count NA values in a vector by summing the output of is.na(). Conversely we can count the number of “not NA” items by using the negation operator (!): # count number of NA values in the alpha0 column sum(is.na(spellman_clean$alpha0)) ## [1] 165 # count number of values that are NOT NA in alpha0 sum(!is.na(spellman_clean$alpha0)) ## [1] 6013 This seems like it should get us close to a solution but sum(is.na(..)) when applied to a data frame counts NAs across the entire data frame, not column-by-column. # doesn&#39;t do what we hoped! sum(is.na(spellman_clean)) ## [1] 52839 If we want sums of NAs by column, we can use the colSums() function instead of sum() as illustrated below: # get number of NAs by column colSums(is.na(spellman_clean)) ## gene cln3-1 cln3-2 clb clb2-2 clb2-1 alpha0 alpha7 ## 0 193 365 6178 454 142 165 525 ## alpha14 alpha21 alpha28 alpha35 alpha42 alpha49 alpha56 alpha63 ## 191 312 267 207 123 257 147 186 ## alpha70 alpha77 alpha84 alpha91 alpha98 alpha105 alpha112 alpha119 ## 185 178 155 329 209 174 222 251 ## cdc15 cdc15_10 cdc15_30 cdc15_50 cdc15_70 cdc15_80 cdc15_90 cdc15_100 ## 6178 677 477 501 608 573 562 606 ## cdc15_110 cdc15_120 cdc15_130 cdc15_140 cdc15_150 cdc15_160 cdc15_170 cdc15_180 ## 570 611 495 574 811 583 571 803 ## cdc15_190 cdc15_200 cdc15_210 cdc15_220 cdc15_230 cdc15_240 cdc15_250 cdc15_270 ## 613 1014 573 741 596 847 379 537 ## cdc15_290 cdc28 cdc28_0 cdc28_10 cdc28_20 cdc28_30 cdc28_40 cdc28_50 ## 426 6178 122 72 67 55 66 56 ## cdc28_60 cdc28_70 cdc28_80 cdc28_90 cdc28_100 cdc28_110 cdc28_120 cdc28_130 ## 82 84 75 237 165 319 312 1439 ## cdc28_140 cdc28_150 cdc28_160 elu elu0 elu30 elu60 elu90 ## 2159 521 543 6178 122 153 175 132 ## elu120 elu150 elu180 elu210 elu240 elu270 elu300 elu330 ## 103 119 111 118 131 110 112 112 ## elu360 elu390 ## 156 114 Columns with all missing values can be more conveniently found by asking for those columns where the number of “not missing” values is zero: # get number of NAs by column colSums(!is.na(spellman_clean)) ## gene cln3-1 cln3-2 clb clb2-2 clb2-1 alpha0 alpha7 ## 6178 5985 5813 0 5724 6036 6013 5653 ## alpha14 alpha21 alpha28 alpha35 alpha42 alpha49 alpha56 alpha63 ## 5987 5866 5911 5971 6055 5921 6031 5992 ## alpha70 alpha77 alpha84 alpha91 alpha98 alpha105 alpha112 alpha119 ## 5993 6000 6023 5849 5969 6004 5956 5927 ## cdc15 cdc15_10 cdc15_30 cdc15_50 cdc15_70 cdc15_80 cdc15_90 cdc15_100 ## 0 5501 5701 5677 5570 5605 5616 5572 ## cdc15_110 cdc15_120 cdc15_130 cdc15_140 cdc15_150 cdc15_160 cdc15_170 cdc15_180 ## 5608 5567 5683 5604 5367 5595 5607 5375 ## cdc15_190 cdc15_200 cdc15_210 cdc15_220 cdc15_230 cdc15_240 cdc15_250 cdc15_270 ## 5565 5164 5605 5437 5582 5331 5799 5641 ## cdc15_290 cdc28 cdc28_0 cdc28_10 cdc28_20 cdc28_30 cdc28_40 cdc28_50 ## 5752 0 6056 6106 6111 6123 6112 6122 ## cdc28_60 cdc28_70 cdc28_80 cdc28_90 cdc28_100 cdc28_110 cdc28_120 cdc28_130 ## 6096 6094 6103 5941 6013 5859 5866 4739 ## cdc28_140 cdc28_150 cdc28_160 elu elu0 elu30 elu60 elu90 ## 4019 5657 5635 0 6056 6025 6003 6046 ## elu120 elu150 elu180 elu210 elu240 elu270 elu300 elu330 ## 6075 6059 6067 6060 6047 6068 6066 6066 ## elu360 elu390 ## 6022 6064 To get the corresponding names of the columns where all values are missing we can do the following: # get names of all columns for which all rows are NA # using standard indexing names(spellman_clean)[colSums(!is.na(spellman_clean)) == 0] ## [1] &quot;clb&quot; &quot;cdc15&quot; &quot;cdc28&quot; &quot;elu&quot; The solution to removing all empty columns thus becomes: empty_columns &lt;- names(spellman_clean)[colSums(!is.na(spellman_clean)) == 0] spellman_clean |&gt; # The minus sign before any_of() indicates negation # So read this as &quot;drop any of these columns&quot; select(-any_of(empty_columns)) |&gt; names() ## [1] &quot;gene&quot; &quot;cln3-1&quot; &quot;cln3-2&quot; &quot;clb2-2&quot; &quot;clb2-1&quot; &quot;alpha0&quot; ## [7] &quot;alpha7&quot; &quot;alpha14&quot; &quot;alpha21&quot; &quot;alpha28&quot; &quot;alpha35&quot; &quot;alpha42&quot; ## [13] &quot;alpha49&quot; &quot;alpha56&quot; &quot;alpha63&quot; &quot;alpha70&quot; &quot;alpha77&quot; &quot;alpha84&quot; ## [19] &quot;alpha91&quot; &quot;alpha98&quot; &quot;alpha105&quot; &quot;alpha112&quot; &quot;alpha119&quot; &quot;cdc15_10&quot; ## [25] &quot;cdc15_30&quot; &quot;cdc15_50&quot; &quot;cdc15_70&quot; &quot;cdc15_80&quot; &quot;cdc15_90&quot; &quot;cdc15_100&quot; ## [31] &quot;cdc15_110&quot; &quot;cdc15_120&quot; &quot;cdc15_130&quot; &quot;cdc15_140&quot; &quot;cdc15_150&quot; &quot;cdc15_160&quot; ## [37] &quot;cdc15_170&quot; &quot;cdc15_180&quot; &quot;cdc15_190&quot; &quot;cdc15_200&quot; &quot;cdc15_210&quot; &quot;cdc15_220&quot; ## [43] &quot;cdc15_230&quot; &quot;cdc15_240&quot; &quot;cdc15_250&quot; &quot;cdc15_270&quot; &quot;cdc15_290&quot; &quot;cdc28_0&quot; ## [49] &quot;cdc28_10&quot; &quot;cdc28_20&quot; &quot;cdc28_30&quot; &quot;cdc28_40&quot; &quot;cdc28_50&quot; &quot;cdc28_60&quot; ## [55] &quot;cdc28_70&quot; &quot;cdc28_80&quot; &quot;cdc28_90&quot; &quot;cdc28_100&quot; &quot;cdc28_110&quot; &quot;cdc28_120&quot; ## [61] &quot;cdc28_130&quot; &quot;cdc28_140&quot; &quot;cdc28_150&quot; &quot;cdc28_160&quot; &quot;elu0&quot; &quot;elu30&quot; ## [67] &quot;elu60&quot; &quot;elu90&quot; &quot;elu120&quot; &quot;elu150&quot; &quot;elu180&quot; &quot;elu210&quot; ## [73] &quot;elu240&quot; &quot;elu270&quot; &quot;elu300&quot; &quot;elu330&quot; &quot;elu360&quot; &quot;elu390&quot; 8.6.1.2 Solution using the where() helper function An alternate approach is to use the where() helper function that works with dplyr::select(). To use where() we need to define a predicate function which returns a Boolean (logical) value depending on whether a vector is “all NA”. Here are two ways to write such a function, one based on summing logical values, the other based on the use of the built-in all() function. all.na &lt;- function(x) { sum(!is.na(x)) == 0 } all.na &lt;- function(x) { all(is.na(x)) } Having defined all.na() above we can now use this with select() and where() as follows: spellman_clean &lt;- spellman_clean |&gt; select(-where(all.na)) dim(spellman_clean) # note number of columns has been reduced ## [1] 6178 78 Version 4.1 of R introduced a new shorthand syntax of “anonymous functions” (sometimes called “lambda expressions” in other programming languages), so we could have defined our predicate function “inline” as so: spellman_clean |&gt; select(-where(\\(x) all(is.na(x)))) |&gt; names() ## [1] &quot;gene&quot; &quot;cln3-1&quot; &quot;cln3-2&quot; &quot;clb2-2&quot; &quot;clb2-1&quot; &quot;alpha0&quot; ## [7] &quot;alpha7&quot; &quot;alpha14&quot; &quot;alpha21&quot; &quot;alpha28&quot; &quot;alpha35&quot; &quot;alpha42&quot; ## [13] &quot;alpha49&quot; &quot;alpha56&quot; &quot;alpha63&quot; &quot;alpha70&quot; &quot;alpha77&quot; &quot;alpha84&quot; ## [19] &quot;alpha91&quot; &quot;alpha98&quot; &quot;alpha105&quot; &quot;alpha112&quot; &quot;alpha119&quot; &quot;cdc15_10&quot; ## [25] &quot;cdc15_30&quot; &quot;cdc15_50&quot; &quot;cdc15_70&quot; &quot;cdc15_80&quot; &quot;cdc15_90&quot; &quot;cdc15_100&quot; ## [31] &quot;cdc15_110&quot; &quot;cdc15_120&quot; &quot;cdc15_130&quot; &quot;cdc15_140&quot; &quot;cdc15_150&quot; &quot;cdc15_160&quot; ## [37] &quot;cdc15_170&quot; &quot;cdc15_180&quot; &quot;cdc15_190&quot; &quot;cdc15_200&quot; &quot;cdc15_210&quot; &quot;cdc15_220&quot; ## [43] &quot;cdc15_230&quot; &quot;cdc15_240&quot; &quot;cdc15_250&quot; &quot;cdc15_270&quot; &quot;cdc15_290&quot; &quot;cdc28_0&quot; ## [49] &quot;cdc28_10&quot; &quot;cdc28_20&quot; &quot;cdc28_30&quot; &quot;cdc28_40&quot; &quot;cdc28_50&quot; &quot;cdc28_60&quot; ## [55] &quot;cdc28_70&quot; &quot;cdc28_80&quot; &quot;cdc28_90&quot; &quot;cdc28_100&quot; &quot;cdc28_110&quot; &quot;cdc28_120&quot; ## [61] &quot;cdc28_130&quot; &quot;cdc28_140&quot; &quot;cdc28_150&quot; &quot;cdc28_160&quot; &quot;elu0&quot; &quot;elu30&quot; ## [67] &quot;elu60&quot; &quot;elu90&quot; &quot;elu120&quot; &quot;elu150&quot; &quot;elu180&quot; &quot;elu210&quot; ## [73] &quot;elu240&quot; &quot;elu270&quot; &quot;elu300&quot; &quot;elu330&quot; &quot;elu360&quot; &quot;elu390&quot; 8.6.2 Dropping columns by matching names Only two time points from the cln3 and clb2 experiments were reported in the original publication. The column names associated with these experiments are of the form “cln3-1”, “cln3-2”, “cln2-1”, etc. Since complete time series are unavailable for these two experimental conditions we will drop them from further consideration. select() can be called be called with a number of “helper functions” (Selection language). Here we’ll illustrate the matches() helper function which matches column names to a “regular expression”. Regular expressions (also referred to as “regex” or “regexp”) are a way of specifying patterns in strings. For the purposes of this document we’ll illustrate regexs by example; for a more detailed explanation of regular expressions see the the regex help(?regex) and the Chapter on Strings in “R for Data Analysis”: Let’s see how to drop all the “cln3” and “clb2” columns from the data frame using matches(): spellman_clean &lt;- spellman_clean |&gt; select(-matches(&quot;cln3&quot;)) |&gt; select(-matches(&quot;clb2&quot;)) If we wanted we could have collapsed our two match statements into one as follows: spellman_clean &lt;- spellman_clean |&gt; select(-matches(&quot;cln3|clb2&quot;)) In this second example, the character “|” is specifing an OR match within the regular expression, so this regular expression matches column names that contain “cln3” OR “clb2”. 8.7 Merging data frames Often you’ll find yourself in the situation where you want to combine information from multiple data sources. The usual requirement is that the data sources have one or more shared columns, that allow you to relate the entities or observations (rows) between the data sets. dplyr provides a variety of join functions to handle different data merging operators. To illustrating merging or joining data sources, we’ll add information about each genes “common name” and a description of the gene functions to our Spellman data set. I’ve prepared a file with this info based on info I downloaded from the Saccharomyces Genome Database. gene.info &lt;- read_csv(&quot;https://github.com/bio304-class/bio304-course-notes/raw/master/datasets/yeast-ORF-info.csv&quot;) ## Rows: 6610 Columns: 3 ## ── Column specification ────────────────── ## Delimiter: &quot;,&quot; ## chr (3): ftr.name, std.name, description ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Having loaded the data, let’s get a quick overview of it’s structure: names(gene.info) ## [1] &quot;ftr.name&quot; &quot;std.name&quot; &quot;description&quot; dim(gene.info) ## [1] 6610 3 head(gene.info) ## # A tibble: 6 × 3 ## ftr.name std.name description ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 YAL069W &lt;NA&gt; Dubious open reading frame; unlikely to encode a functiona… ## 2 YAL068W-A &lt;NA&gt; Dubious open reading frame; unlikely to encode a functiona… ## 3 YAL068C PAU8 Protein of unknown function; member of the seripauperin mu… ## 4 YAL067W-A &lt;NA&gt; Putative protein of unknown function; identified by gene-t… ## 5 YAL067C SEO1 Putative permease; member of the allantoate transporter su… ## 6 YAL066W &lt;NA&gt; Dubious open reading frame; unlikely to encode a functiona… In gene.info, the ftr.name column corresponds to the gene column in our Spellman data set. The std.name column gives the “common” gene name (not every gene has a common name so there are lots of NAs). The description column gives a brief textual description of what the gene product does. To combine spellmean_clean with gene.info we use the left_join function defined in dplyr. As noted in the description of the function, left_join(x, y) returns “all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns.” In addition, we have to specify the column to join by using the by argument to left_join. spellman.merged &lt;- left_join(spellman_clean, gene.info, by = c(&quot;gene&quot; = &quot;ftr.name&quot;)) By default, the joined columns are merged at the end of the data frame, so we’ll reorder variables to bring the std.name and description to the second and thirds columns, preserving the order of all the other columns. The dplyr function relocate() helps us do this: spellman.merged &lt;- spellman.merged |&gt; relocate(std.name, .after = gene) spellman.merged ## # A tibble: 6,178 × 76 ## gene std.n…¹ alpha0 alpha7 alpha14 alpha21 alpha28 alpha35 alpha42 alpha49 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 YAL001C TFC3 -0.15 -0.15 -0.21 0.17 -0.42 -0.44 -0.15 0.24 ## 2 YAL002W VPS8 -0.11 0.1 0.01 0.06 0.04 -0.26 0.04 0.19 ## 3 YAL003W EFB1 -0.14 -0.71 0.1 -0.32 -0.4 -0.58 0.11 0.21 ## 4 YAL004W &lt;NA&gt; -0.02 -0.48 -0.11 0.12 -0.03 0.19 0.13 0.76 ## 5 YAL005C SSA1 -0.05 -0.53 -0.47 -0.06 0.11 -0.07 0.25 0.46 ## 6 YAL007C ERP2 -0.6 -0.45 -0.13 0.35 -0.01 0.49 0.18 0.43 ## 7 YAL008W FUN14 -0.28 -0.22 -0.06 0.22 0.25 0.13 0.34 0.44 ## 8 YAL009W SPO7 -0.03 -0.27 0.17 -0.12 -0.27 0.06 0.23 0.11 ## 9 YAL010C MDM10 -0.05 0.13 0.13 -0.21 -0.45 -0.21 0.06 0.32 ## 10 YAL011W SWC3 -0.31 -0.43 -0.3 -0.23 -0.13 -0.07 0.08 0.12 ## # … with 6,168 more rows, 66 more variables: alpha56 &lt;dbl&gt;, alpha63 &lt;dbl&gt;, ## # alpha70 &lt;dbl&gt;, alpha77 &lt;dbl&gt;, alpha84 &lt;dbl&gt;, alpha91 &lt;dbl&gt;, alpha98 &lt;dbl&gt;, ## # alpha105 &lt;dbl&gt;, alpha112 &lt;dbl&gt;, alpha119 &lt;dbl&gt;, cdc15_10 &lt;dbl&gt;, ## # cdc15_30 &lt;dbl&gt;, cdc15_50 &lt;dbl&gt;, cdc15_70 &lt;dbl&gt;, cdc15_80 &lt;dbl&gt;, ## # cdc15_90 &lt;dbl&gt;, cdc15_100 &lt;dbl&gt;, cdc15_110 &lt;dbl&gt;, cdc15_120 &lt;dbl&gt;, ## # cdc15_130 &lt;dbl&gt;, cdc15_140 &lt;dbl&gt;, cdc15_150 &lt;dbl&gt;, cdc15_160 &lt;dbl&gt;, ## # cdc15_170 &lt;dbl&gt;, cdc15_180 &lt;dbl&gt;, cdc15_190 &lt;dbl&gt;, cdc15_200 &lt;dbl&gt;, … 8.8 Reshaping data with tidyr The tidyr package provides functions for reshaping or tidying data frames. tidyr is yet another component of the tidyverse, and thus was loaded by the library(tidyverse). We’re going to look at two functions tidyr::pivot_longer() and tidyr::extract(), and how they can be combined with now familiar dplyr functions we’ve seen previously. The reading assignment for today’s class session covers a variety of other functions defined in tidyr. The Spellman data, as I provided it to you, is in what we would call “wide” format. Each column (besides the gene column) corresponds to an experimental condition and time point. For example, “alpha0” is the alpha-factor experiment at time point 0 mins; “alpha7” is the alpha-factor experiment at time point 7 mins, etc. The cells within each column correspond to the expression of a corresponding gene (given by the first column which we renamed gene) in that particular experiment at that particular time point. In every expression column, the cells represents the same abstract property of interest – the expression of a gene of interest in a particular experiment/time point. Our first task will be to rearrange our “wide” data frame that consists of many different columns representing gene expression into a “long” data frame with just a single column representing expression. We’ll also create a new column to keep track of which experiment and time point the measurement came from. 8.8.1 Wide to long conversions using pivot_longer() NOTE: tidyr::pivot_longer() and tidyr::pivot_wider() replace the “superseded” tidyr functions, tidyr::gather() and tidyr::spread() that work similarly. I recommend you read the tidyr vignette on pivoting. To use pivot_longer() you need to specify: 1) the columns your are collapsing; 2) the name of a new column that will hold the names of the columns collapsed; and 3) the name of a new column will hold the values of the collapsed columns. Here we want to collapse all 73 of the expression columns – “alpha0” to “elu390” – into two columns: 1) a column to represent the expt/time point of the measurement, and 2) a column to represent the corresponding expression value. The column we don’t want to touch are the gene, std.name, and description. spellman.long &lt;- spellman.merged |&gt; pivot_longer(cols = !c(gene, std.name, description), names_to = &quot;expt.and.time&quot;, values_to = &quot;expression&quot;) Take a moment to look at the data in the “long format”: head(spellman.long) ## # A tibble: 6 × 5 ## gene std.name description expt.…¹ expre…² ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 YAL001C TFC3 Subunit of RNA polymerase III transcription … alpha0 -0.15 ## 2 YAL001C TFC3 Subunit of RNA polymerase III transcription … alpha7 -0.15 ## 3 YAL001C TFC3 Subunit of RNA polymerase III transcription … alpha14 -0.21 ## 4 YAL001C TFC3 Subunit of RNA polymerase III transcription … alpha21 0.17 ## 5 YAL001C TFC3 Subunit of RNA polymerase III transcription … alpha28 -0.42 ## 6 YAL001C TFC3 Subunit of RNA polymerase III transcription … alpha35 -0.44 ## # … with abbreviated variable names ¹​expt.and.time, ²​expression And compare the dimensions of the wide data to the new data: dim(spellman.merged) # for comparison ## [1] 6178 76 dim(spellman.long) ## [1] 450994 5 As you see, we’ve gone from a data frame with 6178 rows and 76 columns (wide format), to a new data frame with 450994 rows and 5 columns (long format). 8.8.2 Extracting information from combined variables using extract() The column expt.and.time violates one of our principles of tidy data: “Each variable must have its own column.”. This column conflates two different types of information – the experiment type and the time point of the measurement. Our next task is to split this information up into two new variables, which will help to facilitate downstream plotting and analysis. One complicating factor is that the different experiments/time combinations have different naming conventions: The “alpha” and “elu” experiments are of the form “alpha0”, “alpha7”, “elu0”, “elu30”, etc. In this case, the first part of the string gives the experiment type (either alpha or elu) and the following digits give the time point. In the “cdc15” and “cdc28” experiments the convention is slightly different; they are of the form “cdc15_0”, “cdc15_10”, “cdc28_0”, “cdc28_10”, etc. Here the part of the string before the underscore gives the experiment type, and the digits after the underscore give the time point. Because of the differences in naming conventions, we will find it easiest to break up spellman.long into a series of sub-data sets corresponding to each experiment type in order to extract out the experiment and time information. After processing each data subset separately, we will join the modified sub-data frames back together. 8.8.3 Subsetting rows Let’s start by getting just the rows corresponding to the “alpha” experiment/times. Here we use dplyr::filter in combination with stringr::str_detect to get all those rows in which the expt.and.time variable contains the string “alpha”. alpha.long &lt;- spellman.long |&gt; filter(str_detect(expt.and.time, &quot;alpha&quot;)) # look at the new data frame dim(alpha.long) ## [1] 111204 5 head(alpha.long, n = 10) ## # A tibble: 10 × 5 ## gene std.name description expt.…¹ expre…² ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha0 -0.15 ## 2 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha7 -0.15 ## 3 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha14 -0.21 ## 4 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha21 0.17 ## 5 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha28 -0.42 ## 6 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha35 -0.44 ## 7 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha42 -0.15 ## 8 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha49 0.24 ## 9 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha56 -0.1 ## 10 YAL001C TFC3 Subunit of RNA polymerase III transcription… alpha63 NA ## # … with abbreviated variable names ¹​expt.and.time, ²​expression 8.8.4 Splitting columns Having subsetted the data, we can now split expt.and.time into two new variables – expt and time. To do this we use tidyr::extract. alpha.long &lt;- alpha.long |&gt; tidyr::extract(col = expt.and.time, # column we&#39;re extracting from into = c(&quot;expt&quot;, &quot;time&quot;), # new columns we&#39;re creating regex=&quot;(alpha)([[:digit:]]+)&quot;, # regexp (see below) convert=TRUE) # automatically convert column types # NOTE: I&#39;m being explicit about saying tidyr::extract because the # magrittr package defines a different extract function Let’s take a moment to look at the regex argument to extract – regex=\"(alpha)([[:digit:]]+)\". The regex is specified as a character string. Each part we want to match and extract is surround by parentheses. In this case we have two sets of parentheses corresponding to the two matches we want to make. The first part of the regex is (alpha); here we’re looking to make an exact match to the string “alpha”. The second part of the regex reads ([[:digit:]]+). [[:digit:]] indicates we’re looking for a numeric digit. The + after [[:digit:]] indicates that we want to match one or more digits (i.e. to get a match we need to find at least one digit, but more than one digit should also be a match). Let’s take a look at the new version of alpha.long following application of extract: head(alpha.long, n = 10) ## # A tibble: 10 × 6 ## gene std.name description expt time expre…¹ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 0 -0.15 ## 2 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 7 -0.15 ## 3 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 14 -0.21 ## 4 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 21 0.17 ## 5 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 28 -0.42 ## 6 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 35 -0.44 ## 7 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 42 -0.15 ## 8 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 49 0.24 ## 9 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 56 -0.1 ## 10 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 63 NA ## # … with abbreviated variable name ¹​expression Notice our two new variables, both of which have appropriate types! A data frame for the elutriation data can be created similarly: elu.long &lt;- spellman.long |&gt; filter(str_detect(expt.and.time, &quot;elu&quot;)) |&gt; tidyr::extract(col = expt.and.time, # column we&#39;re extracting from into = c(&quot;expt&quot;, &quot;time&quot;), # new columns we&#39;re creating regex=&quot;(elu)([[:digit:]]+)&quot;, # regexp (see below) convert=TRUE) # automatically convert column types 8.8.4.1 A fancier regex for the cdc experiments Now let’s process the cdc experiments (cdc15 and cdc28). As before we extract the corresponding rows of the data frame using filter and str_detect. We then split expt.and.time using tidyr::extract. In this case we carry out the two steps in a single code block using pipes: cdc.long &lt;- spellman.long |&gt; # both cdc15 and cdc28 contain &quot;cdc&quot; as a sub-string filter(str_detect(expt.and.time, &quot;cdc&quot;)) |&gt; tidyr::extract(col = expt.and.time, into = c(&quot;expt&quot;, &quot;time&quot;), regex=&quot;(cdc15|cdc28)_([[:digit:]]+)&quot;, # note the fancier regex convert=TRUE) The regex – \"(cdc15|cdc28)_([[:digit:]]+)\" – is slightly fancier in this example. As before there are two parts we’re extracting: (cdc15|cdc28) and ([[:digit:]]+). The first parenthesized regexp is an “OR” – i.e. match “cdc15” or “cdc28”. The second parenthesized regexp is the same as we saw previously. Separating the two parenthesized regexps is an underscore (_). The underscore isn’t parenthesized because we only want to use it to make a match not to extract the corresponding match. 8.8.5 Combining data frames If you have two or more data frames with identical columns, the rows of the data frames can be combined into a single data frame using dplyr::bind_rows . For example, to reassemble the alpha.long, elu.long, and cdc.long data frames into a single data frame we do: spellman.final &lt;- bind_rows(alpha.long, elu.long, cdc.long) # check the dimensions of the new data frame dim(spellman.final) ## [1] 450994 6 8.8.6 Sorting data frame rows Currently the spellman.final data frame is sorted by time point and experiment. head(spellman.final, n = 10) ## # A tibble: 10 × 6 ## gene std.name description expt time expre…¹ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 0 -0.15 ## 2 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 7 -0.15 ## 3 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 14 -0.21 ## 4 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 21 0.17 ## 5 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 28 -0.42 ## 6 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 35 -0.44 ## 7 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 42 -0.15 ## 8 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 49 0.24 ## 9 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 56 -0.1 ## 10 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 63 NA ## # … with abbreviated variable name ¹​expression It might be useful instead to sort by gene and experiment. To do this we can use dplyr::arrange: spellman.final &lt;- spellman.final |&gt; arrange(gene, expt) # look again at the rearranged data head(spellman.final, n = 10) ## # A tibble: 10 × 6 ## gene std.name description expt time expre…¹ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 0 -0.15 ## 2 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 7 -0.15 ## 3 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 14 -0.21 ## 4 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 21 0.17 ## 5 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 28 -0.42 ## 6 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 35 -0.44 ## 7 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 42 -0.15 ## 8 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 49 0.24 ## 9 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 56 -0.1 ## 10 YAL001C TFC3 Subunit of RNA polymerase III transcrip… alpha 63 NA ## # … with abbreviated variable name ¹​expression 8.9 Using your tidy data Whew – that was a fair amount of work to tidy our data! But having done so we can now carry out a wide variety of very powerful analyses. 8.9.1 Visualizing gene expression time series Let’s start by walking through a series of visualizations of gene expression time series. Each plot will show the expression of one or more genes, at different time points, in one or more experimental conditions. Our initial visualizations exploit the “long” versions of the tidy data. First a single gene in a single experimental condition: spellman.final |&gt; filter(expt == &quot;alpha&quot;, gene == &quot;YAL022C&quot;) |&gt; ggplot(aes(x = time, y = expression)) + geom_line() + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression of YAL022C&quot;) We can easily modify the above code block to visualize the expression of multiple genes of interest: genes.of.interest &lt;- c(&quot;YAL022C&quot;, &quot;YAR018C&quot;, &quot;YGR188C&quot;) spellman.final |&gt; filter(gene %in% genes.of.interest, expt == &quot;alpha&quot;) |&gt; ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of multiple genes\\nfollowing synchronization by alpha factor&quot;) By employing facet_wrap() we can visualize the relationship between this set of genes in each of the experiment types: spellman.final |&gt; filter(gene %in% genes.of.interest) |&gt; ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + facet_wrap(~ expt) + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of Multiple Genes\\nAcross experiments&quot;) The different experimental treatments were carried out for varying lengths of time due to the differences in their physiological effects. Plotting them all on the same time scale can obscure that patterns of oscillation we might be interested in, so let’s modify our code block so that plots that share the same y-axis, but have differently scaled x-axes. spellman.final |&gt; filter(gene %in% genes.of.interest) |&gt; ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + facet_wrap(~ expt, scales = &quot;free_x&quot;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of Multiple Genes\\nAcross experiments&quot;) 8.9.2 Finding the most variable genes When dealing with vary large data sets, one ad hoc filtering criteria that is often employed is to focus on those variables that exhibit that greatest variation. To do this, we first need to order our variables (genes) by their variance. Let’s see how we can accomplish this using our long data frame: by.variance &lt;- spellman.final |&gt; group_by(gene) |&gt; summarize(expression.var = var(expression, na.rm = TRUE)) |&gt; arrange(desc(expression.var)) head(by.variance) ## # A tibble: 6 × 2 ## gene expression.var ## &lt;chr&gt; &lt;dbl&gt; ## 1 YLR286C 2.16 ## 2 YNR067C 1.73 ## 3 YNL327W 1.65 ## 4 YGL028C 1.57 ## 5 YHL028W 1.52 ## 6 YKL164C 1.52 The code above calculates the variance of each gene but ignores the fact that we have different experimental conditions. To take into account the experimental design of the data at hand, let’s calculate the average variance across the experimental conditions: by.avg.variance &lt;- spellman.final |&gt; group_by(gene, expt) |&gt; summarize(expression.var = var(expression, na.rm = TRUE)) |&gt; group_by(gene) |&gt; summarize(avg.expression.var = mean(expression.var)) ## `summarise()` has grouped output by ## &#39;gene&#39;. You can override using the ## `.groups` argument. head(by.avg.variance) ## # A tibble: 6 × 2 ## gene avg.expression.var ## &lt;chr&gt; &lt;dbl&gt; ## 1 YAL001C 0.102 ## 2 YAL002W 0.104 ## 3 YAL003W 0.321 ## 4 YAL004W 0.540 ## 5 YAL005C 0.678 ## 6 YAL007C 0.109 Based on the average experession variance across experimental conditions, let’s get the names of the 1000 most variable genes: top.genes.1k &lt;- by.avg.variance |&gt; slice_max(avg.expression.var, n = 1000) |&gt; pull(gene) head(top.genes.1k) ## [1] &quot;YFR014C&quot; &quot;YFR053C&quot; &quot;YBL032W&quot; &quot;YDR274C&quot; &quot;YLR286C&quot; &quot;YMR206W&quot; 8.9.3 Heat maps In our prior visualizations we’ve used line plots to depict how gene expression changes over time. For example here are line plots for 15 genes in the data set, in the cdc28 experimental conditions: genes.of.interest &lt;- c(&quot;YHR084W&quot;, &quot;YBR083W&quot;, &quot;YPL049C&quot;, &quot;YDR480W&quot;, &quot;YGR040W&quot;, &quot;YLR229C&quot;, &quot;YDL159W&quot;, &quot;YBL016W&quot;, &quot;YDR103W&quot;, &quot;YJL157C&quot;, &quot;YNL271C&quot;, &quot;YDR461W&quot;, &quot;YHL007C&quot;, &quot;YHR005C&quot;, &quot;YJR086W&quot;) spellman.final |&gt; filter(expt == &quot;cdc28&quot;, gene %in% genes.of.interest) |&gt; ggplot(aes(x = time, y = expression, color=gene)) + geom_line() + labs(x = &quot;Time (min)&quot;, y = &quot;Expression&quot;) Even with just 10 overlapping line plots, this figure is quite busy and it’s hard to make out the individual behavior of each gene. An alternative approach to depicting such data is a “heat map” which depicts the same information in a grid like form, with the expression values indicated by color. Heat maps are good for depicting large amounts of data and providing a coarse “10,000 foot view”. We can create a heat map using geom_tile as follows: spellman.final |&gt; filter(expt == &quot;cdc28&quot;, gene %in% genes.of.interest) |&gt; ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + xlab(&quot;Time (mins)&quot;) This figure represents the same information as our line plot, but now there is row for each gene, and the expression of that gene at a given time point is represented by color (scale given on the right). Missing data is shown as gray boxes. Unfortunately, the default color scale used by ggplot is a very subtle gradient from light to dark blue. This make it hard to distinguish patterns of change. Let’s now see how we can improve that. 8.9.3.1 Better color schemes with RColorBrewer The RColorBrewer packages provides nice color schemes that are useful for creating heat maps. RColorBrewer defines a set of color palettes that have been optimized for color discrimination, many of which are color blind friendly, etc. Install the RColorBrewer package using the command line or the RStudio GUI. Once you’ve installed the RColorBrewer package you can see the available color palettes as so: library(RColorBrewer) # show representations of the palettes par(cex = 0.5) # reduce size of text in the following plot display.brewer.all() We’ll use the Red-to-Blue (“RdBu”) color scheme defined in RColorBrewer, however we’ll reverse the scheme so blues represent low expression and reds represent high expression. We’ll divide the range of color values into 9 discrete bins. # displays the RdBu color scheme divided into a palette of 9 colors display.brewer.pal(9, &quot;RdBu&quot;) # assign the reversed (blue to red) RdBu palette color.scheme &lt;- rev(brewer.pal(9,&quot;RdBu&quot;)) Now let’s regenerate the heat map we created previously with this new color scheme. To do this we specify a gradient color scale using the scale_fill_gradientn() function from ggplot. In addition to specifying the color scale, we also constrain the limits of the scale to insure it’s symmetric about zero. spellman.final |&gt; filter(expt == &quot;cdc28&quot;, gene %in% genes.of.interest) |&gt; ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme, limits = c(-2.5, 2.5)) + xlab(&quot;Time (mins)&quot;) 8.9.3.2 Looking for patterns using sorted data and heat maps The real power of heat maps becomes apparent when you you rearrange the rows of the heat map to emphasize patterns of interest. For example, let’s create a heat map in which we sort genes by the time of their maximal expression. This is one way to identify genes that reach their peak expression at similar times, which is one criteria one might use to identify genes acting in concert. For simplicities sake we will restrict our attention to the cdc28 experiment, and only consider the 1000 most variables genes with no more than one missing observation in this experimental condition. cdc28 &lt;- spellman.final |&gt; filter(expt == &quot;cdc28&quot;) |&gt; group_by(gene) |&gt; filter(sum(is.na(expression)) &lt;= 1) |&gt; ungroup() # removes grouping information from data frame top1k.genes &lt;- cdc28 |&gt; group_by(gene) |&gt; summarize(expression.var = var(expression, na.rm = TRUE)) |&gt; slice_max(expression.var, n = 1000) |&gt; pull(gene) top1k.cdc28 &lt;- cdc28 |&gt; filter(gene %in% top1k.genes) To find the time of maximum expression we’ll employ the function which.max (which.min), which finds the index of the maximum (minimum) element of a vector. For example to find the index of the maximum expression measurement for YAR018C we could do: top1k.cdc28 |&gt; filter(gene == &quot;YAR018C&quot;) |&gt; pull(expression) |&gt; which.max() ## [1] 8 From the code above we find that the index of the observation at which YAR018C is maximal at 8. To get the corresponding time point we can do something like this: max.index.YAR018C &lt;- top1k.cdc28 |&gt; filter(gene == &quot;YAR018C&quot;) |&gt; pull(expression) |&gt; which.max() top1k.cdc28$time[max.index.YAR018C] ## [1] 70 Thus YAR018C expression peaks at 70 minutes in the cdc28 experiment. To find the index of maximal expression of all genes we can apply the dplyr::group_by() and dplyr::summarize() functions peak.expression.cdc28 &lt;- top1k.cdc28 |&gt; group_by(gene) |&gt; summarise(peak = which.max(expression)) head(peak.expression.cdc28) ## # A tibble: 6 × 2 ## gene peak ## &lt;chr&gt; &lt;int&gt; ## 1 YAL003W 10 ## 2 YAL005C 2 ## 3 YAL022C 17 ## 4 YAL028W 5 ## 5 YAL035C-A 12 ## 6 YAL038W 15 Let’s sort the order of genes by their peak expression: peak.expression.cdc28 &lt;- arrange(peak.expression.cdc28, peak) We can then generate a heatmap where we sort the rows (genes) of the heatmap by their time of peak expression. We introduce a new geom – geom_raster – which is like geom_tile but better suited for large data (hundreds to thousands of rows) The explicit sorting of the data by peak expression is carried out in the call to scale_y_discrete() where the limits (and order) of this axis are set with the limits argument (see scale_y_discrete and discrete_scale in the ggplot2 docs). # we reverse the ordering because geom_raster (and geom_tile) # draw from the bottom row up, whereas we want to depict the # earliest peaking genes at the top of the figure gene.ordering &lt;- rev(peak.expression.cdc28$gene) top1k.cdc28 |&gt; ggplot(aes(x = time, y = gene)) + geom_raster(aes(fill = expression)) + # scale_fill_gradientn(limits=c(-2.5, 2.5), colors=color.scheme) + scale_y_discrete(limits=gene.ordering) + labs(x = &quot;Time (mins)&quot;, y = &quot;Genes&quot;, title = &quot;1000 most variable genes&quot;, subtitle = &quot;Sorted by time of peak expression&quot;) + # the following line suppresses tick and labels on y-axis theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) Figure 8.1: A Heatmap showing genes in the cdc28 experiment, sorted by peak expression The brightest red regions in each row of the heat map correspond to the times of peak expression, and the sorting of the rows helps to highlight those gene whose peak expression times are similar. 8.10 Long-to-wide conversion using pivot_wider() Our long data frame consists of four variables – gene, expt, time, and expression. This made it easy to create visualizations and summaries where time and expression were the primaries variables of interest, and gene and experiment type were categories we could condition on. To facilitate analyses that emphasize comparison between genes, we want to create a new data frame in which each gene is itself treated as a variable of interest along with time, and experiment type remains a categorical variable. In this new data frame rather than just four columns in our data frame, we’ll have several thousand columns – one for each gene. To accomplish this reshaping of data, we’ll use the function tidyr::pivot_wider(). tidyr::pivot_wider() is the inverse of tidyr::pivot_longer(). pivot_longer() took multiple columns and collapsed them together into a smaller number of new columns. By contrast, pivot_wider() creates new columns by spreading values from single columns into multiple columns. Here let’s use pivot_wider to spread the gene name and expression value columns to create a new data frame where the genes are the primary variables (columns) of the data. spellman.wide &lt;- spellman.final |&gt; select(-std.name, -description) |&gt; # drop unneeded columns pivot_wider(names_from = gene, values_from = expression) Now let’s examine the dimensions of this wide version of the data: dim(spellman.wide) ## [1] 73 6180 And here’s a visual view of the first few rows and columns of the wide data: spellman.wide[1:5, 1:8] ## # A tibble: 5 × 8 ## expt time YAL001C YAL002W YAL003W YAL004W YAL005C YAL007C ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alpha 0 -0.15 -0.11 -0.14 -0.02 -0.05 -0.6 ## 2 alpha 7 -0.15 0.1 -0.71 -0.48 -0.53 -0.45 ## 3 alpha 14 -0.21 0.01 0.1 -0.11 -0.47 -0.13 ## 4 alpha 21 0.17 0.06 -0.32 0.12 -0.06 0.35 ## 5 alpha 28 -0.42 0.04 -0.4 -0.03 0.11 -0.01 From this view we infer that the rows of the data set represent the various combination of experimental condition and time points, and the columns represents the 6178 genes in the data set plus the two columns for expt and time. 8.11 Exploring bivariate relationships using “wide” data The “long” version of our data frame proved useful for exploring how gene expression changed over time. By contrast, our “wide” data frame is more convenient for exploring how pairs of genes covary together. For example, we can generate bivariate scatter plots depicting the relationship between two genes of interest: two.gene.plot &lt;- spellman.wide |&gt; filter(!is.na(YAR018C) &amp; !is.na(YAL022C)) |&gt; # remove NAs ggplot(aes(x = YAR018C, y = YAL022C)) + geom_point() + theme(aspect.ratio = 1) two.gene.plot From the scatter plot we infer that the two genes are “positively correlated” with each other, meaning that high values of one tend to be associated with high values of the other (and the same for low values). We can easily extend this visualization to facet the plot based on the experimental conditions: two.gene.plot + facet_wrap(~expt, nrow = 2, ncol = 2) Correlation is a standard measure the degree of association between pairs of continuous variables. Briefly, correlation is a measure of linear association between a pair of variables, and ranges from -1 to 1. A value near zero indicates the variables are uncorrelated (no linear association), while values approaching +1 indicate a strong positive association (the variables tend to get bigger or smaller together) while values near -1 indicate strong negative association (when one variable is larger, the other tends to be small). Let’s calculate the correlation between YAR018C and YAL022C: spellman.wide |&gt; filter(!is.na(YAR018C) &amp; !is.na(YAL022C)) |&gt; summarize(cor = cor(YAR018C, YAL022C)) ## # A tibble: 1 × 1 ## cor ## &lt;dbl&gt; ## 1 0.692 The value of the correlation coefficient for YAR018C and YAL022C, ~0.69, indicates a fairly strong association between the two genes. As we did for our visualization, we can also calculate the correlation coefficients for the two genes under each experimental condition: spellman.wide %&gt;% filter(!is.na(YAR018C) &amp; !is.na(YAL022C)) |&gt; group_by(expt) |&gt; summarize(cor = cor(YAR018C, YAL022C)) ## # A tibble: 4 × 2 ## expt cor ## &lt;chr&gt; &lt;dbl&gt; ## 1 alpha 0.634 ## 2 cdc15 0.575 ## 3 cdc28 0.847 ## 4 elu 0.787 This table suggests that the the strength of correlation between YAR018C and YAL022C may depend on the experimental conditions, with the highest correlations evident in the cdc28 and elu experiments. 8.11.1 Large scale patterns of correlations Now we’ll move from considering the correlation between two specific genes to looking at the correlation between many pairs of genes. As we did in the previous section, we’ll focus specifically on the 1000 most variable genes in the cdc28 experiment. First we filter our wide data set to only consider the cdc28 experiment and those genes in the top 1000 most variable genes in cdc28: top1k.cdc28.wide &lt;- top1k.cdc28 |&gt; select(-std.name, -description) |&gt; pivot_wider(names_from = gene, values_from = expression) With this restricted data set, we can then calculate the correlations between every pair of genes as follows: cdc28.correlations &lt;- top1k.cdc28.wide |&gt; select(-expt, -time) |&gt; # drop expt and time cor(use = &quot;pairwise.complete.obs&quot;) The argument use = \"pairwise.complete.obs\" tells the correlation function that for each pair of genes to use only the pariwse where there is a value for both genes (i.e. neither one can be NA). Given \\(n\\) genes, there are \\(n \\times n\\) pairs of correlations, as seen by the dimensions of the correlation matrix. dim(cdc28.correlations) ## [1] 1000 1000 To get the correlations with a gene of interest, we can index with the gene name on the rows of the correlation matrix. For example, to get the correlations between the gene YAR018C and the first 10 genes in the top 1000 set: cdc28.correlations[&quot;YAR018C&quot;,1:10] ## YAL003W YAL005C YAL022C YAL028W YAL035C-A YAL038W ## 0.07100626 -0.53315493 0.84741624 0.33379901 -0.22316755 -0.03984599 ## YAL044C YAL048C YAL060W YAL062W ## 0.32253692 0.12220221 0.49445700 -0.60972118 In the next statement we extract the names of the genes that have correlations with YAR018C greater than 0.6. First we test genes to see if they have a correlation with YAR018C greater than 0.6, which returns a vector of TRUE or FALSE values. This vector of Boolean values is than used to index into the row names of the correlation matrix, pulling out the gene names where the statement was true. pos.corr.YAR018C &lt;- rownames(cdc28.correlations)[cdc28.correlations[&quot;YAR018C&quot;,] &gt; 0.6] length(pos.corr.YAR018C) ## [1] 65 We then return to our long data to show this set of genes that are strongly positively correlated with YAR018C. top1k.cdc28 |&gt; filter(gene %in% pos.corr.YAR018C) |&gt; ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33) + theme(legend.position = &quot;none&quot;) As is expected, genes with strong positive correlations with YAR018C show similar temporal patterns with YAR018C. We can similarly filter for genes that have negative correlations with YAR018C. neg.corr.YAR018C &lt;- colnames(cdc28.correlations)[cdc28.correlations[&quot;YAR018C&quot;,] &lt;= -0.6] As before we generate a line plot showing these genes: top1k.cdc28 |&gt; filter(gene %in% neg.corr.YAR018C) |&gt; ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33) + theme(legend.position = &quot;none&quot;) 8.11.2 Adding new columns and combining filtered data frames Now let’s create a new data frame by: 1) filtering on our list of genes that have strong positive and negative correlations with YAR018C; and 2) creating a new variable, “corr.with.YAR018C”, which indicates the sign of the correlation. We’ll use this new variable to group genes when we create the plot. pos.corr.df &lt;- top1k.cdc28 |&gt; filter(gene %in% pos.corr.YAR018C) |&gt; mutate(corr.with.YAR018C = &quot;positive&quot;) neg.corr.df &lt;- top1k.cdc28 |&gt; filter(gene %in% neg.corr.YAR018C) |&gt; mutate(corr.with.YAR018C = &quot;negative&quot;) combined.pos.neg &lt;- rbind(pos.corr.df, neg.corr.df) Finally, we plot the data, colored according to the correlation with YAR018C: ggplot(combined.pos.neg, aes(x = time, y = expression, group = gene, color = corr.with.YAR018C)) + geom_line(alpha=0.25) + geom_line(aes(x = time, y = expression), data = filter(top1k.cdc28, gene == &quot;YAR018C&quot;), color = &quot;DarkRed&quot;, size = 2,alpha=0.5) + # changes legend title and values for color sclae scale_color_manual(name = &quot;Correlation with YAR018C&quot;, values = c(&quot;blue&quot;, &quot;red&quot;)) + labs(title = &quot;Genes strongly positively and negatively correlated with YAR018C&quot;, subtitle = &quot;YAR018C shown in dark red&quot;, x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;) 8.11.3 A heat mapped sorted by correlations In our previous heat map example figure, we sorted genes according to peak expression. Now let’s generate a heat map for the genes that are strongly correlated (both positive and negative) with YAR018C. We will sort the genes according to the sign of their correlation. # re-factor gene names so positive and negative genes are spatially distinct in plot combined.pos.neg$gene &lt;- factor(combined.pos.neg$gene, levels = c(pos.corr.YAR018C, neg.corr.YAR018C)) combined.pos.neg |&gt; ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme) + xlab(&quot;Time (mins)&quot;) The breakpoint between the positively and negatively correlated sets of genes is quite obvious in this figure. 8.11.4 A “fancy” figure Recall that we introduced the cowplot library in Chapter 6, as a way to combine different ggplot outputs into subfigures such as you might find in a published paper. Here we’ll make further use cowplot to combine our heat map and line plot visualizations of genes that covary with YAR018C. library(cowplot) cowplot’s draw_plot() function allows us to place plots at arbitrary locations and with arbitrary sizes onto the canvas. The coordinates of the canvas run from 0 to 1, and the point (0, 0) is in the lower left corner of the canvas. We’ll use draw_plot to draw a complex figure with a heatmap on the left, and two smaller line plots on the right. pos.corr.lineplot &lt;- combined.pos.neg |&gt; filter(gene %in% pos.corr.YAR018C) |&gt; ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33, color = &#39;red&#39;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;, title = &quot;Genes Positively correlated\\nwith YAR018C&quot;) neg.corr.lineplot &lt;- combined.pos.neg |&gt; filter(gene %in% neg.corr.YAR018C) |&gt; ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33, color = &#39;blue&#39;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;, title = &quot;Genes negatively correlated\\nwith YAR018C&quot;) heat.map &lt;- ggplot(combined.pos.neg, aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme) + labs(x = &quot;Time (mins)&quot;, y = &quot;Gene&quot;) + theme(legend.position = &quot;none&quot;) The coordinates of the canvas run from 0 to 1, and the point (0, 0) is in the lower left corner of the canvas. We’ll use draw_plot to draw a complex figure with a heatmap on the left, and two smaller line plots on the right. I determined the coordinates below by experimentation to create a visually pleasing layout. fancy.plot &lt;- ggdraw() + draw_plot(heat.map, 0, 0, width = 0.6) + draw_plot(neg.corr.lineplot, 0.6, 0.5, width = 0.4, height = 0.5) + draw_plot(pos.corr.lineplot, 0.6, 0, width = 0.4, height = 0.5) + draw_plot_label(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c(0, 0.6, 0.6), c(1, 1, 0.5), size = 15) fancy.plot "],["vector-algebra.html", "Chapter 9 Vector algebra 9.1 Libraries 9.2 Vector Mathematics in R 9.3 Simple statistics in vector form", " Chapter 9 Vector algebra 9.1 Libraries library(tidyverse) library(magrittr) 9.2 Vector Mathematics in R R vectors support basic arithmetic operations that correspond to the same operations on geometric vectors. For example: &gt; x &lt;- 1:15 &gt; y &lt;- 10:24 &gt; x ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 &gt; y ## [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 &gt; x + y # vector addition ## [1] 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 &gt; x - y # vector subtraction ## [1] -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 &gt; x * 3 # multiplication by a scalar ## [1] 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 R also has an operator for the dot product, denoted %*%. This operator also designates matrix multiplication, which we will discuss in the next chapter. By default this operator returns an object of the R matrix class. If you want a scalar (or the R equivalent of a scalar, i.e. a vector of length 1) you need to use the drop() function. &gt; z &lt;- x %*% x &gt; class(z) # note use of class() function ## [1] &quot;matrix&quot; &quot;array&quot; &gt; z ## [,1] ## [1,] 1240 &gt; drop(z) ## [1] 1240 In lecture we saw that many useful geometric properties of vectors could be expressed in the form of dot products. Let’s start with some two-dimensional vectors where the geometry is easy to visualize: &gt; a &lt;- c(2, 0) # the point (2,0) &gt; b &lt;- c(1, 3) # the point (1,3) To draw our vectors using ggplot, we’ll need to create a data frame with columns representing the x,y coordinates of the end-points of our vectors: df &lt;- data.frame(x.end = c(a[1], b[1]), y.end = c(a[2], b[2]), label = c(&#39;a&#39;, &#39;b&#39;)) ggplot(df) + geom_segment(aes(x=0, y = 0, xend = x.end, yend = y.end, color=label), arrow = arrow()) + labs(x = &quot;x-coordinate&quot;, y = &quot;y-coordinate&quot;) + coord_fixed(ratio = 1) + # insures x and y axis scale are same theme_bw() Let’s see what the dot product can tell us about these vectors. First recall that we can calculate the length of a vector as the square-root of the dot product of the vector with itself (\\(\\vert\\vec{a}\\vert^2 = \\vec{a} \\cdot \\vec{a}\\)) &gt; len.a &lt;- drop(sqrt(a %*% a)) &gt; len.a ## [1] 2 &gt; len.b &lt;- drop(sqrt(b %*% b)) &gt; len.b ## [1] 3.162278 How about the angle between \\(a\\) and \\(b\\)? First we can use the dot product and the previously calculated lengths to calculate the cosine of the angle between the vectors: &gt; cos.ab &lt;- (a %*% b)/(len.a * len.b) &gt; cos.ab ## [,1] ## [1,] 0.3162278 To go from the cosine of the angle to the angle (in radians) we need the arc-cosine function, acos(): &gt; acos(cos.ab) # given angle in radians ## [,1] ## [1,] 1.249046 9.3 Simple statistics in vector form Now let’s turn our attention to seeing how to calculate a variety of simple statistics such as the mean, variance, etc. in terms of vector operations. To illustrate these oeprations we’ll use the I. setosa data from the iris examplar data set. setosa &lt;- filter(iris, Species == &quot;setosa&quot;) 9.3.1 Mean First let’s calculate the mean for the Sepal.Length variable. Referring back to the slides for today’s lecture, we see we can calculate the mean as: \\[ \\bar{x} = \\frac{\\vec{1} \\cdot \\vec{x}}{\\vec{1} \\cdot \\vec{1}} \\] Applying this formula in R: &gt; sepal.length &lt;- setosa$Sepal.Length &gt; ones &lt;- rep(1, length(sepal.length)) # 1-vector of length n &gt; mean.sepal.length &lt;- (ones %*% sepal.length)/(ones %*% ones) &gt; mean.sepal.length %&lt;&gt;% drop # use drop to convert back to scalar &gt; mean.sepal.length ## [1] 5.006 Let’s compare our calculation against the built-in mean function: &gt; mean(sepal.length) ## [1] 5.006 9.3.2 Mean centering Mean centering a vector, means subtracting the mean from each element of that vector: \\[ \\vec{x}_c = \\vec{x} - \\bar{x}\\vec{1} \\] Now let’s create a mean centered vector from sepal.length, which we’ll refer to as the vector of deviates about the mean: &gt; sepal.length.deviates &lt;- sepal.length - mean.sepal.length Note that we didn’t have to explicitly multiply the a one vector by the mean, as R will automatically make the lengths of the sepal.length (a vector of length 150) and mean.sepal.length (a vector of length 1) match by vector recycling. 9.3.3 Variance and standard deviation Using the vector of deviates we can easily calculate the variance and standard deviation of a variable. The variance of a variable, in vector algebraic terms, is: \\[ S_x^2 = \\frac{\\vec{x}_c \\cdot \\vec{x}_c}{n-1} \\] The standard deviation is simply the square root of the variance \\[ S_x = \\sqrt{S_x^2} \\] These calculations for the Sepal.Length variable: &gt; n &lt;- length(sepal.length.deviates) &gt; var.sepal.length &lt;- (sepal.length.deviates %*% sepal.length.deviates)/(n-1) &gt; var.sepal.length ## [,1] ## [1,] 0.124249 &gt; sd.sepal.length &lt;- sqrt(var.sepal.length) &gt; sd.sepal.length ## [,1] ## [1,] 0.3524897 Again, we can compare our calculations to the built-in var() and sd() functions: &gt; var(sepal.length) ## [1] 0.124249 &gt; sd(sepal.length) ## [1] 0.3524897 9.3.4 Covariance and correlation Now let’s consider the common measures of bivariate association, covariance and correlation. Covariance is: \\[ S_{XY} = \\frac{\\vec{x} \\cdot \\vec{y}}{n-1} \\] Correlation is: \\[ r_{XY} = \\frac{\\vec{x} \\cdot \\vec{y}}{|\\vec{x}||\\vec{y}|} = \\frac{S_{XY}}{S_x S_Y} \\] We’ll examine the relationship between sepal length and width: sepal.width &lt;- setosa$Sepal.Width mean.sepal.width &lt;- (ones %*% sepal.width)/(ones %*% ones) sepal.width.deviates &lt;- sepal.width - mean.sepal.width var.sepal.width &lt;- drop((sepal.width.deviates %*% sepal.width.deviates)/(n-1)) sd.sepal.width &lt;- sqrt(var.sepal.width) With the vector of sepal width deviates in hand we can now calculate covariances: &gt; cov.swidth.slength &lt;- (sepal.length.deviates %*% sepal.width.deviates)/(n-1) &gt; cov.swidth.slength ## [,1] ## [1,] 0.09921633 &gt; cov(sepal.length, sepal.width) # and compare to built-in covariance ## [1] 0.09921633 And correlations: &gt; len.sepal.length &lt;- sqrt(sepal.length.deviates %*% sepal.length.deviates) &gt; len.sepal.width &lt;- sqrt(sepal.width.deviates %*% sepal.width.deviates) &gt; &gt; corr.swidth.slength &lt;- + (sepal.length.deviates %*% sepal.width.deviates) / (len.sepal.length * len.sepal.width) &gt; corr.swidth.slength ## [,1] ## [1,] 0.7425467 &gt; cor(sepal.length, sepal.width) # and compare to built-in correlation ## [1] 0.7425467 Alternately, we could have calculated the correlation more simply as follows: &gt; cov.swidth.slength/(sd.sepal.length * sd.sepal.width) ## [,1] ## [1,] 0.7425467 "],["matrices-in-r.html", "Chapter 10 Matrices in R 10.1 Creating matrices in R 10.2 Matrix arithmetic operations in R 10.3 Descriptive statistics as matrix functions 10.4 Matrix Inverse 10.5 Solving sets of simultaneous equations", " Chapter 10 Matrices in R In R matrices are two-dimensional collections of elements all of which have the same mode or type. This is different than a data frame in which the columns of the frame can hold elements of different type (but all of the same length), or from a list which can hold objects of arbitrary type and length. Matrices are more efficient for carrying out most numerical operations, so if you’re working with a very large data set that is amenable to representation by a matrix you should consider using this data structure. library(tidyverse) 10.1 Creating matrices in R There are a number of different ways to create matrices in R. For creating small matrices at the command line you can use the matrix() function. &gt; x &lt;- matrix(1:5) # creates a column vector &gt; x ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 ## [5,] 5 &gt; X &lt;- matrix(1:12, nrow=4) # creates a matrix &gt; X ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 &gt; dim(X) # give the shape of the matrix ## [1] 4 3 matrix() takes a data vector as input and the shape of the matrix to be created is specified by using the nrow and ncol arguments. If the number of elements in the input data vector is less than nrows \\(\\times\\) ncols the elements will be ‘recycled’ as discussed in previous chapters. Without any shape arguments the matrix() function will create a column vector as shown above. By default the matrix() function fills in the matrix in a column-wise fashion. To fill in the matrix in a row-wise fashion use the argument byrow=T. If you have a pre-existing data set in a list or data frame you can use the as.matrix() function to convert it to a matrix. &gt; iris.mtx &lt;- as.matrix(iris) &gt; head(iris.mtx) # NOTE: the elements were all converted to character ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## [1,] &quot;5.1&quot; &quot;3.5&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; ## [2,] &quot;4.9&quot; &quot;3.0&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; ## [3,] &quot;4.7&quot; &quot;3.2&quot; &quot;1.3&quot; &quot;0.2&quot; &quot;setosa&quot; ## [4,] &quot;4.6&quot; &quot;3.1&quot; &quot;1.5&quot; &quot;0.2&quot; &quot;setosa&quot; ## [5,] &quot;5.0&quot; &quot;3.6&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; ## [6,] &quot;5.4&quot; &quot;3.9&quot; &quot;1.7&quot; &quot;0.4&quot; &quot;setosa&quot; Since all elements of an R matrix must be of the same type, when we passed the iris data frame to as.matrix(), everything was converted to a character due to the presence of the Species column in the data frame. &gt; # This is probably more along the lines of what you want &gt; iris.mtx &lt;- iris %&gt;% dplyr::select(-Species) %&gt;% as.matrix &gt; head(iris.mtx) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## [1,] 5.1 3.5 1.4 0.2 ## [2,] 4.9 3.0 1.4 0.2 ## [3,] 4.7 3.2 1.3 0.2 ## [4,] 4.6 3.1 1.5 0.2 ## [5,] 5.0 3.6 1.4 0.2 ## [6,] 5.4 3.9 1.7 0.4 You can use the various indexing operations to get particular rows, columns, or elements. Here are some examples: &gt; X &lt;- matrix(1:12, nrow=4) &gt; X ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 &gt; X[1,] # get the first row ## [1] 1 5 9 &gt; X[,1] # get the first column ## [1] 1 2 3 4 &gt; X[1:2,] # get the first two rows ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 &gt; X[,2:3] # get the second and third columns ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10 ## [3,] 7 11 ## [4,] 8 12 &gt; Y &lt;- matrix(1:12, byrow=T, nrow=4) &gt; Y ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 &gt; Y[4] # see explanation below ## [1] 10 &gt; Y[5] ## [1] 2 &gt; dim(Y) &lt;- c(2,6) # reshape Y &gt; Y ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 7 2 8 3 9 ## [2,] 4 10 5 11 6 12 &gt; Y[5] ## [1] 2 The example above where we create a matrix Y is meant to show that matrices are stored internally in a column wise fashion (think of the columns stacked one atop the other), regardless of whether we use the byrow=T argument. Therefore using single indices returns the elements with respect to this arrangement. Note also the use of assignment operator in conjuction with the dim() function to reshape the matrix. Despite the reshaping, the internal representation in memory hasn’t changed so Y[5] still gives the same element. You can use the diag() function to get the diagonal of a matrix or to create a diagonal matrix as show below: &gt; Z &lt;- matrix(rnorm(16), ncol=4) &gt; Z ## [,1] [,2] [,3] [,4] ## [1,] 0.2523397 0.729350262 0.4769644 -2.0207988 ## [2,] 0.1238106 0.002260243 0.6107511 -0.2396683 ## [3,] -0.6292392 -0.356499431 -2.0468266 0.4721059 ## [4,] 3.4700328 2.466565189 -1.3042726 0.7572256 &gt; diag(Z) ## [1] 0.252339744 0.002260243 -2.046826558 0.757225599 &gt; diag(5) # create the 5 x 5 identity matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 &gt; s &lt;- sqrt(10:13) &gt; diag(s) ## [,1] [,2] [,3] [,4] ## [1,] 3.162278 0.000000 0.000000 0.000000 ## [2,] 0.000000 3.316625 0.000000 0.000000 ## [3,] 0.000000 0.000000 3.464102 0.000000 ## [4,] 0.000000 0.000000 0.000000 3.605551 10.2 Matrix arithmetic operations in R The standard mathematical operations of addition and subtraction and scalar multiplication work element-wise for matrices in the same way as they did for vectors. Matrix multiplication uses the operator %*% which you saw last week for the dot product. To get the transpose of a matrix use the function t(). &gt; A &lt;- matrix(1:12, nrow=4) &gt; A ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 &gt; &gt; B &lt;- matrix(rnorm(12), nrow=4) &gt; B ## [,1] [,2] [,3] ## [1,] 0.9290181 1.06809094 -0.3345056 ## [2,] -1.5126220 -0.41861130 -0.2329943 ## [3,] -1.1397590 0.03192664 -0.9134459 ## [4,] -1.1113718 -0.69490935 -1.5228054 &gt; &gt; t(A) # transpose ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 &gt; A + B # matrix addition ## [,1] [,2] [,3] ## [1,] 1.929018 6.068091 8.665494 ## [2,] 0.487378 5.581389 9.767006 ## [3,] 1.860241 7.031927 10.086554 ## [4,] 2.888628 7.305091 10.477195 &gt; A - B # matrix subtraction ## [,1] [,2] [,3] ## [1,] 0.07098188 3.931909 9.334506 ## [2,] 3.51262199 6.418611 10.232994 ## [3,] 4.13975904 6.968073 11.913446 ## [4,] 5.11137177 8.694909 13.522805 &gt; 5 * A # multiplication by a scalar ## [,1] [,2] [,3] ## [1,] 5 25 45 ## [2,] 10 30 50 ## [3,] 15 35 55 ## [4,] 20 40 60 When applying matrix multiplication, the dimensions of the matrices involved must be conformable. For example, you can’t do this: A %*% B # do you understand why this generates an error? But this works: &gt; A %*% t(B) ## [,1] [,2] [,3] [,4] ## [1,] 3.258923 -5.702627 -9.201139 -18.29117 ## [2,] 4.921526 -7.866855 -11.222417 -21.62025 ## [3,] 6.584130 -10.031083 -13.243695 -24.94934 ## [4,] 8.246733 -12.195310 -15.264974 -28.27843 10.3 Descriptive statistics as matrix functions Assume you have a data set represented as a \\(n \\times p\\) matrix, \\(X\\), with observations in rows and variables in columns. Below I give formulae for calculating some descriptive statistics as matrix functions. 10.3.1 Mean vector and matrix You can calculate a row vector of means, \\(\\mathbf{m}\\), as: \\[ \\mathbf{m} = \\frac{1}{n} \\mathbf{1}^T X \\] where \\(1\\) is a \\(n \\times 1\\) vector of ones. A \\(n \\times p\\) matrix \\(M\\) where each column is filled with the mean value for that column is: \\[ M = \\mathbf{1}\\mathbf{m} \\] 10.3.2 Deviation matrix To re-express each value as the deviation from the variable means (i.e.~each columns is a mean centered vector) we calculate a deviation matrix: \\[ D = X - M \\] 10.3.3 Covariance matrix The \\(p \\times p\\) covariance matrix can be expressed as a matrix product of the deviation matrix: \\[ S = \\frac{1}{n-1} D^T D \\] 10.3.4 Correlation matrix The correlation matrix, \\(R\\), can be calculated from the covariance matrix by: \\[ R = V S V \\] where \\(V\\) is a \\(p \\times p\\) diagonal matrix where \\(V_{ii} = 1/\\sqrt{S_{ii}}\\). 10.4 Matrix Inverse The function solve() can be used to find matrix inverses in R. A &lt;- matrix(1:4, nrow=2) A ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Ainv &lt;- solve(A) Ainv ## [,1] [,2] ## [1,] -2 1.5 ## [2,] 1 -0.5 A %*% Ainv # should give identity matrix ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 Ainv %*% A # should also result in identity matrix ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 Keep in mind that not all square matrices are invertible: C &lt;- matrix(1:16, nrow=4) C ## [,1] [,2] [,3] [,4] ## [1,] 1 5 9 13 ## [2,] 2 6 10 14 ## [3,] 3 7 11 15 ## [4,] 4 8 12 16 Cinv &lt;- solve(C) ## Error in solve.default(C): Lapack routine dgesv: system is exactly singular: U[4,4] = 0 10.5 Solving sets of simultaneous equations The solve() function introduced above can also be used to solve sets of simultaneous equations. For example, given the set of equations below: \\[ \\begin{eqnarray*} x_1 + 3x_2 + 2x_3 &amp; = &amp; 3\\\\ -x_1 + x_2 + 2x_3 &amp; = &amp; -2\\\\ 2x_1 + 4x_2 -x_3 &amp; = &amp; 10 \\end{eqnarray*} \\] We can rewrite this in vector form as: \\[x_1 \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\end{bmatrix} + x_2 \\begin{bmatrix} 3 \\\\ 1 \\\\ 4 \\end{bmatrix} + x_3 \\begin{bmatrix} 2 \\\\ 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 10 \\end{bmatrix} \\] which is equivalent to the matrix form: \\[ \\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\ -1 &amp; 1 &amp; 2\\\\ 2 &amp; 4 &amp; 1 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 10 \\end{bmatrix} \\] solve() takes two arguments: - a – a square numeric matrix containing the coefficients of the linear equations (the left-most matrix above) - b – a vector (or matrix) giving the right hand side of the linear equations First let’s create the matrix of coefficients on the left and right sides: A = matrix( c(c(1, 3 , 2), c(-1, 1, 2), c(2, 4, 1)), nrow=3, ncol=3, byrow=TRUE) b = c(3, -2, 10) Now we solve the equations: x &lt;- solve(A,b) x ## [1] -2.25 4.75 -4.50 Let’s confirm the solution works by multiplying A by x: A %*% x ## [,1] ## [1,] 3 ## [2,] -2 ## [3,] 10 Voila! "],["linear-regression-models-i.html", "Chapter 11 Linear Regression Models I 11.1 Linear functions 11.2 Linear regression 11.3 The optimality criterion for least-squares regression 11.4 Geometry of linear regression 11.5 Solution for the least-squares criterion 11.6 Residuals 11.7 Regression as sum-of-squares decomposition 11.8 Variance “explained” by a regression model 11.9 Goodness of fit 11.10 Interpretting Regression 11.11 Illustrating linear regression with simulated data 11.12 Calculating the regression model using vector arithmetic 11.13 Plotting various aspects of a regression 11.14 Calculating the coefficient of determination", " Chapter 11 Linear Regression Models I Statistical models are quantitative statements about how we think variables are related to each other. Linear models are among the simplest statistical models. In a linear model relating two variables \\(X\\) and \\(Y\\), the general form of the model can be stated as “I assume that \\(Y\\) can be expressed as a linear function of \\(X\\)”. The process of model fitting is then the task of finding the coefficients (parameters) of the linear model which best fit the observed data. As we talk about regression models we’ll frequently use the following terms: Predictors, explanatory, or independent variable – the variables from which we want to make our prediction. Outcomes, dependent, or response variable – the variable we are trying to predict in our models. 11.1 Linear functions A linear function of a single variable \\(X\\) can be written as: \\[ f(X) = a + bX \\] where \\(a\\) and \\(b\\) are constants. In geometric terms \\(b\\) is the slope of the line and \\(a\\) is the value of the function when \\(X\\) is zero (usually the referred to as the “Y-intercept”). The slope tells you have much \\(f(X)\\) changes per unit change of \\(X\\). If we treat \\(X\\) as a vector, this linear function can be expressed as the vector equation: \\[ f(\\vec{\\mathbf{x}}) = a\\vec{\\mathbf{1}} + b\\vec{\\mathbf{x}} = a\\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix} + b\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_p\\end{bmatrix} \\] We can also create linear functions of multiple variables, \\(X_1, X_2, \\ldots, X_m\\). A linear function of multiple variables can be expressed as: \\[ f(X_1, X_2,\\ldots,X_m) = a + b_1X_1 + b_2X_2 + \\cdots + b_mX_m \\] where \\(a\\) is the intercept and \\(b_1, b_2,\\ldots,b_m\\) are slopes with respect to each of the \\(X\\)s. In vector terms this is: \\[ \\begin{eqnarray*} f(\\vec{\\mathbf{x}_1}, \\vec{\\mathbf{x}_2}, \\ldots, \\vec{\\mathbf{x}_m}) &amp;=&amp; a\\vec{\\mathbf{1}} + b_1\\vec{\\mathbf{x}_1} + b_2\\vec{\\mathbf{x}_2} + \\ldots + b_m\\vec{\\mathbf{x}_m} \\\\ &amp;=&amp; a\\begin{bmatrix}1 \\\\ 1 \\\\ \\vdots \\\\ 1\\end{bmatrix} + b_1\\begin{bmatrix}x_{1,1} \\\\ x_{2,1} \\\\ \\vdots \\\\ x_{p,1}\\end{bmatrix} + b_2\\begin{bmatrix}x_{1,2} \\\\ x_{2,2} \\\\ \\vdots \\\\ x_{p,2}\\end{bmatrix} + \\cdots + b_m\\begin{bmatrix}x_{1,m} \\\\ x_{2,m} \\\\ \\vdots \\\\ x_{p,m}\\end{bmatrix} \\end{eqnarray*} \\] If we bundle the individual \\(\\vec{\\mathbf{x}}\\) vectors into a matrix \\(\\mathbf{X}\\) we can rewrite the linear function of multiple variables as \\[ f(\\vec{\\mathbf{x}_1}, \\vec{\\mathbf{x}_2}, \\ldots, \\vec{\\mathbf{x}_m}) = \\mathbf{X}\\vec{\\mathbf{b}} \\] where \\[ \\mathbf{X} = \\begin{bmatrix} 1 &amp; x_{1,1} &amp; x_{1,2} &amp; \\cdots &amp; x_{1,m} \\\\ 1 &amp; x_{2,1} &amp; x_{2,2} &amp; \\cdots &amp; x_{2,m} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n,1} &amp; x_{n,2} &amp; \\cdots &amp; x_{n,m} \\\\ \\end{bmatrix} \\text{ and } \\vec{\\mathbf{b}} = \\begin{bmatrix} a \\\\ b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m \\end{bmatrix} \\] 11.2 Linear regression In bivariate linear regression we assume that our outcome variable (\\(Y\\)) can be expressed as a linear function of a single predictor variable (\\(X\\)) plus an error term. The error term can be thought of as the portion of \\(Y\\) that is “unexplained” by the linear function. \\[ \\begin{eqnarray*} Y &amp;=&amp; f(X) + \\text{error} \\\\ &amp;=&amp; a + bX + \\text{error} \\end{eqnarray*} \\] The vector equivalent is: \\[ \\vec{\\mathbf{y}} = a\\vec{\\mathbf{1}} + b\\vec{\\mathbf{x}} + \\vec{\\mathbf{e}} \\] Similarly, in multiple linear regression we assume that the outcome variable (\\(Y\\)) can be expressed as a linear function of multiple predictor variables (\\(X_1, X_2, \\ldots, X_m\\)) plus an error term, \\(\\epsilon\\): \\[ Y = f(X_1, X_2,\\ldots,X_m) + \\epsilon \\] The vector representation of the multiple regression model is: \\[ \\vec{\\mathbf{y}} = a\\vec{\\mathbf{1}} + b_1\\vec{\\mathbf{x}_1} + b_2\\vec{\\mathbf{x}_2} + \\ldots + b_m\\vec{\\mathbf{x}_m} + \\vec{\\mathbf{e}} \\] In matrix form, this is represented as: \\[ \\vec{\\mathbf{y}} = \\mathbf{X}\\vec{\\mathbf{b}} + \\vec{\\mathbf{e}} \\] where \\(\\mathbf{X}\\vec{\\mathbf{b}}\\) is a matrix of the predictor variables with the ones vector prepended to it as described above. We see that bivariate linear regression is just a special case of multiple regression. In the rest of this document I will replace the linear functions \\(f\\) above with \\(\\widehat{Y}\\) to indicate the component of \\(Y\\) that is predicted (“explained”) by the \\(X\\)s: \\[ Y = \\widehat{Y}+ \\epsilon \\] The equivalent vector representation is: \\[ \\vec{\\mathbf{y}} = \\vec{\\widehat{\\mathbf{y}}} + \\vec{\\mathbf{e}} \\] 11.3 The optimality criterion for least-squares regression There are infinitely many linear functions of \\(X\\) we could define. Which linear function provides the best fit given our observed values of\\(Y\\) and \\(X\\)? In order to fit a model to data, we have to specify some criterion for judging how well alternate models perform. For regression, the optimality criterion can be expressed as “Find the linear function, \\(\\widehat{Y} = f(X)\\), that minimizes the following quantity:” \\[ \\sum \\epsilon^2 = \\sum (y_i - \\widehat{y}_i)^2 \\] In vector terms we are minimizing: \\[ |\\vec{\\mathbf{e}}|^2 = |\\vec{\\mathbf{y}} - \\vec{\\widehat{\\mathbf{y}}} |^2 \\] That is, our goal is to find the linear function of \\(X\\) that minimizes the sum of squared deviations between the predicted values of \\(y\\) and the observed values of \\(y\\). This is known as the least-squares criterion. 11.4 Geometry of linear regression The figure below represents the variable space (A) and subject space (B) representations of bivariate linear regression. Figure 11.1: Graphical representations of bivariate linear least squares regression. A) variable space representation; B) subject space (vector) representation Similarly, the next below represents the variable space (A) and subject space (B) representations of multiple regression of a single outcome variable onto two predictor variables. Figure 11.2: Graphical representations of multiple regression. A) variable space representation; B) subject space (vector) representation Notice that in both representations, the error term (the portion of \\(Y\\) unexplained by the regression model) is orthogonal to the subspace defined by the predictor variables. 11.5 Solution for the least-squares criterion The least-squares optimality criterion tells us to find the best fitting regression model, we need to solve for \\(\\widehat{\\vec{\\mathbf{y}}} = a\\vec{\\mathbf{1}} + b\\vec{\\mathbf{x}}\\) such that \\(|\\vec{\\mathbf{e}}|^2 = |\\vec{\\mathbf{y}} - \\vec{\\widehat{\\mathbf{y}}} |^2\\) is as small as possible. That is we need to find the values \\(a\\) and \\(b\\) that minimize the length of \\(\\vec{\\mathbf{e}}\\). How do we do that? I will discuss how to do solve this optimization problem for mean centered variables in class, using simultaneous linear equations. See Wickens chapters 3 and 4 for the general case. 11.5.1 Bivariate regression, estimating coefficients For the bivariate case, the values of \\(b\\) (slope) and \\(a\\) (intercept) that minimize the sum of squared deviations described above are: \\[\\begin{align} b &amp;= \\frac{s_{xy}}{s^2_x} = r_{xy}\\frac{s_y}{s_x}\\\\ \\\\ a &amp;= \\overline{Y} - b\\overline{X} \\end{align}\\] where \\(r_{xy}\\) is the correlation coefficient between \\(X\\) and \\(Y\\), and \\(s_x\\) and \\(s_y\\) are the standard deviations of \\(X\\) and \\(Y\\) respectively. In vector geometric terms, we calculate the regression coefficient for mean centered vectors as: \\[ b = \\frac{\\vec{x} \\cdot \\vec{y}}{\\vec{x} \\cdot \\vec{x}} \\] 11.5.2 Multiple regression, estimating coefficients For the multiple regression case: \\[ \\vec{\\mathbf{y}} = \\mathbf{X}\\vec{\\mathbf{b}} + \\vec{\\mathbf{e}} \\] we can estimate the vector of regression coefficients as: \\[ \\vec{\\mathbf{b}} = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\] Note the similarity between the matrix solution for the multiple regression case and the vector solution for the bivariate case. 11.6 Residuals Residuals are the difference between the observed values of \\(Y\\) and the predicted values, i.e. the “error” term in our model above. You can think of residuals as the proportion of \\(Y\\) unaccounted for by the model. \\[ \\text{residuals} = \\vec{\\mathbf{e}} = \\vec{\\mathbf{y}} - \\vec{\\widehat{\\mathbf{y}}} \\] When the linear regression model is appropriate to the data, residuals should be approximately normally distributed, centered around zero and should show no strong trends or extreme differences in spread (variance) for different values of \\(X\\). 11.7 Regression as sum-of-squares decomposition Regression can be viewed as a decomposition of the sum-of-squared (SS) deviations. \\[ SS_\\mbox{total} = SS_\\mbox{regression} + SS_\\mbox{residuals} \\] In vector geometric terms this decomposition can be seen as a simple consequence of the Pythagorean theorem: \\[ |\\vec{\\mathbf{y}}|^2 = |\\vec{\\widehat{\\mathbf{y}}}|^2 + |\\vec{\\mathbf{e}}|^2 \\\\ \\] where \\[ \\begin{eqnarray*} SS_\\mbox{total} = |\\vec{\\mathbf{y}}|^2 \\\\ SS_\\mbox{regression} = |\\vec{\\widehat{\\mathbf{y}}}|^2 \\\\ SS_\\mbox{residuals} = |\\vec{\\mathbf{e}}|^2 \\\\ \\end{eqnarray*} \\] 11.8 Variance “explained” by a regression model We can use the sum-of-square decomposition to understand the relative proportion of variance “explained” (accounted for) by the regression model. We call this quantity the “Coefficient of Determination”, designated \\(R^2\\). \\[ R^2 = \\left( 1 - \\frac{SS_{residuals}}{SS_{total}} \\right) \\] In vector geometric terms the coefficient of determination can be calculated as: \\[ R^2 = \\left(1 - \\frac{|\\vec{\\mathbf{e}}|^2}{|\\vec{\\mathbf{y}}|^2}\\right) = \\frac{|\\vec{\\widehat{\\mathbf{y}}}|^2}{|\\vec{\\mathbf{y}}|^2} \\] where the respective vectors are mean centered. 11.9 Goodness of fit The larger the coefficient of determination (maximum 1), the better the fit of the model. Figure 11.3: Vector geometric representations of bivariate linear least squares regressions with A) good fit; b) bad fit. 11.10 Interpretting Regression Here are some things to keep in mind when interpretting a multple regression: In most cases of regression, causal interpretation of the model is not justified. Standard linear regression assumes that the predictor variables ( (\\(X_1, X_2, \\ldots\\)) are observed without error. That is, uncertainty in the regression model is only associated with the outcome variable, not the predictors. For many biological systems/experiments this is NOT the case. Comparing the relative size of regression coefficients only makes sense if all the predictor (explanatory) variables have the same scale If the explanatory variables (\\(X_1, X_2,\\ldots,X_m\\)) are highly correlated, then the regression solution can be “unstable” – a small change in the data could lead to a large change in the regression model. 11.11 Illustrating linear regression with simulated data To illustrate how regression works, we’ll use a simulated data set where we specify the relationship between two variables, \\(X\\) and \\(Y\\). Using a simulation is desirable because it allows us to know what the “true” underlying model that relates \\(X\\) and \\(Y\\) is, so we can evaluate how well we do in terms of recovering the model. Let’s generate two vectors representing the variable, \\(X\\) and \\(Y\\), where \\(Y\\) is a function of \\(X\\) plus some independent noise. As specified below, the “true” relationship is \\(Y = 1.5X + 1.0 + \\epsilon_y\\) where \\(\\epsilon_y\\) is a noise term. # this seeds our random number generator # by setting a seed, we can make random simulation reproducible! set.seed(20190227) npts &lt;- 50 # specify coefficients of the simulation a &lt;- 1.0 b &lt;- 1.5 # simulate draws of X with noise X &lt;- seq(1, 5, length.out = npts) + rnorm(npts) # simulate corresponding Y as a function of X with noise Y &lt;- a + b*X + rnorm(npts, sd = 2) # Y = a + bX + noise df.xy &lt;- data.frame(X = X, Y = Y) Having generated some simulated data, let’s visualize it using ggplot and a new library called ggMarginal (install ggMarginal via the standard package installation mechanism). ggMarginal is a package for adding “marginal” plots to a figure. library(tidyverse) library(ggExtra) # a new library, provides ggMarginal plot (see below) # install if you don&#39;t already have it p &lt;- ggplot(df.xy, aes(x = X, y = Y)) + geom_point() ggMarginal(p, type = &quot;histogram&quot;, bins = 11) As shown here, we plot a scatter plot of X and Y, and on the margins of the figure we graphed the respective histogram of X and Y individually. This is a very useful visualization that allows us to see both the univariate and bivariate patterns in the data in one figure. 11.12 Calculating the regression model using vector arithmetic Fitting the bivariate regression using vector operations is straight foward: # calculate the mean centered vectors X.ctr &lt;- X - mean(X) Y.ctr &lt;- Y - mean(Y) # estimate coefficients b.est &lt;- (X.ctr %*% Y.ctr)/(X.ctr %*% X.ctr) a.est &lt;- mean(Y) - b.est * mean(X) # I use `drop` here to turn this matrices back to scalar values regression &lt;- list(intercept = drop(a.est), slope = drop(b.est)) regression ## $intercept ## [1] 1.819327 ## ## $slope ## [1] 1.197603 11.12.1 Calculating the predicted values and residuals Having fit the model, the fitted (predicted) values \\(\\vec{\\widehat{\\mathbf{y}}}\\) and the residuals, \\(\\vec{\\mathbf{e}}\\) can also be found easily. Yhat &lt;- regression$intercept + regression$slope * X e &lt;- Y - Yhat Let’s create a new data frame that include the observed X and Y values plus the predicted and residual values for easy plotting: regression.df &lt;- tibble(X = X, Y = Y, fitted = Yhat, residuals = e) 11.13 Plotting various aspects of a regression Now let’s visualize the observed values of X and Y (in black), as well as the predicted values of Y (in red). For every black point, representing the observed value of X and the corresponding observed value of Y, there is a corresponding red point that shows the observed value of X and the corresponding predicted value of Y from the regression model. regr.plot &lt;- ggplot(regression.df, aes(X,Y)) + geom_point(color=&#39;black&#39;) + geom_point(mapping = aes(X, fitted), color=&#39;red&#39;) regr.plot We can use geom_abline() to add a straight line with a given slope and intercept regr.plot &lt;- regr.plot + geom_abline(slope = regression$slope, intercept = regression$intercept, color=&#39;red&#39;) regr.plot Finally, let’s add some dashed lines to represent the residuals: regr.plot + geom_segment(aes(xend = X, yend = fitted), color=&#39;red&#39;, linetype=&#39;dashed&#39;, alpha=0.35) One final plot we can create since we’re using simulated data is to compare the estimated model (in red) to the “true” model (in blue). You won’t generally be able to do this for most biological data. ggplot(regression.df, aes(X,Y)) + geom_point(color=&#39;black&#39;) + geom_abline(slope = regression$slope, intercept = regression$intercept, color=&#39;red&#39;) + geom_abline(slope = b, intercept = a, color=&#39;blue&#39;) 11.13.1 Residual plots Another common way to depict the residuals, is to plot the predictor values (the X’s) versus the corresponding residual values, like so: ggplot(regression.df, aes(X, residuals)) + geom_point() + geom_hline(yintercept = 0, color = &#39;red&#39;, linetype = &quot;dashed&quot;) + labs(x = &quot;X&quot;, y = &quot;Residuals&quot;) When the linear regression model is appropriate, residuals should be normally distributed, centered around zero and should show no strong trends or extreme differences in spread (variance) for different values of X. 11.14 Calculating the coefficient of determination As described previously, the coefficient of determination is the standard measures of goodness of fit of a regression model. This can be calculated easily based on our previous calculations as so: Yhat.ctr &lt;- Yhat - mean(Yhat) coeff.determination &lt;- drop(Yhat.ctr %*% Yhat.ctr) / drop(Y.ctr %*% Y.ctr) coeff.determination ## [1] 0.4082507 Approximately 41% of the variation in \\(Y\\) is “explained” by the regression on X. "],["linear-regression-models-ii.html", "Chapter 12 Linear Regression Models II 12.1 New Libraries to install 12.2 Standard libraries 12.3 Specifying Regression Models in R 12.4 Quick bivariate regression plots in ggplot 12.5 More about the data structure returned by lm() 12.6 Broom: a library for converting model results into data frames 12.7 qq-plots 12.8 Multiple regression 12.9 3D Plots 12.10 Fitting the regression model 12.11 Interpretting the regression model 12.12 Exploring the Vector Geometry of the Regression Model 12.13 Exploring the Residuals from the Model Fit 12.14 An alternate model 12.15 Exploring the impact of nearly collinear predictors on regression", " Chapter 12 Linear Regression Models II Last week we reviewed the mathematical basis of linear regression, and we saw how to fit bivariate and regression models using vector operations. This week we’ll look at R’s built-in tools for fitting regression models and we’ll look at a couple of options for producing 3D plots. 12.1 New Libraries to install We’ll be using several new packages for this class session. Install the following packages via one of the standard install mechanisms: broom scatterplot3d 12.2 Standard libraries library(tidyverse) 12.3 Specifying Regression Models in R As one would expect, R has a built-in function for fitting linear regression models. The function lm() can be used to fit bivariate and multiple regression models, as well asanalysis of variance, analysis of covariance, and other linear models. 12.3.1 Example data A study by Whitman et al. (2004) showed that the amount of black coloring on the nose of male lions increases with age, and suggested that this might be used to estimate the age of unknown lions. To establish the relationship between these variables they measured the black coloring on the noses of male lions of known age (represented as a proportion). The variables in this data file are proportionBlack and ageInYears giving the proportion of black pigmentation on the nose of each lion used in the study and the corresponding age of each lion. Data from this study is available in CSV format at: https://github.com/Bio723-class/example-datasets/raw/master/ABD-lion-noses.csv Let’s load the lion data and look at the basic structure: lions &lt;- read_csv(&quot;https://github.com/Bio723-class/example-datasets/raw/master/ABD-lion-noses.csv&quot;) ## Rows: 32 Columns: 2 ## ── Column specification ────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): proportionBlack, ageInYears ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(lions) ## # A tibble: 6 × 2 ## proportionBlack ageInYears ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.21 1.1 ## 2 0.14 1.5 ## 3 0.11 1.9 ## 4 0.13 2.2 ## 5 0.12 2.6 ## 6 0.13 3.2 glimpse(lions) ## Rows: 32 ## Columns: 2 ## $ proportionBlack &lt;dbl&gt; 0.21, 0.14, 0.11, 0.13, 0.12, 0.13, 0.12, 0.18, 0.23, … ## $ ageInYears &lt;dbl&gt; 1.1, 1.5, 1.9, 2.2, 2.6, 3.2, 3.2, 2.9, 2.4, 2.1, 1.9,… Since we want to relate lion age to nose pigmentation, it would be a good idea to look at the bivariate relationship to convince ourselves that a linear model may be appropriate. ggplot(lions, aes(x = proportionBlack, y = ageInYears)) + geom_point() 12.3.2 Fitting the model using lm() The predictor (explanatory) variable is proportionBlack and the outcome (response) variable is ageInYears. We use the lm() function to fit the regression of proportionBlack on ageInYears as so: fit.lions &lt;- lm(ageInYears ~ proportionBlack, lions) The first argument to lm is an R “formula”, the second argument is a data frame.Formulas are R’s way of specifying models, though they find other uses as well (e.g. we saw the formula syntax when we introduced the facet_wrap and facet_grid functions from ggplot). The general form of a formula in R is response variable ~ explanatory variables. In the code example above, we have only a single explanatory variable, and thus our response variable is ageInYears and our explanatory variable is proportionBlack. The lm function returns a list with a number of different components. The ones of most interest to us are fitted.values, coefficients, residuals, and (see the lm documentation for full details.) fit.lions ## ## Call: ## lm(formula = ageInYears ~ proportionBlack, data = lions) ## ## Coefficients: ## (Intercept) proportionBlack ## 0.879 10.647 12.3.3 Interpretting summary output from lm() Calling summary on a fit model provides more detailed output: summary(fit.lions) ## ## Call: ## lm(formula = ageInYears ~ proportionBlack, data = lions) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5449 -1.1117 -0.5285 0.9635 4.3421 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8790 0.5688 1.545 0.133 ## proportionBlack 10.6471 1.5095 7.053 7.68e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.669 on 30 degrees of freedom ## Multiple R-squared: 0.6238, Adjusted R-squared: 0.6113 ## F-statistic: 49.75 on 1 and 30 DF, p-value: 7.677e-08 The summary() function provides textual output reporting a number of features of the model fit. We’ll focus on the section labeled coefficients. For the model we fit above this looks like: Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.8790 0.5688 1.545 0.133 proportionBlack 10.6471 1.5095 7.053 7.68e-08 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The first column of this printed table gives the values of the estimated coefficients. The first coefficient, labeled “(Intercept)” is the intercept of the regression line (\\(a\\) in the formulate \\(\\widehat{Y} = a + bX\\)) – here the estimated value is 0.879. Since our model includes only one predictor variable (proportionBlack) we have only one additional coefficient (b = 10.647). Thus the mathematical equation describing our fit model is $ = 0.88 + 10.65() $ The second column (Std. Error) in the coefficients table gives the standard error of the estimated coefficients (based on assumption of multivariate normality of the data). The third column (t value) gives a calculated t-value for the null hypothesis that the corresponding coefficient is zero, and the fourth column (Pr(&gt;|t|)) is the probability of observing a t-value that large under the null hypothesis that the corresponding has a value of zero (i.e. a p-value). For the intercept, we see that the p-value is 0.133; thus we don’t have strong statistical evidence to reject the null hypothesis that this coefficient is zero. However, for the slope the p-value is &lt; 1e-8. This is strong evidence on which to reject the null hypothesis of a zero coefficient (a zero slope would imply that) 12.4 Quick bivariate regression plots in ggplot Since linear model fitting is a fairly common task, the ggplot library includes a geometric mapping, geom_smooth(), that will fit a linear model (as well as other models, as we’ll see in future lection) for us and generate the corresponding regression plot. We can create this by specifying method=\"lm\" as the argument to geom_smooth(): ggplot(lions, aes(x = proportionBlack, y = ageInYears)) + geom_point(alpha = 0.75) + geom_smooth(method=&quot;lm&quot;, color = &#39;red&#39;, fullrange=TRUE) + xlim(0,1) ## `geom_smooth()` using formula = &#39;y ~ x&#39; By default, geom_smooth draws 95% confidence intervals for the regression model (the shaded gray area around the regression line). These confidence intervals reflect the uncertainty in the estimates of the slope and intercept, as reflected by the standard error of the coefficients discussed above. Note that confidence intervals for a linear regression model are wider far away from the mean values of \\(X\\) and \\(Y\\). 12.5 More about the data structure returned by lm() The data structure returned by lm() is a list-like object with multiple fields: typeof(fit.lions) ## [1] &quot;list&quot; We can access the names of all the fields associated with the fit object: names(fit.lions) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; 12.5.1 Fitted values The component fitted.values gives the predicted values of \\(Y\\) (\\(\\hat{Y}\\) in the equations above) for each observed value of \\(X\\). We can plot these predicted values of \\(Y\\), as shown below. Notice how the predicted values all fall on a line (the regression line itself!) ggplot(lions, aes(x = proportionBlack, y = ageInYears)) + geom_point(alpha=0.7) + # observed data geom_point(aes(x = proportionBlack, y = fit.lions$fitted.values), # predicted data color=&#39;red&#39;, alpha=0.5) + geom_segment(aes(xend = proportionBlack, yend = fit.lions$fitted.values), color=&#39;red&#39;, linetype=&#39;dashed&#39;, alpha=0.25) Figure 12.1: Observed (black) and predicted (red) values in a linear regression of Y on X. Dashed lines indicate the residuals from the regression. 12.5.2 Getting the model coefficients The coefficients components gives the value of the model parameters, namely the intercept and slope. These are the same values reported by the summary() function described above: &gt; fit.lions$coefficients ## (Intercept) proportionBlack ## 0.8790062 10.6471194 Typically these coefficients are given with more precision than we’d want to report. For the purpose of printing, can create character strings with rounded coefficients using the sprintf() function so: As shown above, the estimated slope is 10.65 and the estimated intercept is 0.88. The model estimated by our linear regression is thus \\(\\widehat{\\text{age}} = 0.88 + 10.65\\text{proportion black}\\). 12.6 Broom: a library for converting model results into data frames The data structure we got back when we used the lm function to carry out linear regression, carries lots of useful information it isn’t a particularly “tidy” way to access the data. The R package Broom converts “statistical analysis objects from R into tidy data frames, so that they can more easily be combined, reshaped and otherwise processed with tools like ‘dplyr’, ‘tidyr’ and ‘ggplot2’. The discussion of Broom below is drawn from the Introduction to Broom If you haven’t already done so, install the broom package before proceeding. library(broom) There are three broom functions that are particularly useful for our purposes. They are: tidy – constructs a data frame that summarizes the model’s statistical findings. augment – add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments. glance – construct a concise one-row summary of the model. 12.6.1 broom::tidy tidy applied to a regression model object returns a table giving the estimated coefficients and other information about the uncertainty of those estimates and corresponding p-values. This mirrors the table of coefficients that apply summary() provided to us, but instead of just printing this information we get it in the form of a data frame that we can do further computations on. tidy(fit.lions) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.879 0.569 1.55 0.133 ## 2 proportionBlack 10.6 1.51 7.05 0.0000000768 12.6.2 broom::augment augment creates a data frame that combines the original data with related information from the model fit. lions.augmented &lt;- augment(fit.lions, lions) head(lions.augmented) ## # A tibble: 6 × 8 ## proportionBlack ageInYears .fitted .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.21 1.1 3.11 -2.01 0.0415 1.65 0.0330 -1.23 ## 2 0.14 1.5 2.37 -0.870 0.0584 1.69 0.00894 -0.537 ## 3 0.11 1.9 2.05 -0.150 0.0681 1.70 0.000318 -0.0932 ## 4 0.13 2.2 2.26 -0.0631 0.0615 1.70 0.0000499 -0.0391 ## 5 0.12 2.6 2.16 0.443 0.0647 1.70 0.00261 0.275 ## 6 0.13 3.2 2.26 0.937 0.0615 1.69 0.0110 0.580 Now, in addition to the proportionBlack and ageInYears variables of the original data, we have columns like .fitted (value of Y predicted by the model for the corresponding value of X), .resid (difference between the actual Y and the predicted value), and a variety of other information for evalulating model uncertainty. One thing we can do with this “augmented” data frame is to use it to better visualize and explore the model. For example, if we wanted to generate a figure highlighting the deviations from the model using vertical lines emanating from the regression line, we could do something like this: ggplot(lions.augmented, aes(proportionBlack, ageInYears)) + geom_point() + geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;,se=FALSE) + geom_segment(aes(xend = proportionBlack, yend = .fitted), linetype=&quot;dashed&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; An another example, we can quickly create a residual plot using the augmented data frame as so: ggplot(lions.augmented, aes(proportionBlack, .resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, linetype=&#39;dashed&#39;) + labs(y = &quot;Residuals&quot;, title = &quot;Residual plot the lions regression model.&quot;) 12.6.3 broom::glance glance() provides summary information about the goodness of fit of the model. Most relevant for our current discussion is the column giving the coefficient of determination (r.squared): glance(fit.lions) ## # A tibble: 1 × 12 ## r.squ…¹ adj.r…² sigma stati…³ p.value df logLik AIC BIC devia…⁴ df.re…⁵ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.624 0.611 1.67 49.8 7.68e-8 1 -60.8 128. 132. 83.5 30 ## # … with 1 more variable: nobs &lt;int&gt;, and abbreviated variable names ## # ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual 12.7 qq-plots From our residuals plot of the lions data set, there may be some indication of greater variance of residuals for larger values of the predictor variable. Let’s check how normal the residuals look using a diagnostic plot called a QQ-plot (quantile-quantile plot). A qq-plot is a graphical method for comparing distributions by plotting the respective quantiles against each other. Typically we plot sample quantiles against theoretical quantiles; for example to compare the sample quantiles to the theoretical expectation of normality. In the example below we construct the QQ-plot using “standardized residuals” from the regression which are just z-scores for the residuals. ggplot(lions.augmented, aes(sample = .std.resid)) + geom_qq() + geom_qq_line(color=&quot;firebrick&quot;) Based on the QQ-plot, the residuals seem to diverge somewhat from a normal distirbution, as there’s noticeable curvature in the QQ-plot. When we test for the normality of the residuals using Shapiro-Wilk’s test for normality, we fail to reject the null hypothesis of normality at a significance threshold of \\(\\alpha=0.05\\): shapiro.test(lions.augmented$.resid) ## ## Shapiro-Wilk normality test ## ## data: lions.augmented$.resid ## W = 0.93879, p-value = 0.0692 Even though we failed to reject the null hypothesis of normality for the residuals, but the P-value is very close to significance, suggesting some caution in applying the linear model. 12.8 Multiple regression To illustrate multiple regression in R we’ll use a built in dataset called trees. trees consists of measurements of the girth, height, and volume of 31 black cherry trees (?trees for more info). Let’s assume we’re lumberjacks, but our permit only allows us to harvest a fixed number of trees. We get paid by the total volume of wood we harvest, so we’re interested in predicting a tree’s volume (hard to measure directly) as a function of its girth and height (relatively easy to measure), so we can pick the best trees to harvest. We’ll therefore calculate a multiple regression of volume on height and width. 12.8.1 Exploration of the trees data set We’ll start with some summary tables and diagnostic plots to familiarize ourselves with the data: head(trees) ## # A tibble: 6 × 3 ## Girth Height Volume ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 8.3 70 10.3 ## 2 8.6 65 10.3 ## 3 8.8 63 10.2 ## 4 10.5 72 16.4 ## 5 10.7 81 18.8 ## 6 10.8 83 19.7 We’ll use the GGally::ggpairs() function introduced in problem set 01 to create a scatterplot matrix depicting the pairwise relationships between all the variables library(GGally) ggpairs(trees) As one might expect for morphological measurements related to size, the scatterplot matrix shows that all the variables are positively correlated, and girth and volume have a particularly strong correlation. 12.9 3D Plots ggplot has no built in facilities for 3D scatter plots so we’ll use two new packages, scatterplot3D and rgl, to generate 3D visualizations. 12.9.1 scatterplot3d library(scatterplot3d) # install this package first if needed scatterplot3d(trees, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) The argument pch sets the type of plotting character to use in the plot (for a graphical key of the available plotting characters see this link) and color sets plotting character colors. We can change the angle of the 3D plot using the angle argument: scatterplot3d(trees, pch = 16, color=&quot;steelblue&quot;, angle=75, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) We can add vertical lines to the plot using the type argument and remove the box around the plot: scatterplot3d(trees, pch = 16, color=&quot;steelblue&quot;, angle=75, box = FALSE, type = &quot;h&quot;, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) For more examples of how you can modify plots generated with the scatterplot3d package see this web page). 12.10 Fitting the regression model From the 3D scatter plot it looks like we ought to be able to find a plane through the data that fits the scatter fairly well. Let’s use the lm() function to calculate the multiple regression and summary() to get the details of the model: fit.trees &lt;- lm(Volume ~ Girth + Height, data=trees) summary(fit.trees) ## ## Call: ## lm(formula = Volume ~ Girth + Height, data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4065 -2.6493 -0.2876 2.2003 8.4847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -57.9877 8.6382 -6.713 2.75e-07 *** ## Girth 4.7082 0.2643 17.816 &lt; 2e-16 *** ## Height 0.3393 0.1302 2.607 0.0145 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.882 on 28 degrees of freedom ## Multiple R-squared: 0.948, Adjusted R-squared: 0.9442 ## F-statistic: 255 on 2 and 28 DF, p-value: &lt; 2.2e-16 12.10.1 Visualizing the regression model in scatterplot3d To visualize the multiple regression, let’s use the scatterplot3d package to draw the 3D scatter of plots and the plane that corresponds to the regression model: p &lt;- scatterplot3d(trees, angle=55,type=&#39;h&#39;, pch = 16, color = &quot;steelblue&quot;, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) # add a plane representing the fit of the model p$plane3d(fit.trees, col=&#39;orangered&#39;) From the figures it looks like the regression model fits pretty well, as we anticipated from the pairwise relationships. 12.11 Interpretting the regression model The regression equation is: \\(\\hat{y}\\) = + \\(x_1\\) +\\(x_2\\), where \\(y\\) is Volume, and \\(x_1\\) and \\(x_2\\) are Girth and Height respectively. Since they’re on different scales the coefficients for Girth and Height aren’t directly comparable. Both coefficients are significant at the \\(p&lt;0.05\\) level, but note that Girth is the much stronger predictor. In fact the addition of height explains only a minor additional fraction of variation in tree volume, so from the lumberjack’s perspective the additional trouble of measuring height probably isn’t worth it. 12.12 Exploring the Vector Geometry of the Regression Model Recall the broom:tidy produces a tabular summary of the coefficients of the model and their associated statistics: broom::tidy(fit.trees) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -58.0 8.64 -6.71 2.75e- 7 ## 2 Girth 4.71 0.264 17.8 8.22e-17 ## 3 Height 0.339 0.130 2.61 1.45e- 2 broom:glance provides information about the fit of the model: broom::glance(fit.trees) ## # A tibble: 1 × 12 ## r.squared adj.r.squa…¹ sigma stati…² p.value df logLik AIC BIC devia…³ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.948 0.944 3.88 255. 1.07e-18 2 -84.5 177. 183. 422. ## # … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;, and abbreviated ## # variable names ¹​adj.r.squared, ²​statistic, ³​deviance Let’s use our knowledge of vector geometry to further explore the relationship between the predicted Volume and the predictor variables. By definition the vector representing the predicted values lies in the subspace (in this case a plane) defined by Height and Girth, so let’s do some simple calculations to understand their length and angular relationships: # proportional to length of vectors sd(fit.trees$fitted.values) ## [1] 16.00434 sd(trees$Height) ## [1] 6.371813 sd(trees$Girth) ## [1] 3.138139 # cosines of angles btw vectors cor(trees$Height, trees$Girth) ## [1] 0.5192801 cor(trees$Girth, fit.trees$fitted.values) ## [1] 0.9933158 cor(trees$Height, fit.trees$fitted.values) ## [1] 0.6144545 # angles btw vectors in degrees acos(cor(trees$Girth, trees$Height)) * (180/pi) ## [1] 58.71603 acos(cor(trees$Girth, fit.trees$fitted.values)) * (180/pi) ## [1] 6.628322 acos(cor(trees$Height, fit.trees$fitted.values)) * (180/pi) ## [1] 52.08771 Notice that \\(\\text{Girth}\\) is very highly correlated with the \\(\\widehat{\\text{Volume}}\\) and hence the angle between these two vectors is very small (about 6.6 degrees). By contrast, \\(\\text{Height}\\) is only moderately correlated with \\(\\widehat{\\text{Volume}}\\) and the angle between them is significantly larger (about 52 degrees). 12.13 Exploring the Residuals from the Model Fit Now let’s look at the residuals from the regression. The residuals represent the `unexplained’ variance: trees.augmented &lt;- augment(fit.trees, trees) ggplot(trees.augmented, aes(x = Girth, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color=&#39;red&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;Girth&quot;, y = &quot;Residuals&quot;) Ideally the residuals should be evenly scattered around zero, with no trends as we go from high to low values of the dependent variable. As you can see, the residuals are somewhat u-shaped or j-shaped suggesting that there may be a non-linear aspect of the relationship that our model isn’t capturing. 12.14 An alternate model Let’s think about the relationships we’re actually modeling for a few minutes. For the sake of simplicity let’s consider the trunk of a tree to be a cylinder. How do the dimensions of this cylinder relate to its volume? You can look up the formula for the volume of a cylinder, but the key thing you’ll want to note is that volume of the cylinder should be proportional to a characteristic length of the cylinder cubed (\\(V \\propto \\mathrm{L}^3\\)). This suggests that if we want to fit a linear model we should relate Girth and Height to \\(\\sqrt[3]{\\mathrm{Volume}}\\): trees.cuberoot &lt;- mutate(trees, cuberoot.Volume = Volume^0.33) fit.trees.cuberoot &lt;- lm(cuberoot.Volume ~ Girth + Height, data = trees.cuberoot) broom::glance(fit.trees) # summary of fit of original model ## # A tibble: 1 × 12 ## r.squared adj.r.squa…¹ sigma stati…² p.value df logLik AIC BIC devia…³ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.948 0.944 3.88 255. 1.07e-18 2 -84.5 177. 183. 422. ## # … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;, and abbreviated ## # variable names ¹​adj.r.squared, ²​statistic, ³​deviance broom::glance(fit.trees.cuberoot) # summary of fit of alternate model ## # A tibble: 1 × 12 ## r.squared adj.r.squ…¹ sigma stati…² p.value df logLik AIC BIC devia…³ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.978 0.976 0.0811 612. 7.77e-24 2 35.5 -62.9 -57.2 0.184 ## # … with 2 more variables: df.residual &lt;int&gt;, nobs &lt;int&gt;, and abbreviated ## # variable names ¹​adj.r.squared, ²​statistic, ³​deviance Comparing the summary tables, we see indeed that using the cube root of Volume improves the fit of our model some. Let’s examine the residuals of this alternate model. trees.cuberoot &lt;- broom::augment(fit.trees.cuberoot, trees.cuberoot) ggplot(trees.cuberoot, aes(x = cuberoot.Volume, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color=&#39;red&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;Girth&quot;, y = &quot;Residuals&quot;) As we can see the transformation we applied to the data did seem to make our residuals more uniform across the range of observations. 12.15 Exploring the impact of nearly collinear predictors on regression In lecture we discussed the problems that can arise in regression when your predictor variables are nearly collinear. In this section we’ll illustrate some of these issues. Consider again the trees data set. Recall that two of the variables – Girth and Volume – are highly correlated and thus nearly collinear. cor(trees) ## Girth Height Volume ## Girth 1.0000000 0.5192801 0.9671194 ## Height 0.5192801 1.0000000 0.5982497 ## Volume 0.9671194 0.5982497 1.0000000 Let’s explore what happens when we treat Height as the dependent variable, and Girth and Volume as the predictor variables. fit.Height &lt;- lm(Height ~ Girth + Volume, data = trees) broom::glance(fit.Height) ## # A tibble: 1 × 12 ## r.squ…¹ adj.r…² sigma stati…³ p.value df logLik AIC BIC devia…⁴ df.re…⁵ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.412 0.370 5.06 9.82 5.87e-4 2 -92.6 193. 199. 716. 28 ## # … with 1 more variable: nobs &lt;int&gt;, and abbreviated variable names ## # ¹​r.squared, ²​adj.r.squared, ³​statistic, ⁴​deviance, ⁵​df.residual We can, of course, fit the linear model despite the near collinearity, and we find that the model does have some predictive power, with \\(R^2 = 0.41\\), and with Volume being the more significant predictor. Now, let’s created a slightly different version of the trees data set by add some noise to the three variables. Our goal here is to simulate a data set we might have created had we measured a slightly different set of trees during our sampling. We’ll use the jitter() function to add uniform noise to the data set. jitter.Girth &lt;- jitter(trees$Girth, amount= 0.5 * sd(trees$Girth)) jitter.Height &lt;- jitter(trees$Height, amount= 0.5 * sd(trees$Height)) jitter.Volume &lt;- jitter(trees$Volume, amount= 0.5 * sd(trees$Volume)) jitter.trees &lt;- data.frame(Girth = jitter.Girth, Height = jitter.Height, Volume = jitter.Volume) Here we added uniform noise proportional to the one-quarter the standard deviation of each variable. Let’s take a moment to convince ourselves that our new data set, jitter.trees, is not too different from the trees data set from which it was derived. set.seed(20190227) # compare this to broom::tidy(trees) broom::tidy(jitter.trees) ## # A tibble: 3 × 13 ## column n mean sd median trimmed mad min max range skew kurto…¹ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Girth 31 13.2 3.11 12.3 13.0 1.85 8.02 21.5 13.5 0.732 3.04 ## 2 Height 31 76.3 6.99 76.3 76.7 4.63 60.2 89.2 28.9 -0.567 2.83 ## 3 Volume 31 30.5 17.1 27.0 29.0 9.99 4.64 82.2 77.6 1.02 4.07 ## # … with 1 more variable: se &lt;dbl&gt;, and abbreviated variable name ¹​kurtosis # correlations among jittered variables are # similar to those of the original variables cor(jitter.trees) ## Girth Height Volume ## Girth 1.0000000 0.4730565 0.8581607 ## Height 0.4730565 1.0000000 0.5245206 ## Volume 0.8581607 0.5245206 1.0000000 ## jittered variables are highly correlatd with original variables cor(trees$Height, jitter.trees$Height) ## [1] 0.9659041 cor(trees$Girth, jitter.trees$Girth) ## [1] 0.9508859 cor(trees$Volume, jitter.trees$Volume) ## [1] 0.9464283 Now that we’ve convinced ourselves that our jittered data set is a decent approximation to our original data set, let’s re-calculate the linear regression, and compare the coefficients of the jittered model to the original model: fit.Height.jitter &lt;- lm(Height ~ Girth + Volume, data = jitter.trees) broom::tidy(fit.Height) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 83.3 9.09 9.17 6.33e-10 ## 2 Girth -1.86 1.16 -1.61 1.19e- 1 ## 3 Volume 0.576 0.221 2.61 1.45e- 2 broom::tidy(fit.Height.jitter) ## # A tibble: 3 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 68.1 6.39 10.7 2.34e-11 ## 2 Girth 0.196 0.704 0.278 7.83e- 1 ## 3 Volume 0.183 0.128 1.44 1.62e- 1 We see that the coefficients of the linear model have changed substantially between the original data and the jittered data. Our model is unstable to relatively modest changes to the data! Let’s draw some plots to illustrate how different the models fit to the original and jittered data are: # draw 3d scatter plots with small points so as not to obscure regression planes p &lt;- scatterplot3d(x=trees$Girth, y=trees$Volume, z=trees$Height, angle=15, type=&#39;p&#39;, pch=&#39;.&#39;) # original model p$plane3d(fit.Height, col=&#39;orangered&#39;) # jittered model p$plane3d(fit.Height.jitter, col=&#39;blue&#39;) Let’s do the same comparison for the multiple regression of Volume on Height and Girth. In this case the predictor variables are nearly collinear. fit.Volume &lt;- lm(Volume ~ Girth + Height, data = trees) fit.Volume.jitter &lt;- lm(Volume ~ Girth + Height, data = jitter.trees) coefficients(fit.Volume) ## (Intercept) Girth Height ## -57.9876589 4.7081605 0.3392512 coefficients(fit.Volume.jitter) ## (Intercept) Girth Height ## -55.4318168 4.3381867 0.3746981 For this model, we see that the coefficients have changed only a small amount. The underlying data, jitter.trees, is the same in both cases, but now our model is stable because the predictor variables are only modestly correlated with each other. Let’s generate another plot to illustrate the similarity of the models fit to the original and jittered data when Girth and Height are used to predict Volume. p &lt;- scatterplot3d(x=trees$Girth, y=trees$Height, z=trees$Volume, angle=55, type=&#39;p&#39;, pch=&#39;.&#39;) p$plane3d(fit.Volume, col=&#39;orangered&#39;) p$plane3d(fit.Volume.jitter, col=&#39;blue&#39;) Finally, let’s do some vector calculations to quantify how the angular deviation between the fit data and the predictor variables changes between the original and jittered data set for the two different multiple regressions: # write a quickie fxn to express angle between vectors in degrees vec.angle &lt;- function(x,y) { acos(cor(x,y)) * (180/pi)} # vector angles for fit of Height ~ Girth + Volume (orig) vec.angle(fit.Height$fitted.values, trees$Girth) ## [1] 36.02644 vec.angle(fit.Height$fitted.values, trees$Volume) ## [1] 21.29297 # vector angles for fit of Height ~ Girth + Volume (jittered) vec.angle(fit.Height.jitter$fitted.values, jitter.trees$Girth) ## [1] 26.02139 vec.angle(fit.Height.jitter$fitted.values, jitter.trees$Volume) ## [1] 4.867913 Now the same comparison for the non-collinear model Volume ~ Girth + Height. # vector angles for fit of Volume ~ Girth + Height (orig) vec.angle(fit.Volume$fitted.values, trees$Girth) ## [1] 6.628322 vec.angle(fit.Volume$fitted.values, trees$Height) ## [1] 52.08771 # vector angles for fit of Volume ~ Girth + Height (jittered) vec.angle(fit.Volume.jitter$fitted.values, jitter.trees$Girth) ## [1] 8.912219 vec.angle(fit.Volume.jitter$fitted.values, jitter.trees$Height) ## [1] 52.8549 As these calculation illustrate, the change in the regression plane in the jittered date is much smaller when the dependent variable are not nearly colinear. "],["principal-components-analysis.html", "Chapter 13 Principal Components Analysis 13.1 Libraries 13.2 Matrices as linear transformations 13.3 Eigenanalysis in R 13.4 Principal Components Analysis in R 13.5 Drawing Figures to Represent PCA", " Chapter 13 Principal Components Analysis 13.1 Libraries library(tidyverse) library(broom) library(GGally) library(cowplot) 13.2 Matrices as linear transformations In lecture we introduced the notion that pre-multiplying a vector, \\(x\\), by a matrix \\(\\mathbf{A}\\) represents a linear transformation of \\(x\\). Let’s explore that visually for 2D vectors. For illustration let’s generate a 2D vector representing the coordinates of points on the sine function. sin.xy &lt;- data_frame(x = seq(0,2*pi, length.out = 50) - pi, y = sin(x)) ggplot(sin.xy, aes(x = x, y= y)) + geom_point() Let’s start with some of the transformations we discussed in lecture. First we look at reflection about the x-axis: # this matrix represents reflection about the x-axis A &lt;- matrix(c(1, 0, 0, -1), byrow=TRUE, nrow=2) # apply this transformation to each of the points in our vector `sin.xy` Ax &lt;- A %*% t(sin.xy) # creae a data frame from this transformation Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) # plot the transformed points ggplot(Ax.df, aes(x = x, y = y)) + geom_point() If instead we apply a matrix that represents a shear in the x-axis we get: # shear parallel to the x-axis A &lt;- matrix(c(1, 1.5, 0, 1), byrow=TRUE, nrow=2) Ax &lt;- A %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) ggplot(Ax.df, aes(x = x, y= y)) + geom_point() And finally rotation: # rotation by pi/2 radians (90 degrees) A &lt;- matrix(c(cos(pi/2), -sin(pi/2), sin(pi/2), cos(pi/2)), byrow=TRUE, nrow=2) Ax &lt;- A %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) ggplot(Ax.df, aes(x = x, y= y)) + geom_point() 13.3 Eigenanalysis in R As we discussed in lecture, the eigenvectors of a square matrix, \\(\\mathbf{A}\\), point in the directions that are unchanged by the transformation specified by \\(\\mathbf{A}\\). Let’s start with yet another transformation matrix, \\(\\mathbf{A}\\), the effects of which are illustrated below: A &lt;- matrix(c(1.0, 1.5, 0, 2.0), byrow=TRUE, nrow=2) Ax &lt;- A %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) ggplot(mapping=aes(x = x, y = y)) + geom_point(data = sin.xy, shape=16, alpha=0.5) + geom_point(data = Ax.df, shape=17, alpha=0.5, color=&#39;red&#39;) This transformation is a combination of stretching (dilation) in the Y-axis, and shear parallel to the x-axis. The eigen() function computes the eigenvalues and eigenvectors of a square matrix: eigen.A &lt;- eigen(A) eigen.A ## eigen() decomposition ## $values ## [1] 2 1 ## ## $vectors ## [,1] [,2] ## [1,] 0.8320503 1 ## [2,] 0.5547002 0 The following relationships relate \\(\\mathbf{A}\\) to it’s eigenvectors and eigenvalues: \\[\\mathbf{V}^{-1}\\mathbf{A}\\mathbf{V} = \\mathbf{D} \\] \\[\\mathbf{A} = \\mathbf{V}\\mathbf{D}\\mathbf{V}^{-1}\\] where \\(\\mathbf{V}\\) is a matrix where the columns represent the eigenvectors, and \\(\\mathbf{D}\\) is a diagonal matrix of eigenvalues. Let’s confirm that those relationships hold for our example: V &lt;- eigen.A$vectors D &lt;- diag(eigen.A$values) # diagonal matrix of eigenvalues Vinv &lt;- solve(V) V %*% D %*% Vinv # reconstruct our original matrix (see lecture slides) ## [,1] [,2] ## [1,] 1 1.5 ## [2,] 0 2.0 all.equal(Vinv %*% A %*% V, D) # test &#39;near equality&#39; ## [1] TRUE V[,1] %*% V[,2] # note that the eigenvectors are NOT orthogonal. Why? ## [,1] ## [1,] 0.8320503 Since we’re dealing with a 2D transformation matrix, we can easily calculate the slopes of the eigenvectors: eigenvec.A.slope1 = eigen.A$vectors[2,1]/eigen.A$vectors[1,1] eigenvec.A.slope2 = eigen.A$vectors[2,2]/eigen.A$vectors[1,2] Now we plot the eigenvectors to show the directions that are unaffected by the transformation represented by the matrix \\(\\mathbf{A}\\): # Note that the slopes can be infinite (inf), indicating a vertical eigenvector (parallel to y-axis) # this function is to draw the eigenvectors correctly regardless of the slope geom_ab_or_vline &lt;- function(slope, intercept, ...) { if (is.infinite(slope)) { return(geom_vline(yintercept = intercept, ...)) } else { return(geom_abline(slope=slope, intercept=intercept, ...)) } } ggplot(mapping=aes(x = x, y = y)) + geom_point(data = sin.xy, alpha=0.5) + geom_point(data = Ax.df, color=&#39;red&#39;, alpha=0.5) + geom_ab_or_vline(slope = eigenvec.A.slope1,intercept=0, color=&#39;red&#39;,linetype=&#39;dashed&#39;) + geom_ab_or_vline(slope = eigenvec.A.slope2,intercept=0, color=&#39;red&#39;,linetype=&#39;dashed&#39;) + coord_fixed() Referring back to the eigenvectors, we see that it’s the second eigenvector that is represented by the horizontal line: eigen.A$vectors ## [,1] [,2] ## [1,] 0.8320503 1 ## [2,] 0.5547002 0 The implies that lines pointing in the horizontal are unchanged in the direction by the transformation represented by A. To drive home this point, here’s another configuration of points and the corresponding points under the same transformation: horiz.x &lt;- seq(-1,1,by=0.1) horiz.y &lt;- rep(0, length(horiz.x)) horiz.x2 &lt;- seq(-0.5,0.5,by=0.1) horiz.y2 &lt;- rep(0.5, length(horiz.x2)) horiz.x3 &lt;- seq(-0.5,0.5,by=0.1) horiz.y3 &lt;- rep(-0.5, length(horiz.x3)) vert.y &lt;- seq(-1,1,by=0.1) vert.x &lt;- rep(0, length(vert.y)) x &lt;- c(horiz.x, horiz.x2, horiz.x3, vert.x) y &lt;- c(horiz.y, horiz.y2, horiz.y3, vert.y) cross.df &lt;- data_frame(x = x, y = y) Ax &lt;- A %*% t(cross.df) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) plot.A &lt;- ggplot(cross.df, mapping = aes(x=x,y=y)) + geom_point() + lims(x=c(-1.5,1.5), y=c(-2,2)) + coord_fixed() + labs(x = &quot;X&quot;, y = &quot;Y&quot;, title=&quot;Points prior to transformation&quot;) plot.B &lt;- ggplot(Ax.df, mapping = aes(x=x,y=y)) + geom_point(color=&#39;red&#39;) + lims(x=c(-1.5,1.5), y=c(-2,2)) + coord_fixed() + labs(x = &quot;X&quot;, y = &quot;Y&quot;, title=&quot;Points after transformation&quot;) plot_grid(plot.A, plot.B) 13.4 Principal Components Analysis in R There are two functions in R for carrying out PCA - princomp() and prcomp(). The princomp() function uses the eigen() function to carry out the analysis on the covariance matrix or correlation matrix, while carries out an equivalent analysis, starting from a data matrix, using a technique called singular value decomposition (SVD). The SVD routine has greater numerical accuracy, so the prcomp() function should generally be preferred. The princomp() function is useful when you don’t have access to the original data, but you do have a covariance or correlation matrix (a frequent situation when re-analyzing data from the literature). We’ll concentrate on using the prcomp() function. 13.4.1 Bioenv dataset To demonstrate PCA we’ll use a dataset called `bioenv.txt’ (see class wiki), obtained from a book called “Biplots in Practice” (M. Greenacre, 2010). Here is Greenacre’s description of the dataset: The context is in marine biology and the data consist of two sets of variables observed at the same locations on the sea-bed: the first is a set of biological variables, the counts of five groups of species, and the second is a set of four environmental variables. The data set, called “bioenv”, is shown in Exhibit 2.1. The species groups are abbreviated as “a” to “e”. The environmental variables are “pollution”, a composite index of pollution combining measurements of heavy metal concentrations and hydrocarbons; depth, the depth in metres of the sea-bed where the sample was taken; “temperature”, the temperature of the water at the sampling point; and “sediment”, a classification of the substrate of the sample into one of three sediment categories. We’ll start by reading the bioenv.txt data set from the Github repository: bioenv &lt;- read_tsv(&#39;https://github.com/Bio723-class/example-datasets/raw/master/bioenv.txt&#39;) ## New names: ## Rows: 30 Columns: 10 ## ── Column specification ## ──────────────────────────────────── Delimiter: &quot;\\t&quot; chr ## (2): ...1, Sediment dbl (8): a, b, c, d, e, Pollution, ## Depth, Temperature ## ℹ Use `spec()` to retrieve the full column specification ## for this data. ℹ Specify the column types or set ## `show_col_types = FALSE` to quiet this message. ## • `` -&gt; `...1` names(bioenv) ## [1] &quot;...1&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ## [6] &quot;e&quot; &quot;Pollution&quot; &quot;Depth&quot; &quot;Temperature&quot; &quot;Sediment&quot; Notice that the first column got assigned the generic name ...1. This is because there is a missing column header in the bioenv.txt file. This first column corresponds to the sampling sites. Before we move on let’s give this column a more meaningful name: bioenv &lt;- bioenv %&gt;% rename(Site = &quot;...1&quot;) names(bioenv) ## [1] &quot;Site&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ## [6] &quot;e&quot; &quot;Pollution&quot; &quot;Depth&quot; &quot;Temperature&quot; &quot;Sediment&quot; The columns labeled a to e contain the counts of the five species at each site, while the remaining columns give additional information about the physical properties of each sampling site. For our purposes today we’ll confine our attention to the abundance data. abundance &lt;- bioenv %&gt;% dplyr::select(Site, a, b, c, d, e) head(abundance) ## # A tibble: 6 × 6 ## Site a b c d e ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 s1 0 2 9 14 2 ## 2 s2 26 4 13 11 0 ## 3 s3 0 10 9 8 0 ## 4 s4 0 0 15 3 0 ## 5 s5 13 5 3 10 7 ## 6 s6 31 21 13 16 5 The data is currently in a “wide” format. For the purposes of plotting it will be more convenient to generate a “long” version of the data using functions from the tidyr library (see the Data Wrangling chapter). long.abundance &lt;- abundance %&gt;% tidyr::gather(Species, Count, -Site) head(long.abundance) ## # A tibble: 6 × 3 ## Site Species Count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 s1 a 0 ## 2 s2 a 26 ## 3 s3 a 0 ## 4 s4 a 0 ## 5 s5 a 13 ## 6 s6 a 31 ggplot(long.abundance, aes(x = Species, y = Count)) + geom_boxplot() + labs(x = &quot;Species&quot;, y = &quot;Count&quot;, title=&quot;Distribution of\\nSpecies Counts per Site&quot;) From the boxplot it looks like the counts for species `e’ are smaller on average, and less variable. The mean and variance functions confirm that. long.abundance %&gt;% group_by(Species) %&gt;% summarize(mean(Count), var(Count)) ## # A tibble: 5 × 3 ## Species `mean(Count)` `var(Count)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 13.5 158. ## 2 b 8.73 83.4 ## 3 c 8.4 73.6 ## 4 d 10.9 44.4 ## 5 e 2.97 15.7 A correlation matrix suggests weak to moderate associations between the variables: abundance.only &lt;- abundance %&gt;% dplyr::select(-Site) # drop the Site column cor(abundance.only) ## a b c d e ## a 1.0000000 0.67339954 -0.23992888 0.358192050 0.273522301 ## b 0.6733995 1.00000000 -0.08041947 0.501834036 0.036914702 ## c -0.2399289 -0.08041947 1.00000000 0.081504483 -0.343540453 ## d 0.3581921 0.50183404 0.08150448 1.000000000 -0.004048517 ## e 0.2735223 0.03691470 -0.34354045 -0.004048517 1.000000000 However a scatterplot matrix generated by the GGally::ggapirs() function suggests that many of the relationships have a strong non-linear element. ggpairs(abundance.only) 13.4.2 PCA of the Bioenv dataset Linearity is not a requirement for PCA, as it’s simply a rigid rotation of the original data. So we’ll continue with our analysis after taking a moment to read the help on the prcomp() function that is used to carry-out PCA in R. abundance.pca &lt;- prcomp(abundance.only, center=TRUE, retx=TRUE) # center=TRUE mean centers the data # retx=TRUE returns the PC scores # if you want to do PCA on the correlation matrix set scale.=TRUE # -- notice the period after scale! summary(abundance.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 ## Standard deviation 14.8653 8.8149 6.2193 5.03477 3.48231 ## Proportion of Variance 0.5895 0.2073 0.1032 0.06763 0.03235 ## Cumulative Proportion 0.5895 0.7968 0.9000 0.96765 1.00000 We see that approximately 59% of the variance in the data is capture by the first PC, and approximately 90% by the first three PCs. Let’s compare the values return by PCA to what we would get if we carried out eigenanalysis of the covariance matrix that corresponds to our data. First the list object return by prcomp(): abundance.pca ## Standard deviations (1, .., p=5): ## [1] 14.865306 8.814912 6.219250 5.034774 3.482308 ## ## Rotation (n x k) = (5 x 5): ## PC1 PC2 PC3 PC4 PC5 ## a 0.81064462 0.07052882 -0.53108427 0.18442140 -0.14771336 ## b 0.51264394 -0.27799671 0.47711910 -0.63418946 0.17342177 ## c -0.16235135 -0.88665551 -0.40897655 -0.01149647 0.14173943 ## d 0.22207108 -0.31665237 0.56250980 0.72941223 -0.04422938 ## e 0.06616623 0.17696554 -0.08141111 0.17781482 0.96231977 And now the corresponding values returned by eigenanaysis of the covariance matrix generated from the abundance data: eig.abundance &lt;- eigen(cov(abundance.only)) eig.abundance$vectors # compare to rotation matrix of PCA ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.81064462 -0.07052882 0.53108427 0.18442140 -0.14771336 ## [2,] 0.51264394 0.27799671 -0.47711910 -0.63418946 0.17342177 ## [3,] -0.16235135 0.88665551 0.40897655 -0.01149647 0.14173943 ## [4,] 0.22207108 0.31665237 -0.56250980 0.72941223 -0.04422938 ## [5,] 0.06616623 -0.17696554 0.08141111 0.17781482 0.96231977 sqrt(eig.abundance$values) # compare to sdev of PCA ## [1] 14.865306 8.814912 6.219250 5.034774 3.482308 Notice that the rotation object returned by the prcomp() represents the scaled eigenvectors (scaled to have length 1). The standard deviations of the PCA are the square roots of the eigenvalues of the covariance matrix. 13.4.3 Calculating Factor Loadings Let’s calculate the “factor loadings” associated with the PCs: V &lt;- abundance.pca$rotation # eigenvectors L &lt;- diag(abundance.pca$sdev) # diag mtx w/sqrts of eigenvalues on diag. abundance.loadings &lt;- V %*% L abundance.loadings ## [,1] [,2] [,3] [,4] [,5] ## a 12.0504801 0.6217053 -3.3029460 0.92852016 -0.5143835 ## b 7.6206090 -2.4505164 2.9673232 -3.19300085 0.6039081 ## c -2.4134024 -7.8157898 -2.5435276 -0.05788214 0.4935804 ## d 3.3011545 -2.7912626 3.4983893 3.67242602 -0.1540203 ## e 0.9835813 1.5599356 -0.5063161 0.89525751 3.3510942 The magnitude of the factor loadings is what you want to focus on. For example, species a and b contribute most to the first PC, while species c has the largest influence on PC2. You can think of the factor loadings, as defined above, as the components (i.e lengths of the projected vectors) of the original variables with respect to the PC basis vectors. Since vector length is proportional to the standard deviation of the variables they represent, you can think of the loadings as giving the standard deviation of the original variables with respect the PC axes. This implies that the loadings squared sum to the total variance in the original data, as illustrated below. sum(abundance.loadings**2) ## [1] 374.8345 abundance.only %&gt;% purrr::map_dbl(var) %&gt;% sum ## [1] 374.8345 13.5 Drawing Figures to Represent PCA 13.5.1 PC Score Plots The simplest PCA figure is to depict the PC scores, i.e. the projection of the observations into the space defined by the PC axes. Let’s make a figure with three subplots, depicting PC1 vs PC2, PC1 vs PC3, and PC2 vs. PC3. pca.scores.df &lt;- as.data.frame(abundance.pca$x) coord.system &lt;- coord_fixed(ratio=1, xlim=c(-30,30),ylim=c(-30,30)) pc1v2 &lt;- pca.scores.df %&gt;% ggplot(aes(x = PC1, y= PC2)) + geom_point() + coord.system pc1v3 &lt;- pca.scores.df %&gt;% ggplot(aes(x = PC1, y= PC3)) + geom_point() + coord.system pc2v3 &lt;- pca.scores.df %&gt;% ggplot(aes(x = PC2, y= PC3)) + geom_point() + coord.system cowplot::plot_grid(pc1v2, pc1v3, pc2v3, align=&quot;hv&quot;) When plotting PC scores, it is very important to keep the aspect ratio with a value of 1, so that the distance between points in the plot is an accurate representation of the distance in the PC space. Note too that I used the xlim and ylim arguments to keep the axis limits the same in all plots; comparable scaling of axes is important when comparing plots. Also note the use of the align=\"hv\" argument to plot_grid() to keep my plots the same size when combining them into a single figure. 13.5.2 Simultaneous Depiction of Observations and Variables in the PC Space Let’s return to our simple PC score plot. As we discussed above, the loadings are components of the original variables in the space of the PCs. This implies we can depict those loadings in the same PC basis that we use to depict the scores. First let’s create a data frame with the loadings from the first two PCs as well as a column representation the variable names: loadings.1and2 &lt;- data.frame(abundance.loadings[,1:2]) %&gt;% rename(PC1.loading = X1, PC2.loading = X2) %&gt;% mutate(variable = row.names(abundance.loadings)) loadings.1and2 ## # A tibble: 5 × 3 ## PC1.loading PC2.loading variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 12.1 0.622 a ## 2 7.62 -2.45 b ## 3 -2.41 -7.82 c ## 4 3.30 -2.79 d ## 5 0.984 1.56 e With this data frame in hand, we can now draw a set of vector to represent the original variables projected into the subspace defined by PCs 1 and 2: pc1v2.biplot &lt;- pc1v2 + geom_segment(data=loadings.1and2, aes(x = 0, y = 0, xend = PC1.loading, yend = PC2.loading), color=&#39;red&#39;, arrow = arrow(angle=15, length=unit(0.1,&quot;inches&quot;))) + geom_text(data=loadings.1and2, aes(x = PC1.loading, y = PC2.loading, label=variable), color=&#39;red&#39;, nudge_x = 1, nudge_y = 1) pc1v2.biplot The figure above is called a “biplot”“, as it simultaneously depicts both the observations and variables in the same space. From this biplot we can immediately see that variable a is highly correlated with PC1, but only weakly associated with PC2. Conversely, variable c is strongly correlated with PC2 but only weakly so with PC1. We can also approximate the correlations among the variables themselves – for example b and d are fairly strongly correlated, but weakly correlated with c. Keep in mind however that with respect to the relationships among the variables, this visualization is a 2D projection of a 5D space so the geometry is approximate. The biplot is a generally useful tool for multivariate analyses and there are a number of different ways to define biplots. We’ll study biplots more formally in a few weeks after we’ve covered singular value decomposition. "],["canonical-variates-analysis.html", "Chapter 14 Canonical Variates Analysis 14.1 Libraries 14.2 Discriminant Analysis in R 14.3 Estimating confidence regions for group means in CVA 14.4 Calculating the Within and Between Group Covariance Matrices", " Chapter 14 Canonical Variates Analysis Canonical Variates Analysis (CVA) is also referred to in the literature as “Linear Discrimination Analysis” (LDA). Confusingly, there is also a technique usualled called Canonical Correlation Analysis that is sometimes referred to as “Canonical Variates Analysis” in the literature. Canonical variate analysis is used for analyzing group structure in multivariate data. Canonical variate axes are directions in multivariate space that maximally separate (discriminate) the pre-defined groups of interest specified in the data. Unlike PCA, canonical variate axes are not, in general, orthogonal in the space of the original variables. 14.1 Libraries library(tidyverse) library(cowplot) library(broom) library(magrittr) 14.2 Discriminant Analysis in R The function lda(), found in the R library MASS, carries out linear discriminant analysis (i.e. canonical variates analysis). library(MASS) lda.iris &lt;-lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, iris, prior=c(1,1,1)/3) lda.iris ## Call: ## lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, ## data = iris, prior = c(1, 1, 1)/3) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 5.006 3.428 1.462 0.246 ## versicolor 5.936 2.770 4.260 1.326 ## virginica 6.588 2.974 5.552 2.026 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 0.8293776 0.02410215 ## Sepal.Width 1.5344731 2.16452123 ## Petal.Length -2.2012117 -0.93192121 ## Petal.Width -2.8104603 2.83918785 ## ## Proportion of trace: ## LD1 LD2 ## 0.9912 0.0088 When using lda() we specify a formula, with the grouping variable on the left and the quantitative variables on which you want to bases the discriminant axes, on the left. The prior argument given in the lda() function call isn’t strictly necessary because by default the lda() function will assign equal probabilities among the groups. However I included this argument call to illustrate how to change the prior if you wanted. The output give some simple summary statistics for the group means for each of the variables and then gives the coefficients of the canonical variates. The `Proportion of trace’ output above tells us that 99.12% of the between-group variance is captured along the first discriminant axis. 14.2.1 Shorthand Formulae in R You’ve encountered the use of model formulae in R throughout the course. Relevant to our current example is a shorthand way for specifying multiple variables in a formula. In the example above we called the |lda()| function with a formula of the form: Species ~ Sepal.Length + Sepal.Width + .... Writing the names of all those variables is tedious and error prone and would be unmanageable if we were analyzing a data set with tens or hundreds of variables. Luckily we can use the shorthand name . to specify all other variables in the data frame except the variable on the left. For example, we can rewrite the lda() call above as: iris.lda &lt;- lda(Species ~ ., data = iris) 14.2.2 Working with the output of lda() The object returned by lda() is of class “lda” with a number of components (see ?lda for details): class(iris.lda) ## [1] &quot;lda&quot; names(iris.lda) ## [1] &quot;prior&quot; &quot;counts&quot; &quot;means&quot; &quot;scaling&quot; &quot;lev&quot; &quot;svd&quot; &quot;N&quot; ## [8] &quot;call&quot; &quot;terms&quot; &quot;xlevels&quot; The scaling component gives the coefficients of the CVA that we can use to calculate the “scores” of the observations in the space of the canonical variates. iris.lda$scaling ## LD1 LD2 ## Sepal.Length 0.8293776 0.02410215 ## Sepal.Width 1.5344731 2.16452123 ## Petal.Length -2.2012117 -0.93192121 ## Petal.Width -2.8104603 2.83918785 The columns LD1 and LD2 give the coffiecients, \\(\\bf{a}\\), that we can use in the formula \\(\\bf{y}_\\text{discrim} = \\bf{Xa}\\) iris.sub &lt;- iris %&gt;% dplyr::select(-Species) %&gt;% # drop Species column as.matrix # cast to matrix for calculations # calculate CV scores CVA.scores &lt;- iris.sub %*% iris.lda$scaling # create data frame with scores iris.CV &lt;- data.frame(CVA.scores) iris.CV$Species &lt;- iris$Species Having calculated the CVA scores we can now generate a plot: iris.cva.plot &lt;- ggplot(iris.CV, aes(x = LD1, y = LD2)) + geom_point(aes(color=Species, shape=Species), alpha=0.5) + labs(x = &quot;CV1&quot;, y = &quot;CV2&quot;) + coord_fixed(ratio=1) # keep the unit scaling of the plot fixed at 1 iris.cva.plot Since most of the between group variation is captured by CV1, a density plot is an alternative in this case: ggplot(iris.CV, aes(x = LD1)) + geom_density(aes(color=Species)) + labs(x = &quot;CV1&quot;) The density plot of CV1 makes it clear how well the first canonical variate does in separating the three groups. 14.3 Estimating confidence regions for group means in CVA I stated in lecture that for the canonical variate diagram we can estimate the \\(100(1-\\alpha)\\) confidence region for a group mean as a circle centered at the mean having a radius \\((\\chi^{2}_{\\alpha,r}/n_i)^{1/2}\\) where \\(r\\) is the number of canonical variate dimensions considered. Using similar reasoning the \\(100(1-\\alpha)\\) “tolerance regions” for the whole population is given by a hypersphere centered at the mean with radius \\((\\chi^{2}_{\\alpha,r})^{1/2}\\). These tolerance regions are the regions in the CVA space where we expect approximately \\(100(1-\\alpha)\\) percent of samples belong to a given group to be found. To calculate these confidence regions you could look up the appropriate value of the the \\(\\chi^2\\) distribution in a book of statistical tables, or we can use the |qchisq()| function which gives the inverse cumulative probability distribution for the \\(\\chi^2\\) function: chi2 = qchisq(0.05,2, lower.tail=FALSE) chi2 ## [1] 5.991465 CIregions.mean.and.pop &lt;- iris.CV %&gt;% group_by(Species) %&gt;% summarize(CV1.mean = mean(LD1), CV2.mean = mean(LD2), mean.radii = sqrt(chi2/n()), popn.radii = sqrt(chi2)) CIregions.mean.and.pop ## # A tibble: 3 × 5 ## Species CV1.mean CV2.mean mean.radii popn.radii ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.50 6.88 0.346 2.45 ## 2 versicolor -3.93 5.93 0.346 2.45 ## 3 virginica -7.89 7.17 0.346 2.45 14.3.1 Drawing the CVA confidence regions Surprisingly, ggplot2 has no built-in functions for drawing circles, despite having geom_rect() and geom_polygon() functions. Instead we turn to a package called ggforce which provides a convenient geom_circle() function as well as a number of other useful extensions of ggplot. Install “ggforce” through the normal package installation mechanism and then load it. library(ggforce) We can then use ggforce::geom_circle() to draw confidence regions for the mean and population in our 2D CVA plot: iris.cva.plot2 &lt;- iris.cva.plot + geom_circle(data = CIregions.mean.and.pop, mapping = aes(x0 = CV1.mean, y0 = CV2.mean, r = mean.radii), inherit.aes = FALSE) + geom_circle(data = CIregions.mean.and.pop, mapping = aes(x0 = CV1.mean, y0 = CV2.mean, r = popn.radii), linetype = &quot;dashed&quot;, inherit.aes = FALSE) iris.cva.plot2 Let’s put the finishing touch on our plots by adding some color coded rug plots to the first CV axis. iris.cva.plot2 + geom_rug(aes(color=Species), sides = &quot;b&quot;) 14.4 Calculating the Within and Between Group Covariance Matrices The lda() function conveniently carries out the key steps of a canonical variates analysis for you. However, what if we wanted some of the intermediate matrices relevant to the analysis such as the within- and between group covariances matrices? The code below shows you how to calculate these: nobs &lt;- nrow(iris) ngroups &lt;- nlevels(iris$Species) # calculate deviations around grand mean tot.deviates &lt;- iris %&gt;% dplyr::select(-Species) %&gt;% # review the course notes on dplyr to remind # yourself about how the mutate_all() and funs() fxns work mutate_all(funs(. - mean(.))) %&gt;% as.matrix # Total SSQ and covariance matrix ssq.tot &lt;- t(tot.deviates) %*% tot.deviates cov.tot &lt;- ssq.tot/nobs # calculate deviations around group means win.deviates &lt;- iris %&gt;% group_by(Species) %&gt;% mutate_all(funs(. - mean(.))) %&gt;% ungroup %&gt;% dplyr::select(-Species) %&gt;% as.matrix ## `mutate_all()` ignored the following grouping variables: ## • Column `Species` ## ℹ Use `mutate_at(df, vars(-group_cols()), myoperation)` to ## silence the message. # Within group SSQ and covariance ssq.win &lt;- t(win.deviates) %*% win.deviates cov.win &lt;- ssq.win/(nobs - ngroups) # Between group deviates btw.deviates &lt;- iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) %&gt;% dplyr::select(-Species) %&gt;% mutate_all(funs(. - mean(.))) %&gt;% as.matrix # Between group SSQ and covariance ssq.btw &lt;- ngroups * t(btw.deviates) %*% btw.deviates cov.btw &lt;- ssq.btw/(ngroups-1) 14.4.1 Recapitulating the CVA analysis of lda() If we wanted to recapitulate the calculations that the lda() function carries out, we can do so based on the within- and between-group covariance matrices we estimated in the previous code block: # Cacluate the eigenvectors of W^{-1}B WinvB = solve(cov.win) %*% cov.btw eigin.WinvB = eigen(WinvB) cva.vecs &lt;- Re(eigin.WinvB$vectors)[,1:ngroups-1] cva.vals &lt;- Re(eigin.WinvB$values)[1:ngroups-1] unscaled.scores &lt;- win.deviates %*% cva.vecs # figure out scaling so group covariance matrix is spherical scaling &lt;- diag(1/sqrt((t(unscaled.scores) %*% unscaled.scores)/(nobs-ngroups))) # compare to &quot;scaling&quot; component object returned by lda() scaled.cva.vecs &lt;- cva.vecs %*% diag(scaling) cva.scores &lt;- iris.sub %*% scaled.cva.vecs colnames(cva.scores) &lt;- c(&quot;CV1&quot;,&quot;CV2&quot;) cva.scores &lt;- as.data.frame(cva.scores) cva.scores$Species &lt;- iris$Species Let’s plot the set of CVA scores that we calculated “by hand” to visually confirm our analysis produced similar results to the lda() function: ggplot(cva.scores, aes(x = CV1, y = CV2)) + geom_point(aes(color=Species, shape=Species)) + coord_fixed(ratio=1) Note that the CVA ordination above is “flipped” left-right relative to our earlier CVA figures. Canonical variates, like principal components, are identical with respect to reflection. "],["clustering-in-r.html", "Chapter 15 Clustering in R 15.1 Libraries 15.2 Data set 15.3 Hierarchical Clustering in R 15.4 Manipulating hierarchical clusterings with dendextend 15.5 Plotting dendrograms in dendextend 15.6 Cutting dendrograms 15.7 Looking at clusters 15.8 Generating a heat map from a cluster 15.9 Working with sub-trees 15.10 Setting dendrogram parameters in dendextend 15.11 Combining heatmaps and dendrograms 15.12 K-means/K-medoids Clustering in R 15.13 Combining clusters and correlation matrix heatmaps 15.14 Depicting the data within clusters", " Chapter 15 Clustering in R 15.1 Libraries library(tidyverse) library(RColorBrewer) 15.2 Data set To illustrate clustering method, we’ll use a subset of the Spellman et al. gene expression data set we introduced in the Data Wrangling chapter. Recall that, Spellman and colleagues tried to identify all the genes in the yeast genome (&gt;6000 genes) that exhibited oscillatory behaviors suggestive of cell cycle regulation. To do so, they measured gene expression over time, using six different types of cell cycle synchronization experiments. Rather than working with the whole genome, we’re going to focus on clustering ~700 genes that Spellman and colleagues identified as oscillatory. This data set can be loaded from this link. spellman &lt;- read_csv(&quot;https://github.com/Bio723-class/example-datasets/raw/master/spellman-wide.csv&quot;) dim(spellman) ## [1] 73 726 This data is provided in a tidy “wide” format with genes in columns. The first two columns give the experiment name and the corresponding time point at which the sample was collected. Let’s confirm that the case by looking at the first few rows and columns of the data: spellman[1:5,1:8] ## # A tibble: 5 × 8 ## expt time YAL022C YAL040C YAL053W YAL067C YAR003W YAR007C ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alpha 0 -0.36 1.04 0.21 0.01 -0.3 -0.48 ## 2 alpha 7 -0.42 0.19 -0.2 0.07 -0.45 -0.42 ## 3 alpha 14 0.29 0.47 0.2 0.17 0.75 0.87 ## 4 alpha 21 -0.14 -1.03 0.15 NA 0.37 0.92 ## 5 alpha 28 -0.19 -0.63 0.38 -0.1 0.27 0.67 15.3 Hierarchical Clustering in R Hierarchical clustering in R can be carried out using the hclust() function. The method argument to hclust determines the group distance function used (single linkage, complete linkage, average, etc.). The input to hclust() is a dissimilarity matrix. The function dist() provides some of the basic dissimilarity measures (e.g. Euclidean, Manhattan, Canberra; see method argument of dist) but you can convert an arbitrary square matrix to a distance object by applying the as.dist function to the matrix. We’re primarily interested in clustering the variables of our data set – genes – in order to discover what sets of gene are expressed in similar patterns (motivated by the idea that genes that are expressed in a similar manner are likely regulated by the same sets of transcription factors). So we need an appropriate similarity/dissimilarity measure for variables. The correlation coefficient is a suitable measure of linear association between variables. Correlations range from 1 for perfectly correlated variables to -1 for anti-correlated variables. Uncorrelated variables have values near zero. Correlation is a measure of similarity so we’ll turn it to into a measure of dissimilarity before passing it to the as.dist function. spellman.cor &lt;- dplyr::select(spellman, -time, -expt) %&gt;% cor(use=&quot;pairwise.complete.obs&quot;) spellman.dist &lt;- as.dist(1 - spellman.cor) The use argument to the cor function specifies that when there are missing values, the function should use all the available observations that have data necessary to calculate a correlation for any given pair of genes. We then turn the correlation into a distance measure by subtracting it from 1 (so perfectly positively correlated variables have distance 0) and passing it to the as.dist function. We’re now ready to put the hclust function to use. We first generate the hierarchical clustering, use the “complete linkage” method (see lecture slides): spellman.tree &lt;- hclust(spellman.dist, method=&quot;complete&quot;) Having generated the tree object, we can plot it using the multipurpose plot() function (Note that plot() is part of the base R graphics package, and hence unrelated to ggplot): plot(spellman.tree) Ugh - that’s an ugly plot! One major problem is that the text labels at the bottom of the tree are too large, and they’re overlapping each other. We can tweak that a little by changing the text size with the cex parameter. plot(spellman.tree, cex=0.2) That’s a little better, but we’re going to need some additional tools to wrangle this dendrogram into a more usable state. 15.4 Manipulating hierarchical clusterings with dendextend To work with and manipulate hierarchical clusterings and to create nicer dendrograms we’re going to use a package called dendextend. Install dendextend via one of the standard package installation mechanisms before proceeding. library(dendextend) First we’ll create a dendrogram object from our clustering tree, and use some dendextend features to examine a few of the properties of the dendrogram. spellman.dend &lt;- as.dendrogram(spellman.tree) # create dendrogram object dendextend includes a number of functions for examing the tree. For example, to examine the number of “leaves” (= # of genes we clustered) or nodes (= # of leaves + number of internal joins) in the tree we can do the following: nleaves(spellman.dend) # number of leaves in tree ## [1] 724 nnodes(spellman.dend) # number of nodes (=leaves + joins) in tree ## [1] 1447 15.5 Plotting dendrograms in dendextend The plot function for dendextend dendrogram objects (see ?plot.dendrogram) has a number of additional parameters that allows us to tweak the plot. For example, for large dendrograms it often makes sense to remove the leaf labels entirely as they will often be too small to read. This can be accomplished using the leaflab argument: plot(spellman.dend, leaflab = &quot;none&quot;) 15.6 Cutting dendrograms When looking at the figure we just generated it looks like there may be four major clusters. We’ll use the cutree function to cut the tree into four pieces and examine the implied clustering (note that the cutree function can also be used to cut the tree at a specified height). clusters &lt;- cutree(spellman.dend, k=4) table(clusters) ## clusters ## 1 2 3 4 ## 189 144 227 164 When we cut the tree we got four clusters, whose size is given by the table command above. If you examine the cutree documentation (reminder: use ?cutree from the command line) you will see that it returns a vector of integers, giving the corresponding cluster to which each variable (gene) is assigned. The code below shows the cluster assignments for the first six genes. clusters[1:6] ## YAL022C YAL040C YAL053W YAL067C YAR003W YAR007C ## 1 2 3 4 3 3 Next let’s create a nicer plot that highlights each of the clusters. The function color_branches() does essentially the same thing as cutree but it returns information that the plot function can use to appropriately color branches of the tree according to cluster membership. plot(color_branches(spellman.dend, k=4),leaflab=&quot;none&quot;) Now we’re getting somewhere! However, our clusters are still pretty big. Let’s check out the clusterings we get when we cut with eight clusters rather than four. plot(color_branches(spellman.dend, k=8),leaflab=&quot;none&quot;) And here are the number of genes in each cluster: clusters &lt;- cutree(spellman.dend, k=8, order_clusters_as_data = FALSE) table(clusters) ## clusters ## 1 2 3 4 5 6 7 8 ## 164 169 58 81 108 74 16 54 15.7 Looking at clusters To further explore the clusters, let’s create a data frame that holds the information about genes and their cluster assignments: clusters.df &lt;- data.frame(gene = names(clusters), cluster = clusters) Having created this data frame, it’s straightforward to lookup the cluster to which a gene belongs: clusters.df[&quot;YAL022C&quot;,] ## # A tibble: 1 × 2 ## gene cluster ## &lt;chr&gt; &lt;int&gt; ## 1 YAL022C 5 or to get all the names of genes in a given cluster: cluster3.genes &lt;- filter(clusters.df, cluster == 3)$gene cat(as.character(cluster3.genes[1:10]), quote=FALSE,sep=&quot;\\n&quot;); ## YEL068C ## YOR242C ## YKL172W ## YBR087W ## YJR043C ## YLR154C ## YGR276C ## YGR042W ## YPL232W ## YOR355W ## FALSE Note the use of the cat function to print out a list of the gene names for cluster 7, with the names separated by returns (\"\\n\"). This is useful if you want to cut and paste the gene names into a document, or an online analysis tool such as various Gene Ontology (GO) browsers (we’ll talk about these in a later class session). 15.8 Generating a heat map from a cluster Let’s generate a heat map showing the expression of all the genes in the alpha factor experiment for the first cluster that we found above. To generate this visualization it will be convenient to work with the data in a tidy long format, so we use dplyr::gather to restructure the data first: spellman.long &lt;- gather(spellman, gene, expression, -time, -expt) head(spellman.long) ## # A tibble: 6 × 4 ## expt time gene expression ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 alpha 0 YAL022C -0.36 ## 2 alpha 7 YAL022C -0.42 ## 3 alpha 14 YAL022C 0.29 ## 4 alpha 21 YAL022C -0.14 ## 5 alpha 28 YAL022C -0.19 ## 6 alpha 35 YAL022C -0.52 Having restructured the data we can now generate our desired heat map: color.scheme &lt;- rev(brewer.pal(8,&quot;RdBu&quot;)) # generate the color scheme to use spellman.long %&gt;% filter(gene %in% cluster3.genes &amp; expt == &quot;alpha&quot;) %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme, limits = c(-2,2)) + theme(axis.text.y = element_text(size = 6)) # set size of y axis labels 15.9 Working with sub-trees The cutree() function illustrated above gives us the groupings implied by cutting the tree at a certain height. However, it does not explicitly return objects representing the sub-trees themselves. If you want to do computations or generate figures of the sub-trees, you’ll need to use the cut() function. # note that I determined the height to cut at by looking at the colored dendrogram # plot above for 8 clusters sub.trees &lt;- cut(spellman.dend, h = 1.48) The cut function returns multiple sub-trees designated upper and lower. The upper tree is the tree “above” the cut, while the multiple “lower” trees represent the disconnected sub-trees “below” the cut. For purposes of clustering you usually are most interested in the sub-trees (clusters) below the cut. sub.trees$lower ## [[1]] ## &#39;dendrogram&#39; with 2 branches and 164 members total, at height 1.469983 ## ## [[2]] ## &#39;dendrogram&#39; with 2 branches and 169 members total, at height 1.178098 ## ## [[3]] ## &#39;dendrogram&#39; with 2 branches and 58 members total, at height 1.32836 ## ## [[4]] ## &#39;dendrogram&#39; with 2 branches and 81 members total, at height 1.231597 ## ## [[5]] ## &#39;dendrogram&#39; with 2 branches and 108 members total, at height 1.366951 ## ## [[6]] ## &#39;dendrogram&#39; with 2 branches and 74 members total, at height 1.463623 ## ## [[7]] ## &#39;dendrogram&#39; with 2 branches and 16 members total, at height 1.041689 ## ## [[8]] ## &#39;dendrogram&#39; with 2 branches and 54 members total, at height 1.188419 We can retrieve any particular sub-tree by indexing into the list: cluster3.tree &lt;- sub.trees$lower[[3]] cluster3.tree ## &#39;dendrogram&#39; with 2 branches and 58 members total, at height 1.32836 nleaves(cluster3.tree) ## [1] 58 15.10 Setting dendrogram parameters in dendextend dendextend has a generic function – set() – for changing the parameters associated with dendrograms. The basic form of the function is set(object, what, value), where object is the dendrogram you’re working with, what is a character string indicating the parameter you want to change, and value is the setting you wishing to assign to that parameter. A full list of dendrogram parameters that can be changed is provided in the dendextend documentation. We’ll use the set function to make a nice diagram of our cluster 3 sub-tree: cluster3.tree %&gt;% set(&quot;labels_cex&quot;, 0.45) %&gt;% set(&quot;labels_col&quot;, &quot;red&quot;) %&gt;% plot(horiz = TRUE) # plot horizontally 15.11 Combining heatmaps and dendrograms A common visualization used in transcriptome studies is to combine dendrograms and heatmaps. To do this with a minimum of fuss we’ll use a package called “gplots” which includes a heatmap function that will also plot dendrograms next. Install “gplots” via the standard package installation mechanism. library(gplots) The gplots function we will use is called heatmap.2. This function requires as input our data in the form of a matrix. If you provide no other information heatmap.2 will carry out clustering for you, clustering both the rows and columns of the data matrix. However, here we want to draw a dendrogram (representing similarity among variables) we’ve already calculated, and to create a heat map just for the alpha factor data, so we need to do some pre-calculations and tweak the heatmap.2 arguments: # subset out the alpha factor data alpha.factor &lt;- filter(spellman, expt == &quot;alpha&quot;) # create matrix after dropping time and expt columns alpha.mtx &lt;- as.matrix(dplyr::select(alpha.factor, -time, -expt)) # drop time, expt columns # set row names to corresponding time points for nice plotting row.names(alpha.mtx) &lt;- alpha.factor$time # transpose the matrix so genes are drawn in rows transposed.alpha.mtx &lt;- t(alpha.mtx) Having defined the data we want to plot in the heatmap we can now use heatmap.2 as follows # this is a large figure, so if working in RMarkdown document I suggest specifying # the code block header as so to make the figure large # {r, fig.width = 8, fig.height = 8} heatmap.2(transposed.alpha.mtx, Rowv = cluster3.tree, # use the dendrogram previously calculated Colv = NULL, # don&#39;t mess with my columns! (i.e. keep current ordering ) dendrogram = &quot;row&quot;, # only draw row dendrograms breaks = seq(-2, 2, length.out = 9), # OPTIONAL: set break points for colors col = color.scheme, # use previously defined colors trace = &quot;none&quot;, density.info = &quot;none&quot;, # remove distracting elements of plot xlab = &quot;Time (mins)&quot;) 15.12 K-means/K-medoids Clustering in R The kmeans() function calculates standard k-means clusters in R. However, we’re actually going to use a related function that calculates “k-medoids” clustering. K-medoids clustering differs from k-means in that the objective function is to minimize the sum of dissimilarities from the cluster centers (“medoids”) rather then the sum of squared distances. K-medoids clustering tends to be more robust to outliers than K-means. Another advantage for our purposes is that the k-medoids algorithm, unlike the standard implementation of k-means, can accept a distance or dissimilarity matrix as input. K-medoids clustering is implemented in the function pam() (Partitioning Around Medoids), which is found in a package called cluster that is included with the standard R installation. library(cluster) spellman.kmedoids &lt;- pam(spellman.dist, 8) # create k-medoids clustering with 8 clusters kclusters &lt;- spellman.kmedoids$cluster table(kclusters) ## kclusters ## 1 2 3 4 5 6 7 8 ## 121 104 70 95 85 94 65 90 K-medoids (or K-means) will, in general, find different clusterings than our hierarchical clustering illustrated previously. For comparison with our earlier hierarchical clustering results, lets plot the k-medoids inferred clusters back onto our earlier dendrogram. # reorder genes so they match the dendrogram kclusters.reordered &lt;- kclusters[order.dendrogram(spellman.dend)] # get branch colors so we&#39;re using the same palette dend.colors &lt;- unique(get_leaves_branches_attr(color_branches(spellman.dend, k=8), attr=&quot;col&quot;)) # color the branches according to their k-means cluster assignment plot(branches_attr_by_clusters(spellman.dend, kclusters.reordered , dend.colors),leaflab=&quot;none&quot;) Comparing the inferred k-medoids clustering to our previous complete linkage clustering we see some clusters that are similar between the two, but there are also significant differences. 15.12.1 Heat map from k-medoids cluster In the same manner we generated a heat map for one of the hierarchical clustering sub-trees, we can generate a similar heat map for a k-medoids cluster. Let’s examine cluster 4: kcluster4.genes &lt;- names(kclusters[kclusters == 4]) spellman.long %&gt;% filter(gene %in% kcluster4.genes &amp; expt == &quot;alpha&quot;) %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme, limits=c(-2,2)) + labs(title = &quot;K-medoids Clustering of Spellman et al. Data\\nCluster 4&quot;) + theme(axis.text.y = element_text(size = 6)) # set size of y axis labels 15.13 Combining clusters and correlation matrix heatmaps How do we know if we have a sensible clustering? For example, above we illustrated how to use complete linkage clustering and the k-medoids algorithm to produce two different clusterings of the same data. We produce a result with 8 clusters for each, but there were significant differences in terms of the membership of genes in each cluster. Which one of these methods produced a “better” result for the data in hand? There are many criteria – biological, mathematical, computational – one might apply to answer such a question, but for now let’s consider the idea that a good clustering is one that produces “natural groups” in data. In the lecture slides , I proferred the following “common sense” definition of natural groups: Natural Groups Groups of objects where the similarity between objects is higher within groups than between groups. How might we evaluate a proposed clustering with respect to this definition of natural groups? One way to do so is the examine our matrix of similarity according to the implied clusters to see if the clusters are consistent with high within group similarity and low between group similarity. In the clustering applications we’ve looked at so far, our measure of similarity has been based on correlations. We’ll first look at the “raw” correlation matrix, unsorted with respect to the implied clusterings, and then we’ll take a look at the correlation matrix sorted by hierarchical clustering and then k-medoids clustering. We can use the heatmap.2() function to visualize the correlation matrix. color.scheme &lt;- rev(brewer.pal(10,&quot;RdBu&quot;)) # generate the color scheme to use heatmap.2(spellman.cor, Rowv = NULL, # don&#39;t cluster rows Colv = NULL, # don&#39;t cluster columns dendrogram = &quot;none&quot;, # don&#39;t draw dendrograms trace = &quot;none&quot;, density.info = &quot;none&quot;, col = color.scheme, keysize = 1, labRow = FALSE, labCol = FALSE, xlab = &quot;genes&quot;, ylab = &quot;genes&quot;) A correlation matrix is a square symmetric matrix. The dark red line down the diagonal represents correlations of genes with themselves (i.e. perfectly correlated). Off diagonal elements range from blue (negative correlations) to gray (near zero correlations) to red (positive correlations). 15.13.1 Hierarchical clustering, visualized on correlation matrix Now let’s re-generate the heatmap of the correlation matrix with the genes sorted by our hierarchical clustering: heatmap.2(spellman.cor, Rowv = ladderize(spellman.dend), Colv = ladderize(spellman.dend), dendrogram = &quot;both&quot;, revC = TRUE, # rev column order of dendrogram so conforms to natural representation trace = &quot;none&quot;, density.info = &quot;none&quot;, col = color.scheme, key = FALSE, labRow = FALSE, labCol = FALSE) This reordered correlation matrix (and accompanying dendrograms) are useful for a number of purposes. First, the block structure along the diagonal indicates the correlation structure within the implied clusters. Uniformity in sign and magnitude of the on-diagongal blocks is an indicator of “within cluster” similarity. The off-diagonal blocks indicate the relationship between clusters. For example, the four “major” clusterings implied by the dendrogram are apparent in the block structure of the correlation matrix, but we also see that the two clusters at the bottom of the figure show some patches of weak or even negative correlations, suggesting that those may not be natural clusters. Also apparent are relationships between clusters – for example we see that genes in the first cluster (upper left of diagram) share strong positive correlations with many genes in the right half of the second cluster. The same relationship, though to a lesser extant, is apparent between clusters one and four. 15.13.2 K-medoids clustering visualized on the correlation matrix Let’s do a similar sorting of the correlation matrix based on k-medoids clustering. Since there is no dendrogram to pass to the heatmap.2 function, we’ll sort the correlation matrix ourselves by indexing on the output of the order function applied to the cluster assignments (read the order function help for more info). # reorder correlation matrix by ordering given by clustering kmedoids.cor &lt;- spellman.cor[order(kclusters), order(kclusters)] Having reordered the correlation matrix by the k-clusters, we can again use the heatmap.2 function to visualize the results. heatmap.2(kmedoids.cor, Rowv = NULL, Colv = NULL, dendrogram = &quot;none&quot;, trace = &quot;none&quot;, density.info = &quot;none&quot;, col = color.scheme, key = FALSE, labRow = FALSE, labCol = FALSE) The diagram has the same interpretation as the previous figure. However, here we notice that the clusters seem to be more internally consistent, as evidence by the greater within cluster uniformity of correlations. Positive and negative relationships between clusters are also readily apparent in this figure. 15.14 Depicting the data within clusters As a final step we’ll generate a figure depicting the time series behavior of genes in different clusters. Overlain over the individual gene expression time series, we’ll overlay the average time series for each cluster. I illustrate this with clusters generated by hierarchical clustering. This is easily adapted to clusterings generated by other methods. Create a data frame holding cluster membership for each gene. clusters.df &lt;- data.frame(gene = names(kclusters), cluster = as.factor(kclusters)) For the purposes of easy plotting in ggplot, we want to combine the information about cluster assignment to the original data. However the cluster assignments and corresponding names aren’t necessarily in the same order as our original data frame. We’ll use a “left join” to combine the two data frames, matching on the “gene” column: # do a left_join, combining the information in spellman.long # with clusters.df (matched on gene). This effectively adds the # cluster information as a new column to spellman.long data frame # keeping the appropriate matches by gene name alpha.long &lt;- spellman.long %&gt;% filter(expt == &quot;alpha&quot;) %&gt;% left_join(clusters.df, by = c(&quot;gene&quot;)) Next we calculate cluster means: # calculate the mean at each time point within each cluster cluster.means &lt;- alpha.long %&gt;% group_by(cluster, time) %&gt;% summarize(mean.exp = mean(expression, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;cluster&#39;. You can ## override using the `.groups` argument. And finally we plot the time series data for each cluster, with the clusters means overlain: # draw a figure showing time varying gene expression # in each cluster, overlain with the each clusters # mean time series alpha.long %&gt;% ggplot(aes(time, expression, group=gene)) + geom_line(alpha=0.25) + geom_line(aes(time, mean.exp, group=NULL,color=cluster), data = cluster.means, size=1.1) + ylim(-2.5, 2.5) + facet_wrap(~cluster, ncol=4) To my eye, oscillatory behavior is fairly apparent in most of the clusters except cluster 7. "],["non-linear-regression-models.html", "Chapter 16 Non-linear regression models 16.1 LOESS regression 16.2 Logistic regression", " Chapter 16 Non-linear regression models 16.1 LOESS regression LOESS (`Locally estimated scatterplot smoothing’, aka LOWESS; ‘Locally weighted scatterplot smoothing’) is a modeling technique that fits a curve (or surface) to a set of data using a large number of local linear regressions. Local weighted regressions are fit at numerous regions across the data range, using a weighting function that drops off as you move away from the center of the fitting region (hence the “local aspect). LOESS combines the simplicity of least squares fitting with the flexibility of non-linear techniques and doesn’t require the user to specify a functional form ahead of time in order to fit the model. It does however require relatively dense sampling in order to produce robust fits. Formally, at each point \\(x_i\\) we estimate the regression coefficients \\(\\hat{\\beta}_j(x)\\) as the values that minimize: \\[ \\sum_{k=1}^n w_k(x_i)(y_k - \\beta_0 - \\beta_1 x_k - \\ldots - \\beta_d x_k^2)^2 \\] where \\(d\\) is the degree of the polynomial (usually 1 or 2) and \\(w_k\\) is a weight function. The most common choice of weighting function is called the “tri-cube” function as is defined as: \\[\\begin{align*} w_k(x_i) &amp;= (1-|x_i|^3)^3, \\mbox{for}\\ |x_i| \\lt 1 \\\\ &amp;= 0, \\mbox{for}\\ |x_i| \\geq 1 \\end{align*}\\] where \\(|x_i|\\) is the normalized distance (as determined by the span parameter of the LOESS model) of the observation \\(x_i\\) from the focal observation \\(x_k\\). The primary parameter that a user must decide on when using LOESS is the size of the neighborhood function to apply (i.e. over what distance should the weight function drop to zero). This is referred to as the “span” in the R documentation, or as the parameter \\(\\alpha\\) in many of the papers that discuss LOESS. The appropriate span can be determined by experimentation or, more rigorously by cross-validation. We’ll illustrate fitting a Loess model using data on Barack Obama’s approval ratings over the period from 2008 to 2001 (obama-polls.txt). polls &lt;- read_delim(&#39;https://github.com/Bio723-class/example-datasets/raw/master/obama-polls-2008-2011.txt&#39;, delim=&quot;\\t&quot;, trim_ws=TRUE) # note that we needed to use &quot;trim_ws&quot; above because there were # some lurking spaces in the fields of that tab delimited data file head(polls) ## # A tibble: 6 × 6 ## Pollster Dates `N/Pop` Approve Disapprove Undecided ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Rasmussen 9/17-19/11 1500 LV 46 52 - ## 2 Rasmussen 9/14-16/11 1500 LV 45 55 - ## 3 Gallup 9/13-15/11 1500 A 39 52 - ## 4 CBS/Times 9/10-15/11 1452 A 43 50 7 ## 5 Marist/McClatchy 9/13-14/11 825 RV 39 52 9 ## 6 Rasmussen 9/11-13/11 1500 LV 45 54 - Notice that the Dates column is not very tidy. Each “date” is actually a range of dates of the form Month/DayStart-DayEnd/Year (e.g. “9/1/09” is September 01, 2009). Even nastier, some dates are in the form Month/Day/Year (only a single day) or MonthStart/DayStart-MonthEnd/DayEnd/Year (e.g. “2/26-3/1/11” is February 26,2011 to March 01, 2011) . Whoever formatted the data in this fashion must really hate tidy data! To deal with this nightmare we’re going to use the tidyr::extract() function to employ regular expressions (regex) to parse this complicated data field into it’s constituent parts. For more details on regular expression see the R Regular Expession Cheat Sheet and R for Data Science. polls &lt;- polls %&gt;% # first separate left most and right most fields as month and year respectively tidyr::extract(&quot;Dates&quot;, c(&quot;month&quot;, &quot;day.range&quot;, &quot;year&quot;), regex=&quot;(\\\\d+)/(.+)/(\\\\d+$)&quot;, convert = TRUE) %&gt;% # now deal with the complicated middle field. For simplicities sake we&#39;re just # going to focus on extracting the start day tidyr::extract(&quot;day.range&quot;, c(&quot;day.start&quot;, &quot;day.other&quot;), regex = &quot;(\\\\d+)(.+)&quot;, convert = TRUE) %&gt;% # finally convert YY to 20YY mutate(year = 2000 + year) head(polls) ## # A tibble: 6 × 9 ## Pollster month day.start day.o…¹ year `N/Pop` Approve Disap…² Undec…³ ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Rasmussen 9 17 -19 2011 1500 LV 46 52 - ## 2 Rasmussen 9 14 -16 2011 1500 LV 45 55 - ## 3 Gallup 9 13 -15 2011 1500 A 39 52 - ## 4 CBS/Times 9 10 -15 2011 1452 A 43 50 7 ## 5 Marist/McClatchy 9 13 -14 2011 825 RV 39 52 9 ## 6 Rasmussen 9 11 -13 2011 1500 LV 45 54 - ## # … with abbreviated variable names ¹​day.other, ²​Disapprove, ³​Undecided For the next steps we’ll need the lubridate library (install if needed): library(lubridate) polls &lt;- polls %&gt;% mutate(date = make_date(year = year, month=month, day = day.start)) head(polls) ## # A tibble: 6 × 10 ## Polls…¹ month day.s…² day.o…³ year `N/Pop` Approve Disap…⁴ Undec…⁵ date ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; ## 1 Rasmus… 9 17 -19 2011 1500 LV 46 52 - 2011-09-17 ## 2 Rasmus… 9 14 -16 2011 1500 LV 45 55 - 2011-09-14 ## 3 Gallup 9 13 -15 2011 1500 A 39 52 - 2011-09-13 ## 4 CBS/Ti… 9 10 -15 2011 1452 A 43 50 7 2011-09-10 ## 5 Marist… 9 13 -14 2011 825 RV 39 52 9 2011-09-13 ## 6 Rasmus… 9 11 -13 2011 1500 LV 45 54 - 2011-09-11 ## # … with abbreviated variable names ¹​Pollster, ²​day.start, ³​day.other, ## # ⁴​Disapprove, ⁵​Undecided polls.plot &lt;- polls %&gt;% ggplot(aes(x = date, y = Approve)) + geom_point(alpha=0.5, pch=1) + labs(x = &quot;Date&quot;, y = &quot;Approval Rating&quot;, title = &quot;Barack Obama&#39;s Approval Ratings, 2008-2011&quot;) polls.plot We can fit the LOESS as so, and get back the predicted values using the predict() function: loess.approval &lt;- loess(Approve ~ as.numeric(date), data = polls) loess.predicted.values &lt;- predict(loess.approval) head(loess.predicted.values) ## [1] 44.55653 44.59349 44.60572 44.64216 44.60572 44.63006 Usually we’ll want to visualize the LOESS regression, which we can conveniently do with ggplot::geom_smooth without having to explicitly calculate the LOESS: polls.plot + geom_smooth(color=&#39;red&#39;, method=&quot;loess&quot;, se=FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Here’s the same data fit with a smaller span (the paramater that controls the “local neighborhood” size in LOESS): polls.plot + geom_smooth(color=&#39;red&#39;, method=&quot;loess&quot;, se=FALSE, span=0.1) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The high density of the polling justifies the smaller span, and the additional deviations apparent when the LOESS is fit with the smaller span likely reflect real world changes in approval, induced by a variety of political and other news events. For example, we can zoom in on 2011: polls.plot + geom_smooth(color=&#39;red&#39;, method=&quot;loess&quot;, se=FALSE, span=0.1) + coord_cartesian(xlim=c(ymd(20110101), ymd(20110901)), ylim=c(35,65)) + scale_x_date(date_breaks=&quot;1 month&quot;, date_label=&quot;%B&quot;) + labs(title=&quot;Barack Obama&#39;s Approval Ratings, Jan - Sep 2011&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Increased approval ratings in January coincide with the approval of a tax deal and a speech to the nation following the shooting of congresswoman Gabbie Giffords in Tuscson, AZ (https://www.cnbc.com/id/41139968). The spike apparent in early May coincides with the death of Osama Bin Laden. You might take a look at major policitcal events in other years to see if you can identify drivers behind other approval rating shifts. 16.2 Logistic regression Logistic regression is used when the dependent variable is discrete (often binary). The explanatory variables may be either continuous or discrete. Examples: Whether a gene is turned off (=0) or on (=1) as a function of levels of various proteins Whether an individual is healthy (=0) or diseased (=1) as a function of various risk factors. Whether an individual died (=0) or survived (=1) some selective event as a function of behavior, morphology, etc. We model the binary response variable, \\(Y\\), as a function of the predictor variables, \\(X_1\\), \\(X_2\\), etc as : \\[ P(Y = 1|X_1,\\ldots,X_p) = f(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p) \\] So we’re modeling the probability of the state of Y as a function of a linear combination of the predictor variables. For logistic regression, \\(f\\) is the logistic function: \\[ f(z) = \\frac{e^z}{1+e^z} = \\frac{1}{1 + e^{-z}} \\] Therefore, the bivariate logistic regression is given by: \\[ P(Y = 1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}} \\] Note that \\(\\beta_0\\) here is akin to the intercept in our standard linear regression. 16.2.1 A web app to explore the logistic regression equation To help you develop an intuition for the logistic regression equation, I’ve developed a small web app, that allows you to explore how the shape of the regression curve responds to changes in the regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\). Open the app in another browser window and play with the sliders that control the coeffients \\(B_0\\) and \\(B_1\\). In the assignment associated with today’s class you’ll be asked to answer some specific questions based on this app. 16.2.2 Titanic data set titanic.csv contains information about passengers on the Titanic. Variables in this data set include information such as sex, age, passenger class (1st, 2nd, 3rd), and whether or not they survived the sinking of the ship (0 = died, 1 = survived). library(tidyverse) library(broom) library(cowplot) library(ggthemes) titanic &lt;- read_csv(&quot;http://bit.ly/bio304-titanic-data&quot;) names(titanic) ## [1] &quot;pclass&quot; &quot;survived&quot; &quot;name&quot; &quot;sex&quot; &quot;age&quot; &quot;sibsp&quot; ## [7] &quot;parch&quot; &quot;ticket&quot; &quot;fare&quot; &quot;cabin&quot; &quot;embarked&quot; &quot;boat&quot; ## [13] &quot;body&quot; &quot;home.dest&quot; 16.2.3 Subsetting the data We’ve all heard the phrase, “Women and children first”, so we might expect that the probability that a passenger survived the sinking of the Titanic is related to their sex and/or age. Let’s create separate data subsets for male and female passengers. male &lt;- filter(titanic, sex == &quot;male&quot;) female &lt;- filter(titanic, sex == &quot;female&quot;) 16.2.4 Visualizing survival as a function of age Let’s create visualizations of survival as a function of age for the male and female passengers. fcolor = &quot;lightcoral&quot; mcolor = &quot;lightsteelblue&quot; female.plot &lt;- ggplot(female, aes(x = age, y = survived)) + geom_jitter(width = 0, height = 0.05, color = fcolor) + labs(title = &quot;Female Passengers&quot;) male.plot &lt;- ggplot(male, aes(x = age, y = survived)) + geom_jitter(width = 0, height = 0.05, color = mcolor) + labs(title = &quot;Male Passengers&quot;) plot_grid(female.plot, male.plot) The jittered points with Y-axis value around one are passengers who survived, the point jittered around zero are those who died. 16.2.5 Fitting the logistic regression model The function glm (generalized linear model) can be used to fit the logistic regression model (as well as other models). Setting the argument family = binomial gives us logistic regression. Note that when fitting the model the dependent variable needs to be numeric, so if the data is provided as Boolean (logical) TRUE/FALSE values, they should be converted to integers using as.numeric(). First we fit the regression for the famale passengers. fit.female &lt;- glm(survived ~ age, family = binomial, female) tidy(fit.female) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.493 0.254 1.94 0.0523 ## 2 age 0.0225 0.00854 2.64 0.00834 The column “estimate” gives the coefficients of the model. The “intercept”” estimate corresponds to \\(B_0\\) in the logistic regression equation, the “age” estimate corresponds to the coefficient \\(B_1\\) in the equation. Now we repeat the same step for the male passengers. fit.male &lt;- glm(survived ~ age, family = binomial, male) tidy(fit.male) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.661 0.225 -2.94 0.00329 ## 2 age -0.0238 0.00728 -3.27 0.00109 Notice that the female coefficients are both positive, while the male coefficients are negative. We’ll visualize what this means in terms of the model below. 16.2.6 Visualizing the logistic regression To visualize the logistic regression fit, we first use the predict function to generate the model predictions about probability of survival as a function of age. ages &lt;- seq(0, 75, 1) # predict survival for ages 0 to 75 predicted.female &lt;- predict(fit.female, newdata = data.frame(age = ages), type = &quot;response&quot;) predicted.male &lt;- predict(fit.male, newdata = data.frame(age = ages), type = &quot;response&quot;) Having generated the predicted probabilities of survival we can then add these prediction lines to our previous plot using geom_line. female.logistic.plot &lt;- female.plot + geom_line(data = data.frame(age = ages, survived = predicted.female), color = fcolor, size = 1) male.logistic.plot &lt;- male.plot + geom_line(data = data.frame(age = ages, survived = predicted.male), color = mcolor, size = 1) plot_grid(female.logistic.plot, male.logistic.plot) We see that for the female passengers, the logistic regression predicts that the probability of survival increases with passenger age. In contrast, the model fit to the male passengers suggests that the probability of survival decreases with passenger age. For the male passengers, the data is consistent with “children first”; for female passengers this model doesn’t seem to hold. However, there are other factors to consider as we’ll see below. 16.2.7 Quick and easy visualization Here’s an alternative “quick and easy” way to generate the plot above using the awesome power of ggplot. The downside of this approach is we don’t generate the detailed information on the model, which is something you’d certainly want to have in any real analysis. ggplot(titanic, aes(x=age, y=survived, color=sex)) + geom_jitter(width = 0, height = 0.05) + geom_smooth(method=&quot;glm&quot;, method.args = list(family=&quot;binomial&quot;)) + labs(x = &quot;Age&quot;, y = &quot;P(Survival)&quot;) + facet_wrap(~ sex) + scale_color_manual(values = c(fcolor, mcolor)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 16.2.8 Impact of sex and passenger class on the models In our previous analysis we considered the relationship between survival and age, conditioned (facted) on passenger sex. In a complex data set like this one, it is often useful to condition on multiple variables simultaneously. Lets extend our visualization to look at the regression faceted on both class and sex, using facet_grid: ggplot(titanic, aes(x=age, y=survived, color=sex)) + geom_jitter(width = 0, height = 0.05) + geom_smooth(method=&quot;glm&quot;, method.args = list(family=&quot;binomial&quot;)) + labs(x = &quot;Age&quot;, y = &quot;P(Survival)&quot;) + facet_grid(pclass ~ sex) + scale_color_manual(values = c(fcolor, mcolor)) + theme_few() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Having conditioned on both sex and ticket class, our figure now reveals a much more complex relationship between age and survival. Almost all first class female passengers survived, regardless of age. For second calss female passengers, the logistic regression suggests a very modest decrease in survival with increasing age. The negative relationship between age and survival is stronger still for third class females. Male passengers on the other hand show a negative relationship between sex and survival, regardless of class, but the models suggest that there are still class specific differences in this relationship. 16.2.9 Fitting multiple models based on groupings use dplyr::do In the figure above we used ggplot and facet_grid to visualize logistic regression of survival on age, conditioned on both sex and class. What if we wanted to calculate the terms of the logistic regressions for each combination of these two categorical variables? There are three passenger classes and two sexes, meaning we’d have to create six data subsets and fit the model six times if we used the same approach we used previously. Luckily, dplyr provides a powerful function called do() that allows us to carry out arbitrary computations on grouped data. There are two ways to use do(). The first way is to give the expressions you evaluate in do() a name, in which case do() will store the results in a column. The second way to use do() is for the expression to return a data frame. In this first example, the model fits are stored in the fits column. When using do() you can refer to the groupings using a period (.): grouped.models &lt;- titanic %&gt;% group_by(sex, pclass) %&gt;% do(fits = glm(survived ~ age, family = binomial, data = .)) grouped.models ## # A tibble: 6 × 3 ## sex pclass fits ## &lt;chr&gt; &lt;dbl&gt; &lt;list&gt; ## 1 female 1 &lt;glm&gt; ## 2 female 2 &lt;glm&gt; ## 3 female 3 &lt;glm&gt; ## 4 male 1 &lt;glm&gt; ## 5 male 2 &lt;glm&gt; ## 6 male 3 &lt;glm&gt; Notice that the “fits” column doesn’t explicitly print out the details of the model. The object returned by glm() can’t be simply represented as text string (it’s a list), so we seea place holder string that tells us that there is data here represented a glm object. However, we can access the the columns with the fits just like any other variable: # get the summary of the second logistic regression (Female, 2nd Class) tidy(grouped.models$fits[[2]]) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.49 0.904 3.86 0.000112 ## 2 age -0.0450 0.0255 -1.77 0.0773 Now we illustrate the second approach to using do(). When no name is provided, do() expects its expression to return a dataframe. Here we use the broom::tidy() function to get the key results of each fit model into a data frame: titanic %&gt;% group_by(sex, pclass) %&gt;% do(tidy(glm(survived ~ age, family = binomial, data = .))) ## # A tibble: 12 × 7 ## sex pclass term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 1 (Intercept) 2.90 1.24 2.34 0.0193 ## 2 female 1 age 0.00960 0.0326 0.294 0.769 ## 3 female 2 (Intercept) 3.49 0.904 3.86 0.000112 ## 4 female 2 age -0.0450 0.0255 -1.77 0.0773 ## 5 female 3 (Intercept) 0.289 0.341 0.846 0.398 ## 6 female 3 age -0.0178 0.0136 -1.31 0.190 ## 7 male 1 (Intercept) 0.880 0.527 1.67 0.0951 ## 8 male 1 age -0.0374 0.0128 -2.93 0.00335 ## 9 male 2 (Intercept) 1.04 0.599 1.74 0.0818 ## 10 male 2 age -0.112 0.0249 -4.50 0.00000672 ## 11 male 3 (Intercept) -0.761 0.342 -2.23 0.0259 ## 12 male 3 age -0.0339 0.0134 -2.53 0.0113 Using this approach we get a nice data frame showing the logistic regression coefficients, and associated statistics (standard error, P-values, etc) for the regression of survival on age, for each combination of sex and class. "],["randomization-jackknife-and-bootstrap.html", "Chapter 17 Randomization, Jackknife, and Bootstrap 17.1 Libraries 17.2 Randomization 17.3 Using randomization to test for a difference in means 17.4 Using randomization to test for equality of variances 17.5 Jackknifing in R 17.6 Bootstrapping in R", " Chapter 17 Randomization, Jackknife, and Bootstrap 17.1 Libraries library(tidyverse) library(broom) set.seed(20180404) 17.2 Randomization There are a number of packages (e.g. coin) that include functions for carrying out randomization/permutation tests in R. However, it’s often just as easy to write a quick set of functions to carry out such tests yourself. We’ll illustrate a simple example of this using an example from Manly (2007). Consider the following data set composed of measures of mandible lengths (in mm) for male and female golden jackals. This set of measurements was taken from a set of skeletons in the collections of the British Museum of Natural History. Females: 110, 111, 107, 108, 110, 105, 107, 106, 111, 111 Males: 120, 107, 110, 116, 114, 111, 113, 117, 114, 112 Let’s first two vectors to represent this set of measurements, and then create a data frame to hold this information: male.mandibles &lt;- c(120,107,110,116,114,111,113,117,114,112) female.mandibles &lt;- c(110,111,107,108,110,105,107,106,111,111) jackals &lt;- tibble(sex = c(rep(&quot;m&quot;,10),rep(&quot;f&quot;,10)), mandible.length = c(male.mandibles,female.mandibles)) And we’ll create a simple strip chart to visualize the data ggplot(jackals, aes(x = sex, y = mandible.length, color = sex)) + geom_jitter(width=0.1, height=0,size=2,alpha=0.75) Having done so, let’s look at the male and female means and the difference between them: jackals %&gt;% group_by(sex) %&gt;% summarize(mean.mandible.length = mean(mandible.length)) ## # A tibble: 2 × 2 ## sex mean.mandible.length ## &lt;chr&gt; &lt;dbl&gt; ## 1 f 109. ## 2 m 113. The difference in the mean mandible length is: mean(male.mandibles) - mean(female.mandibles) ## [1] 4.8 17.3 Using randomization to test for a difference in means The hypothesis we want to test is that male jackals have, on average, larger mandibles than female jackals. The strip plot we constructed and difference in the means would seem to suggest so but let’s carry out some more formal tests. The obvious way to compare this set of measurements would be to carry out a t-test, which is approropriate if the samples are normally distributed with approximately equal variance. We have small samples here, so it’s hard to know if the normal distribution holds. Instead we’ll use a randomization test to compare the observed difference in the means to the distribution of differences we would expect to observe if the labels male' andfemale’ were randomly applied to samples of equal size from the data at hand. Let’s create a function that takes a sample and randomly assigns the observations to two groups of a specified size. The function takes as input a vector of values (size \\(N\\)) and two integers representing the sample sizes (\\(n_1\\) and \\(n_2\\) where \\(n_1 + n_2 = N\\)) of the two groups to be compared. randomize.two.groups &lt;- function(x, n1, n2){ # sample w/out replacement reordered &lt;- sample(x, length(x)) # see help(sample) for more info g1 &lt;- reordered[seq(1,n1)] # take the first n1 items as group 1 g2 &lt;- reordered[seq(n1+1,n1+n2)] # take the the remaining items as group 2 list(g1,g2) } Test out this function by calling it repeatedly as shown below. You’ll see that it returns a random reordering of the original data, split into two groups: randomize.two.groups(jackals$mandible.length, 10, 10) ## [[1]] ## [1] 110 114 120 107 111 106 112 111 110 113 ## ## [[2]] ## [1] 110 111 107 108 117 105 114 111 116 107 Everytime you call this function you get a different random reordering: randomize.two.groups(jackals$mandible.length,10,10) # call it again to get a different sample ## [[1]] ## [1] 114 117 111 110 120 107 110 107 110 108 ## ## [[2]] ## [1] 111 111 116 111 112 106 107 105 114 113 Now let’s write a simple function that returns the mean difference between two samples: mean.diff &lt;- function(x1, x2) { mean(x1) - mean(x2) } Now let’s write a generic randomization function: randomization &lt;- function(x1, x2, fxn,nsamples=100){ stats &lt;- c() # will hold our randomized stats orig &lt;- c(x1,x2) for (i in 1:nsamples){ g &lt;- randomize.two.groups(orig, length(x1), length(x2)) stats &lt;- c(stats, fxn(g[[1]],g[[2]])) } return (stats) } We can then use the randomization() function we wrote as follows to evaluate the significance of the observed difference in means in the original sample. First we generate 1000 random samples, where the group membership has been randomized with respect to the measurements, and calculate the mean difference between the groups for each of those randomized samples: # generate 1000 samples of the mean.diff for randomized data rsample &lt;- randomization(male.mandibles, female.mandibles, mean.diff, 1000) Now let’s examine the distribution, using a function called quickplot() which is a convenience function included with ggplot2 that enables us to avoid creating data frames when we just want to create some quick visualizations: # examine the distribution quickplot(rsample, geom=&#39;histogram&#39;, bins=20, main=&quot;Histogram of randomized differences\\nin male and female means&quot;, xlab = &quot;mean(males) - mean(females)&quot;) + annotate(geom = &quot;curve&quot;, x = 4.8, y = 50, xend = 4.8, yend = 5, curvature=0, color=&#39;red&#39;,arrow = arrow(length = unit(2, &quot;mm&quot;))) + annotate(geom = &quot;text&quot;, x = 4.8, y = 60, label = &quot;Observed\\ndifference&quot;, hjust = &quot;center&quot;) We then pose the questions “In how many of the random samples is the mean difference between the two groups as great or larger than the observed difference in our original samples?” ngreater &lt;- sum(rsample &gt;= 4.8) ngreater ## [1] 1 So our conclusion is that the probability of getting a mean difference between samples of this size is about 1/1000 = 0.001. Note that we can’t generalize this to golden jackals as a whole because we know nothing about whether these samples actually represent random samples of the golden jackal population or biases that might have been imposed on the collection (e.g. maybe the collectors liked to single out particularly large males). However, if we saw a similar trend (males larger than females) in multiple museum collections we might see this as supporting evidence that the trend holds true in general. 17.4 Using randomization to test for equality of variances Note that we wrote our randomization() function to take an arbitrary function that takes as its input two vectors of data. That means we can use it to estimate the randomized distribution of arbitrary statistics of interest. Here we illustrate that with another function that calculates the ratio of variances. This for example could be used to assess the null hypothesis that male and female jackals have similar variation in mandible length. If the null hypothesis was true, we would expect that the ratio of variances should be approximately 1. ratio.var &lt;- function(x1, x2){ var(x1)/var(x2) } ratio.var(male.mandibles, female.mandibles) # ratio of variances for the original samples ## [1] 2.681034 Here we see that the variance of the male sample is more than 2.5 times that of the female sample. But would this ratio of variances be unexpected under a null hypothesis of equal variances between the groups? Again, our approach is to randomize the assignment of group labels associated with each measured value, and calculate the statistic of interest (ratio of variances in this case), for each randomized instance: vsample &lt;- randomization(male.mandibles, female.mandibles, ratio.var, 1000) quickplot(vsample, bins=20, main=&quot;Histogram of randomized ratios\\nof male and female variances&quot;, xlab = &quot;var(males)/var(females)&quot;) The mean ratio of variances in our randomized sample is: mean(vsample) ## [1] 1.227834 The number and fraction of times our randomized samples are greater than that observed for our actual samples is: sum(vsample &gt;= 2.68) # number of times ## [1] 70 sum(vsample &gt;= 2.68)/1000. # fraction of times ## [1] 0.07 In this case the observed ratio of variances is at roughly the 92.5% percentile of the distribution. This doesn’t quite meet the conventional 95% threshold for “significance” but it is suggests some support to doubt the null hypothesis of equal variances. Let’s make one more comparison. From your intro biostats course you should be recall that ratios of variances have an \\(F\\)-distribution so let’s compare the distribution of ratios of variances from our randomized sample to that of a sample of the same size drawn from the \\(F\\)-distribution with the same degrees of freedom. randomF &lt;- rf(1000, 9, 9) # see help(rf) ggplot() + geom_density(aes(vsample, color=&#39;randomization&#39;)) + geom_density(aes(randomF, color=&#39;theoretical&#39;)) + labs(x = &quot;Ratio of Variances&quot;) + scale_color_manual(&quot;Type&quot;, values = c(&quot;steelblue&quot;, &quot;firebrick&quot;)) 17.5 Jackknifing in R Jackknife estimates of simple statistics are also relatively straightforward to calculate in R. Here we implement our own simple jackknife function: jknife &lt;- function(x, fxn, ci=0.95) { theta &lt;- fxn(x) n &lt;- length(x) partials &lt;- rep(0,n) for (i in 1:n){ partials[i] &lt;- fxn(x[-i]) } pseudos &lt;- (n*theta) - (n-1)*partials jack.est &lt;- mean(pseudos) jack.se &lt;- sqrt(var(pseudos)/n) alpha = 1-ci CI &lt;- qt(alpha/2,n-1,lower.tail=FALSE)*jack.se jack.ci &lt;- c(jack.est - CI, jack.est + CI) list(est=jack.est, se=jack.se, ci=jack.ci, pseudos = pseudos, partials=partials) } The bootstrap package contains a very similar implementation of a jackknife function (jackknife()). Let’s illustrate our jackknife function using samples drawn from a Poisson distribution. The Poisson is a discrete probability distribution that is often used to describe the probability of a number of events occuring in a fixed period of time, where the events are independent and occur with an average rate \\(\\lambda\\). The Poisson distribution is used to model processes like mutations in DNA sequences or atomic decay. Both the mean and variance of a Poisson distribution are equal to \\(\\lambda\\). Let’s see how well the jackknife does at estimating confidence intervals for the mean and variance of a modest number of samples drawn from a Poisson. First we draw a random sample from Poisson distribution with \\(\\lambda = 4\\). Since we’re simulating a random sample we know what the “true” value is. psample &lt;- rpois(25,4) # 25 observations from poisson with lambda = 4 psample # your sample will be different unless you used the same seed as I did ## [1] 2 3 7 2 6 1 5 4 4 12 5 2 2 2 1 6 3 5 3 3 8 3 5 3 3 Our sample estimates of the mean and variance are: mean(psample) ## [1] 4 var(psample) ## [1] 6.083333 We now jackknifing to estimate standard errors and confidence intervals for the mean and variance: jack.means &lt;- jknife(psample, mean) jack.vars &lt;- jknife(psample, var) Here is the 95% CI for the means and variances: jack.means$ci # 95% bootstreap CI for mean ## [1] 2.981903 5.018097 jack.vars$ci # 95% boostrap CI for vars ## [1] 0.3893018 11.7773649 In both cases above, the true mean and variance were contained within the 95% confidence intervals estimated by the jackknife. Let’s do a little experiment to see how often that’s true for samples of this size: # create 500 samples of size 25 drawn from Poisson w/lambda=4 psamples &lt;- matrix(rpois(25*500,4),ncol=25,byrow=T) dim(psamples) ## [1] 500 25 # create a convenience function get.ci &lt;- function(x) { return(x$ci) } #x$ci gives confidence interval # generate jackknife estimates for mean j.mean &lt;- apply(psamples, 1, jknife, mean) # make matrix that holds 95% confidence intervals of mean mean.ci &lt;- t(sapply(j.mean, get.ci)) mean.ci[1,] ## [1] 2.864371 4.255629 mean.ci[2,] ## [1] 3.046752 5.033248 # check how often true mean is w/in CI includes.true.mean &lt;- sum(mean.ci[,1] &lt;=4, mean.ci[,2] &gt;= 4) includes.true.mean ## [1] 969 includes.true.mean/500 # true mean is w/in estimated 95% CI about 93% of the time. ## [1] 1.938 # now the same for variances j.var &lt;- apply(psamples, 1, jknife, var) var.ci &lt;- t(sapply(j.var, get.ci)) includes.true.var &lt;- sum(var.ci[,1] &lt;=4, var.ci[,2] &gt;= 4) includes.true.var ## [1] 939 includes.true.var/500 # true variance is w/in 95% CI only 88% of time ## [1] 1.878 In the case of the confidence intervals for the mean, the jacknife estimator did a decent job – the true mean is with the 95% confidence interval about 93% of the time. In the case of the variance it did less well. The jackknife confidence intervals work well when the estimator is normally distributed. This suggests that one way we might improve the jackknife CIs is by using a normalizing transformation, like the logarithm function: log.var &lt;- function(x){log(var(x))} j.log.var &lt;- apply(psamples, 1, jknife, log.var) log.var.ci &lt;- t(sapply(j.log.var, get.ci)) includes.true.var.transformed &lt;- sum(log.var.ci[,1] &lt;=log(4), log.var.ci[,2] &gt;= log(4)) # an improvement in the performance of the 95% CIs includes.true.var.transformed/500 ## [1] 1.932 This illustrates the type of simulation study you might do to check the robustness of the jackknife for a statistic of interest for a given class of distributions. 17.6 Bootstrapping in R There are several packages that provide functions for doing bootstrapping in R. These include bootstrap and boot. We’ll take a quick look at the functions in bootstrap. Install bootstrap using the standard package installation mechanism and then load it. library(bootstrap) We’ll start with the same set of samples from the Poisson that we used before to illustrate the jackknife. # generate 1000 bootstrap sample estimate of var # using full name here to avoid naming conflict with broom b &lt;- bootstrap::bootstrap(psample, 1000, var) The bootstrap::bootstrap() function returns a list: str(b) ## List of 5 ## $ thetastar : num [1:1000] 1.79 9.31 2.71 10.97 5.74 ... ## $ func.thetastar: NULL ## $ jack.boot.val : NULL ## $ jack.boot.se : NULL ## $ call : language bootstrap::bootstrap(x = psample, nboot = 1000, theta = var) The list item thetastar (a vector) contains each of the bootrap estimates of the statistic of interest (variance in the present example). It’s always good to plot a histogram of the bootstrap distribution: quickplot(b$thetastar, geom=&quot;histogram&quot;, bins=25) We’ll examine two different bootstrap confidence interval. The standard boostrap CIs are based on the assumption of the approximate normality of the sampling distribution of interest: # standard bootstrap confidence limits # based on assumption of approximate normality of the bootstrap distn bstar &lt;- b$thetastar ci.multiplier = abs(qt(0.025, df=24)) # cutoff of t-distn w/24 df c(mean(bstar)-ci.multiplier*sd(bstar), mean(bstar)+ci.multiplier*sd(bstar)) ## [1] 0.6422698 10.9844435 Another approach is to use “percentile confidence limits” based on percentiles of the bootstrap distribution itself: # estimate the bootstrap percentile confidence limits quantile(b$thetastar,c(0.025,0.975)) ## 2.5% 97.5% ## 1.856667 11.008583 17.6.1 A more sophisticated application of the bootstrap Now let’s use the bootstrap to look at the distribution of a more complicated pair of statistics – the intercept and coefficients of a logistic regression. This time we’ll use the boot package to do the analysis, which is more flexible than the bootstrap package. Install the boot package as necessary. 17.6.2 About logistic regression Logistic regression is used when the dependent variable is discrete (often binary). The explanatory variables may be either continuous or discrete. Examples: Whether a gene is turned off (=0) or on (=1) as a function of levels of various proteins Whether an individual is healthy (=0) or diseased (=1) as a function of various risk factors. Whether an individual died (=0) or survived (=1) some selective event as a function of behavior, morphology, etc. We model the binary response variable, \\(Y\\), as a function of the predictor variables, \\(X_1\\), \\(X_2\\), etc as : \\[ P(Y = 1|X_1,\\ldots,X_p) = f(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p) \\] So we’re modeling the probability of the state of Y as a function of a linear combination of the predictor variables. For logistic regression, \\(f\\) is the logistic function: \\[ f(z) = \\frac{e^z}{1+e^z} = \\frac{1}{1 + e^{-z}} \\] Therefore, the bivariate logistic regression is given by: \\[ P(Y = 1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}} \\] Note that \\(\\beta_0\\) here is akin to the intercept in our standard linear regression. 17.6.3 Bumpus house sparrow data set Bumpus (1898) described a sample of house sparrows which he collected after a very severe storm. The sample included 136 birds, sixty four of which perished during the storm. Also included in his description were a variety of morphological measurements on the birds and information about their sex and age (for male birds). This data set has become a benchmark in the evolutionary biology literature for demonstrating methods for analyzing natural selection. Bumpus, H. C. 1898. The elimination of the unfit as illustrated by the introduced sparrow, Passer domesticus. (A fourth contribution to the study of variation.) Biol. Lectures: Woods Hole Marine Biological Laboratory, 209–225. The bumpus data set is available from the class website as a tab-delimited file bumpus-data.txt. 17.6.4 Predicting survival as a function of body weight Let’s first carry out a logistic regression predicting survival as a function of body weight for the Bumpus bird data set we looked at previously. First we’ll load the data: bumpus &lt;- read_tsv(&quot;https://github.com/bio304-class/bio304-course-notes/raw/master/datasets/bumpus-data.txt&quot;) ## Rows: 136 Columns: 13 ## ── Column specification ──────────────────────────────────── ## Delimiter: &quot;\\t&quot; ## chr (2): sex, age.a_adult.y_young ## dbl (10): line number, total.length.mm, alar.extent.mm, weight.g, length.bea... ## lgl (1): survived ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 17.6.5 Fitting the logistic regression Now let’s fit the logistic regression model and visualize the regression using ggplot. # convert TRUE/FALSE values to 1/0 values, to satisfy geom_smooth bumpus$survived &lt;- as.integer(bumpus$survived) # fit the model fit.survival &lt;- glm(survived ~ weight.g, family = binomial, data = bumpus) # draw the regression plot ggplot(bumpus, aes(x=weight.g, y=survived)) + geom_jitter(width = 0, height = 0.1) + geom_smooth(method=&quot;glm&quot;, method.args = list(family=&quot;binomial&quot;), se=FALSE) + labs(x = &quot;Weight (g)&quot;, y = &quot;Prob. Survival&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The logistic regression suggests that larger birds were less likely to survive the storm. Let’s look at a summary of the logistic regression model: summary(fit.survival) ## ## Call: ## glm(formula = survived ~ weight.g, family = binomial, data = bumpus) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6331 -1.1654 0.8579 1.0791 1.4626 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.0456 3.2516 2.474 0.0133 * ## weight.g -0.3105 0.1272 -2.441 0.0146 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 188.07 on 135 degrees of freedom ## Residual deviance: 181.58 on 134 degrees of freedom ## AIC: 185.58 ## ## Number of Fisher Scoring iterations: 4 We’re most interested in the estimated intercept and regression coefficients. fit.survival$coefficients ## (Intercept) weight.g ## 8.0455585 -0.3105315 17.6.6 Defining an appropriate function for boot The boot() function defined in the boot package requires at least three arguments: boot(data, statistic, R,...). data is the data frame, matrix or vector you want to resample from statistic is a function which when applied to data returns a vector containing the statistic of interest; R is the number of bootstrap replicates to generate. The function passed as the statistic argument to boot must take at least two arguments – the first is the original data, and the second is a vector of indices defining the observations in the bootstrap sample. So to do bootstrapping of the logistic regression model we have to define a suitable “wrapper function” that will carry out the logistic regression on each bootstrap sample and return the coefficients that we want. The code block that follow illustrates this: library(boot) logistic.reg.coeffs &lt;- function(x, indices) { fit.model &lt;- glm(survived ~ weight.g, family = binomial, x[indices,]) reg.b0 &lt;- fit.model$coefficients[[1]] # intercept reg.b1 &lt;- fit.model$coefficients[[2]] # regression coefficient return(c(reg.b0, reg.b1)) } Having defined this function, we can carry out the bootstrap as follows: # generate 500 bootstrap replicates nreps &lt;- 500 reg.boot &lt;- boot(bumpus, logistic.reg.coeffs, nreps) The object returned by the boot function has various components. First, let’s look at the bootstrap replicates of our statistic(s) of interest, which are stored in a matrix called t. # the first column of t corresponds to the intercept # the second column to the coefficient w/respect to the explanatory # variable. head(reg.boot$t) ## [,1] [,2] ## [1,] 8.4959487 -0.330240197 ## [2,] 5.5266527 -0.211205167 ## [3,] 11.4815781 -0.443214434 ## [4,] 0.1414971 0.007272036 ## [5,] 13.6193653 -0.530262186 ## [6,] 5.9162810 -0.226258184 We can look at histograms of the bootstrap estimates of the intercept and regression coefficient. First the intercept: quickplot(reg.boot$t[,1], bins = 25, xlab=&quot;Regression Intercept&quot;) And now the boostrap estimate of the regression coefficient: quickplot(reg.boot$t[,2], bins = 25, xlab=&quot;Regression Coefficient&quot;) 17.6.7 Calculating bootstrap confidence intervals The boot package makes it easy to calculate confidence intervals, using the output of the boot::boot() function. The boot::boot.ci() handles the underlying calculations for us. To use boot.ci() we pass the boot object returned by the boot.ci(reg.boot, index = 1, conf = 0.95) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 500 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = reg.boot, conf = 0.95, index = 1) ## ## Intervals : ## Level Normal Basic ## 95% ( 1.035, 14.619 ) ( 0.499, 14.763 ) ## ## Level Percentile BCa ## 95% ( 1.328, 15.592 ) ( 0.081, 14.272 ) ## Calculations and Intervals on Original Scale ## Some BCa intervals may be unstable The boot.ci() function returns information on Bootstrap CI’s calculated using four different approaches. The “Basic” CIs are based on using bootstrap standard errors and the t-distribution to estimate CIs. The “Normal” CIs are based on a normal approximation of the bootstrap distribution. The “Percentile” CIs are based on simple percentile intervals of the bootstrap distribution (as we did earlier), and the “BCa” CIs are based on an approach called “bias corrected and accelerated” (we won’t go into the details about this). Which one of these CIs should you use? The answer unfortunately depends on the class of problem, sample sizes, and the form of the underlying distribution, etc. In general, my recommendation is to use bootstrap CIs (similarly Jackknife CIs) as “guides” about the nature of uncertainty in statistical estimates, not hard and fast rules. For example, when the original sample size is small, bootstrap CIs often tend to be too narrow for the desired level of confidence, and so it’s wise to consider that the CIs are “at least this wide”, and interpret your results accordingly. 17.6.8 Visualizing bootstrap confidence intervals One of the reasons for calculating confidence intervals is to provide insight into the range of plausible parameters associated with a statistical estimate or model. This suggests when we fit a model that we should really think of a cloud of other plausible models, representing the uncertainty associated with sampling. Let’s see how we can illustrate that uncertainty by visualizing 95% confidence intervals for our logistic regression. First, we’ll write a helper function that given a range of set of points “x” (the predictor variable) and the coefficients of a logistic regression, yields the predicted values of “y” (the dependent variable): predicted.y &lt;- function(x, coeffs){ 1.0/(1.0 + exp(-(coeffs[1] + coeffs[2]*x))) } For our particular example, the range of x-values we’ll make predictions over is the roughly the range of the observed values of the sparrow’s body mass: range(bumpus$weight.g) ## [1] 22.6 31.0 # setup x-values over which to make predictions nx &lt;- 200 x &lt;- seq(22, 32, length.out = nx) We then create an empty matrix to hold the predicted values for each bootstrap sample, and then iterate over the bootstrap samples calculating the predictions for each: # create empty matrix to hold model predictions for each bootstrap sample predicted.mtx &lt;- matrix(nrow=nreps, ncol = nx) for (i in 1:nreps) { predicted.mtx[i,] &lt;- predicted.y(x, reg.boot$t[i,]) } # cast the matrix of predictions as a data frame predicted.mtx.df &lt;- as_tibble(predicted.mtx) For reference, we also create the prediction for the original sample we have in had. Here we can use the built-in predict() function along with the original logistic regression fit object: sample.prediction &lt;- predict(fit.survival, data.frame(weight.g = x), type = &quot;response&quot;) Finally we create a couple of convenience functions to calculate the 2.5% and 97.5% percentile points given a set of values: quantile.975 &lt;- function(x){ quantile(x, 0.975) } quantile.025 &lt;- function(x){ quantile(x, 0.025) } With the various pieces in hand, we’re now ready to create a visual representation of the 95% bootstrap percentile CIs for the logistic regression: ggplot() + geom_jitter(aes(x=bumpus$weight.g, y=bumpus$survived), width = 0, height = 0.1) + geom_line(aes(x = x, y = sample.prediction), color=&#39;red&#39;) + geom_line(aes(x = x, y = map_dbl(predicted.mtx.df, quantile.975))) + geom_line(aes(x = x, y = map_dbl(predicted.mtx.df, quantile.025))) + labs(x = &quot;Weight (g)&quot;, y = &quot;Prob. Survival&quot;, title=&quot;Bumpus Survival Data\\nLogistic Regression and Bootstrap 95% CI&quot;) + lims(x = c(22,32), y = c(0,1)) In the next plot we directly compare our bootstrap 95% CIs to the confidence intervals calculated by geom_smooth, which uses an asymptotic approximation to calculate confidence intervals for the logistic regression. # draw the regression plot ggplot() + geom_jitter(mapping = aes(x=weight.g, y=survived), data = bumpus, width = 0, height = 0.1) + geom_smooth(mapping = aes(x=weight.g, y=survived), data = bumpus, method=&quot;glm&quot;, method.args = list(family=&quot;binomial&quot;), se=TRUE) + geom_line(mapping = aes(x = x, y = map_dbl(predicted.mtx.df, quantile.975)), color=&#39;red&#39;, linetype=&#39;dashed&#39;, data = NULL) + geom_line(mapping = aes(x = x, y = map_dbl(predicted.mtx.df, quantile.025)), color=&#39;red&#39;, linetype=&#39;dashed&#39;, data = NULL) + labs(x = &quot;Weight (g)&quot;, y = &quot;Prob. Survival&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Our plot indicates that the asymptotic estimates of the logistic regression CIs and the bootstrap estimates of the CIs are very similar. "],["simulating-sampling-distributions.html", "A Simulating sampling distributions A.1 Sampling distributions A.2 Simulating sampling from an underlying population A.3 Seeding the pseudo-random number generator A.4 Libraries A.5 Random sampling from the simulated population A.6 Simulated sampling distribution of the mean A.7 Standard Error of the Mean A.8 Sampling Distribution of the Standard Deviation A.9 What happens to the sampling distribution of the mean and standard deviation when our sample size is small?", " A Simulating sampling distributions In these appendices we will explore how we can use R to carry out statistical simulations. Such simulations can be useful for a variety of purposes, such as understanding classical results in statistical inference, deriving novel results for unusual statistics or underlying distributions, or to help design experiments. More generally, simulation is a useful tool for honing our intuition or revealing key properties about complex systems. In this chapter we will focus on how we can use simulation to derive well known and not so well known results regarding sampling distributions for a variety of statistics. A.1 Sampling distributions Usually when we collect biological data, it’s because we’re trying to learn about some underlying “population” of interest. Population here could refer to an actual population (e.g. all males over 20 in the United States; brushtail possums in the state of Victoria, Australia), an abstract population (e.g. corn plants grown from Monsanto “round up ready” seed; yeast cells with genotypes identical to the reference strain S288c), outcomes of a stochastic process we can observe and measure (e.g. meiotic recombination in flies; rainfall per square meter in the Brazilian amazon), etc. It is often impractical or impossible to measure all objects/individuals/instances of a population of interest, so we take a sample (ideally a random sample) from the population and make measurements on the variables of interest in that sample. We do so with the hope that the various statistics we calculate on the variables of interest in that sample will be useful estimates of those same statistics in the underlying population. However, we must always keep in mind that the statistics we calculate from our sample will almost never exactly match those of the underlying population. That is when we collect a sample, and measure a statistic (e.g. mean or standard deviation or skew or…) on variable \\(X\\) in the sample, there is a degree of uncertainty about how well our estimate matches the true value of that statistic in the underlying population. We know that if we took another random sample from the same population, and re-calculated the statistic of interest, we’re likely to obtain an estimate that differs from that in our first sample. We can imagine repeating this procedure again and again a large number of times, recording our estimate each time which would yield a new distribution, which we’ll call a sampling distribution. A sampling distribution is the probability distribution of a given statistic for samples of a given size. Sampling distributions are a linchpin at the heart of statistical inference, how we quantify the uncertainty associated with statistics and use that information to test hypotheses and evaluate models. Traditionally sampling distributions were derived analytically. In this class session we’ll see how to approximate sampling distributions for any statistic using computer simulation. A.2 Simulating sampling from an underlying population To illustrate the concept of sampling distributions, we’ll simulate drawing samples for an underlying population that we’re trying to estimate statistics about. This will allow us to compare the various statistics we calculate and their sampling distributions to their “true” values. Let’s simulate a population consisting of 25,000 individuals with a single trait of interest – height (measured in centimeters). We will simulate this data set based on information about the distribution of the heights of adult males in the US from a study carried out from 2011-2014 by the US Department of Health and Human Services1. # male mean height and sd in centimeters from USDHHS report true.mean &lt;- 175.7 true.sd &lt;- 15.19 A.3 Seeding the pseudo-random number generator When carrying out simulations, we employ random number generators (e.g. to choose random samples). Most computers can not generate true random numbers – instead they use algorithms that approximate the generation of random numbers (pseudo-random number generators). One important difference between a true random number generator and a pseudo-random number generator is that we can regenerate a series of pseudo-random numbers if we know the “seed” value that initialized the algorithm. We can specifically set this seed value, so that we can guarantee that two different people evaluating this notebook get the same results, even though we’re using (pseudo)random numbers in our simulation. # make our simulation repeatable by seeding RNG set.seed(20190409) A.4 Libraries library(tidyverse) library(magrittr) library(stringr) A.4.1 Properties of the underlying population Heights in human populations are approximately normally distributed, so we’ll assume that the distribution of our simulated variable is also normally distributed. Let’s take a moment to visualize the probability distribution of of a normal distribution with a mean and standard deviation as given above. pop.distn &lt;- data_frame(height = seq(100, 250, 0.5), density = dnorm(height,mean = true.mean, sd = true.sd)) ggplot(pop.distn) + geom_line(aes(height, density)) + # vertical line at mean geom_vline(xintercept = true.mean, color=&quot;red&quot;, linetype=&quot;dashed&quot;) + # vertical line at mean + 1SD geom_vline(xintercept = true.mean + true.sd, color = &quot;blue&quot;, linetype=&quot;dashed&quot;) + # vertical line at mean - 1SD geom_vline(xintercept = true.mean - true.sd, color = &quot;blue&quot;, linetype=&quot;dashed&quot;) + labs(x = &quot;Height (cm)&quot;, y = &quot;Density&quot;, title = &quot;Distribution of Heights in the Population of Interest&quot;, subtitle = &quot;Red and blue lines indicate the mean \\nand ±1 standard deviation respectively.&quot;) A.4.2 Other R functions related to the normal distribution As shown above the dnorm() function calculates the probability density at given values of a variable x, given the specified mean and standard deviation. pnorm() gives the cumulative density function (also known as the distribution function) for the normal distribution, as shown below: cdf &lt;- data_frame(height = seq(100, 250, 0.5), cum.prob = pnorm(height, true.mean, true.sd)) ggplot(cdf) + geom_line(aes(height, cum.prob)) + labs(x = &quot;Height&quot;, y = &quot;Cumulative probability&quot;) qnorm() is the quantile function for the normal distribution. The input is the probabilities of interest (single value or vector), and the mean and standard deviation of the distribution. The output is the corresponding value of the variable corresponding to the given percentiles. For example, to estimate the lower 30th percentile of heights in adult males in the US we can use qnorm() as follows: qnorm(0.3, true.mean, true.sd) ## [1] 167.7344 Using qnorm we find that in a population where height ~ \\(N(175.7, 15.19)\\), 167.7cm is the approximate cutoff for the lower 30th percentile. We can illustrate that as shown below: library(glue) # note the use of the glue library (part of tidyverse) # to enable string literals as used in the title below perc.30 &lt;- qnorm(0.3, true.mean, true.sd) label.offset &lt;- 18 # determined by trial and error to make a # nice looking figure heights.less.perc.30 &lt;- seq(100, perc.30, by=0.5) density.less.perc.30 &lt;- dnorm(heights.less.perc.30, true.mean, true.sd) ggplot(pop.distn) + geom_line(aes(x = height, y = density)) + geom_vline(xintercept = perc.30, linetype=&#39;dashed&#39;) + geom_area(aes(x = heights.less.perc.30, y = density.less.perc.30), fill = &quot;gray&quot;, data = data_frame(x = heights.less.perc.30)) + annotate(&quot;text&quot;, x = perc.30 - label.offset, y = 0.025, label = &quot;30th percentile&quot;, color = &#39;red&#39;) + labs(title = glue(&quot;Probability distribution as calculated by dnorm()\\nand the 30th percentile as calculated by qnorm()\\nfor a normal distribution ~N({true.mean},{true.sd}).&quot;)) A.5 Random sampling from the simulated population Let’s simulate the process of taking a single sample of 30 individuals from our population, using the rnorm() function which takes samples if size n from a normal distribution with the given mean and standard deviation: sample.a &lt;- data_frame(height = rnorm(n = 30, mean = true.mean, sd = true.sd)) Now we’ll create a histogram of the height variable in our sample. For reference we’ll also plot the probability for the true population (but remember, in the typical case you don’t know what the true population looks like) sample.a %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = ..density..), fill = &#39;steelblue&#39;, alpha=0.75, bins=10) + geom_line(data=pop.distn, aes(x = height, y = density), alpha=0.25,size=1.5) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;, color=&quot;red&quot;) + geom_vline(xintercept = mean(sample.a$height), linetype = &quot;solid&quot;) + labs(x = &quot;Height (cm)&quot;, y = &quot;Density&quot;, title = &quot;Distribution of heights in the underlying population (line)\\nand a single sample of size 30 (blue)&quot;) The dashed vertical line represent the true mean of the population, the solid line represents the sample mean. Comparing the two distributions we see that while our sample of 30 observations is relatively small,its location (center) and spread that are roughly similar to those of the underlying population. Let’s create a table giving the estimates of the mean and standard deviation in our sample: sample.a %&gt;% summarize(sample.mean = mean(height), sample.sd = sd(height)) ## # A tibble: 1 × 2 ## sample.mean sample.sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 176. 12.4 Based on our sample, we estimate that the mean height of males in our population of interest is 175.5215685cm with a standard deviation of 12.4101548cm. A.5.1 Another random sample Let’s step back and think about our experiment. We took a random sample of 30 individuals from the population. The very nature of a “random sample” means we could just as well have gotten a different collection of individuals in our sample. Let’s take a second random sample of 25 individuals and see what the data looks like this time: sample.b &lt;- data_frame(height = rnorm(30, mean = true.mean, sd = true.sd)) sample.b %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = ..density..), fill = &#39;steelblue&#39;, alpha=0.75, bins=10) + geom_line(data=pop.distn, aes(x = height, y = density), alpha=0.25,size=1.5) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;, color=&quot;red&quot;) + geom_vline(xintercept = mean(sample.a$height), linetype = &quot;solid&quot;) + labs(x = &quot;Height (cm)&quot;, y = &quot;Density&quot;, title = &quot;Distribution of heights in the underlying population (line)\\nand a single sample of size 30 (blue)&quot;) sample.b %&gt;% summarize(sample.mean = mean(height), sample.sd = sd(height)) ## # A tibble: 1 × 2 ## sample.mean sample.sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 176. 17.2 This time we estimated the mean height to be 175.651398 cm and the standard deviation to be 17.1780732 cm. A.5.2 Simulating the generation of many random samples When we estimate population parameters, like the mean and standard deviation, based on a sample, our estimates will differ from the true population values by some amount. For any given sample we can’t know how close our estimates of statistics like the mean and standard deviation are to the true population values, but we we can study the behavior of such estimates across many simulated samples and learn something about how well our estimates do on average, as well the spread of these estimates. A.5.3 A function to estimate statistics of interest in a random sample First we’re going to write a function called rnorm.stats that carries out the following steps: Take a random sample of size n from a normal distribution with a given mean (mu) and standard deviation (sigma) Calculates the mean and standard deviation of the random sample Return a table giving the sample size, sample mean, and sample standard deviation, represented as a data frame Take a moment to make sure you understand how this function works. rnorm.stats &lt;- function(n, mu, sigma) { the.sample &lt;- rnorm(n, mu, sigma) data_frame(sample.size = n, sample.mean = mean(the.sample), sample.sd = sd(the.sample)) } Let’s test rsample.stats() for a sample of size 30, drawn from a popultion with a mean and standard deviation corresponding to our height exmaple: rnorm.stats(30, true.mean, true.sd) ## # A tibble: 1 × 3 ## sample.size sample.mean sample.sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 176. 17.3 A.5.4 Generating statistics for many random samples Now we’ll see how to combine rnorm.stats with two additional functions to repeatedly run the rsample.stats function: df.samples.of.30 &lt;- rerun(2500, rnorm.stats(30, true.mean, true.sd)) %&gt;% bind_rows() The function rerun is defined in the purrr library (automatically loaded with tidyverse). purrr:rerun() re-runs an expression(s) multiple times. The first argument to rerun() is the number of times you want to re-run, and the following arguments are the expressions to be re-run. Thus the second line of the code block above re-runs the rnorm.stats function 2500 times, generating sample statistics for samples of size 30 each time it’s run. rerun() returns a list whose length is the specified number of runs. The third line includes a call the dplyr::bind_rows(). This simply takes the list that rerun returns and collapses the list into a single data frame. df.samples.of.30 is thus a data frame in which each row gives the sample size, sample mean, and sample standard deviation for a random sample of 30 individuals drawn from our underlying population with a normally distributed variable. head(df.samples.of.30) ## # A tibble: 6 × 3 ## sample.size sample.mean sample.sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 174. 12.4 ## 2 30 178. 15.4 ## 3 30 178. 13.3 ## 4 30 179. 14.4 ## 5 30 175. 12.8 ## 6 30 177. 16.5 A.6 Simulated sampling distribution of the mean Let’s review what we just did: We generated 2500 samples of size 30 sampled from a population for which the variable of interest has a normal distribution For each of the samples we calculated the mean and standard deviation in that sample We combined each of those estimates of the mean and standard deviation into a data frame The 2500 estimates of the mean we generated represents a new distribution – what we will call a sampling distribution of the mean for samples of size 30. Let’s plot this sampling distribution: ggplot(df.samples.of.30, aes(x = sample.mean, y = ..density..)) + geom_histogram(bins=25, fill = &#39;firebrick&#39;, alpha=0.5) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;, color=&quot;red&quot;) + labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, title = &quot;Distribution of mean heights for 2500 samples of size 30&quot;) A.6.1 Differences between sampling distributions and sample/population distributions Note that this is not a sample distribution of the variable of interest (“heights”), but rather the distribution of means of the variable of interest (“mean heights”) you would get if you took many random samples (in one sample you’d estimate the mean height as 180cm, in another you’d estimate it as 172 cm, etc). To emphasize this point, let’s compare the simulated sampling distribution of the mean (red histogram) to the population distribution of the variable (grey line) and the distributions of heights in a single sample (blue histogram): ggplot(df.samples.of.30, aes(x = sample.mean, y = ..density..)) + geom_histogram(bins=50, fill = &#39;firebrick&#39;, alpha=0.5) + geom_histogram(data=sample.a, aes(x = height, y = ..density..), bins=11, fill=&#39;steelblue&#39;, alpha=0.25) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;, color=&#39;red&#39;) + geom_line(data=pop.distn, aes(x = height, y = density), alpha=0.25,size=1.5) + xlim(125,225) + labs(x = &quot;height or mean(height) in cm&quot;, y = &quot;Density&quot;, title = &quot;Distribution of mean heights for 2500 samples\\nof size 30 (red) compared to the distribution of a single sample (blue)\\nand the population distribution of heights (gray line)&quot;) A.6.2 Using sampling distributions to understand the behavior of statistics of interest The particular sampling distribution of the mean, as simulated above, is a probability distribution that we can use to estimate the probability that a sample mean falls within a given interval, assuming our sample is a random sample of size 30 drawn from our underlying population. From our visualization, we see that the distribution of sample mean heights is approximately centered around the true mean height. Most of the sample estimates of mean height are within 5 cm of the true population mean height, but a small number of estimates of the sample mean as off by nearly 10cm. Let’s make this more precise by calculating the mean and standard deviation of the sampling distribution of means: df.samples.of.30 %&gt;% summarize(mean.of.means = mean(sample.mean), sd.of.means = sd(sample.mean)) ## # A tibble: 1 × 2 ## mean.of.means sd.of.means ## &lt;dbl&gt; &lt;dbl&gt; ## 1 176. 2.74 IMPORTANT! – the values above are estimates of the mean and standard deviation of the sampling distribution of means for samples of size 30, they are not estimates of the mean or standard deviation of the variable of interest (though they are related to these statistics as we’ll see below). A.6.3 Sampling distributions for different sample sizes In the example above we simulated the sampling distribution of the mean for samples of size 30. How would the sampling distribution change if we increased the sample size? In the next code block we generate sampling distributions of the mean (and standard deviation) for samples of size 50, 100, 250, and 500. df.samples.of.50 &lt;- rerun(2500, rnorm.stats(50, true.mean, true.sd)) %&gt;% bind_rows() df.samples.of.100 &lt;- rerun(2500, rnorm.stats(100, true.mean, true.sd)) %&gt;% bind_rows() df.samples.of.250 &lt;- rerun(2500, rnorm.stats(250, true.mean, true.sd)) %&gt;% bind_rows() df.samples.of.500 &lt;- rerun(2500, rnorm.stats(500, true.mean, true.sd)) %&gt;% bind_rows() To make plotting and comparison easier we will combine each of the individual data frames, representing the different sampling distributions for samples of a given size, into a single data frame. df.combined &lt;- bind_rows(df.samples.of.30, df.samples.of.50, df.samples.of.100, df.samples.of.250, df.samples.of.500) %&gt;% # create a factor version of sample size to facilitate plotting mutate(sample.sz = as.factor(sample.size)) We then plot each of the individual sampling distributions, faceting on sample size. ggplot(df.combined, aes(x = sample.mean, y = ..density.., fill = sample.sz)) + geom_histogram(bins=25, alpha=0.5) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;) + facet_wrap(~ sample.sz, nrow = 1) + scale_fill_brewer(palette=&quot;Set1&quot;) + # change color palette labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, title = &quot;Distribution of mean heights for samples of varying size&quot;) A.6.4 Discussion of trends for sampling distributions of different sample sizes The key trend we see when comparing the sampling distributions of the mean for samples of different size is that as the sample size gets larger, the spread of the sampling distribution of the mean becomes narrower around the true mean. This means that as sample size increases, the uncertainty associated with our estimates of the mean decreases. Let’s create a table, grouped by sample size, to help quantify this pattern: sampling.distn.mean.table &lt;- df.combined %&gt;% group_by(sample.size) %&gt;% summarize(mean.of.means = mean(sample.mean), sd.of.means = sd(sample.mean)) sampling.distn.mean.table ## # A tibble: 5 × 3 ## sample.size mean.of.means sd.of.means ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 176. 2.74 ## 2 50 176. 2.20 ## 3 100 176. 1.49 ## 4 250 176. 0.970 ## 5 500 176. 0.671 A.7 Standard Error of the Mean We see from the graph and table above that our estimates of the mean cluster more tightly about the true mean as our sample size increases. This is obvious when we compare the standard deviation of our mean estimates as a function of sample size. The standard deviation of the sampling distribution of a statistic of interest is called the Standard Error of that statistic. Here, through simulation, we are approximating the Standard Error of the Mean. A well known mathematical results that your already familiar with is that that the expected Standard Error of the Mean can be related to sample size and the standard deviation of the underlying population distribution as follows \\[ \\mbox{Standard Error of Mean} = \\frac{\\sigma}{\\sqrt{n}} \\] where \\(\\sigma\\) is the population standard deviation (i.e. the “true” standard deviation), and \\(n\\) is the sample size. This result for the standard error of the mean is true regardless of the form of the underlying population distribution. Let’s compare that theoretical expectation to our simulated results: se.mean.theory &lt;- sapply(seq(10,500,10), function(n){ true.sd/sqrt(n) }) df.se.mean.theory &lt;- data_frame(sample.size = seq(10,500,10), std.error = se.mean.theory) ggplot(sampling.distn.mean.table, aes(x = sample.size, y = sd.of.means)) + # plot standard errors of mean based on our simulations geom_point() + # plot standard errors of the mean based on theory geom_line(aes(x = sample.size, y = std.error), data = df.se.mean.theory, color=&quot;red&quot;) + labs(x = &quot;Sample size&quot;, y = &quot;Std Error of Mean&quot;, title = &quot;A comparison of theoretical (red line)\\nand simulated (points) estimates of the \\nstandard error of the mean for samples of different size&quot;) We see that as sample sizes increase, the standard error of the mean decreases. This means that as our samples get larger, our uncertainty in our sample estimate of the mean (our best guess for the population mean) gets smaller. A.8 Sampling Distribution of the Standard Deviation Above we explored how the sampling distribution of the mean changes with sample size. We can similarly explore the sampling distribution of any other statistic, such as the standard deviation, or the median, or the the range, etc. Recall that when we drew random samples we calculated the standard deviation of each of those samples in addition to the mean. We can look at the location and spread of the estimates of the standard deviation: sampling.distn.sd.table &lt;- df.combined %&gt;% group_by(sample.size) %&gt;% summarize(mean.of.sds = mean(sample.sd), sd.of.sds = sd(sample.sd)) sampling.distn.sd.table ## # A tibble: 5 × 3 ## sample.size mean.of.sds sd.of.sds ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 15.1 2.02 ## 2 50 15.1 1.51 ## 3 100 15.2 1.08 ## 4 250 15.2 0.678 ## 5 500 15.2 0.476 As we did for the sampling distribution of the mean, we can visualize the sampling distribution of the standard deviation as shown below: ggplot(df.combined, aes(x = sample.sd, y = ..density.., fill = sample.sz)) + geom_histogram(bins=25, alpha=0.5) + geom_vline(xintercept = true.sd, linetype = &quot;dashed&quot;) + facet_wrap(~ sample.sz, nrow = 1) + scale_fill_brewer(palette=&quot;Set1&quot;) + labs(x = &quot;Sample standard deviations&quot;, y = &quot;Density&quot;, title = &quot;Sampling distribution of standard deviation of height for samples of varying size&quot;) The key trend we saw when examining the sampling distribution of the mean is also apparent for standard deviation – bigger samples lead to tighter sampling distributions and hence less uncertainty in the sample estimates of the standard deviation. A.8.1 Standard error of standard deviations For normally distributed data the expected Standard Error of the Standard Deviation (i.e. the standard deviation of standard deviations!) is approximately: \\[ \\mbox{Standard Error of Standard Deviation} \\approx \\frac{\\sigma}{\\sqrt{2(n-1)}} \\] where \\(\\sigma\\) is the population standard deviation, and \\(n\\) is the sample size. As before, let’s visually compare the theoretical expectation to our simulated estimates. se.sd.theory &lt;- sapply(seq(10, 500, 5), function(n){ true.sd/sqrt(2*(n-1))}) df.se.sd.theory &lt;- data_frame(sample.size = seq(10,500,5), std.error = se.sd.theory) ggplot(sampling.distn.sd.table, aes(x = sample.size, y = sd.of.sds)) + # plot standard errors of mean based on our simulations geom_point() + # plot standard errors of the mean based on theory geom_line(aes(x = sample.size, y = std.error), data = df.se.sd.theory, color=&quot;red&quot;) + labs(x = &quot;Sample size&quot;, y = &quot;Std Error of Standard Deviation&quot;, title = &quot;A comparison of theoretical (red line) and\\nsimulated (points) estimates of\\nthe standard error of the standard deviation\\n for samples of different size&quot;) A.9 What happens to the sampling distribution of the mean and standard deviation when our sample size is small? We would hope that, regardless of sample size, the sampling distributions of both the mean and standard deviation should be centered around the true population value, \\(\\mu\\) and \\(\\sigma\\) respectively. That seemed to be the case for the modest to large sample sizes we’ve looked at so far (30 to 500 observations). Does this also hold for small samples? Let’s use simulation to explore how well this is expectation is met for small samples. As we’ve done before, we simulate the sampling distribution of the mean and standard deviation for samples of varying size. Because we’re dealing with small samples we’ll use larger simulations (5000 samples) so we get better estimates of their behavior (in general, the noisier the process you’re simulating the more simulations you should do) # A new version of rnorm.stats that includes the sample estimate of the # standard error of the mean rnorm.stats.2 &lt;- function(n, mu, sigma) { the.sample &lt;- rnorm(n, mu, sigma) data_frame(sample.size = n, sample.mean = mean(the.sample), sample.sd = sd(the.sample), sample.estimate.SE = sample.sd/sqrt(sample.size)) } # sample sizes we&#39;ll consider ssizes &lt;- c(2, 3, 4, 5, 7, 10, 20, 30, 50) # number of samples to draw *for each sample size* nsamples &lt;- 2500 # create a data frame with empty columns df.combined.small &lt;- data_frame(sample.size = double(), sample.mean = double(), sample.sd = double(), sample.estimate.SE = double()) for (i in ssizes) { df.samples.of.size.i &lt;- rerun(nsamples, rnorm.stats.2(i, true.mean, true.sd)) %&gt;% bind_rows() df.combined.small &lt;- bind_rows(df.combined.small, df.samples.of.size.i) } df.combined.small %&lt;&gt;% mutate(sample.sz = as.factor(sample.size)) A.9.1 For small samples, sample standard deviations systematically underestimate the population standard deviation Let’s examine how the well centered the sampling distributions of the mean and standard deviation are around their true values, as a function of sample size. First a table summarizing this information: by.sample.size &lt;- df.combined.small %&gt;% group_by(sample.size) %&gt;% summarize(mean.of.means = mean(sample.mean), mean.of.sds = mean(sample.sd)) by.sample.size ## # A tibble: 9 × 3 ## sample.size mean.of.means mean.of.sds ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 176. 12.4 ## 2 3 176. 13.5 ## 3 4 176. 14.1 ## 4 5 176. 14.1 ## 5 7 176. 14.6 ## 6 10 176. 14.8 ## 7 20 176. 14.9 ## 8 30 176. 15.1 ## 9 50 176. 15.1 We see that the sampling distributions of means are well centered around the true mean for both small and medium, and there is no systematic bias one way or the other. By contrast the sampling distribution of standard deviations tends to underestimate the true standard deviation when the samples are small (less than 30 observations). We can visualize this bias as shown here: ggplot(by.sample.size, aes(x = sample.size, y = mean.of.sds)) + geom_point(color = &#39;red&#39;) + geom_line(color = &#39;red&#39;) + geom_hline(yintercept = true.sd, color = &#39;black&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;Sample Size&quot;, y = &quot;Mean of Sampling Distn of Std Dev&quot;) The source of this bias is clear if we look at the sampling distribution of the standard deviation for samples of size 3, 5, and 30. filtered.df &lt;- df.combined.small %&gt;% filter(sample.size %in% c(3, 5, 30)) ggplot(filtered.df, aes(x = sample.sd, y = ..density.., fill = sample.sz)) + geom_histogram(bins=50, alpha=0.65) + facet_wrap(~sample.size, nrow = 1) + geom_vline(xintercept = true.sd, linetype = &#39;dashed&#39;) + labs(x = &quot;Std Deviations&quot;, y = &quot;Density&quot;, title = &quot;Sampling distributions of the standard deviation\\nas a function of sample size&quot;) There’s very clear indication that the the sampling distribution of standard deviations is not centered around the true value for \\(n=3\\) and for \\(n=5\\), however with samples of size 30 the sampling distribution of the standard deviation appears fairly well centered around the true value of the underlying population. A.9.2 Underestimates of the standard deviation for small \\(n\\) lead to understimates of the SE of the mean When sample sizes are small, sample estimates of the standard deviation, \\(s_x\\), tend to underestimate the true standard deviation, \\(\\sigma\\). Given this, it follows that sample estimates of the standard error of the mean, \\(SE_{\\overline{x}} = \\frac{s_x}{\\sqrt{n}}\\), must tend to understimate the true standard error of the mean, \\(SE_\\mu = \\frac{\\sigma}{\\sqrt{n}}\\). A.9.3 The t-distribution is the appropriate distribution for describing the sampling distribution of the mean when \\(n\\) is small The problem associated with estimating the standard error of the mean for small sample sizes was recognized in the early 20th century by William Gosset, an employee at the Guinness Brewing Company. He published a paper, under the pseudonym “Student”, giving the appropriate distribution for describing the standard error of the mean as a function of the sample size \\(n\\). Gosset’s distribution is known as the “t-distribution” or “Student’s t-distribution”. The t-distribution is specified by a single parameter, called degrees of freedom (\\(df\\)) where \\({df} = n - 1\\). As \\(df\\) increases, the t-distribution becomes more and more like the normal such that when \\(n \\geq 30\\) it’s nearly indistinguishable from the standard normal distribution. In the figures below we compare the t-distribution and the standard normal distribution for sample sizes \\(n = {3, 5, 30}\\). x &lt;- seq(-6, 6, length.out = 200) n &lt;- c(3, 5, 30) # sample sizes distns.df &lt;- data_frame(sample.size = double(), z.or.t = double(), norm.density = double(), t.density = double()) for (i in n) { norm.density &lt;- dnorm(x, mean = 0, sd = 1) t.density &lt;- dt(x, df = i - 1) df.temp &lt;- data_frame(sample.size = i, z.or.t = x, norm.density = norm.density, t.density = t.density) distns.df &lt;- bind_rows(distns.df, df.temp) } distns.df %&lt;&gt;% mutate(df = as.factor(sample.size - 1)) ggplot(distns.df, aes(x = z.or.t, y = t.density, color = df)) + geom_line() + geom_line(aes(y = norm.density), color=&#39;black&#39;, linetype=&quot;dotted&quot;) + labs(x = &quot;z or t value&quot;, y = &quot;Probablity density&quot;, title = &quot;Standard normal distribution (black dotted line)\\nversus t-distributions for different degrees of freedom&quot;) US Dept. of Health and Human Services; et al. (August 2016). “Anthropometric Reference Data for Children and Adults: United States, 2011–2014” (PDF). National Health Statistics Reports. 11. https://www.cdc.gov/nchs/data/series/sr_03/sr03_039.pdf↩︎ "],["simulating-confidence-intervals.html", "B Simulating confidence intervals B.1 Confidence intervals B.2 Generic formulation for confidence intervals B.3 Example: Confidence intervals for the mean B.4 A problem arises! B.5 Confidence intervals under sample estimates of standard errors B.6 t-distribution for confidence intervals of the mean", " B Simulating confidence intervals Recall the concept of the sampling distribution of a statistic – this is simply the probability distribution of the statistic of interest you would observe if you took a large number of random samples of a given size from a population of interest and calculated that statistic for each of the samples. You learned that the standard deviation of the sampling distribution of a statistic has a special name – the standard error of that statistic. The standard error of a statistic provides a way to quantify the uncertainty of a statistic across random samples. Here we show how to use information about the standard error of a statistic to calculate plausible ranges for a statistic of interest that take into account the uncertainty of our estimates. We call such plausible ranges Confidence Intervals. B.1 Confidence intervals We know that given a random sample from a population of interest, the value of a statistic of interest is unlikely to be exactly equally to the true population value of that statistic. However, our simulations have taught us a number of things: As sample size increases, the sample estimate of the given statistic is more likely to be close to the true value of that statistic As sample size increases, the standard error of the statistic decreases We will define an “X% percent confidence interval for a statistic of interest”, as an interval (upper and lower bound) that when calculated from a random sample, would include the true population value of the statistic of interest, X% of the time. This quote from the NIST page on confidence intervals, which I’ve adapted to refer to any statistic, helps to make this concrete regarding confidence intervals: As a technical note, a 95% confidence interval does not mean that there is a 95% probability that the interval contains the true [statistic]. The interval computed from a given sample either contains the true [statistic] or it does not. Instead, the level of confidence is associated with the method of calculating the interval … That is, for a 95% confidence interval, if many samples are collected and the confidence interval computed, in the long run about 95% of these intervals would contain the true [statistic]. The idea behind a 95% confidence interval is illustrated in the following figure: Figure B.1: Point estimates and confidence intervals for a theoretical statistic of interest. B.2 Generic formulation for confidence intervals We define the \\((100\\times\\beta)\\)% confidence interval for the statistic \\(\\phi\\) as the interval: \\[ CI_\\beta = \\phi_{n} \\pm (z \\times {SE}_{\\phi,n}) \\] Where: \\(\\phi_{n}\\) is the statistic of interest in a random sample of size \\(n\\) \\({SE}_{\\phi,n}\\) is the standard error of the statistic \\(\\phi\\) (via simulation or analytical solution) And the value of \\(z\\) is chosen so that: across many different random samples of size \\(n\\), the true value of the \\(\\phi\\) in the population of interest would fall within the interval approximately \\((100\\times\\beta)\\)% of the time So rather than estimating a single value of \\(\\phi\\) from our data, we will use our observed data plus knowledge about the sampling distribution of \\(\\phi\\) to estimate a range of plausible values for \\(\\phi\\). The size of this interval will be chosen so that if we considered many possible random samples, the true population value of \\(\\phi\\) would be bracketed by the interval in \\((100\\times\\beta)\\)% of the samples. B.3 Example: Confidence intervals for the mean To make the idea of a confidence interval more concrete, let’s consider confidence intervals for the mean of a normally distributed variable. Recall that if a variable \\(X\\) is normally distributed in a population of interest, \\(X \\sim N(\\mu, \\sigma)\\), then the sampling distribution of the mean of \\(X\\) is also normally distributed with mean \\(\\mu\\), and standard error \\({SE}_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\): \\[ \\overline{X} \\sim N \\left( \\mu, \\frac{\\sigma}{\\sqrt{n}}\\ \\right) \\] In our simulation we will explore how varying the value of \\(z\\) in the formula \\(CI_\\beta = \\phi_{n} \\pm (z \\times {SE}_{\\phi,n})\\) changes the percentage of times that the confidence interval brackets the true population mean. B.3.1 Simulation of means In our simulation we’re going to generate a large number of samples, and for each sample we will calculate the sample estimate of the mean, and then quantify how much each sample mean differs from the true mean in terms of units of the population standard error of the mean. We’ll then use this information to calibrate the wide of our confidence intervals. For the sake of simplicity we’ll simulate sampling from the “Standard Normal Distribution” – a normal distribution with mean \\(\\mu=0\\), and standard deviation \\(\\sigma=1\\). First we load our standard libraries: library(tidyverse) library(magrittr) library(cowplot) set.seed(20190416) # initialize RNG Then we write our basic framework for our simulations: rnorm.stats &lt;- function(n, mu, sigma) { s &lt;- rnorm(n, mu, sigma) df &lt;- data_frame(sample.size = n, sample.mean = mean(s), sample.sd = sd(s), pop.SE = sigma/sqrt(n)) } And then use this to simulate samples of size 50. true.mean &lt;- 0 true.sd &lt;- 11 n &lt;- 50 samples.50 &lt;- rerun(10000, rnorm.stats(n, true.mean, true.sd)) %&gt;% bind_rows() B.3.2 Distance between sample means and true means We append a new column to our samples.50 data frame, which is the result of calculating the distance of each sample mean from the true mean, expressed in terms of units of the population standard error of the mean: samples.50 &lt;- samples.50 %&gt;% mutate(z.pop = (sample.mean - true.mean)/pop.SE) Since the sampling distribution of the mean of a normally distributed variable (\\(N(\\mu,\\sigma)\\)) is itself normally distributed (\\(N(\\mu, SE_{\\overline{X}})\\)), then the distribution of \\(z = \\frac{\\overline{X} - \\mu}{SE}\\) is \\(N(0,1)\\). This is illustrated in the figure below where we compare our simulated z-scores to the theoretical expectation: SE &lt;- 1 ggplot(samples.50) + geom_histogram(aes(x = z.pop, y=..density..), bins=50) + stat_function(fun = function(x){dnorm(x, mean=0, sd=SE)}, color=&quot;red&quot;) For a given value of \\(z\\) we can ask what fraction of our simulated means fall within \\(\\pm z\\) standard errors of the true mean. samples.50 %&gt;% summarize(frac.win.1SE = sum(abs(z.pop) &lt;= 1)/n(), frac.win.2SE = sum(abs(z.pop) &lt;= 2)/n()) ## # A tibble: 1 × 2 ## frac.win.1SE frac.win.2SE ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.683 0.957 We see that roughly 68% of our sample means are within 1 SE of the true mean; ~95% are within 2 SEs. If we wanted to get exact multiples of the SE corresponding to different percentiles of the distribution of z-scores, based on the theoretical result (z scores ~ \\(N(0,1)\\)), we can use the qnorm() function: frac.of.interest &lt;- c(0.68, 0.90, 0.95, 0.99) # we use 1 - frac to get left most critical value # we divide by two here two account for area under left and right tails left.critical.value &lt;- qnorm((1 - frac.of.interest)/2, mean = 0, sd=1) data_frame(Percentile = frac.of.interest * 100, Critical.value = abs(left.critical.value)) ## # A tibble: 4 × 2 ## Percentile Critical.value ## &lt;dbl&gt; &lt;dbl&gt; ## 1 68 0.994 ## 2 90 1.64 ## 3 95 1.96 ## 4 99 2.58 B.3.3 Calculating a CI If we knew the standard error of the mean for variable of interest, in order to a confidence interval we could simply look up the corresponding critical value for our percentile of interst in a table like the one above and calculate our CI as: \\[ \\overline{X} \\pm \\text{critical value} \\times SE_{\\overline{X}} \\] For example, we see that the critical value for 95% CIs is ~1.96. B.4 A problem arises! If you’re a critical reader you should have noticed that calculating confidence intervals using the above formula presumes we know the standard error of the mean for the variable of interest. If we knew the standard deviation, \\(\\sigma\\), of our variable, we could calculate this as \\(SE_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) but in general we do not know \\(\\sigma\\) either. Instead we must estimate the standard error of the mean using our sample standard deviation: \\[ \\widehat{SE}_{\\overline{x}} = \\frac{s_x}{\\sqrt{n}} \\] This introduces another level of uncertainty and also a complication. The complication is due to the fact that for small samples, sample estimates of the standard deviation tend to be biased (smaller) relative to the true population standard deviation (see previous chapter). B.5 Confidence intervals under sample estimates of standard errors Let’s extend our simulations to gain insight into how using sample estimates of standard errors affect our calculations of confidence interval. First we’ll rewrite our rnorm.stats function to include columns for both the true values of the mean, standard deviation, and standard error of the mean as well as the sample estimates of those statistics. rnorm.stats &lt;- function(n, mu, sigma) { s &lt;- rnorm(n, mu, sigma) df &lt;- data_frame(sample.size = n, true.mean = mu, true.sd = sigma, true.se = sigma/sqrt(n), sample.mean = mean(s), sample.sd = sd(s), estimated.se = sample.sd/sqrt(n)) df } B.5.1 The spread of sample estimates of the mean in units of true and estimates SEs Now we run simulations for samples of size 5 and 50, and calculate z-scores for the sample means relative to the true mean in terms of units of standard error (both true and estiamted). samples.5 &lt;- rerun(10000, rnorm.stats(5, true.mean, true.sd)) %&gt;% bind_rows() samples.5 &lt;- samples.5 %&gt;% mutate(z.true = (sample.mean - true.mean)/true.se, z.est = (sample.mean - true.mean)/estimated.se) samples.50 &lt;- rerun(10000, rnorm.stats(50, true.mean, true.sd)) %&gt;% bind_rows() samples.50 &lt;- samples.50 %&gt;% mutate(z.true = (sample.mean - true.mean)/true.se, z.est = (sample.mean - true.mean)/estimated.se) First we’ll look at the distribution of z-scores for samples of size 50: samples.50 %&gt;% ggplot() + geom_density(aes(x=z.true), alpha=0.5, color=&#39;blue&#39;, fill=&#39;blue&#39;) + geom_density(aes(x=z.est), alpha=0.25, color=&#39;red&#39;, fill=&#39;red&#39;) + labs(title=&quot;Distribution of z.true(blue) and z.est(red)\\nfor 10,000 samples of size 50&quot;) We see that the distributions are similar whether we used the true SE of the mean, or the estimated SEs. This reflects the fact that the sample estimate of the SE of the mean is a good estimator when sample sizes are relatively large. Now we do the same for samples of size 5: samples.5 %&gt;% ggplot() + geom_density(aes(x=z.true), alpha=0.5, color=&#39;blue&#39;, fill=&#39;blue&#39;) + geom_density(aes(x=z.est), alpha=0.25, color=&#39;red&#39;, fill=&#39;red&#39;) + labs(title=&quot;Distribution of z.true(blue) and z.est(red)\\nfor 10,000 samples of size 5&quot;) + xlim(-10,10) Unlike in the previous case, for samples of sie 5 we find that the distribution of z-scores is significantly wider based on samples estimates of the SE of the mean, rather than the true value of the SE of the mean. This reflects the fact that our sample estimates of the SE of the mean systematically underestimate the true value when sample sizes are small. B.5.2 Quantifying deviations of sample estimates of the mean To quantify the patterns that is apparent in the figures we generated previously, we can ask, “What fraction of sample means are less than on, two, and three standard errors away from the true mean?” We will ask this question using z-scores based on both the true SE of the mean as well as the estimated SEs of the mean. B.5.2.1 Samples of size 5 First for samples of size 5, based on the true SEs: samples.5 %&gt;% summarize(less.1SE = sum(abs(z.true) &lt; 1)/n(), less.2SE = sum(abs(z.true) &lt; 2)/n(), less.3SE = sum(abs(z.true) &lt; 3)/n()) ## # A tibble: 1 × 3 ## less.1SE less.2SE less.3SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.686 0.953 0.997 Now for the same samples of size 5, based on the estimated SEs: samples.5 %&gt;% summarize(less.1SE = sum(abs(z.est) &lt; 1)/n(), less.2SE = sum(abs(z.est) &lt; 2)/n(), less.3SE = sum(abs(z.est) &lt; 3)/n()) ## # A tibble: 1 × 3 ## less.1SE less.2SE less.3SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.624 0.886 0.966 Interpretation: For samples of size 5, if we use sample estimates of the standard error of the mean as our “ruler”, we must use an interval of about 3 SE units to capture 95% of the distribution of sample means. Contrast this with the situation in which we knew the true SE; in that case we only had to consider a spread of approximately +/- 2 SE units to capture 95% of the sampling distribution of the mean. B.5.2.2 Samples of size 50 Now we ask the same questions for samples of size 50: samples.50 %&gt;% summarize(less.1SE = sum(abs(z.true) &lt; 1)/n(), less.2SE = sum(abs(z.true) &lt; 2)/n(), less.3SE = sum(abs(z.true) &lt; 3)/n()) ## # A tibble: 1 × 3 ## less.1SE less.2SE less.3SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.683 0.956 0.997 samples.50 %&gt;% summarize(less.1SE = sum(abs(z.est) &lt; 1)/n(), less.2SE = sum(abs(z.est) &lt; 2)/n(), less.3SE = sum(abs(z.est) &lt; 3)/n()) ## # A tibble: 1 × 3 ## less.1SE less.2SE less.3SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.681 0.949 0.995 Interpretation: For samples of size 50, we get similar estimats of the spread of the sampling distribution of the mean regardless of whether we use the true SE or the estiamted SEs. B.6 t-distribution for confidence intervals of the mean The “Student’s t-distribution” is the appropriate distribution to use when we want to estimate the sampling distribution of the mean of a normally distributed variable when samples sizes are small (very frequently) and/or the standard deviation of the population we’re sampling from is unknown (almost always the case). You can think of values of \\(t\\) as multiples of the estimated standard error. The t-distribution is specified by a single parameter, called degrees of freedom (\\(df\\)) where \\({df} = n - 1\\). As \\(df\\) increases, the t-distribution becomes more and more like the “standard normal distribution” (\\(N(0,1)\\). You can use the function dt() to calculate the t-distribution for appropriate degrees of freedom. For example, here we compare the t-distributions with 4 and 49 degrees of freedom, corresponding to samples of size 5 and 50: npts &lt;- 500 df.t5 &lt;- data_frame(sample.size = rep(5, npts), t = seq(-10, 10, length.out = npts), density = dt(t, df=4)) df.t50 &lt;- data_frame(sample.size = rep(50, npts), t = seq(-10, 10, length.out = npts), density = dt(t, df=49)) df.t &lt;- bind_rows(df.t5, df.t50) df.t %&gt;% ggplot(aes(x = t, y = density, color=as.factor(sample.size))) + geom_line() B.6.1 t-distribution vs standard normal distribution For large degrees of freedom, the t-distribution and the standard normal distribution are very similar. For example, let’s compare these disetributions when df=49: df.norm &lt;- data_frame(t = seq(-10, 10, length.out = npts), density = dnorm(t)) t.vs.normal.df49 &lt;- df.t50 %&gt;% ggplot(aes(x = t, y = density)) + geom_line(color=&#39;red&#39;) + geom_line(data=df.norm, color = &#39;blue&#39;) + labs(title = &quot;Standard normal distribution (blue) vs t(df=49) distn.&quot;) + theme(plot.title = element_text(size = 10)) t.vs.normal.df49 However, when the degrees of freedom is small, the t-distribution and the standard normal distribution differ appreciably. b) Draw a figure comparing the t-distribution for samples of size 5 to the standard normal distribution. Draw another figure comparing the t-distribution for samples of size 50 to the standard normal distribution. Use cowplot to display these two figure as &quot;A&quot; and &quot;B&quot; subfigures. [3 pts] t.vs.normal.df4&lt;- df.t5 %&gt;% ggplot(aes(x = t, y = density)) + geom_line(color=&#39;red&#39;) + geom_line(data=df.norm, color = &#39;blue&#39;) + labs(title = &quot;Standard normal distribution (blue) vs t(df=4) distn.&quot;) + theme(plot.title = element_text(size = 10)) t.vs.normal.df4 B.6.2 Formula for confidence intervals of the mean based on sample estimates of the SE and the t-distribution To calculate the lower and upper bounds of the 95% confidence intervals for the mean based on the t-distribution, for a sample of size \\(n\\), we can use the following formulas: \\[ \\begin{align} \\text{lower bound:}\\ \\overline{x} + t_{0.025}(df = n-1) \\times \\widehat{SE}_\\overline{x} \\\\ \\text{upper bound:}\\ \\overline{x} + t_{0.975}(df = n-1) \\times \\widehat{SE}_\\overline{x} \\end{align} \\] where \\(t_{0.025}(df = n-1)\\) and \\(t_{0.975}(df = n-1)\\) are the 2.5-percentile and 97.5-percentile of the t-distribution with \\(n-1\\) degrees of freedom. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
