[
["index.html", "Biology 723: Statistical Computing for Biologists Chapter 1 Introduction 1.1 Accessing older versions of the course notes 1.2 How to use these lecture notes", " Biology 723: Statistical Computing for Biologists Paul M. Magwene 2019-04-17 Chapter 1 Introduction Bio 723 is a course I offer at Duke University. The focus of this course is statistical computing for the biological sciences with an emphasis on common multivariate statistical methods and techniques for exploratory data analysis. A major goal of the course is to help graduate students in the biological sciences develop practical insights into methods that they are likely to encounter in their own research, and the potential advantages and pitfalls that come with their use. In terms of mathematical perspectives, the course emphasize a geometric approach to understanding multivariate statistics. I try to help students develop an intuition for the geometry of vector spaces and discuss topics like correlation, regression, and principal components analysis in terms of angles between vectors, dot products, and projection. This course also provides an introduction to the R language/statistical computing environment. 1.1 Accessing older versions of the course notes The material covered in Bio 723 changes a bit from year to year. If you’d like to access older versions of the course notes, I will be making these available on the “Releases” page of the Github site for this book. 1.2 How to use these lecture notes In this and future materials to be posted on the course website you’ll encounter blocks of R code. Your natural intuition will be to cut and paste commands or code blocks into the R interpretter to save yourself the typing. DO NOT DO THIS!! In each of the examples below, I provide example input, but I don’t show you the output. It’s your job to type in these examples at the R console, evaluate what you typed, and to look at and think critically about the output. You will make mistakes and generate errors! Part of learning any new skill is making mistakes, figuring out where you went wrong, and correcting those mistakes. In the process of fixing those errors, you’ll learn more about how R works, and how to avoid such errors, or correct bugs in your own code in the future. If you cut and paste the examples into the R interpretter the code will run, but you will learn less than if you input the code yourself and you’ll be less capable of apply the concepts in new situations. The R interpretter, like all programming languages, is very exacting. A mispelled variable or function name, a stray period, or an unbalanced parenthesis will raise an error and finding the sources of such errors can sometimes be tedious and frustrating. Persist! If you read your code critically, think about what your doing, and seek help when needed (teaching team, R help, Google, etc) you’ll eventually get better at debugging your code. But keep in mind that like most new skills, learning to write and debug your code efficiently takes time and experience. "],
["getting-started-with-r.html", "Chapter 2 Getting Started with R 2.1 What is R? 2.2 What is RStudio? 2.3 Entering commands in the console 2.4 Comments 2.5 Using R as a Calculator 2.6 Variable assignment 2.7 Data types 2.8 Packages 2.9 The R Help System", " Chapter 2 Getting Started with R 2.1 What is R? R is a statistical computing environment and programming language. It is free, open source, and has a large and active community of developers and users. There are many different R packages (libraries) available for conducting out a wide variety of different analyses, for everything from genome sequence data to geospatial information. 2.2 What is RStudio? RStudio (http://www.rstudio.com/) is an open source integrated development environment (IDE) that provides a nicer graphical interface to R than does the default R GUI. The figure below illustrates the RStudio interface, in it’s default configuration. For the exercises below you’ll be primarily entering commands in the “console” window. We’ll review key parts of the RStudio interface in greater detail in class. Figure 2.1: RStudio window with the panes labeled 2.3 Entering commands in the console You can type commands directly in the console. When you hit Return (Enter) on your keyboard the text you typed is evaluated by the R interpreter. This means that the R program reads your commands, makes sure there are no syntax errors, and then carries out any commands that were specified. Try evaluating the following arithmetic commands in the console: 10 + 5 10 - 5 10 / 5 10 * 5 If you type an incomplete command and then hit Return on your keyboard, the console will show a continuation line marked by a + symbol. For example enter the incomplete statement (10 + 5 and then hit Enter. You should see something like this. &gt; (10 + 5 + The continuation line tells you that R is waiting for additional input before it evaluates what you typed. Either complete your command (e.g. type the closing parenthesis) and hit Return, or hit the “Esc” key to exit the continuation line without evaluating what you typed. 2.4 Comments When working in the R console, or writing R code, the pound symbol (#) indicates the start of a comment. Anything after the #, up to the end of the current line, is ignored by the R interpretter. # This line will be ignored 5 + 4 # the first part of this line, up to the #, will be evaluated Throughout this course I will often include short explanatory comments in my code examples. When I want to display the output generated by an R statement typed at the console I will generally use a display convention in which I prepend the results with the symbols ##. 5 + 4 # same as above but with output displayed ## [1] 9 2.5 Using R as a Calculator The simplest way to use R is as a fancy calculator. Evaluate each of the following statements in the console. 10 + 2 # addition 10 - 2 # subtraction 10 * 2 # multiplication 10 / 2 # division 10 ^ 2 # exponentiation 10 ** 2 # alternate exponentiation pi * 2.5^2 # R knows about some constants such as Pi 10 %% 3 # modulus operator -- gives remainder after division 10 %/% 3 # integer division Be aware that certain operators have precedence over others. For example multiplication and division have higher precedence than addition and subtraction. Use parentheses to disambiguate potentially confusing statements. (10 + 2)/4-5 # was the output what you expected? (10 + 2)/(4-5) # compare the answer to the above Division by zero produces an object that represents infinite numbers. Infinite values can be either positive or negative 1/0 ## [1] Inf -1/0 ## [1] -Inf Invalid calculations produce a objected called NaN which is short for “Not a Number”: 0/0 # invalid calculation ## [1] NaN 2.5.1 Common mathematical functions Many commonly used mathematical functions are built into R. Here are some examples: abs(-3) # absolute value ## [1] 3 cos(pi/3) # cosine ## [1] 0.5 sin(pi/3) # sine ## [1] 0.8660254 log(10) # natural logarithm ## [1] 2.302585 log10(10) # log base 10 ## [1] 1 log2(10) # log base 2 ## [1] 3.321928 exp(1) # exponential function ## [1] 2.718282 sqrt(10) # square root ## [1] 3.162278 10^0.5 # same as square root ## [1] 3.162278 2.6 Variable assignment An important programming concept in all programming languages is that of “variable assignment”. Variable assignment is the act of creating labels that point to particular data values in a computers memory, which allows us to apply operations to the labels rather than directly to specific. Variable assignment is an important mechanism of abstracting and generalizing computational operations. Variable assignment in R is accomplished with the assignment operator, which is designated as &lt;- (left arrow, constructed from a left angular brack and the minus sign). This is illustrated below: x &lt;- 10 # assign the variable name &#39;x&#39; the value 10 sin(x) # apply the sin function to the value x points to ## [1] -0.5440211 x &lt;- pi # x now points to a different value sin(x) # the same function call now produces a different result ## [1] 1.224647e-16 # note that sin(pi) == 0, but R returns a floating point value very # very close to but not zero 2.6.1 Valid variable names As described in the R documentation, “A syntactically valid name consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number. Names such as ‘.2way’ are not valid, and neither are the reserved words.” Here are some examples of valid and invalid variable names. Mentally evaluate these based on the definition above, and then evaluate these in the R interpetter to confirm your understanding : x &lt;- 10 x.prime &lt;- 10 x_prime &lt;- 10 my.long.variable.name &lt;- 10 another_long_variable_name &lt;- 10 _x &lt;- 10 .x &lt;- 10 2.x &lt;- 2 * x 2.7 Data types The phrase “data types” refers to the representations of information that a programming language provides. In R, there are three core data types representing numbes, logical values, and strings. You can use the function typeof() to get information about an objects type in R. 2.7.1 Numeric data types There are three standard types of numbers in R. “double” – this is the default numeric data type, and is used to represent both real numbers and whole numbers (unless you explicitly ask for integers, see below). “double” is short for “double precision floating point value”. All of the previous computations you’ve seen up until this point used data of type double. typeof(10.0) # real number ## [1] &quot;double&quot; typeof(10) # whole numbers default to doubles ## [1] &quot;double&quot; “integer” – when your numeric data involves only whole numbers, you can get slighly better performance using the integer data type. You must explicitly ask for numbers to be treated as integers. typeof(as.integer(10)) # now treated as an integer ## [1] &quot;integer&quot; “complex” – R has a built-in data type to represent complex numbers – numbers with a “real” and “imaginary” component. We won’t encounter the use of complex numbers in this course, but they do have many important uses in mathematics and engineering and also have some interesting applications in biology. typeof(1 + 0i) ## [1] &quot;complex&quot; sqrt(-1) # sqrt of -1, using doubles ## [1] NaN sqrt(-1 + 0i) # sqrt of -1, using complex numbers ## [1] 0+1i 2.7.2 Logical values When we compare values to each other, our calculations no longer return “doubles” but rather TRUE and FALSE values. This is illustrated below: 10 &lt; 9 # is 10 less than 9? ## [1] FALSE 10 &gt; 9 # is 10 greater than 9? ## [1] TRUE 10 &lt;= (5 * 2) # less than or equal to? ## [1] TRUE 10 &gt;= pi # greater than or equal to? ## [1] TRUE 10 == 10 # equals? ## [1] TRUE 10 != 10 # does not equal? ## [1] FALSE TRUE and FALSE objects are of “logical” data type (known as “Booleans” in many other languages, after the mathematician George Boole). typeof(TRUE) typeof(FALSE) x &lt;- FALSE typeof(x) # x points to a logical x &lt;- 1 typeof(x) # the variable x no longer points to a logical When working with numerical data, tests of equality can be tricky. For example, consider the following two comparisons: 10 == (sqrt(10)^2) # Surprised by the result? See below. 4 == (sqrt(4)^2) # Even more confused? Mathematically we know that both \\((\\sqrt{10})^2 = 10\\) and \\((\\sqrt{4})^2 = 4\\) are true statements. Why does R tell us the first statement is false? What we’re running into here are the limits of computer precision. A computer can’t represent \\(\\sqrt 10\\) exactly, whereas \\(\\sqrt 4\\) can be exactly represented. Precision in numerical computing is a complex subject and a detailed discussion is beyond the scope of this course. However, it’s important to be aware of this limitation (this limitation is true of any programming language, not just R). To test “near equality” R provides a function called all.equal(). This function takes two inputs – the numerical values to be compared – and returns TRUE if their values are equal up to a certain level of tolerance (defined by the built-in numerical precision of your computer). all.equal(10, sqrt(10)^2) ## [1] TRUE Here’s another example where the simple equality operator returns an unexpected result, but all.equal() produces the comparison we’re likely after. sin(pi) == 0 ## [1] FALSE all.equal(sin(pi), 0) ## [1] TRUE 2.7.2.1 Logical operators Logical values support Boolean operations, like logical negation (“not”), “and”, “or”, “xor”, etc. This is illustrated below: !TRUE # logical negation -- reads as &quot;not x&quot; ## [1] FALSE TRUE &amp; FALSE # AND: are x and y both TRUE? ## [1] FALSE TRUE | FALSE # OR: are either x or y TRUE? ## [1] TRUE xor(TRUE,FALSE) # XOR: is either x or y TRUE, but not both? ## [1] TRUE The function isTRUE can be useful for evaluating the state of a variable: x &lt;- sample(1:10, 1) # sample a random number in the range 1 to 10 isTRUE(x &gt; 5) # was the random number picked greater than 5? ## [1] TRUE 2.7.3 Character strings Character strings (“character”) represent single textual characters or a longer sequence of characters. They are created by enclosing the characters in text either single our double quotes. typeof(&quot;abc&quot;) # double quotes ## [1] &quot;character&quot; typeof(&#39;abc&#39;) # single quotes ## [1] &quot;character&quot; Character strings have a length, which can be found using the nchar function: first.name &lt;- &quot;jasmine&quot; nchar(first.name) ## [1] 7 last.name &lt;- &#39;smith&#39; nchar(last.name) ## [1] 5 There are a number of built-in functions for manipulating character strings. Here are some of the most common ones. 2.7.3.1 Joining strings The paste() function joins two characters strings together: paste(first.name, last.name) # join two strings ## [1] &quot;jasmine smith&quot; paste(&quot;abc&quot;, &quot;def&quot;) ## [1] &quot;abc def&quot; Notice that paste() adds a space between the strings? If we didn’t want the space we can call the paste() function with an optional argument called sep (short for separator) which specifies the character(s) that are inserted between the joined strings. paste(&quot;abc&quot;, &quot;def&quot;, sep = &quot;&quot;) # join with no space; &quot;&quot; is an empty string ## [1] &quot;abcdef&quot; paste(&quot;abc&quot;, &quot;def&quot;, sep = &quot;|&quot;) # join with a vertical bar ## [1] &quot;abc|def&quot; 2.7.3.2 Splitting strings The strsplit() function allows us to split a character string into substrings according to matches to a specified split string (see ?strsplit for details). For example, we could break a sentence into it’s constituent words as follows: sentence &lt;- &quot;Call me Ishmael.&quot; words &lt;- strsplit(sentence, &quot; &quot;) # split on space words ## [[1]] ## [1] &quot;Call&quot; &quot;me&quot; &quot;Ishmael.&quot; Notice that strsplit() is the reverse of paste(). 2.7.3.3 Substrings The substr() function allows us to extract a substring from a character object by specifying the first and last positions (indices) to use in the extraction: substr(&quot;abcdef&quot;, 2, 5) # get substring from characters 2 to 5 ## [1] &quot;bcde&quot; substr(first.name, 1, 3) # get substring from characters 1 to ## [1] &quot;jas&quot; 2.8 Packages Packages are libraries of R functions and data that provide additional capabilities and tools beyond the standard library of functions included with R. Hundreds of people around the world have developed packages for R that provide functions and related data structures for conducting many different types of analyses. Throughout this course you’ll need to install a variety of packages. Here I show the basic procedure for installing new packages from the console as well as from the R Studio interface. 2.8.1 Installing packages from the console The function install.packages() provides a quick and conveniet way to install packages from the R console. 2.8.2 Install the tidyverse package To illustrate the use of install.packages(), we’ll install a collection of packages (a “meta-package”) called the tidyverse. Here’s how to install the tidyverse meta-package from the R console: install.packages(&quot;tidyverse&quot;, dependencies = TRUE) The first argument to install.packages gives the names of the package we want to install. The second argument, dependencies = TRUE, tells R to install any additional packages that tidyverse depends on. 2.8.3 Installing packages from the RStudio dialog You can also install packages using a graphical dialog provided by RStudio. To do so pick the Packages tab in RStudio, and then click the Install button. Figure 2.2: The Packages tab in RStudio In the packages entry box you can type the name of the package you wish to install. Let’s install another useful package called “stringr”. Type the package name in the “Packages” field, make sure the “Install dependencies” check box is checked, and then press the “Install” button. Figure 2.3: Package Install Dialog 2.8.4 Loading packages with the library() function Once a package is installed on your computer, the package can be loaded into your R session using the library function. To insure our previous install commands worked correctly, let’s load one of the packages we just installed. library(tidyverse) Since the tidyverse pacakge is a “meta-package” it provides some additional info about the sub-packages that got loaded. When you load tidyverse, you will also see a message about “Conflicts” as several of the functions provided in the dplyr package (a sub-package in tidyverse) conflict with names of functions provided by the “stats” package which usually gets automically loaded when you start R. The conflicting funcdtions are filter and lag. The conflicting functions in the stats package are lag and filter which are used in time series analysis. The dplyr functions are more generally useful. Furthermore, if you need these masked functions you can still access them by prefacing the function name with the name of the package (e.g. stats::filter). We will use the “tidyverse” package for almost every class session and assignment in this class. Get in the habit of including the library(tidyverse) statement in all of your R documents. 2.9 The R Help System R comes with fairly extensive documentation and a simple help system. You can access HTML versions of the R documentation under the Help tab in Rstudio. The HTML documentation also includes information on any packages you’ve installed. Take a few minutes to browse through the R HTML documentation. In addition to the HTML documentation there is also a search box where you can enter a term to search on (see red arrow in figure below). Figure 2.4: The RStudio Help tab 2.9.1 Getting help from the console In addition to getting help from the RStudio help tab, you can directly search for help from the console. The help system can be invoked using the help function or the ? operator. help(&quot;log&quot;) ?log If you are using RStudio, the help results will appear in the “Help” tab of the Files/Plots/Packages/Help/Viewer (lower right window by default). What if you don’t know the name of the function you want? You can use the help.search() function. help.search(&quot;log&quot;) In this case help.search(\"log\") returns all the functions with the string log in them. For more on help.search type ?help.search. Other useful help related functions include apropos() and example(), vignette(). apropos returns a list of all objects (including function names) in the current session that match the input string. apropos(&quot;log&quot;) ## [1] &quot;as.data.frame.logical&quot; &quot;as.logical&quot; ## [3] &quot;as.logical.factor&quot; &quot;dlogis&quot; ## [5] &quot;is.logical&quot; &quot;log&quot; ## [7] &quot;log10&quot; &quot;log1p&quot; ## [9] &quot;log2&quot; &quot;logb&quot; ## [11] &quot;Logic&quot; &quot;logical&quot; ## [13] &quot;logLik&quot; &quot;loglin&quot; ## [15] &quot;plogis&quot; &quot;qlogis&quot; ## [17] &quot;rlogis&quot; &quot;SSlogis&quot; example() provides examples of how a function is used. example(log) ## ## log&gt; log(exp(3)) ## [1] 3 ## ## log&gt; log10(1e7) # = 7 ## [1] 7 ## ## log&gt; x &lt;- 10^-(1+2*1:9) ## ## log&gt; cbind(x, log(1+x), log1p(x), exp(x)-1, expm1(x)) ## x ## [1,] 1e-03 9.995003e-04 9.995003e-04 1.000500e-03 1.000500e-03 ## [2,] 1e-05 9.999950e-06 9.999950e-06 1.000005e-05 1.000005e-05 ## [3,] 1e-07 1.000000e-07 1.000000e-07 1.000000e-07 1.000000e-07 ## [4,] 1e-09 1.000000e-09 1.000000e-09 1.000000e-09 1.000000e-09 ## [5,] 1e-11 1.000000e-11 1.000000e-11 1.000000e-11 1.000000e-11 ## [6,] 1e-13 9.992007e-14 1.000000e-13 9.992007e-14 1.000000e-13 ## [7,] 1e-15 1.110223e-15 1.000000e-15 1.110223e-15 1.000000e-15 ## [8,] 1e-17 0.000000e+00 1.000000e-17 0.000000e+00 1.000000e-17 ## [9,] 1e-19 0.000000e+00 1.000000e-19 0.000000e+00 1.000000e-19 The vignette() function gives longer, more detailed documentation about libraries. Not all libraries include vignettes, but for those that do it’s usually a good place to get started. For example, the stringr package (which we installed above) includes a vignette. To read it’s vignette, type the following at the console vignette(&quot;stringr&quot;) "],
["r-markdown-and-r-notebooks.html", "Chapter 3 R Markdown and R Notebooks 3.1 R Notebooks 3.2 Creating an R Notebook 3.3 The default R Notebook template 3.4 Code and Non-code blocks 3.5 Running a code chunk 3.6 Running all code chunks above 3.7 “Knitting an” R Markdown to HTML 3.8 Sharing your reproducible R Notebook", " Chapter 3 R Markdown and R Notebooks RStudio comes with a useful set of tools, collectively called R Markdown, for generating “literate” statistical analyses. The idea behind literate statistical computing is that we should try to carry out our analyses in a manner that is transparent, self-explanatory, and reproducible. Literate statistical computing helps to ensure your research is reproducible because: The steps of your analyses are explicitly described, both as written text and the code and function calls used. Analyses can be more easily checked for correctness and reproduced from your literate code. Your literate code can serve as a template for future analyses, saving you time and the trouble of remembering all the gory details. As we’ll see, R Markdown will allow us to produce statistical documents that integrate prose, code, figures, and nicely formatted mathematics so that we can share and explain our analyses to others. Sometimes those “others” are advisors, supervisors, or collaborators; sometimes the “other” is you six months from now. For the purposes of this class, you will be asked to complete problem sets in the form of R Markdown documents. R Markdown documents are written in a light-weight markup language called Markdown. Markdown provides simple plain text “formatting” commands for specifying the structured elements of a document. Markdown was invented as a lightweight markup language for creating web pages and blogs, and has been adopted to a variety of different purposes. This chaptern provides a brief introduction to the capabilities of R Markdown. For more complete details, including lots of examples, see the R Markdown Website. 3.1 R Notebooks We’re going to create a type of R Markdown document called an “R Notebook”. The R Notebook Documentation describes R Notebooks as so: “An R Notebook is an R Markdown document with code chunks that can be executed independently and interactively, with output visible immediately beneath the input.” 3.2 Creating an R Notebook To create an R Notebook select File &gt; New File &gt; R Notebook from the files menu in RStudio. Figure 3.1: Using the File menu to create a new R Notebook. 3.3 The default R Notebook template The standard template that RStudio creates for you includes a header section like the following where you can specify document properties such as the title, author, and change the look and feel of the generated HTML document. --- title: &quot;R Notebook&quot; output: html_notebook --- The header is followed by several example sections that illustrate a few of the capabilities of R Markdown. Delete these and replace them with your own code as necessary. 3.4 Code and Non-code blocks R Markdown documents are divided into code blocks (also called “chunks”) and non-code blocks. Code blocks are sets of R commands that will be evalauted when the R Markdown document is run or “knitted” (see below). Non-code blocks include explanatory text, embedded images, etc. The default notebook template includes both code and non-code blocks. 3.4.1 Non-code blocks The first bit of text in the default notebook template is a non-code block that tells you how to use the notebook: This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. The text of non-code blocks can include lightweight markup information that can be used to format HTML or PDF output generated from the R Markdown document. Here are some examples: # Simple textual formatting This is a paragraph with plain text. Nothing fancy will happen here. This is a second paragraph with *italic*, **bold**, and `verbatim` text. # Lists ## Bullet points lists This is a list with bullet points: * Item a * Item b * Item c ## Numbered lists This is a numbered list: 1. Item 1 #. Item 2 #. Item 3 ## Mathematics R Markdown supports mathematical equations, formatted according to LaTeX conventions. Dollar signs ($) are used to offset mathematics like so: $x^2 + y^2 = z^2$. Notice from the example above that R Markdown supports LaTeX style formatting of mathematical equations. For example, $x^2 + y^2 = z^2$ appears as \\(x^2 + y^2 = z^2\\). 3.4.2 Code blocks Code blocks are delimited by matching sets of three backward ticks (```). Everything within a code block is interpretted as an R command and is evaluated by the R interpretter. Here’s the first code block in the default notebook template: ``` {r} plot(cars) ``` 3.5 Running a code chunk You can run a single code block by clicking the small green “Run” button in the upper right hand corner of the code block as shown in the image below. Figure 3.2: Click the Run button to execute a code chunk. If you click this button the commands within this code block are executed, and any generated output is shown below the code block. Try running the first code block in the default template now. After the code chunk is executed you should see a plot embedded in your R Notebook as shown below: Figure 3.3: An R Notebook showing an embedded plot after executing a code chunk. 3.6 Running all code chunks above Next to the “Run” button in each code chunk is a button for “Run all chunks above” (see figure below). This is useful when the code chunk you’re working on depends on calculations in earlier code chunks, and you want to evaluated those earlier code chunks prior to running the focal code chunk. Figure 3.4: Use the ‘Run all chunks above’ button to evaluate all previous code chunks. 3.7 “Knitting an” R Markdown to HTML Save your R Notebook as first_rnotebook.Rmd (RStudio will automatically add the .Rmd extension so you don’t need to type it). You can generate an HTML version of your notebook by clicking the “Preview” menu on the Notebook taskbar and then choosing “Knit to HTML” (see image below). Figure 3.5: Use the ‘Knit to HTML’ menu to generate HTML output from your R Notebook When an RMarkdown document is “knit”, all of the code and non-code blocks are executed in a “clean” environment, in order from top to bottom. An output file is generated (HTML or one of the other available output types) that shows the results of executing the notebook. By default RStudio will pop-up a window showing you the HTML output you generated. Knitting a document is a good way to make sure your analysis is reproducible. If your code compiles correctly when the document is knit, and produces the expected output, there’s a good chance that someone else will be able to reproduce your analyses independently starting with your R Notebook document (after accounting for differences in file locations). 3.8 Sharing your reproducible R Notebook To share your R Notebook with someone else you just need to send them the source R Markdown file (i.e. the file with the .Rmd extension). Assuming they have access to the same source data, another user should be able to open the notebook file in RStudio and regenerate your analyses by evaluating the individual code chunks or knitting the document. In this course you will be submitting homework assignments in the form of R Notebook markdown files. "],
["data-structures.html", "Chapter 4 Data structures 4.1 Vectors 4.2 Lists 4.3 Data frames", " Chapter 4 Data structures In computer science, the term “data structure” refers to the ways that data are stored, retrieved, and organized in a computer’s memory. Common examples include lists, hash tables (also called dictionaries), sets, queues, and trees. Different types of data structures are used to support different types of operations on data. In R, the three basic data structures are vectors, lists, and data frames. 4.1 Vectors Vectors are the core data structure in R. Vectors store an ordered lists of items, all of the same type (i.e. the data in a vector are “homogenous” with respect to their type). The simplest way to create a vector at the interactive prompt is to use the c() function, which is short hand for “combine” or “concatenate”. x &lt;- c(2,4,6,8) # create a vector, assignn it the variable name `x` x ## [1] 2 4 6 8 Vectors in R always have a type (accessed with the typeof() function) and a length (accessed with the length() function). length(x) ## [1] 4 typeof(x) ## [1] &quot;double&quot; Vectors don’t have to be numerical; logical and character vectors work just as well. y &lt;- c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE) y ## [1] TRUE TRUE FALSE TRUE FALSE FALSE typeof(y) ## [1] &quot;logical&quot; length(y) ## [1] 6 z &lt;- c(&quot;How&quot;, &quot;now&quot;, &quot;brown&quot;, &quot;cow&quot;) z ## [1] &quot;How&quot; &quot;now&quot; &quot;brown&quot; &quot;cow&quot; typeof(z) ## [1] &quot;character&quot; length(z) ## [1] 4 You can also use c() to concatenate two or more vectors together. x &lt;- c(2, 4, 6, 8) y &lt;- c(1, 3, 5, 7, 9) # create another vector, labeled y xy &lt;- c(x,y) # combine two vectors xy ## [1] 2 4 6 8 1 3 5 7 9 z &lt;- c(pi/4, pi/2, pi, 2*pi) xyz &lt;- c(x, y, z) # combine three vectors xyz ## [1] 2.0000000 4.0000000 6.0000000 8.0000000 1.0000000 3.0000000 5.0000000 ## [8] 7.0000000 9.0000000 0.7853982 1.5707963 3.1415927 6.2831853 4.1.1 Vector Arithmetic The basic R arithmetic operations work on numeric vectors as well as on single numbers (in fact, behind the scenes in R single numbers are vectors!). x &lt;- c(2, 4, 6, 8, 10) x * 2 # multiply each element of x by 2 ## [1] 4 8 12 16 20 x - pi # subtract pi from each element of x ## [1] -1.1415927 0.8584073 2.8584073 4.8584073 6.8584073 y &lt;- c(0, 1, 3, 5, 9) x + y # add together each matching element of x and y ## [1] 2 5 9 13 19 x * y # multiply each matching element of x and y ## [1] 0 4 18 40 90 x/y # divide each matching element of x and y ## [1] Inf 4.000000 2.000000 1.600000 1.111111 Basic numerical functions operate element-wise on numerical vectors: sin(x) ## [1] 0.9092974 -0.7568025 -0.2794155 0.9893582 -0.5440211 cos(x * pi) ## [1] 1 1 1 1 1 log(x) ## [1] 0.6931472 1.3862944 1.7917595 2.0794415 2.3025851 4.1.2 Vector recycling When vectors are not of the same length R `recycles’ the elements of the shorter vector to make the lengths conform. x &lt;- c(2, 4, 6, 8, 10) length(x) ## [1] 5 z &lt;- c(1, 4, 7, 11) length(z) ## [1] 4 x + z ## [1] 3 8 13 19 11 In the example above z was treated as if it was the vector (1, 4, 7, 11, 1). 4.1.3 Simple statistical functions for numeric vectors Now that we’ve introduced vectors as the simplest data structure for holding collections of numerical values, we can introduce a few of the most common statistical functions that operate on such vectors. First let’s create a vector to hold our sample data of interest. Here I’ve taken a random sample of the lengths of the last names of students enrolled in Bio 723 during Spring 2018. len.name &lt;- c(7, 7, 6, 2, 9, 9, 7, 4, 10, 5) Some common statistics of interest include minimum, maximum, mean, median, variance, and standard deviation: sum(len.name) ## [1] 66 min(len.name) ## [1] 2 max(len.name) ## [1] 10 mean(len.name) ## [1] 6.6 median(len.name) ## [1] 7 var(len.name) # variance ## [1] 6.044444 sd(len.name) # standard deviation ## [1] 2.458545 The summary() function applied to a vector of doubles produce a useful table of some of these key statistics: summary(len.name) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 5.25 7.00 6.60 8.50 10.00 4.1.4 Indexing Vectors Accessing the element of a vector is called “indexing”. Indexing is the process of specifying the numerical positions (indices) that you want to take access from the vector. For a vector of length \\(n\\), we can access the elements by the indices \\(1 \\ldots n\\). We say that R vectors (and other data structures like lists) are `one-indexed’. Many other programming languages, such as Python, C, and Java, use zero-indexing where the elements of a data structure are accessed by the indices \\(0 \\ldots n-1\\). Indexing errors are a common source of bugs. Indexing a vector is done by specifying the index in square brackets as shown below: x &lt;- c(2, 4, 6, 8, 10) length(x) ## [1] 5 x[1] # return the 1st element of x ## [1] 2 x[4] # return the 4th element of x ## [1] 8 Negative indices are used to exclude particular elements. x[-1] returns all elements of x except the first. x[-1] ## [1] 4 6 8 10 You can get multiple elements of a vector by indexing by another vector. In the example below, x[c(3,5)] returns the third and fifth element of x`. x[c(3,5)] ## [1] 6 10 4.1.5 Comparison operators applied to vectors When the comparison operators, such as“greater than” (&gt;), “less than or equal to” (&lt;=), equality (==), etc. are applied tonumeric vectors, they return logical vectors: x &lt;- c(2, 4, 6, 8, 10, 12) x &lt; 8 # returns TRUE for all elements lass than 8 ## [1] TRUE TRUE TRUE FALSE FALSE FALSE Here’s a fancier example: x &gt; 4 &amp; x &lt; 10 # greater than 4 AND less than 10 ## [1] FALSE FALSE TRUE TRUE FALSE FALSE 4.1.6 Combining Indexing and Comparison of Vectors A very powerful feature of R is the ability to combine the comparison operators (which return TRUE or FALSE values) with indexing. This facilitates data filtering and subsetting. Here’s an example: x &lt;- c(2, 4, 6, 8, 10) x[x &gt; 5] ## [1] 6 8 10 In the first example we retrieved all the elements of x that are larger than 5 (read as “x where x is greater than 5”). Notice how we got back all the elements where the statement in the brackets was TRUE. You can string together comparisons for more complex filtering. x[x &lt; 4 | x &gt; 8] # less than four OR greater than 8 ## [1] 2 10 In the second example we retrieved those elements of x that were smaller than four or greater than six. Combining indexing and comparison is a powerful concept which we’ll use repeatedly in this course. 4.1.7 Vector manipulation You can combine indexing with assignment to change the elements of a vectors: x &lt;- c(2, 4, 6, 8, 10) x[2] &lt;- -4 x ## [1] 2 -4 6 8 10 You can also use indexing vectors to change multiple values at once: x &lt;- c(2, 4, 6, 8, 10) x[c(1, 3, 5)] &lt;- 6 x ## [1] 6 4 6 8 6 Using logical vectors to manipulate the elements of a vector also works: x &lt;- c(2, 4, 6, 8, 10) x[x &gt; 5] = 5 # truncate all values to have max value 5 x ## [1] 2 4 5 5 5 4.1.8 Vectors from regular sequences There are a variety of functions for creating regular sequences in the form of vectors. 1:10 # create a vector with the integer values from 1 to 10 ## [1] 1 2 3 4 5 6 7 8 9 10 20:11 # a vector with the integer values from 20 to 11 ## [1] 20 19 18 17 16 15 14 13 12 11 seq(1, 10) # like 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 seq(1, 10, by = 2) # 1:10, in steps of 2 ## [1] 1 3 5 7 9 seq(2, 4, by = 0.25) # 2 to 4, in steps of 0.25 ## [1] 2.00 2.25 2.50 2.75 3.00 3.25 3.50 3.75 4.00 4.1.9 Additional functions for working with vectors The function unique() returns the unique items in a vector: x &lt;- c(5, 2, 1, 4, 6, 9, 8, 5, 7, 9) unique(x) ## [1] 5 2 1 4 6 9 8 7 rev() returns the items in reverse order (without changing the input vector): y &lt;- rev(x) y ## [1] 9 7 5 8 9 6 4 1 2 5 x # x is still in original order ## [1] 5 2 1 4 6 9 8 5 7 9 There are a number of useful functions related to sorting. Plain sort() returns a new vector with the items in sorted order: sorted.x &lt;- sort(x) # returns items of x sorted sorted.x ## [1] 1 2 4 5 5 6 7 8 9 9 x # but x remains in its unsorted state ## [1] 5 2 1 4 6 9 8 5 7 9 The related function order() gives the indices which would rearrange the items into sorted order: order(x) ## [1] 3 2 4 1 8 5 9 7 6 10 order() can be useful when you want to sort one list by the values of another: students &lt;- c(&quot;fred&quot;, &quot;tabitha&quot;, &quot;beatriz&quot;, &quot;jose&quot;) class.ranking &lt;- c(4, 2, 1, 3) students[order(class.ranking)] # get the students sorted by their class.ranking ## [1] &quot;beatriz&quot; &quot;tabitha&quot; &quot;jose&quot; &quot;fred&quot; any() and all(), return single boolean values based on a specified comparison provided as an argument: y &lt;- c(2, 4, 5, 6, 8) any(y &gt; 5) # returns TRUE if any of the elements are TRUE ## [1] TRUE all(y &gt; 5) # returns TRUE if all of the elements are TRUE ## [1] FALSE which() returns the indices of the vector for which the input is true: which(y &gt; 5) ## [1] 4 5 4.2 Lists R lists are like vectors, but unlike a vector where all the elements are of the same type, the elements of a list can have arbitrary types (even other lists). Lists are a powerful data structure for organizing information, because there are few constraints on the shape or types of the data included in a list. Lists are easy to create: l &lt;- list(&#39;Bob&#39;, pi, 10) Note that lists can contain arbitrary data. Lists can even contain other lists: l &lt;- list(&#39;Bob&#39;, pi, 10, list(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;, &quot;qux&quot;)) Lists are displayed with a particular format, distinct from vectors: l ## [[1]] ## [1] &quot;Bob&quot; ## ## [[2]] ## [1] 3.141593 ## ## [[3]] ## [1] 10 ## ## [[4]] ## [[4]][[1]] ## [1] &quot;foo&quot; ## ## [[4]][[2]] ## [1] &quot;bar&quot; ## ## [[4]][[3]] ## [1] &quot;baz&quot; ## ## [[4]][[4]] ## [1] &quot;qux&quot; In the example above, the correspondence between the list and its display is obvious for the first three items. The fourth element may be a little confusing at first. Remember that the fourth item of l was another list. So what’s being shown in the output for the fourth item is the nested list. An alternative way to display a list is using the str() function (short for “structure”). str() provides a more compact representation that also tells us what type of data each element is: str(l) ## List of 4 ## $ : chr &quot;Bob&quot; ## $ : num 3.14 ## $ : num 10 ## $ :List of 4 ## ..$ : chr &quot;foo&quot; ## ..$ : chr &quot;bar&quot; ## ..$ : chr &quot;baz&quot; ## ..$ : chr &quot;qux&quot; 4.2.1 Length and type of lists Like vectors, lists have length: length(l) ## [1] 4 But the type of a list is simply “list”, not the type of the items within the list. This makes sense because lists are allowed to be heterogeneous (i.e. hold data of different types). typeof(l) ## [1] &quot;list&quot; 4.2.2 Indexing lists Lists have two indexing operators. Indexing a list with single brackets, like we did with vectors, returns a new list containing the element at index \\(i\\). Lists also support double bracket indexing (x[[i]]) which returns the bare element at index \\(i\\) (i.e. the element without the enclosing list). This is a subtle but important point so make sure you understand the difference between these two forms of indexing. 4.2.2.1 Single bracket list indexing First, let’s demonstrate single bracket indexing of the lists l we created above. l[1] # single brackets, returns list(&#39;Bob&#39;) ## [[1]] ## [1] &quot;Bob&quot; typeof(l[1]) # notice the list type ## [1] &quot;list&quot; When using single brackets, lists support indexing with ranges and numeric vectors: l[3:4] ## [[1]] ## [1] 10 ## ## [[2]] ## [[2]][[1]] ## [1] &quot;foo&quot; ## ## [[2]][[2]] ## [1] &quot;bar&quot; ## ## [[2]][[3]] ## [1] &quot;baz&quot; ## ## [[2]][[4]] ## [1] &quot;qux&quot; l[c(1, 3, 5)] ## [[1]] ## [1] &quot;Bob&quot; ## ## [[2]] ## [1] 10 ## ## [[3]] ## NULL 4.2.2.2 Double bracket list indexing If double bracket indexing is used, the object at the given index in a list is returned: l[[1]] # double brackets, return plain &#39;Bob&#39; ## [1] &quot;Bob&quot; typeof(l[[1]]) # notice the &#39;character&#39; type ## [1] &quot;character&quot; Double bracket indexing does not support multiple indices, but you can chain together double bracket operators to pull out the items of sublists. For example: # second item of the fourth item of the list l[[4]][[2]] ## [1] &quot;bar&quot; 4.2.3 Naming list elements The elements of a list can be given names when the list is created: p &lt;- list(first.name=&#39;Alice&#39;, last.name=&quot;Qux&quot;, age=27, years.in.school=10) You can retrieve the names associated with a list using the names function: names(p) ## [1] &quot;first.name&quot; &quot;last.name&quot; &quot;age&quot; &quot;years.in.school&quot; If a list has named elements, you can retrieve the corresponding elements by indexing with the quoted name in either single or double brackets. Consistent with previous usage, single brackets return a list with the corresponding named element, whereas double brackets return the bare element. For example, make sure you understand the difference in the output generated by these two indexing calls: p[&quot;first.name&quot;] ## $first.name ## [1] &quot;Alice&quot; p[[&quot;first.name&quot;]] ## [1] &quot;Alice&quot; 4.2.4 The $ operator Retrieving named elements of lists (and data frames as we’ll see), turns out to be a pretty common task (especially when doing interactive data analysis) so R has a special operator to make this more convenient. This is the $ operator, which is used as illustrated below: p$first.name # equivalent to p[[&quot;first.name&quot;]] ## [1] &quot;Alice&quot; p$age # equivalent to p[[&quot;age&quot;]] ## [1] 27 4.2.5 Changing and adding lists items Combining indexing and assignment allows you to change items in a list: suspect &lt;- list(first.name = &quot;unknown&quot;, last.name = &quot;unknown&quot;, aka = &quot;little&quot;) suspect$first.name &lt;- &quot;Bo&quot; suspect$last.name &lt;- &quot;Peep&quot; suspect[[3]] &lt;- &quot;LITTLE&quot; str(suspect) ## List of 3 ## $ first.name: chr &quot;Bo&quot; ## $ last.name : chr &quot;Peep&quot; ## $ aka : chr &quot;LITTLE&quot; By combining assignment with a new name or an index past the end of the list you can add items to a list: suspect$age &lt;- 17 # add a new item named age suspect[[5]] &lt;- &quot;shepardess&quot; # create an unnamed item at position 5 Be careful when adding an item using indexing, because if you skip an index an intervening NULL value is created: # there are only five items in the list, what happens if we # add a new item at position seven? suspect[[7]] &lt;- &quot;wanted for sheep stealing&quot; str(suspect) ## List of 7 ## $ first.name: chr &quot;Bo&quot; ## $ last.name : chr &quot;Peep&quot; ## $ aka : chr &quot;LITTLE&quot; ## $ age : num 17 ## $ : chr &quot;shepardess&quot; ## $ : NULL ## $ : chr &quot;wanted for sheep stealing&quot; 4.2.6 Combining lists The c (combine) function we introduced to create vectors can also be used to combine lists: list.a &lt;- list(&quot;little&quot;, &quot;bo&quot;, &quot;peep&quot;) list.b &lt;- list(&quot;has lost&quot;, &quot;her&quot;, &quot;sheep&quot;) list.c &lt;- c(list.a, list.b) list.c ## [[1]] ## [1] &quot;little&quot; ## ## [[2]] ## [1] &quot;bo&quot; ## ## [[3]] ## [1] &quot;peep&quot; ## ## [[4]] ## [1] &quot;has lost&quot; ## ## [[5]] ## [1] &quot;her&quot; ## ## [[6]] ## [1] &quot;sheep&quot; 4.2.7 Converting lists to vectors Sometimes it’s useful to convert a list to a vector. The unlist() function takes care of this for us. # a homogeneous list ex1 &lt;- list(2, 4, 6, 8) unlist(ex1) ## [1] 2 4 6 8 When you convert a list to a vector make sure you remember that vectors are homogeneous, so items within the new vector will be “coerced” to have the same type. # a heterogeneous list ex2 &lt;- list(2, 4, 6, c(&quot;bob&quot;, &quot;fred&quot;), list(1 + 0i, &#39;foo&#39;)) unlist(ex2) ## [1] &quot;2&quot; &quot;4&quot; &quot;6&quot; &quot;bob&quot; &quot;fred&quot; &quot;1+0i&quot; &quot;foo&quot; Note that unlist() also unpacks nested vectors and lists as shown in the second example above. 4.3 Data frames Along with vectors and lists, data frames are one of the core data structures when working in R. A data frame is essentially a list which represents a data table, where each column in the table has the same number of rows and every item in the a column has to be of the same type. Unlike standard lists, the objects (columns) in a data frame must have names. We’ve seen data frames previously, for example when we loaded data sets using the read_csv function. 4.3.1 Creating a data frame While data frames will often be created by reading in a data set from a file, they can also be created directly in the console as illustrated below: age &lt;- c(30, 26, 21, 29, 25, 22, 28, 24, 23, 20) sex &lt;- rep(c(&quot;M&quot;,&quot;F&quot;), 5) wt.in.kg &lt;- c(88, 76, 67, 66, 56, 74, 71, 60, 52, 72) df &lt;- data.frame(age = age, sex = sex, wt = wt.in.kg) Here we created a data frame with three columns, each of length 10. 4.3.2 Type and class for data frames Data frames can be thought of as specialized lists, and in fact the type of a data frame is “list” as illustrated below: typeof(df) ## [1] &quot;list&quot; To distinguish a data frame from a generic list, we have to ask about it’s “class”. class(df) # the class of our data frame ## [1] &quot;data.frame&quot; class(l) # compare to the class of our generic list ## [1] &quot;list&quot; The term “class” comes from a style/approach to programming called “object oriented programming”. We won’t go into explicit detail about how object oriented programming works in this class, though we will exploit many of the features of objects that have a particular class. 4.3.3 Length and dimension for data frames Applying the length() function to a data frame returns the number of columns. This is consistent with the fact that data frames are specialized lists: length(df) ## [1] 3 To get the dimensions (number of rows and columns) of a data frame, we use the dim() function. dim() returns a vector, whose first value is the number of rows and whose second value is the number of columns: dim(df) ## [1] 10 3 We can get the number of rows and columns individually using the nrow() and ncol() functions: nrow(df) # number of rows ## [1] 10 ncol(df) # number of columsn ## [1] 3 4.3.4 Indexing and accessing data frames Data frames can be indexed by either column index, column name, row number, or a combination of row and column numbers. 4.3.4.1 Single bracket indexing of the columns of a data frame The single bracket operator with a single numeric index returns a data frame with the corresponding column. df[1] # get the first column (=age) of the data frame ## # A tibble: 10 x 1 ## age ## &lt;dbl&gt; ## 1 30 ## 2 26 ## 3 21 ## 4 29 ## 5 25 ## 6 22 ## 7 28 ## 8 24 ## 9 23 ## 10 20 The single bracket operator with multiple numeric indices returns a data frame with the corresponding columns. df[1:2] # first two columns ## # A tibble: 10 x 2 ## age sex ## &lt;dbl&gt; &lt;fct&gt; ## 1 30 M ## 2 26 F ## 3 21 M ## 4 29 F ## 5 25 M ## 6 22 F ## 7 28 M ## 8 24 F ## 9 23 M ## 10 20 F df[c(1, 3)] # columns 1 (=age) and 3 (=wt) ## # A tibble: 10 x 2 ## age wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 30 88 ## 2 26 76 ## 3 21 67 ## 4 29 66 ## 5 25 56 ## 6 22 74 ## 7 28 71 ## 8 24 60 ## 9 23 52 ## 10 20 72 Column names can be substituted for indices when using the single bracket operator: df[&quot;age&quot;] ## # A tibble: 10 x 1 ## age ## &lt;dbl&gt; ## 1 30 ## 2 26 ## 3 21 ## 4 29 ## 5 25 ## 6 22 ## 7 28 ## 8 24 ## 9 23 ## 10 20 df[c(&quot;age&quot;, &quot;wt&quot;)] ## # A tibble: 10 x 2 ## age wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 30 88 ## 2 26 76 ## 3 21 67 ## 4 29 66 ## 5 25 56 ## 6 22 74 ## 7 28 71 ## 8 24 60 ## 9 23 52 ## 10 20 72 4.3.4.2 Single bracket indexing of the rows of a data frame To get specific rows of a data frame, we use single bracket indexing with an additional comma following the index. For example to get the first row a data frame we would do: df[1,] # first row ## # A tibble: 1 x 3 ## age sex wt ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 30 M 88 This syntax extends to multiple rows: df[1:2,] # first two rows ## # A tibble: 2 x 3 ## age sex wt ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 30 M 88 ## 2 26 F 76 df[c(1, 3, 5),] # rows 1, 3 and 5 ## # A tibble: 3 x 3 ## age sex wt ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 30 M 88 ## 2 21 M 67 ## 3 25 M 56 4.3.4.3 Single bracket indexing of both the rows and columns of a data frame Single bracket indexing of data frames extends naturally to retrieve both rows and columns simultaneously: df[1, 2] # first row, second column ## [1] M ## Levels: F M df[1:3, 2:3] # first three rows, columns 2 and 3 ## # A tibble: 3 x 2 ## sex wt ## &lt;fct&gt; &lt;dbl&gt; ## 1 M 88 ## 2 F 76 ## 3 M 67 # you can even mix numerical indexing (rows) with named indexing of columns df[5:10, c(&quot;age&quot;, &quot;wt&quot;)] ## # A tibble: 6 x 2 ## age wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 25 56 ## 2 22 74 ## 3 28 71 ## 4 24 60 ## 5 23 52 ## 6 20 72 4.3.4.4 Double bracket and $ indexing of data frames Whereas single bracket indexing of a data frame always returns a new data frame, double bracket indexing and indexing using the $ operator, returns vectors. df[[&quot;age&quot;]] ## [1] 30 26 21 29 25 22 28 24 23 20 typeof(df[[&quot;age&quot;]]) ## [1] &quot;double&quot; df$wt ## [1] 88 76 67 66 56 74 71 60 52 72 typeof(df$wt) ## [1] &quot;double&quot; 4.3.5 Logical indexing of data frames Logical indexing using boolean values works on data frames in much the same way it works on vectors. Typically, logical indexing of a data frame is used to filter the rows of a data frame. For example, to get all the subject in our example data frame who are older than 25 we could do: # NOTE: the comma after 25 is important to insure we&#39;re indexing rows! df[df$age &gt; 25, ] ## # A tibble: 4 x 3 ## age sex wt ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 30 M 88 ## 2 26 F 76 ## 3 29 F 66 ## 4 28 M 71 Similarly, to get all the individuals whose weight is between 60 and 70 kgs we could do: df[(df$wt &gt;= 60 &amp; df$wt &lt;= 70),] ## # A tibble: 3 x 3 ## age sex wt ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 21 M 67 ## 2 29 F 66 ## 3 24 F 60 4.3.6 Adding columns to a data frame Adding columns to a data frame is similar to adding items to a list. The easiest way to do so is using named indexing. For example, to add a new column to our data frame that gives the individuals ages in number of days, we could do: df[[&quot;age.in.days&quot;]] &lt;- df$age * 365 dim(df) ## [1] 10 4 "],
["introduction-to-ggplot2.html", "Chapter 5 Introduction to ggplot2 5.1 Loading ggplot2 5.2 Example data set: Anderson’s Iris Data 5.3 Template for single layer plots in ggplot2 5.4 An aside about function arguments 5.5 Strip plots 5.6 Histograms 5.7 Faceting to depict categorical information 5.8 Density plots 5.9 Violin or Beanplot 5.10 Boxplots 5.11 Building complex visualizations with layers 5.12 Useful combination plots 5.13 ggplot layers can be assigned to variables 5.14 Adding titles and tweaking axis labels 5.15 ggplot2 themes 5.16 Other aspects of ggplots can be assigned to variables 5.17 Bivariate plots 5.18 Bivariate density plots 5.19 Combining Scatter Plots and Density Plots with Categorical Information 5.20 Density plots with fill 5.21 2D bin and hex plots 5.22 The cowplot package", " Chapter 5 Introduction to ggplot2 Pretty much any statistical plot can be thought of as a mapping between data and one or more visual representations. For example, in a scatter plot we map two ordered sets of numbers (the variables of interest) to points in the Cartesian plane (x,y-coordinates). The representation of data as points in a plane can be thought of as a type of geometric mapping. In a histogram, we divide the range of a variable of interest into bins, count the number of observations in each bin, and represent those counts as bars. The process of counting the data in bins is a type of statistical transformation (summing in this case), while the representation of the counts as bars is another example of a geometric mapping. Both types of plots can be further embellished with additional information, such as coloring the points or bars based on a categorical variable of interest, changing the shape of points, etc. These are examples of aesthetic mappings. An additional operation that is frequently useful is faceting (also called conditioning), in which a series of subplots are created to show particular subsets of the data. The package ggplot2 is based on a formalized approach for building statistical graphics as a combination of geometric mappings, aesthetic mappings, statistical transformations, and faceting (conditioning). In ggplot2, complex figures are built up by combining layers – where each layer includes a geometric mapping, an aesthetic mapping, and a statistical transformation – along with any desired faceting information. Many of the key ideas behind ggplot2 (and its predecessor,“ggplot”) are based on a book called “The Grammar of Graphics” (Leland Wilkinson, 1985). The “grammar of graphics” is the “gg” in the ggplot2 name. 5.1 Loading ggplot2 ggplot2 is one of the packages included in the tidyverse meta-package we installed during the previous class session (see the previous lecture notes for instruction if you have not installed tidyverse). If we load the tidyverse package, ggplot2 is automatically loaded as well. library(tidyverse) However if we wanted to we could load only ggplot2 as follows: library(ggplot2) # not necessary if we already loaded tidyverse 5.2 Example data set: Anderson’s Iris Data To illustrate ggplot2 we’ll use a dataset called iris. This data set was made famous by the statistician and geneticist R. A. Fisher who used it to illustrate many of the fundamental statistical methods he developed (Recall that Fisher was one of the key contributors to the modern synthesis in biology, reconciling evolution and genetics in the early 20th century). The data set consists of four morphometric measurements for specimens from three different iris species (Iris setosa, I. versicolor, and I. virginica). Use the R help to read about the iris data set (?iris). We’ll be using this data set repeatedly in future weeks so familiarize yourself with it. The iris data is included in a standard R package (datasets) that is made available automatically when you start up R. As a consequence we don’t need to explicitly load the iris data from a file. Let’s take a few minutes to explore this iris data set before we start generating plots: names(iris) # get the variable names in the dataset ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## [5] &quot;Species&quot; dim(iris) # dimensions given as rows, columns ## [1] 150 5 head(iris) # can you figure out what the head function does? ## # A tibble: 6 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.100 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.600 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa tail(iris) # what about the tail function? ## # A tibble: 6 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 6.7 3.3 5.7 2.5 virginica ## 2 6.7 3 5.2 2.300 virginica ## 3 6.3 2.5 5 1.9 virginica ## 4 6.5 3 5.2 2 virginica ## 5 6.2 3.4 5.4 2.300 virginica ## 6 5.9 3 5.100 1.8 virginica 5.3 Template for single layer plots in ggplot2 A basic template for building a single layer plot using ggplot2 is shown below. When creating a plot, you need to replace the text in brackets (e.g. &lt;DATA&gt;) with appropriate objects, functions, or arguments: # NOTE: this is pseudo-code. It will not run! ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) The base function ggplot() is responsible for creating the coordinate system in which the plot will be display. To this coordinate system we add a geometric mapping (called a “geom” for short) that specifies how data gets mapped into the coordinate system (e.g. points, bars, etc). Included as an input to the geom function is the aesthetic mapping function that specifies which variables to use in the geometric mapping (e.g. which variables to treat as the x- and y-coordinates), colors, etc. For example, using this template we can create a scatter plot that show the relationship between the variables Sepal.Width and Petal.Width. To do so we subsitute iris for &lt;DATA&gt;, geom_point for &lt;GEOM_FUNCTION&gt;, and x = Sepal.Width and y = Petal.Width for &lt;MAPPINGS&gt;. ggplot(data = iris) + geom_point(mapping = aes(x = Sepal.Width, y = Petal.Width)) If we were to translate this code block to English, we might write it as “Using the iris data frame as the source of data, create a point plot using each observeration’s Sepal.Width variable for the x-coordinate and the Petal.Width variable for the y-coordinate.” 5.4 An aside about function arguments The inputs to a function are also known as “arguments”. In R, when you call a function you can specify the arguments by keyword (i.e. using names specified in the function definition) or by position (i.e. the order of the inputs). In our bar plot above, we’re using using keyword arguments. For example, in the line ggplot(data = iris), iris is treated as the “data” argument. Similarly, in the second line, aes(x = Sepal.Width, y = Petal.Width) is the “mapping” argument to geom_bar. Note that aes is itself a function (see ?aes) that takes arguments that can be specified positionally or with keywords. If we wanted to, we could instead use position arguments when calling a function, by passing inputs to the function corresponding to the order they are specified in the function definition. For example, take a minute to read the documentation for the ggplot function (?ggplot). Near the top of the help page you’ll see a description of how the function is called under “Usage”. Reading the Usage section you’ll see that the the “data” argument is the first positional argument to ggplot. Similarly, if you read the docs for the geom_point function you’ll see that mapping is the first positional argument for that function. The equivalent of our previous example, but now using positional arguments is: ggplot(iris) + # note we dropped the &quot;data = &quot; part # note we dropped the &quot;mapping = &quot; part from the geom_point call geom_point(aes(x = Sepal.Width, y = Petal.Width)) The upside of using positional arguments is that it means less typing, which is useful when working interactively at the console (or in an R Notebok). The downside to using positional arguments is you need to remember or lookup the order of the arguments. Using positional arguments can also make your code less “self documenting” in the sense that it is less explicit about how the inputs are being treated. While the argument “x” is the first argument to the aes function, I chose to explicitly include the argument name to make it clear what variable I’m plotting on the x-axis. We will cover function arguments in greater detail a class session or two from now, when we learn how to write our own functions. 5.5 Strip plots One of the simplest visualizations of a continuous variable is to draw points along a number line, where each point represent the value of one of the observations. This is sometimes called a “strip plot”. First, we’ll use the geom_point function as shown below to generate a strip plot for the Sepal.Width variable in the iris data set. ggplot(data = iris) + geom_point(aes(x = Sepal.Width, y = 0)) 5.5.1 Jittering data There should have been 150 points plotted in the figure above (one for each of the iris plants in the data set), but visually it looks like only about 25 or 30 points are shown. What’s going on? If you examine the iris data, you’ll see that the all the measures are rounded to the nearest tenth of a centimer, so that there are a large number of observations with identical values of Sepal.Width. This is a limitation of the precision of measurements that was used when generating the data set. To provide a visual clue that there are multiple observations that share the same value, we can slightly “jitter” the values (randomly move points a small amount in either in the vertical or horizontal direction). Jittering is used solely to enhance visualization, and any statistical analyses you carry out would be based on the original data. When presenting your data to someone else, should note when you’ve used jittering so as not to misconvey the actual data. Jittering can be accomplished using geom_jitter, which is derived from geom_point: ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = 0), width = 0.05, height = 0, alpha = 0.25) The width and height arguments specify the maximum amount (as fractions of the data) to jitter the observed data points in the horizontal (width) and vertical (height) directions. Here we only jitter the data in the horizontal direction. The alpha argument controls the transparency of the points – the valid range of alpha values is 0 to 1, where 0 means completely transparent and 1 is completely opaque. Within a geom, arguments outside of the aes mapping apply uniformly across the visualization (i.e. they are fixed values). For example, setting `alpha = 0.25’ made all the points transparent. 5.5.2 Adding categorical information Recall that are three different species represented in the data: Iris setosa, I. versicolor, and I. virginica. Let’s see how to generate a strip plot that also includes a breakdown by species. ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species), width=0.05, height=0.1, alpha=0.5) That was easy! All we had to do was change the aesthetic mapping in geom_jitter, specifying “Species” as the y variable. I also added a little vertical jitter as well to better separate the points. Now we have a much better sense of the data. In particular it’s clear that the I. setosa specimens generally have wider sepals than samples from the other two species. Let’s tweak this a little by also adding color information, to further emphasize the distinct groupings. We can do this by adding another argument to the aesthetic mapping in geom_jitter. ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species, color=Species), width=0.05, height=0.1, alpha=0.5) 5.5.3 Rotating plot coordinates What if we wanted to rotate this plot 90 degrees, depicting species on the x-axis and sepal width on the y-axis. For this example, it would be easy to do this by simpling swapping the variables in the aes mapping argument. However an alternate way to do this is with a coordinate transformation function. Here we use coord_flip to flip the x- and y-axes: ggplot(data = iris) + geom_jitter(aes(x = Sepal.Width, y = Species, color=Species), width=0.05, height=0.1, alpha=0.5) + coord_flip() We’ll see other uses of coordinate transformations in later lectures. 5.6 Histograms Histograms are probably the most common way to depict univariate data. In a histogram rather than showing individual observations, we divide the range of the data into a set of bins, and use vertical bars to depict the number (frequency) of observations that fall into each bin. This gives a good sense of the intervals in which most of the observations are found. The geom, geom_histogram, takes care of both the geometric representation and the statistical transformations of the data necessary to calculate the counts in each binn. Here’s the simplest way to use geom_histogram: ggplot(iris) + geom_histogram(aes(x = Sepal.Width)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The default number of bins that geom_histogram uses is 30. For modest size data sets this is often too many bins, so it’s worth exploring how the histogram changes with different bin numbers: ggplot(iris) + geom_histogram(aes(x = Sepal.Width), bins = 10) ggplot(iris) + geom_histogram(aes(x = Sepal.Width), bins = 12) One important thing to note when looking at these histograms with different numbers of bins is that the number of bins used can change your perception of the data. For example, the number of peaks (modes) in the data can be very sensitive to the bin number as can the perception of gaps. 5.6.1 Variations on histograms when considering categorical data As before, we probably want to break the data down by species. Here we’re faced with some choices about how we depict that data. Do we generate a “stacked histogram” to where the colors indicate the number of observations in each bin that belong to each species? Do we generate side-by-side bars for each species? Or Do we generate separate histograms for each species, and show them overlapping? Stacked histograms are the default if we associate a categorical variable with the bar fill color: ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12) To get side-by-side bars, specify “dodge” as the position argument to geom_histogram. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12, position = &quot;dodge&quot;) If you want overlapping histograms, use position = \"identity\" instead. When generating overlapping histograms like this, you probably want to make the bars semi-transparent so you can can distinguish the overlapping data. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12, position = &quot;identity&quot;, alpha = 0.4) 5.7 Faceting to depict categorical information Yet another way to represent the histograms for the three species is to using faceting, the create subplots for each species. Faceting is the operation of subsetting the data with respect to a discrete or categorical variable of interest, and generating the same plot type for each subset. Here we use the “ncol” argument to the facet_wrap function to specify that the subplots should be drawn in a single vertical column to facilitate comparison of the distributions. ggplot(iris) + geom_histogram(aes(x = Sepal.Width, fill = Species), bins = 12) + facet_wrap(~Species, ncol = 1) 5.8 Density plots One shortcoming of histograms is that they are sensitive to the choice of bin margins and the number of bins. An alternative is a “density plot”, which you can think of as a smoothed version of a histogram. ggplot(iris) + geom_density(aes(x = Sepal.Width, fill = Species), alpha=0.25) Density plots still make some assumptions that affect the visualization, in particular a “smoothing bandwidth” (specified by the argument bw) which determines how course or granular the density estimation is. Note that the vertical scale on a density plot is no longer counts (frequency) but probability density. In a density plot, the total area under the plot adds up to one. Intervals in a density plot therefore have a probabilistic intepretation. 5.9 Violin or Beanplot A violin plot (sometimes called a bean plot) is closely related to a density plot. In fact you can think of a violin plot as a density plot rotated 90 degress and mirrored left/right. ggplot(iris) + geom_violin(aes(x = Species, y = Sepal.Width, color = Species, fill=Species), alpha = 0.25) 5.10 Boxplots Boxplots are another frequently used univariate visualization. Boxplots provide a compact summary of single variables, and are most often used for comparing distributions between groups. A standard box plot depicts five useful features of a set of observations: 1) the median (center most line); 2 and 3) the first and third quartiles (top and bottom of the box); 4) the whiskers of a boxplot extend from the first/third quartile to the highest value that is within 1.5 * IQR, where IQR is the inter-quartile range (distance between the first and third quartiles); 5) points outside of the whiskers are usually consider extremal points or outliers. There are many variants on box plots, particularly with respect to the “whiskers”. It’s always a good idea to be explicit about what a box plot you’ve created shows. ggplot(iris) + geom_boxplot(aes(x = Species, y = Sepal.Width, color = Species)) Boxplots are most commonly drawn with the cateogorical variable on the x-axis. 5.11 Building complex visualizations with layers All of our ggplot2 examples up to now have involved a single geom. We can think of geoms as “layers” of information in a plot. One of the powerful features of plotting useing ggplot2 is that it is trivial to combine layers to make more complex plots. The template for multi-layered plots is a simple extension of the single layer: ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION1&gt;(mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION2&gt;(mapping = aes(&lt;MAPPINGS&gt;)) 5.12 Useful combination plots Boxplot or violin plots represent visual summaries/simplifications of the underlying data. This is useful but sometimes key information is lost in the process of summarizing. Combining these plots with a strip plot give you both the “birds eye view” as well as granular information. 5.12.1 Boxplot plus strip plot Here’s an example of combining box plots and strip plots: ggplot(iris) + # outlier.shape = NA suppresses the depiction of outlier points in the boxplot geom_boxplot(aes(x = Species, y = Sepal.Width), outlier.shape = NA) + # size sets the point size for the jitter plot geom_jitter(aes(x = Species, y = Sepal.Width), width=0.2, height=0.05, alpha=0.35, size=0.75) Note that I suppressed the plotting of outliers in geom_boxplot so as not to draw the same points twice (the individual data are drawn by geom_jitter). 5.12.2 Setting shared aesthetics The example above works well, but you might have noticed that there’s some repetition of code. In particular, we set the same aesthetic mapping in both geom_boxplot and geom_jitter. It turns out that creating layers that share some of the same aesthetic values is a common case. To deal with such cases, you can specify shared aesthetic mappings as an argument to the ggplot function and then set additional aesthetics specific to each layer in the individual geoms. Using this approach, our previous example can be written more compactly as follow. ggplot(iris, mapping = aes(x = Species, y = Sepal.Width)) + geom_boxplot(outlier.shape = NA) + # note how we specify a layer specific aesthetic in geom_jitter geom_jitter(aes(color = Species), width=0.2, height=0.05, alpha=0.5, size=0.75) 5.13 ggplot layers can be assigned to variables The function ggplot() returns a “plot object” that we can assign to a variable. The following example illustrates this: # create base plot object and assign to variable p # this does NOT draw the plot p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width)) In the code above we created a plot object and assigned it to the variable p. However, the plot wasn’t drawn. To draw the plot object we evaluate it as so: p # try to draw the plot object The code block above didn’t generate an image, because we haven’t added a geom to the plot to determine how our data should be drawn. We can add a geom to our pre-created plot object as so: # add a point geom to our base layer and draw the plot p + geom_boxplot() If we wanted to we could have assigned the geom to a variable as well: box.layer &lt;- geom_boxplot() p + box.layer In this case we don’t really gain anything by creating an intermediate variable, but for more complex plots or when considering different versions of a plot this can be very useful. 5.13.1 Violin plot plus strip plot Here is the principle of combining layers, applied to a combined violin plot + strip plot. Again, we set shared aesthetic mappings in ggplot function call and this time we assign individual layers of the plot to variables. p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width, color = Species)) violin.layer &lt;- geom_violin() jitter.layer &lt;- geom_jitter(width=0.15, height=0.05, alpha=0.5, size=0.75) p + violin.layer + jitter.layer # combined layers of plot and draw 5.14 Adding titles and tweaking axis labels ggplot2 automatically adds axis labels based on the variable names in the data frame passed to ggplot. Sometimes these are appropriate, but more presentable figures you’ll usually want to tweak the axis labs (e.g. adding units). The labs (short for labels) function allows you to do so, and also let’s you set a title for your plot. We’ll illustrate this by modifying our previous figure. Note that we save considerable amounts of re-typing since we had already assigned three of the plot layers to variables in the previous code block: p + violin.layer + jitter.layer + labs(x = &quot;Species&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Sepal Width Distributions for Three Iris Species&quot;) 5.15 ggplot2 themes By now you’re probably familiar with the default “look” of plots generated by ggplot2, in particular the ubiquitous gray background with a white grid. This default works fairly well in the context of RStudio notebooks and HTML output, but might not work as well for a published figure or a slide presentation. Almost every individual aspect of a plot can be tweaked, but ggplot2 provides an easier way to make consistent changes to a plot using “themes”. You can think of a theme as adding another layer to your plot. Themes should generally be applied after all the other graphical layers are created (geoms, facets, labels) so the changes they create affect all the prior layers. There are eight default themes included with ggplot2, which can be invoked by calling the corresponding theme functions: theme_gray, theme_bw, theme_linedraw, theme_light, theme_dark, theme_minimal, theme_classic, and theme_void (See http://ggplot2.tidyverse.org/reference/ggtheme.html for a visual tour of all the default themes) For example, let’s generate a boxplot using theme_bw which get’s rid of the gray background: # create another variable to hold combination of three previous # ggplot layers. I&#39;m doing this because I&#39;m going to keep re-using # the same plot in the following code blocks violin.plus.jitter &lt;- p + violin.layer + jitter.layer violin.plus.jitter + theme_bw() Another theme, theme_classic, remove the grid lines completely, and also gets rid of the top-most and right-most axis lines. violin.plus.jitter + theme_classic() 5.15.1 Further customization with ggplot2::theme In addition to the eight complete themes, there is a theme function in ggplot2 that allows you to tweak particular elements of a theme (see ?theme for all the possible options). For example, to tweak just the aspect ratio of a plot (the ratio of width to height), you can set the aspect.ratio argument in theme: violin.plus.jitter + theme_classic() + theme(aspect.ratio = 1) Theme related function calls can be combined to generate new themes. For example, let’s create a theme called my.theme by combining theme_classic with a call to theme: my.theme &lt;- theme_classic() + theme(aspect.ratio = 1) We can then apply this theme as so: violin.plus.jitter + my.theme 5.16 Other aspects of ggplots can be assigned to variables Plot objects, geoms and themes are not the only aspects of a figure that can be assigned to variables for later use. For example, we can create a label object: my.labels &lt;- labs(x = &quot;Species&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Sepal Width Distributions for Three Iris Species&quot;) Combining all of our variables as so, we generate our new plot: violin.plus.jitter + my.labels + my.theme 5.17 Bivariate plots Now we turn our attention to some useful representations of bivariate distributions. For the purposes of these illustrations I’m initially going to restrict my attention to just one of the three species represented in the iris data set – the I. setosa specimens. This allows us to introduce a vary useful base function called subset(). subset() will return subsets of a vector or data frames that meets the specified conditions. This can also be accomplished with conditional indexing but subset() is usually less verbose. # create a new data frame composed only of the I. setosa samples setosa.only &lt;- subset(iris, Species == &quot;setosa&quot;) In the examples that follow, I’m going to illustrate different ways of representing the same bivariate distribution – the joint distribution of Sepal Length and Sepal Width – over and over again. To avoid repitition, let’s assign the base ggplot layer to a variable as we did in our previous examples. We’ll also pre-create a label layer. setosa.sepals &lt;- ggplot(setosa.only, mapping = aes(x = Sepal.Length, y = Sepal.Width)) sepal.labels &lt;- labs(x = &quot;Sepal Length (cm)&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Relationship between Sepal Length and Width&quot;, caption = &quot;data from Anderson (1935)&quot;) 5.17.1 Scatter plots A scatter plot is one of the simplest representations of a bivariate distribution. Scatter plots are simple to create in ggplot2 by specifying the appropriate X and Y variables in the aesthetic mapping and using geom_point for the geometric mapping. setosa.sepals + geom_point() + sepal.labels 5.17.2 Adding a trend line to a scatter plot ggplot2 makes it easy to add trend lines to plots. I use “trend lines” here to refer to representations like regression lines, smoothing splines, or other representations mean to help visualize the relationship between pairs of variables. We’ll spend a fair amount of time exploring the mathematics and interpetation of regression lines and related techniques in later lectures, but for now just think about trends lines as summary representations for bivariate relationships. Trend lines can be created using geom_smooth. Let’s add a default trend line to our I. setosa scatter plot of the Sepal Width vs Sepal Length: setosa.sepals + geom_jitter() + # using geom_jitter to avoid overplotting of points geom_smooth() + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The defaul trend line that geom_smooth fits is generated by a technique called “LOESS regression”. LOESS regression is a non-linear curve fitting method, hence the squiggly trend line we see above. The smoothness of the LOESS regression is controlled by a parameter called span which is related to the proportion of points used. We’ll discuss LOESS in detail in a later lecture, but here’s an illustration how changing the span affects the smoothness of the fit curve: setosa.sepals + geom_jitter() + # using geom_jitter to avoid overplotting of points geom_smooth(span = 0.95) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 5.17.2.1 Linear trend lines If instead we want a straight trend line, as would typically be depicted for a linear regression model we can specify a different statistical method: setosa.sepals + geom_jitter() + # using geom_jitter to avoid overplotting of points geom_smooth(method = &quot;lm&quot;, color = &quot;red&quot;) + # using linear model (&quot;lm&quot;) sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme 5.18 Bivariate density plots The density plot, which we introduced as a visualization for univariate data, can be extended to two-dimensional data. In a one dimensional density plot, the height of the curve was related to the relatively density of points in the surrounding region. In a 2D density plot, nested contours (or contours plus colors) indicate regions of higher local density. Let’s illustrate this with an example: setosa.sepals + geom_density2d() + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme The relationship between the 2D density plot and a scatter plot can be made clearer if we combine the two: setosa.sepals + geom_density_2d() + geom_jitter(alpha=0.35) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme 5.19 Combining Scatter Plots and Density Plots with Categorical Information As with many of the univariate visualizations we explored, it is often useful to depict bivariate relationships as we change a categorical variable. To illustrate this, we’ll go back to using the full iris data set. all.sepals &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) all.sepals + geom_point(aes(color = Species, shape = Species), size = 2, alpha = 0.6) + sepal.labels + labs(subtitle = &quot;All species&quot;) + my.theme Notice how in our aesthetic mapping we specified that both color and shape should be used to represent the species categories. The same thing can be accomplished with a 2D density plot. all.sepals + geom_density_2d(aes(color = Species)) + sepal.labels + labs(subtitle = &quot;All species&quot;) + my.theme As you can see, in the density plots above, when you have multiple categorical variables and there is significant overlap in the range of each sub-distribution, figures can become quite busy. As we’ve seen previously, faceting (conditioning) can be a good way to deal with this. Below a combination of scatter plots and 2D density plots, combined with faceting on the species variable. all.sepals + geom_density_2d(aes(color = Species), alpha = 0.5) + geom_point(aes(color = Species), alpha=0.5, size=1) + facet_wrap(~ Species) + sepal.labels + labs(subtitle = &quot;All species&quot;) + theme_bw() + theme(aspect.ratio = 1, legend.position = &quot;none&quot;) # get rid of legend In this example I went back to using a theme that includes grid lines to facilitate more accurate comparisons of the distributions across the facets. I also got rid of the legend, because the information there was redundant. 5.20 Density plots with fill Let’s revisit our earlier single species 2D density plot. Instead of simply drawing contour lines, let’s use color information to help guide the eye to areas of higher density. To draw filled contours, we use a sister function to geom_density_2d called stat_density_2d: setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme Using the default color scale, areas of low density are drawn in dark blue, whereas areas of high density are drawn in light blue. I personally find this dark -to-light color scale non-intuitive for density data, and would prefer that darker regions indicate area of higher density. If we want to change the color scale, we can use the a scale function (in this case scale_fill_continuous) to set the color values used for the low and high values (this function we’ll interpolate the intervening values for us). NOTE: when specifying color names, R accepts standard HTML color names (see the Wikipedia page on web colors for a list). We’ll also see other ways to set color values in a later class session. setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + # lavenderblush is the HTML standard name for a light purplish-pink color scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme The two contour plots we generated looked a little funny because the contours are cutoff due to the contour regions being outside the limits of the plot. To fix this, we can change the plot limits using the lims function as shown in the following code block. We’ll also add the scatter (jittered) to the emphasize the relationship between the levels, and we’ll change the title for the color legend on the right by specifying a text label associated with the fill arguments in the labs function. setosa.sepals + stat_density_2d(aes(fill = ..level..), geom = &quot;polygon&quot;) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + geom_jitter(alpha=0.5, size = 1.1) + # customize labels, including legend label for fill labs(x = &quot;Sepal Length(cm)&quot;, y = &quot;Sepal Width (cm)&quot;, title = &quot;Relationship between sepal length and width&quot;, subtitle = &quot;I. setosa specimens only&quot;, fill = &quot;Density&quot;) + # Set plot limits lims(x = c(4,6), y = c(2.5, 4.5)) + my.theme 5.21 2D bin and hex plots Two dimensional bin and hex plots are alterative ways to represent the joint density of points in the Cartesian plane. Here are examples of to generate these plot types. Compare them to our previous examples. A 2D bin plot can be tought of as a 2D histogram: setosa.sepals + geom_bin2d(binwidth = 0.2) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme A hex plot is similar to a 2D bin plot but uses hexagonal regions instead of squares. Hexagonal bins are useful because they can somtimes avoid visual artefacts sometimes apparent with square bins: setosa.sepals + geom_hex(binwidth = 0.2) + scale_fill_continuous(low=&quot;lavenderblush&quot;, high=&quot;red&quot;) + sepal.labels + labs(subtitle = &quot;I. setosa data only&quot;) + my.theme 5.22 The cowplot package A common task when preparing visualizations for scientific presentations and manuscripts is combining different plots as subfigures of a larger figure. To accomplish this we’ll use a package called cowplot that compliments the power of ggplot2. Install cowplot either via the command line or the R Studio GUI (see Section 2.8). library(cowplot) # assumes package has been installed cowplot allows us to create individual plots using ggplot, and then arrange them in a grid-like fashion with labels for each plot of interest, as you would typically see in publications. The core function of cowplot is plot_grid(), which allows the user to layout the sub-plots in an organized fashion and add labels as necesary. To illustrate plot_grid() let’s create three different representations of the distribution of sepal width in the irisu data set, and combine them into a single figure: p &lt;- ggplot(iris, mapping = aes(x = Species, y = Sepal.Width, color = Species)) # for the histogram we&#39;re going to override the mapping because # geom_histogram only takes an x argument plot.1 &lt;- p + geom_histogram(bins=12, mapping = aes(x = Sepal.Width), inherit.aes = FALSE) plot.2 &lt;- p + geom_boxplot() plot.3 &lt;- p + geom_violin() plot_grid(plot.1, plot.2, plot.3) If instead, we wanted to layout the plots in a single row we could change the call to plot_grid as so: plot_grid(plot.1, plot.2, plot.3, nrow = 1, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) Notice we also added labels to our sub-plots. "],
["introduction-to-dplyr.html", "Chapter 6 Introduction to dplyr 6.1 Libraries 6.2 Reading data with the readr package 6.3 A note on “tibbles” 6.4 Data filtering and transformation with dplyr 6.5 dplyr’s “verbs” 6.6 Pipes", " Chapter 6 Introduction to dplyr In today’s class we introduce a new package, dplyr, which, along with ggplot2 will be used in almost every class session. We will also introduce the readr package, for reading tabular data. 6.1 Libraries Both readr and dplyr are members of the tidyverse, so a single invocation of library() makes the functions defined in these two packages available for our use: library(tidyverse) 6.2 Reading data with the readr package The readr package defines a number of functions for reading data tables from common file formats like Comma-Separated-Value (CSV) and Tab-Separated-Value (TSV) files. The two most frequently used readr functions we’ll use in this class are read_csv() and read_tsv() for reading CSV and TSV files respectively. There are some variants of these basic function, which you can read about by invoking the help system (?read_csv). 6.2.1 Reading Excel files The tidyverse also includes a package called readxl which can be used to read Excel spreadsheets (recent versions with .xls and .xlsx extensions). Excel files are somewhat more complicated to deal with because they can include separate “sheets”. We won’t use readxl in this class, but documentation and examples of how readxl is used can be found at the page linked above. 6.2.2 Example data: NC Births For today’s hands on session we’ll use a data set that contains information on 150 cases of mothers and their newborns in North Carolina in 2004. This data set is available at the following URL: https://github.com/Bio723-class/example-datasets/raw/master/nc-births.txt The births data is a TSV file, so we’ll use the read_tsv() function to read it: births &lt;- read_tsv(&quot;https://github.com/Bio723-class/example-datasets/raw/master/nc-births.txt&quot;) ## Parsed with column specification: ## cols( ## fAge = col_double(), ## mAge = col_double(), ## weeks = col_double(), ## premature = col_character(), ## visits = col_double(), ## gained = col_double(), ## weight = col_double(), ## sexBaby = col_character(), ## smoke = col_character() ## ) Notice that when you used read_tsv() the function printed information about how it “parsed” the data (i.e. the types it assigned to each of the columns). The variables in the data set are: father’s age (fAge), mother’s age (mAge), weeks of gestation (weeks) whether the birth was premature or full term (premature) number of OB/GYN visits (visits) mother’s weight gained in pounds (gained) babies birth weight (weight) sex of the baby (sexBaby) whether the mother was a smoker (smoke). Notice too that we read the TSV file directly from a remote location via a URL. If instead, you wanted to load a local file on your computer you would specify the “path” – i.e. the location on your hard drive where you stored the file. For example, here is how I would load the same file if it was stored in the Downloads directory on my Mac laptop: # load the data from a local file births &lt;- read_tsv(&quot;/Users/pmagwene/Downloads/nc-births.txt&quot;) 6.3 A note on “tibbles” You may have noticed that most of the functions defined in tidyverse related packages return not data frames, but rather something called a “tibble”. You can think about tibbles as light-weight data frames. In fact if you ask about the “class” of a tibble you’ll see that it includes data.frame as one of it’s classes as well as tbl and tbl_df. class(births) ## [1] &quot;spec_tbl_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; There are some minor differences between data frame and tibbles. For example, tibbles print differently in the console and don’t automatically change variable names and types in the same way that standard data frames do. Usually tibbles can be used wherever a standard data frame is expected, but you may occasionally find a function that only works with a standard data frame. It’s easy to convert a tibble to a standard data frame using the as.data.frame function: births.std.df &lt;- as.data.frame(births) For more details about tibbles, see the Tibbles chapter in R for Data Analysis. 6.4 Data filtering and transformation with dplyr dplyr is powerful tool for data filter and transformation. In the same way that ggplot2 attempts to provide a “grammar of graphics”, dplyr aims to provide a “grammar of data manipulation”. In today’s material we will see how dplyr complements and simplifies standard data frame indexing and subsetting operations. However, dplyr is focused only on data frames and doesn’t completely replace the basic subsetting operations, and so being adept with both dplyr and the indexing approaches we’ve seen previously is important. If you’re curious about the name “dplyr”, the package’s originator Hadley Wickham says it’s supposed to invoke the idea of pliers for data frames (Github: Meaning of dplyrs name) 6.5 dplyr’s “verbs” The primary functions in the dplyr package can be thought of as a set of “verbs”, each verb corresponding to a common data manipulation task. Some of the most frequently used verbs/functions in dplyr include: select – select columns filter – filter rows mutate – create new columns arrange– reorder rows summarize – summarize values group_by – split data frame on some grouping variable. Can be powerfully combined with summarize All of these functions return new data frames rather than modifying the existing data frame (though some of the functions support in place modification of data frames via optional arguments).We illustrate these below by example using the NC births data. 6.5.1 select The select function subsets the columns (variables) of a data frame. For example, to select just the weeks and weight columns from the births data set we could do: wks.weight &lt;- select(births, weeks, weight) dim(wks.weight) # dim should be 50 x 2 ## [1] 150 2 head(wks.weight) ## # A tibble: 6 x 2 ## weeks weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 39 6.88 ## 2 39 7.69 ## 3 40 8.88 ## 4 40 9 ## 5 40 7.94 ## 6 40 8.25 The equivalent using standard indexing would be: wks.wt.alt &lt;- births[c(&quot;weeks&quot;, &quot;weight&quot;)] dim(wks.wt.alt) ## [1] 150 2 head(wks.wt.alt) ## # A tibble: 6 x 2 ## weeks weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 39 6.88 ## 2 39 7.69 ## 3 40 8.88 ## 4 40 9 ## 5 40 7.94 ## 6 40 8.25 Notes: * The first argument to all of the dplyr functions is the data frame you’re operating on When using functions defined in dplyr and ggplot2 variable names are (usually) not quoted or used with the $ operator. This is a design feature of these libraries and makes it easier to carry out interactive analyes because it saves a fair amount of typing. 6.5.2 filter The filter function returns those rows of the data set that meet the given logical criterion. For example, to get all the premature babies in the data set we could use filter as so: premies &lt;- filter(births, premature == &quot;premie&quot;) dim(premies) ## [1] 21 9 The equivalent using standard indexing would be: premies.alt &lt;- births[births$premature == &quot;premie&quot;,] The filter function will work with more than one logical argument, and these are joined together using Boolean AND logic (i.e. intersection). For example, to find those babies that were premature and whose mothers were smokers we could do: smoking.premies &lt;- filter(births, premature == &quot;premie&quot;, smoke == &quot;smoker&quot;) The equivalent call using standard indexing is: # don&#39;t forget the trailing comma to indicate rows! smoking.premies.alt &lt;- births[(births$premature == &quot;premie&quot;) &amp; (births$smoke == &quot;smoker&quot;),] filter also accepts logical statements chained together using the standard Boolean operators. For example, to find babies who were premature or whose moms were older than 35 you could use the OR operator |: premies.or.oldmom &lt;- filter(births, premature == &quot;premie&quot; | fAge &gt; 35) 6.5.3 mutate The mutate function creates a new data frame that is the same as input data frame but with additional variables (columns) as specified by the function arguments. In the example below, I create two new variables, weight.in.kg and a mom.smoked: # to make code more readable it&#39;s sometime useful to spread out # function arguments over multiple lines like I&#39;ve done here births.plus &lt;- mutate(births, weight.in.kg = weight / 2.2, mom.smoked = (smoke == &quot;smoker&quot;)) head(births.plus) ## # A tibble: 6 x 11 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smok… ## 2 34 36 39 full term 5 35 7.69 male nons… ## 3 36 35 40 full term 12 29 8.88 male nons… ## 4 41 40 40 full term 13 30 9 female nons… ## 5 42 37 40 full term NA 10 7.94 male nons… ## 6 37 28 40 full term 12 35 8.25 male smok… ## # … with 2 more variables: weight.in.kg &lt;dbl&gt;, mom.smoked &lt;lgl&gt; The equivalent using standard indexing would be to create a new data frame from births, appending the new variables to the end as so: births.plus.alt &lt;- data.frame(births, weight.in.kg = births$weight / 2.2, mom.smoked = (births$smoke == &quot;smoker&quot;)) 6.5.4 arrange Arrange creates a new data frame where the rows are sorted according to their values for one or more variables. For example, to sort by mothers age we could do: young.moms.first &lt;- arrange(births, mAge) head(young.moms.first) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18 15 37 full term 12 76 8.44 male nonsmoker ## 2 NA 16 40 full term 4 12 6 female nonsmoker ## 3 21 16 38 full term 15 75 7.56 female smoker ## 4 26 17 38 full term 11 30 9.5 female nonsmoker ## 5 17 17 29 premie 4 10 2.63 female nonsmoker ## 6 20 17 40 full term 17 38 7.19 male nonsmoker The equivalent to arrange using standard indexing would be to use the information returned by the order function: young.moms.first.alt &lt;- births[order(births$mAge),] head(young.moms.first.alt) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18 15 37 full term 12 76 8.44 male nonsmoker ## 2 NA 16 40 full term 4 12 6 female nonsmoker ## 3 21 16 38 full term 15 75 7.56 female smoker ## 4 26 17 38 full term 11 30 9.5 female nonsmoker ## 5 17 17 29 premie 4 10 2.63 female nonsmoker ## 6 20 17 40 full term 17 38 7.19 male nonsmoker When using arrange, multiple sorting variables can be specified: sorted.by.moms.and.dads &lt;- arrange(births, mAge, fAge) head(sorted.by.moms.and.dads) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 18 15 37 full term 12 76 8.44 male nonsmoker ## 2 21 16 38 full term 15 75 7.56 female smoker ## 3 NA 16 40 full term 4 12 6 female nonsmoker ## 4 17 17 29 premie 4 10 2.63 female nonsmoker ## 5 20 17 40 full term 17 38 7.19 male nonsmoker ## 6 26 17 38 full term 11 30 9.5 female nonsmoker If you want to sort in descending order, you can combing arrange with the desc (=descend) function, also defined in dplyr: old.moms.first &lt;- arrange(births, desc(mAge)) head(old.moms.first) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 NA 41 33 premie 13 0 5.69 female nonsmoker ## 2 41 40 40 full term 13 30 9 female nonsmoker ## 3 33 40 36 premie 13 23 7.81 female nonsmoker ## 4 40 40 38 full term 13 38 7.31 male nonsmoker ## 5 46 39 38 full term 10 35 6.75 male smoker ## 6 NA 38 32 premie 10 16 2.19 female smoker 6.5.5 summarize summarize applies a function of interest to one or more variables in a data frame, reducing a vector of values to a single value and returning the results in a data frame. This is most often used to calculate statistics like means, medians, count, etc. As we’ll see below, this is powerful when combined with the group_by function. summarize(births, mean.wt = mean(weight), median.wks = median(weeks)) ## # A tibble: 1 x 2 ## mean.wt median.wks ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7.046 39 You’ll need to be diligent if your data has missing values (NAs). For example, by default the mean function returns NA if any of the input values are NA: summarize(births, mean.gained = mean(gained)) ## # A tibble: 1 x 1 ## mean.gained ## &lt;dbl&gt; ## 1 NA However, if you read the mean docs (?mean) you’ll see that there is an na.rm argument that indicates whether NA values should be removed before computing the mean. This is what we want so we instead call summarize as follows: summarize(births, mean.gained = mean(gained, na.rm = TRUE)) ## # A tibble: 1 x 1 ## mean.gained ## &lt;dbl&gt; ## 1 32.45 6.5.6 group_by The group_by function implicitly adds grouping information to a data frame. # group the births by whether mom smoked or not by_smoking &lt;- group_by(births, smoke) The object returned by group_by is a “grouped data frame”: class(by_smoking) ## [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Some functions, like count() and summarize() (see below) know how to use the grouping information. For example, to count the number of births conditional on mother smoking status we could do: count(by_smoking) ## # A tibble: 2 x 2 ## smoke n ## &lt;chr&gt; &lt;int&gt; ## 1 nonsmoker 100 ## 2 smoker 50 group_by also works with multiple grouping variables, with each added grouping variable specified as an additional argument: by_smoking.and.mAge &lt;- group_by(births, smoke, mAge &gt; 35) 6.5.7 Combining grouping and summarizing Grouped data frames can be combined with the summarize function we saw above. For example, if we wanted to calculate mean birth weight, broken down by whether the baby’s mother smoked or not we could call summarize with our by_smoking grouped data frame: summarize(by_smoking, mean.wt = mean(weight)) ## # A tibble: 2 x 2 ## smoke mean.wt ## &lt;chr&gt; &lt;dbl&gt; ## 1 nonsmoker 7.180 ## 2 smoker 6.779 Similarly to get the mean birth weight of children conditioned on mothers smoking status and age: summarize(by_smoking.and.mAge, mean(weight)) ## # A tibble: 4 x 3 ## smoke `mAge &gt; 35` `mean(weight)` ## &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 nonsmoker FALSE 7.171 ## 2 nonsmoker TRUE 7.258 ## 3 smoker FALSE 6.832 ## 4 smoker TRUE 6.302 6.5.8 Scoped variants of mutate and summarize Both the mutate() and summarize() functions provide “scoped” alternatives, that allow us to apply the operation on a selection of variables. These variants are often used in combination with grouping. We’ll look at the summarize versions – summarize_all(), summarize_at(), and summarize_if(). See the documentation (?mutate_all) for descriptions of the mutate versions. 6.5.8.1 summarize_all() summarize_all() applies a one or more functions to all columns in a data frame. Here we illustrate a simple version of this with the iris data: # group by species by_species &lt;- group_by(iris, Species) # calculate the mean of every variable, grouped by species summarize_all(by_species, mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.006 3.428 1.462 0.246 ## 2 versicolor 5.936 2.77 4.26 1.326 ## 3 virginica 6.588 2.974 5.552 2.026 Note that if we try and apply summarize_all() in the same way to the grouped data frame by_smoking we’ll get a bunch of warning messages: summarize_all(by_smoking, mean) ## # A tibble: 2 x 9 ## smoke fAge mAge weeks premature visits gained weight sexBaby ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker NA 26.9 38.55 NA NA NA 7.180 NA ## 2 smoker NA 26 38.54 NA 10.8 NA 6.779 NA Here’s an example of one of these warnings: Warning messages: 1: In mean.default(premature) : argument is not numeric or logical: returning NA This message is telling us that we can’t apply the mean() function to the data frame column premature because this is not a numerical or logical vector. Despite this and the other similar warnings, summarize_all() does return a result, but the means for any non-numeric values are replaced with NAs, as shown below: # A tibble: 2 x 9 smoke fAge mAge weeks premature visits gained weight sexBaby &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 nonsmoker NA 26.9 38.6 NA NA NA 7.18 NA 2 smoker NA 26.0 38.5 NA 10.8 NA 6.78 NA If you examine the output above, you’ll see that there are several variables that are numeric, however we still got NAs when we calculated the grouped means. This is because those variables contain NA values. The mean function has an optional argument, na.rm, which tells the function to remove any missing data before calculating the mean. Thus we can modify our call to summarize_all as follows: # calculate mean of all variables, grouped by smoking status summarize_all(by_smoking, mean, na.rm = TRUE) ## # A tibble: 2 x 9 ## smoke fAge mAge weeks premature visits gained weight sexBaby ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 29.81 26.9 38.55 NA 11.86 32.55 7.180 NA ## 2 smoker 29.71 26 38.54 NA 10.8 32.27 6.779 NA Note that the non-numeric data columns still lead to NA values. 6.5.8.2 summarize_if() summarize_if() is similar to summarize_all(), except it only applies the function of interest to those variables that match a particular predicate (i.e. are TRUE for a particular TRUE/FALSE test). Here we use summarize_if() to apply the mean() function to only those variables (columns) that are numeric. # calculate mean of all numeric variables, grouped by smoking status summarize_if(by_smoking, is.numeric, mean, na.rm = TRUE) ## # A tibble: 2 x 7 ## smoke fAge mAge weeks visits gained weight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 29.81 26.9 38.55 11.86 32.55 7.180 ## 2 smoker 29.71 26 38.54 10.8 32.27 6.779 6.5.8.3 summarize_at() summarize_at() allows us to apply functions of interest only to specific variables. # calculate mean of gained and weight variables, grouped by smoking status summarize_at(by_smoking, c(&quot;gained&quot;, &quot;weight&quot;), mean, na.rm = TRUE) ## # A tibble: 2 x 3 ## smoke gained weight ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 32.55 7.180 ## 2 smoker 32.27 6.779 All three of the scoped summarize functions can also be used to apply multiple functions, by wrapping the function names in a call to dplyr::funs(): # calculate mean and std deviation of # gained and weight variables, grouped by smoking status summarize_at(by_smoking, c(&quot;gained&quot;, &quot;weight&quot;), funs(mean, sd), na.rm = TRUE) ## # A tibble: 2 x 5 ## smoke gained_mean weight_mean gained_sd weight_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 32.55 7.180 15.23 1.434 ## 2 smoker 32.27 6.779 16.65 1.597 summarize_at() accepts as the the argument for variables a character vector of column names, a numeric vector of column positions, or a list of columns generated by the dplyr::vars() function, which can be be used as so: # reformatted to promote readability of arguments summarize_at(by_smoking, vars(gained, weight), funs(mean, sd), na.rm = TRUE) ## # A tibble: 2 x 5 ## smoke gained_mean weight_mean gained_sd weight_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 nonsmoker 32.55 7.180 15.23 1.434 ## 2 smoker 32.27 6.779 16.65 1.597 6.5.9 Combining summarize with grouping aesthetics in ggplot2 We’ve already seen an instance of grouping (conditioning) when we used aesthetics like color or fill to distinguish subgroups in different types of statistical graphics. Below is an example where we integrate information from a group_by/summarize operation into a plot: # calculate mean weights, conditioned on smoking status wt.by.smoking &lt;- summarize(by_smoking, mean_weight = mean(weight, na.rm = TRUE)) # create density plot for all the data # and then use geom_vline to draw vertical lines at the means for # each group ggplot(births) + geom_density(aes(x = weight, color = smoke)) + # data drawn from births geom_vline(data = wt.by.smoking, # note use of different data frame! mapping = aes(xintercept = mean_weight, color = smoke), linetype = &#39;dashed&#39;) 6.6 Pipes dplyr includes a very useful operator available called a pipe available to us. Pipes are powerful because they allow us to chain together sets of operations in a very intuitive fashion while minimizing nested function calls. We can think of pipes as taking the output of one function and feeding it as the first argument to another function call, where we’ve already specified the subsequent arguments. Pipes are actually defined in another packaged called magrittr. We’ll look at the basic pipe operator and then look at a few additional “special” pipes that magrittr provides. 6.6.1 Install and load magrittr In magrittr in not already installed, install it via the command line or the RStudio GUI. Having done so, you will need to load magrittr via the library() function: library(magrittr) 6.6.2 The basic pipe operator The pipe operator is designated by %&gt;%. Using pipes, the expression x %&gt;% f() is equivalent to f(x) and the expression x %&gt;% f(y) is equivalent to f(x,y). The documentation on pipes (see ?magrittr) uses the notation lhs %&gt;% rhs where lhs and rhs are short for “left-hand side” and “right-hand side” respectively. I’ll use this same notation in some of the explanations that follow. births %&gt;% head() # same as head(births) ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker births %&gt;% head # you can even leave the parentheses out ## # A tibble: 6 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker births %&gt;% head(10) # same as head(births, 10) ## # A tibble: 10 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 31 30 39 full term 13 1 6.88 male smoker ## 2 34 36 39 full term 5 35 7.69 male nonsmoker ## 3 36 35 40 full term 12 29 8.88 male nonsmoker ## 4 41 40 40 full term 13 30 9 female nonsmoker ## 5 42 37 40 full term NA 10 7.94 male nonsmoker ## 6 37 28 40 full term 12 35 8.25 male smoker ## 7 35 35 28 premie 6 29 1.63 female nonsmoker ## 8 28 21 35 premie 9 15 5.5 female smoker ## 9 22 20 32 premie 5 40 2.69 male smoker ## 10 36 25 40 full term 13 34 8.75 female nonsmoker Multiple pipes can be chained together, such that x %&gt;% f() %&gt;% g() %&gt;% h() is equivalent to h(g(f(x))). # equivalent to: head(arrange(births, weight), 10) births %&gt;% arrange(weight) %&gt;% head(10) ## # A tibble: 10 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 35 35 28 premie 6 29 1.63 female nonsmoker ## 2 NA 18 33 premie 7 40 1.69 male smoker ## 3 NA 38 32 premie 10 16 2.19 female smoker ## 4 17 17 29 premie 4 10 2.63 female nonsmoker ## 5 22 20 32 premie 5 40 2.69 male smoker ## 6 38 37 26 premie 5 25 3.63 male nonsmoker ## 7 25 22 34 premie 10 20 3.75 male nonsmoker ## 8 NA 24 38 full term 16 50 3.75 female nonsmoker ## 9 30 25 35 premie 15 40 4.5 male smoker ## 10 19 20 34 premie 13 6 4.5 male nonsmoker When there are multiple piping operations, I like to arrange the statements vertically to help emphasize the flow of processing and to facilitate debugging and/or modification. I would usually rearrange the above code block as follows: births %&gt;% arrange(weight) %&gt;% head(10) ## # A tibble: 10 x 9 ## fAge mAge weeks premature visits gained weight sexBaby smoke ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 35 35 28 premie 6 29 1.63 female nonsmoker ## 2 NA 18 33 premie 7 40 1.69 male smoker ## 3 NA 38 32 premie 10 16 2.19 female smoker ## 4 17 17 29 premie 4 10 2.63 female nonsmoker ## 5 22 20 32 premie 5 40 2.69 male smoker ## 6 38 37 26 premie 5 25 3.63 male nonsmoker ## 7 25 22 34 premie 10 20 3.75 male nonsmoker ## 8 NA 24 38 full term 16 50 3.75 female nonsmoker ## 9 30 25 35 premie 15 40 4.5 male smoker ## 10 19 20 34 premie 13 6 4.5 male nonsmoker 6.6.3 An example without pipes To illustrate how pipes help us, first let’s look at an example set of analysis steps without using pipes. Let’s say we wanted to explore the relationship between father’s age and baby’s birth weight. We’ll start this process of exploration by generating a bivariate scatter plot. Being good scientists we want to express our data in SI units, so we’ll need to converts pounds to kilograms. You’ll also recall that a number of the cases have missing data on father’s age, so we’ll want to remove those before we plot them. Here’s how we might accomplish these steps: # add a new column for weight in kg births.kg &lt;- mutate(births, weight.kg = weight / 2.2) # filter out the NA fathers filtered.births &lt;- filter(births.kg, !is.na(fAge)) # create our plot ggplot(filtered.births, aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) Notice that we created two “temporary” data frames along the way – births.kg and filtered.births. These probably aren’t of particular interest to us, but we needed to generate them to build the plot we wanted. If you were particularly masochistic you could avoid these temporary data frames by using nested functions call like this: # You SHOULD NOT write nested code like this. # Code like this is hard to debug and understand! ggplot(filter(mutate(births, weight.kg = weight / 2.2), !is.na(fAge)), aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) 6.6.4 The same example using pipes The pipe operator makes the output of one statement (lhs) as the first input of a following function (rhs). This simplifies the above example to: births %&gt;% mutate(weight.kg = weight / 2.2) %&gt;% filter(!is.na(fAge)) %&gt;% ggplot(aes(x = fAge, y = weight.kg)) + geom_point() + labs(x = &quot;Father&#39;s Age (years)&quot;, y = &quot;Birth Weight (kg)&quot;) In the example above, we feed the data frame into the mutate function. mutate expects a data frame as a first argument, and subsequent arguments specify the new variables to be created. births %&gt;% mutate(weight.kg = weight / 2.2) is thus equivalent to mutate(births, weight.kg = weight / 2.2)). We then pipe the output to filter, removing NA fathers, and then pipe that output as the input to ggplot. As mentioned previously, it’s good coding style to write each discrete step as its own line when using piping. This make it easier to understand what the steps of the analysis are as well as facilitating changes to the code (commenting out lines, adding lines, etc) 6.6.5 Assigning the output of a statement involving pipes to a variable It’s important to recognize that pipes are simply a convenient way to chain together a series of expression. Just like any other compound expression, the output of a series of pipe statements can be assigned to a variable, like so: stats.old.moms &lt;- births %&gt;% filter(mAge &gt; 35) %&gt;% summarize(median.gestation = median(weeks), mean.weight = mean(weight)) stats.old.moms ## # A tibble: 1 x 2 ## median.gestation mean.weight ## &lt;dbl&gt; &lt;dbl&gt; ## 1 38 6.939 Note that our summary table, stats.old.moms, is itself a data frame. 6.6.6 Compound assignment pipe operator A fairly common operation when working interactively in R is to update an existing data frame. magrittr defines another pipe operator – %&lt;&gt;% – called the “compound assignment” pipe operator, to facilitate this. The compound assignment pipe operator has the basic usage lhs %&lt;&gt;% rhs. This operator evaluates the function on the rhs using the lhs as the first argument, and then updates the lhs with the resulting value. This is simply shorthand for writing lhs &lt;- lhs %&gt;% rhs. stats.old.moms %&lt;&gt;% # note compound pipe operator! mutate(mean.weight.kg = mean.weight / 2.2) 6.6.7 The dot operator with pipes When working with pipes, sometimes you’ll want to use the lhs in multiple places on the rhs, or as something other than the first argument to the rhs. magrittr provides for this situation by using the dot (.) operator as a placeholder. Using the dot operator, the expression y %&gt;% f(x, .) is equivalent to f(x,y). c(&quot;dog&quot;, &quot;cakes&quot;, &quot;sauce&quot;, &quot;house&quot;) %&gt;% # create a vector sample(1) %&gt;% # pick a random single element of that vector str_c(&quot;hot&quot;, .) # string concatenate the pick with the word &quot;hot&quot; ## [1] &quot;hotcakes&quot; 6.6.8 The exposition pipe operator magrittr defines another operator called the “exposition pipe operator”, designed %$%. This operator exposes the names in the lhs to the expression on the rhs. Here is an example of using the exposition pipe operator to simply return the vector of weights: births %&gt;% filter(premature == &quot;premie&quot;) %$% # note the different pipe operator! weight ## [1] 1.63 5.50 2.69 6.50 7.81 4.75 3.75 2.19 6.81 4.69 6.75 4.50 5.94 4.50 ## [15] 5.06 5.69 1.69 6.31 2.63 5.88 3.63 If we wanted to calculate the minimum and maximum weight of premature babies in the data set we could do the following (though I’d usually prefer summarize() unless I needed the results in the form of a vector): births %&gt;% filter(mAge &gt; 35) %$% # note the different pipe operator! c(min(weight), max(weight)) ## [1] 2.19 10.13 "],
["data-wrangling.html", "Chapter 7 Data wrangling 7.1 Libraries 7.2 Data 7.3 Renaming data frame columms 7.4 Dropping unneeded columns 7.5 Merging data frames 7.6 Reshaping data with tidyr 7.7 Using your tidy data 7.8 Long-to-wide conversion using tidyr::spread 7.9 Exploring bivariate relationships using “wide” data", " Chapter 7 Data wrangling In the real world you’ll often create a data set (or be given one) in a format that is less than ideal for analysis. This can happen for a number of reasons. For example, the data may have been recorded in a manner convenient for collection and visual inspection, but which does not work well for analysis and plotting. Or the data may be an amalgamation of multiple experiments, in which each of the experimenters used slightly different naming conventions. Or the data may have been produced by an instrument that produces output with a fixed format. Sometimes important experimental information is included in the column headers of a spreadsheet. Whatever the case, we often find ourselves in the situation where we need to “wrangle” our data into a “tidy” format before we can proceed with visualization and analysis. The “R for Data Science” text discusses some desirable rules for “tidy” data in order to facilitate downstream analyses. These are: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. In this lecture we’re going to walk through an extended example of wrangling some data into a “tidy” format. 7.1 Libraries library(magrittr) library(stringr) library(tidyverse) library(cowplot) 7.2 Data To illustrate a data wrangling pipeline, we’re going to use a gene expression microarray data set, based on the following paper: Spellman PT, et al. 1998. Comprehensive identification of cell cycle-regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization. Mol Biol Cell 9(12): 3273-97. In this paper, Spellman and colleagues tried to identify all the genes in the yeast genome (&gt;6000 genes) that exhibited oscillatory behaviors suggestive of cell cycle regulation. To do so, they combined gene expression measurements from six different types of cell cycle synchronization experiments. Download the Spellman data to your filesystem from this link (right-click the “Download” button and save to your Downloads folder or similar). I suggest that once you download the data, you open it in a spreadsheet program (e.g. Excel) or use the RStudio Data Viewer to get a sense of what the data looks like. Let’s load it into R, using the read_tsv() function, using the appropriate file path. # the filepath may differ on your computer spellman &lt;- read_tsv(&quot;~/Downloads/spellman-combined.txt&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## X1 = col_character(), ## clb = col_logical(), ## alpha = col_logical(), ## cdc15 = col_logical(), ## cdc28 = col_logical(), ## elu = col_logical() ## ) ## See spec(...) for full column specifications. The initial dimenions of the data frame are: dim(spellman) ## [1] 6178 83 The six types of cell cycle synchronization experiments included in this data set are: synchronization by alpha-factor = “alpha” synchronization by cdc15 temperature sensitive mutants = “cdc15” synchronization by cdc28 temperature sensitive mutants = “cdc28” synchronization by elutration = “elu” synchronization by cln3 mutatant strains = “cln3” synchronization by clb2 mutant strains = “clb2” 7.3 Renaming data frame columms Notice that when we imported the data we got a warning message: Missing column names filled in: 'X1' [1]. In a data frame, every column must have a name. The first column of our data set did not have a name in the header, so read_tsv automatically gave it the name X1. Our first task is to give the first column a more meaningful name. This column gives “systematic gene names” – a standardized naming scheme for genes in the yeast genome. We’ll use dplyr::rename to rename X1 to gene. Note that rename can take multiple arguments if you need to rename multiple columns simultaneously. spellman.clean &lt;- spellman %&gt;% rename(gene = X1) Note the use of the compound assingment operator – %&lt;&gt;% – from the magrittr package, which we introduced in our last class session. 7.4 Dropping unneeded columns Take a look at the Spellman data again in your spreadsheet program (or the RStudio data viewer). You’ll notice there are some blank columns. For example there is a column with the header “alpha” that has no entries. These are simply visual organizing elements that the creator of the spreadsheet added to separate the different experiments that are included in the data set. We can use dplyr::select() to drop columns by prepending column names with the negative sign: # drop the alpha column keeping all others spellman.clean %&lt;&gt;% select(-alpha) Note that usually select() keeps only the variables you specify. However if the first expression is negative, select will instead automatically keep all variables, dropping only those you specify. 7.4.1 Finding all empty columns In the example above, we looked at the data and saw that the “alpha” column was empty, and thus dropped it. This worked because there are only a modest number of columns in the data frame in it’s initial form. However, if our data frame contained thousands of columns, this “look and see” procedure would not be efficient. Can we come up with a general solution for removing empty columns from a data frame? When you load a data frame from a spreadsheet, empty cells are given the value NA. In previous class sessions we were introduced to the function is.na() which tests each value in a vector or data frame for whether it’s NA or not. We can count NA values in a vector by summing the output of is.na(). Conversely we can count the number of “not NA” items by using the negation operator (!): # count number of NA values in the alpha0 column sum(is.na(spellman$alpha0)) ## [1] 165 # count number of values that are NOT NA in alpha0 sum(!is.na(spellman$alpha0)) ## [1] 6013 This seems like it should get us close to a solution but sum(is.na(..)) when applied to a data frame counts NAs across the entire data frame, not column-by-column. # doesn&#39;t do what we hoped! sum(is.na(spellman)) ## [1] 59017 If we want sums of NAs by column, we instead use the colSums() function: # get number of NAs by column colSums(is.na(spellman)) ## X1 cln3-1 cln3-2 clb clb2-2 clb2-1 alpha ## 0 193 365 6178 454 142 6178 ## alpha0 alpha7 alpha14 alpha21 alpha28 alpha35 alpha42 ## 165 525 191 312 267 207 123 ## alpha49 alpha56 alpha63 alpha70 alpha77 alpha84 alpha91 ## 257 147 186 185 178 155 329 ## alpha98 alpha105 alpha112 alpha119 cdc15 cdc15_10 cdc15_30 ## 209 174 222 251 6178 677 477 ## cdc15_50 cdc15_70 cdc15_80 cdc15_90 cdc15_100 cdc15_110 cdc15_120 ## 501 608 573 562 606 570 611 ## cdc15_130 cdc15_140 cdc15_150 cdc15_160 cdc15_170 cdc15_180 cdc15_190 ## 495 574 811 583 571 803 613 ## cdc15_200 cdc15_210 cdc15_220 cdc15_230 cdc15_240 cdc15_250 cdc15_270 ## 1014 573 741 596 847 379 537 ## cdc15_290 cdc28 cdc28_0 cdc28_10 cdc28_20 cdc28_30 cdc28_40 ## 426 6178 122 72 67 55 66 ## cdc28_50 cdc28_60 cdc28_70 cdc28_80 cdc28_90 cdc28_100 cdc28_110 ## 56 82 84 75 237 165 319 ## cdc28_120 cdc28_130 cdc28_140 cdc28_150 cdc28_160 elu elu0 ## 312 1439 2159 521 543 6178 122 ## elu30 elu60 elu90 elu120 elu150 elu180 elu210 ## 153 175 132 103 119 111 118 ## elu240 elu270 elu300 elu330 elu360 elu390 ## 131 110 112 112 156 114 Columns with all missing values can be more conveniently found by asking for those columns where the number of “not missing” values is zero: # get names of all columns for which all rows are NA # useing standard indexing names(spellman)[colSums(!is.na(spellman)) == 0] ## [1] &quot;clb&quot; &quot;alpha&quot; &quot;cdc15&quot; &quot;cdc28&quot; &quot;elu&quot; We can combine the colSums(!is.na()) idiom with the dplyr::select_if function to quickly remove all empty columns as so: # this broke in dplyr 0.80 due to a bug (https://github.com/tidyverse/dplyr/issues/4213) # supposedly a fix is on the way. In the iterim, a work around is included #spellman.clean %&lt;&gt;% # keep ONLY the non-empty columns #select_if(colSums(!is.na(.)) &gt; 0) not.all.na &lt;- function(x) { if (sum(!is.na(x)) &gt; 0){ return(TRUE) } else { return(FALSE) } } spellman.clean %&lt;&gt;% select_if(not.all.na) 7.4.2 Dropping columns by matching names Only two time points from the cln3 and clb2 experiments were reported in the original publication. Since complete time series are unavailable for these two experimental conditions we will drop them from further consideration. select() can be called be called with a number of “helper function” (?select_helpers). Here we’ll illustrate the matches() helper function which matches column names to a “regular expression”. Regular expressions (also referred to as “regex” or “regexp”) are a way of specifying patterns in strings. For the purposes of this document we’ll illustrate regexs by example; for a more detailed explanation of regular expressions see the the regex help(?regex) and the Chapter on Strings in “R for Data Analysis”: Let’s see how to drop all the “cln3” and “clb2” columns from the data frame using matches(): spellman.clean %&lt;&gt;% select(-matches(&quot;cln3&quot;)) %&gt;% select(-matches(&quot;clb2&quot;)) If we wanted we could have collapsed our two match statements into one as follows: spellman.clean %&lt;&gt;% select(-matches(&quot;cln3|clb2&quot;)) In this second example, the character “|” is specifing an OR match within the regular expression, so this regular expression matches column names that contain “cln3” OR “clb2”. 7.5 Merging data frames Often you’ll find yourself in the situation where you want to combine information from multiple data sources. The usual requirement is that the data sources have one or more shared columns, that allow you to relate the entities or observations (rows) between the data sets. dplyr provides a variety of join functions to handle different data merging operators. To illustrating merging or joining data sources, we’ll add information about each genes “common name” and a description of the gene functions to our Spellman data set. I’ve prepared a file with this info based on info I downloaded from the Saccharomyces Genome Database. gene.info &lt;- read_csv(&quot;https://github.com/bio304-class/bio304-course-notes/raw/master/datasets/yeast-ORF-info.csv&quot;) ## Parsed with column specification: ## cols( ## ftr.name = col_character(), ## std.name = col_character(), ## description = col_character() ## ) Having loaded the data, let’s get a quick overview of it’s structure: names(gene.info) ## [1] &quot;ftr.name&quot; &quot;std.name&quot; &quot;description&quot; dim(gene.info) ## [1] 6610 3 head(gene.info) ## # A tibble: 6 x 3 ## ftr.name std.name description ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 YAL069W &lt;NA&gt; Dubious open reading frame; unlikely to encode a func… ## 2 YAL068W-A &lt;NA&gt; Dubious open reading frame; unlikely to encode a func… ## 3 YAL068C PAU8 Protein of unknown function; member of the seripauper… ## 4 YAL067W-A &lt;NA&gt; Putative protein of unknown function; identified by g… ## 5 YAL067C SEO1 Putative permease; member of the allantoate transport… ## 6 YAL066W &lt;NA&gt; Dubious open reading frame; unlikely to encode a func… In gene.info, the ftr.name column corresponds to the gene column in our Spellman data set. The std.name column gives the “common” gene name (not every gene has a common name so there are lots of NAs). The description column gives a brief textual description of what the gene product does. To combine spellmean.clean with gene.info we use the left_join function defined in dplyr. As noted in the description of the function, left_join(x, y) returns “all rows from x, and all columns from x and y. Rows in x with no match in y will have NA values in the new columns.” In addition, we have to specify the column to join by using the by argument to left_join. spellman.merged &lt;- left_join(spellman.clean, gene.info, by = c(&quot;gene&quot; = &quot;ftr.name&quot;)) By default, the joined columns are merged at the end of the data frame, so we’ll reorder variables to bring the std.name and description to the second and thirds columns, preserving the order of all the other colums. spellman.merged %&lt;&gt;% select(gene, std.name, description, everything()) spellman.merged ## # A tibble: 6,178 x 76 ## gene std.name description alpha0 alpha7 alpha14 alpha21 alpha28 alpha35 ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 YAL0… TFC3 Subunit of… -0.15 -0.15 -0.21 0.17 -0.42 -0.44 ## 2 YAL0… VPS8 Membrane-b… -0.11 0.1 0.01 0.06 0.04 -0.26 ## 3 YAL0… EFB1 Translatio… -0.14 -0.71 0.1 -0.32 -0.4 -0.580 ## 4 YAL0… &lt;NA&gt; Dubious op… -0.02 -0.48 -0.11 0.12 -0.03 0.19 ## 5 YAL0… SSA1 ATPase inv… -0.05 -0.53 -0.47 -0.06 0.11 -0.07 ## 6 YAL0… ERP2 Member of … -0.6 -0.45 -0.13 0.35 -0.01 0.49 ## 7 YAL0… FUN14 Integral m… -0.28 -0.22 -0.06 0.22 0.25 0.13 ## 8 YAL0… SPO7 Putative r… -0.03 -0.27 0.17 -0.12 -0.27 0.06 ## 9 YAL0… MDM10 Subunit of… -0.05 0.13 0.13 -0.21 -0.45 -0.21 ## 10 YAL0… SWC3 Protein of… -0.31 -0.43 -0.3 -0.23 -0.13 -0.07 ## # … with 6,168 more rows, and 67 more variables: alpha42 &lt;dbl&gt;, ## # alpha49 &lt;dbl&gt;, alpha56 &lt;dbl&gt;, alpha63 &lt;dbl&gt;, alpha70 &lt;dbl&gt;, ## # alpha77 &lt;dbl&gt;, alpha84 &lt;dbl&gt;, alpha91 &lt;dbl&gt;, alpha98 &lt;dbl&gt;, ## # alpha105 &lt;dbl&gt;, alpha112 &lt;dbl&gt;, alpha119 &lt;dbl&gt;, cdc15_10 &lt;dbl&gt;, ## # cdc15_30 &lt;dbl&gt;, cdc15_50 &lt;dbl&gt;, cdc15_70 &lt;dbl&gt;, cdc15_80 &lt;dbl&gt;, ## # cdc15_90 &lt;dbl&gt;, cdc15_100 &lt;dbl&gt;, cdc15_110 &lt;dbl&gt;, cdc15_120 &lt;dbl&gt;, ## # cdc15_130 &lt;dbl&gt;, cdc15_140 &lt;dbl&gt;, cdc15_150 &lt;dbl&gt;, cdc15_160 &lt;dbl&gt;, ## # cdc15_170 &lt;dbl&gt;, cdc15_180 &lt;dbl&gt;, cdc15_190 &lt;dbl&gt;, cdc15_200 &lt;dbl&gt;, ## # cdc15_210 &lt;dbl&gt;, cdc15_220 &lt;dbl&gt;, cdc15_230 &lt;dbl&gt;, cdc15_240 &lt;dbl&gt;, ## # cdc15_250 &lt;dbl&gt;, cdc15_270 &lt;dbl&gt;, cdc15_290 &lt;dbl&gt;, cdc28_0 &lt;dbl&gt;, ## # cdc28_10 &lt;dbl&gt;, cdc28_20 &lt;dbl&gt;, cdc28_30 &lt;dbl&gt;, cdc28_40 &lt;dbl&gt;, ## # cdc28_50 &lt;dbl&gt;, cdc28_60 &lt;dbl&gt;, cdc28_70 &lt;dbl&gt;, cdc28_80 &lt;dbl&gt;, ## # cdc28_90 &lt;dbl&gt;, cdc28_100 &lt;dbl&gt;, cdc28_110 &lt;dbl&gt;, cdc28_120 &lt;dbl&gt;, ## # cdc28_130 &lt;dbl&gt;, cdc28_140 &lt;dbl&gt;, cdc28_150 &lt;dbl&gt;, cdc28_160 &lt;dbl&gt;, ## # elu0 &lt;dbl&gt;, elu30 &lt;dbl&gt;, elu60 &lt;dbl&gt;, elu90 &lt;dbl&gt;, elu120 &lt;dbl&gt;, ## # elu150 &lt;dbl&gt;, elu180 &lt;dbl&gt;, elu210 &lt;dbl&gt;, elu240 &lt;dbl&gt;, elu270 &lt;dbl&gt;, ## # elu300 &lt;dbl&gt;, elu330 &lt;dbl&gt;, elu360 &lt;dbl&gt;, elu390 &lt;dbl&gt; 7.6 Reshaping data with tidyr The tidyr package provides functions for reshaping or tidying data frames. tidyr is yet another component of the tidyverse, and thus was loaded by the library(tidyverse). We’re going to look at two functions tidyr::gather() and tidyr::extract(), and how they can be combined with now familiar dplyr functions we’ve seen previously. The reading assignment for today’s class session covers a variety of other functions defined in tidyr. The Spellman data, as I provided it to you, is in what we would call “wide” format. Each column (besides the gene column) corresponds to an experimental condition and time point. For example, “alpha0” is the alpha-factor experiment at time point 0 mins; “alpha7” is the alpha-factor experiment at time point 7 mins, etc. The cells within each column correspond to the expression of a corresponding gene (given by the first column which we renamed gene) in that particular experiment at that particular time point. In every column (except “gene”), the cells represents the same abstract property of interest – the expression of a gene of interest in a particular experiment/time point. Our first task will be to rearrange our “wide” data frame that consists of many different columns representing gene expression into a “long” data frame with just a single column representing expression. We’ll also create a new column to keep track of which experiment and time point the measurement came from. 7.6.1 Wide to long conversions using tidyr::gather tidyr::gather() takes multiple columns, and collapses them together into a smaller number of new columns. When using gather() you give the names of the new columns to create, as well as the names of any existing columns gather() should not collect together. Here we want to collapse all 73 or the expression columns – “alpha0” to “elu390” – into two columns: 1) a column to represent the expt/time point of the measurement, and 2) a column to represent the corresponding expression value. The column we don’t want to touch are the gene, std.name, and description. # convert &quot;wide&quot; data to &quot;long&quot; spellman.long &lt;- spellman.merged %&gt;% gather(expt.and.time, expression, -gene, -std.name, -description) Take a moment to look at the data in the “long format”: head(spellman.long) ## # A tibble: 6 x 5 ## gene std.name description expt.and.time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III t… alpha0 -0.15 ## 2 YAL00… VPS8 Membrane-binding component of t… alpha0 -0.11 ## 3 YAL00… EFB1 Translation elongation factor 1… alpha0 -0.14 ## 4 YAL00… &lt;NA&gt; Dubious open reading frame; unl… alpha0 -0.02 ## 5 YAL00… SSA1 ATPase involved in protein fold… alpha0 -0.05 ## 6 YAL00… ERP2 Member of the p24 family involv… alpha0 -0.6 And compare the dimensions of the wide data to the new data: dim(spellman.merged) # for comparison ## [1] 6178 76 dim(spellman.long) ## [1] 450994 5 As you see, we’ve gone from a data frame with 6178 rows and 76 columns (wide format), to a new data frame with 450994 rows and 5 columns (long format). 7.6.2 Extracting information from combined variables using tidyr::extract The column expt.and.time violates one of our principles of tidy data: “Each variable must have its own column.”. This column conflates two different types of information – the experiment type and the time point of the measurement. Our next task is to split this information up into two new variables, which will help to facilitate downstream plotting and analysis. One complicating factor is that the different experiments/time combinations have different naming conventions: The “alpha” and “elu” experiments are of the form “alpha0”, “alpha7”, “elu0”, “elu30”, etc. In this case, the first part of the string gives the experiment type (either alpha or elu) and the following digits give the time point. In the “cdc15” and “cdc28” experiments the convention is slightly different; they are of the form “cdc15_0”, “cdc15_10”, “cdc28_0”, “cdc28_10”, etc. Here the part of the string before the underscore gives the experiment type, and the digits after the underscore give the time point. Because of the differences in naming conventions, we will find it easiest to break up spellman.long into a series of sub-data sets corresponding to each experiment type in order to extract out the experiment and time information. After processing each data subset separately, we will join the modified sub-data frames back together. 7.6.3 Subsetting rows Let’s start by getting just the rows corresponding to the “alpha” experiment/times. Here we use dplyr::filter in combination with stringr::str_detect to get all those rows in which the expt.and.time variable contains the string “alpha”. alpha.long &lt;- spellman.long %&gt;% filter(str_detect(expt.and.time, &quot;alpha&quot;)) # look at the new data frame dim(alpha.long) ## [1] 111204 5 head(alpha.long, n = 10) ## # A tibble: 10 x 5 ## gene std.name description expt.and.time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III … alpha0 -0.15 ## 2 YAL00… VPS8 Membrane-binding component of … alpha0 -0.11 ## 3 YAL00… EFB1 Translation elongation factor … alpha0 -0.14 ## 4 YAL00… &lt;NA&gt; Dubious open reading frame; un… alpha0 -0.02 ## 5 YAL00… SSA1 ATPase involved in protein fol… alpha0 -0.05 ## 6 YAL00… ERP2 Member of the p24 family invol… alpha0 -0.6 ## 7 YAL00… FUN14 Integral mitochondrial outer m… alpha0 -0.28 ## 8 YAL00… SPO7 Putative regulatory subunit of… alpha0 -0.03 ## 9 YAL01… MDM10 Subunit of both the ERMES and … alpha0 -0.05 ## 10 YAL01… SWC3 Protein of unknown function; c… alpha0 -0.31 7.6.4 Splitting columns Having subsetted the data, we can now split expt.and.time into two new variables – expt and time. To do this we use tidyr::extract. alpha.long %&lt;&gt;% tidyr::extract(expt.and.time, # column we&#39;re extracting from c(&quot;expt&quot;, &quot;time&quot;), # new columns we&#39;re creating regex=&quot;(alpha)([[:digit:]]+)&quot;, # regexp (see below) convert=TRUE) # automatically convert column types # NOTE: I&#39;m being explict about saying tidyr::extract because the # magrittr package defines a different extract function Let’s take a moment to look at the regex argument to extract – regex=\"(alpha)([[:digit:]]+)\". The regex is specified as a character string. Each part we want to match and extract is surround by parentheses. In this case we have two sets of parentheses corresponding to the two matches we want to make. The first part of the regex is (alpha); here we’re looking to make an exact match to the string “alpha”. The second part of the regex reads ([[:digit:]]+). [[:digit:]] indicates we’re looking for a numeric digit. The + after [[:digit:]] indicates that we want to match one or more digits (i.e. to get a match we need to find at least one digit, but more than one digit should also be a match). Let’s take a look at the new version of alpha.long following application of extract: head(alpha.long, n = 10) ## # A tibble: 10 x 6 ## gene std.name description expt time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 0 -0.15 ## 2 YAL00… VPS8 Membrane-binding component of th… alpha 0 -0.11 ## 3 YAL00… EFB1 Translation elongation factor 1 … alpha 0 -0.14 ## 4 YAL00… &lt;NA&gt; Dubious open reading frame; unli… alpha 0 -0.02 ## 5 YAL00… SSA1 ATPase involved in protein foldi… alpha 0 -0.05 ## 6 YAL00… ERP2 Member of the p24 family involve… alpha 0 -0.6 ## 7 YAL00… FUN14 Integral mitochondrial outer mem… alpha 0 -0.28 ## 8 YAL00… SPO7 Putative regulatory subunit of N… alpha 0 -0.03 ## 9 YAL01… MDM10 Subunit of both the ERMES and th… alpha 0 -0.05 ## 10 YAL01… SWC3 Protein of unknown function; com… alpha 0 -0.31 Notice our two new variables, both of which have appropriate types! A data frame for the elutriation data can be created similarly: elu.long &lt;- spellman.long %&gt;% filter(str_detect(expt.and.time, &quot;elu&quot;)) %&gt;% tidyr::extract(expt.and.time, # column we&#39;re extracting from c(&quot;expt&quot;, &quot;time&quot;), # new columns we&#39;re creating regex=&quot;(elu)([[:digit:]]+)&quot;, # regexp (see below) convert=TRUE) # automatically convert column types 7.6.4.1 A fancier regex for the cdc experiments Now let’s process the cdc experiments (cdc15 and cdc28). As before we extract the corresponding rows of the data frame using filter and str_detect. We then split expt.and.time using tidyr::extract. In this case we carry out the two steps in a single code block using pipes: cdc.long &lt;- spellman.long %&gt;% # both cdc15 and cdc28 contain &quot;cdc&quot; as a sub-string filter(str_detect(expt.and.time, &quot;cdc&quot;)) %&gt;% tidyr::extract(expt.and.time, c(&quot;expt&quot;, &quot;time&quot;), regex=&quot;(cdc15|cdc28)_([[:digit:]]+)&quot;, # note the fancier regex convert=TRUE) The regex – \"(cdc15|cdc28)_([[:digit:]]+)\" – is slightly fancier in this example. As before there are two parts we’re extracting: (cdc15|cdc28) and ([[:digit:]]+). The first parenthesized regexp is an “OR” – i.e. match “cdc15” or “cdc28”. The second parenthesized regexp is the same as we saw previously. Separating the two parenthesized regexps is an underscore (_). The underscore isn’t parenthesized because we only want to use it to make a match not to extract the corresponding match. 7.6.5 Combining data frames If you have two or more data frames with identical columns, the rows of the data frames can be combined into a single data frame using rbind (defined in the base package). For example, to reassemble the alpha.long, elu.long, and cdc.long data frames into a single data frame we do: spellman.final &lt;- rbind(alpha.long, elu.long, cdc.long) # check the dimensions of the new data frame dim(spellman.final) ## [1] 450994 6 7.6.6 Sorting data frame rows Currently the spellman.final data frame is sorted by time point and experiment. head(spellman.final, n = 10) ## # A tibble: 10 x 6 ## gene std.name description expt time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 0 -0.15 ## 2 YAL00… VPS8 Membrane-binding component of th… alpha 0 -0.11 ## 3 YAL00… EFB1 Translation elongation factor 1 … alpha 0 -0.14 ## 4 YAL00… &lt;NA&gt; Dubious open reading frame; unli… alpha 0 -0.02 ## 5 YAL00… SSA1 ATPase involved in protein foldi… alpha 0 -0.05 ## 6 YAL00… ERP2 Member of the p24 family involve… alpha 0 -0.6 ## 7 YAL00… FUN14 Integral mitochondrial outer mem… alpha 0 -0.28 ## 8 YAL00… SPO7 Putative regulatory subunit of N… alpha 0 -0.03 ## 9 YAL01… MDM10 Subunit of both the ERMES and th… alpha 0 -0.05 ## 10 YAL01… SWC3 Protein of unknown function; com… alpha 0 -0.31 It might be useful instead to sort by gene and experiment. To do this we can use dplyr::arrange: spellman.final %&lt;&gt;% arrange(gene, expt) # look again at the rearranged data head(spellman.final, n = 10) ## # A tibble: 10 x 6 ## gene std.name description expt time expression ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 0 -0.15 ## 2 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 7 -0.15 ## 3 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 14 -0.21 ## 4 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 21 0.17 ## 5 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 28 -0.42 ## 6 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 35 -0.44 ## 7 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 42 -0.15 ## 8 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 49 0.24 ## 9 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 56 -0.1 ## 10 YAL00… TFC3 Subunit of RNA polymerase III tr… alpha 63 NA 7.7 Using your tidy data Whew – that was a fair amount of work to tidy our data! But having done so we can now carry out a wide variety of very powerful analyses. 7.7.1 Visualizing gene expression time series Let’s start by walking through a series of visualizations of gene expression time series. Each plot will show the expression of one or more genes, at different time points, in one or more experimental conditions. Our initial visualizations exploit the “long” versions of the tidy data. First a single gene in a single experimental condition: spellman.final %&gt;% filter(expt == &quot;alpha&quot;, gene == &quot;YAL022C&quot;) %&gt;% ggplot(aes(x = time, y = expression)) + geom_line() + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression of YAL022C&quot;) We can easily modify the above code block to visualize the expression of multiple genes of interest: genes.of.interest &lt;- c(&quot;YAL022C&quot;, &quot;YAR018C&quot;, &quot;YGR188C&quot;) spellman.final %&gt;% filter(gene %in% genes.of.interest, expt == &quot;alpha&quot;) %&gt;% ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of multiple genes\\nfollowing synchronization by alpha factor&quot;) By employing facet_wrap() we can visualize the relationship between this set of genes in each of the experiment types: spellman.final %&gt;% filter(gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + facet_wrap(~ expt) + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of Multiple Genes\\nAcross experiments&quot;) The different experimental treatments were carried out for varying lengths of time due to the differences in their physiological effects. Plotting them all on the same time scale can obscure that patterns of oscillation we might be interested in, so let’s modify our code block so that plots that share the same y-axis, but have differently scaled x-axes. spellman.final %&gt;% filter(gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = expression, color = gene)) + geom_line() + facet_wrap(~ expt, scales = &quot;free_x&quot;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Normalized expression&quot;, title = &quot;Expression of Multiple Genes\\nAcross experiments&quot;) 7.7.2 Finding the most variable genes When dealing with vary large data sets, one ad hoc filtering criteria that is often employed is to focus on those variables that exhibit that greatest variation (variation is measure of the spread of data; we will give a precise definition in a later lecture). To do this, we first need to order our variables (genes) by their variation. Let’s see how we can accomplish this using our long data frame: by.variance &lt;- spellman.final %&gt;% group_by(gene) %&gt;% summarize(expression.var = var(expression, na.rm = TRUE)) %&gt;% arrange(desc(expression.var)) head(by.variance) ## # A tibble: 6 x 2 ## gene expression.var ## &lt;chr&gt; &lt;dbl&gt; ## 1 YLR286C 2.157 ## 2 YNR067C 1.733 ## 3 YNL327W 1.653 ## 4 YGL028C 1.571 ## 5 YHL028W 1.521 ## 6 YKL164C 1.515 The code above calculates the variance of each gene but ignores the fact that we have different experimental conditions. To take into account the experimental design of the data at hand, let’s calculate the average variance across the experimental conditions: by.avg.variance &lt;- spellman.final %&gt;% group_by(gene, expt) %&gt;% summarize(expression.var = var(expression, na.rm = TRUE)) %&gt;% group_by(gene) %&gt;% summarize(avg.expression.var = mean(expression.var)) %&gt;% arrange(desc(avg.expression.var)) head(by.avg.variance) ## # A tibble: 6 x 2 ## gene avg.expression.var ## &lt;chr&gt; &lt;dbl&gt; ## 1 YFR014C 3.579 ## 2 YFR053C 2.377 ## 3 YBL032W 2.299 ## 4 YDR274C 2.173 ## 5 YLR286C 2.128 ## 6 YMR206W 1.937 Based on the average experession variance across experimental conditions, let’s get the names of the 1000 most variable genes: top.genes.1k &lt;- by.avg.variance[1:1000,]$gene head(top.genes.1k) ## [1] &quot;YFR014C&quot; &quot;YFR053C&quot; &quot;YBL032W&quot; &quot;YDR274C&quot; &quot;YLR286C&quot; &quot;YMR206W&quot; 7.7.3 Heat maps In our prior visualizations we’ve used line plots to depict how gene expression changes over time. For example here are line plots for 15 genes in the data set, in the cdc28 experimental conditions: genes.of.interest &lt;- c(&quot;YHR084W&quot;, &quot;YBR083W&quot;, &quot;YPL049C&quot;, &quot;YDR480W&quot;, &quot;YGR040W&quot;, &quot;YLR229C&quot;, &quot;YDL159W&quot;, &quot;YBL016W&quot;, &quot;YDR103W&quot;, &quot;YJL157C&quot;, &quot;YNL271C&quot;, &quot;YDR461W&quot;, &quot;YHL007C&quot;, &quot;YHR005C&quot;, &quot;YJR086W&quot;) spellman.final %&gt;% filter(expt == &quot;cdc28&quot;, gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = expression, color=gene)) + geom_line() + labs(x = &quot;Time (min)&quot;, y = &quot;Expression&quot;) Even with just 10 overlapping line plots, this figure is quite busy and it’s hard to make out the individual behavior of each gene. An alternative approach to depicting such data is a “heat map” which depicts the same information in a grid like form, with the expression values indicated by color. Heat maps are good for depicting large amounts of data and providing a coarse “10,000 foot view”. We can create a heat map using geom_tile as follows: spellman.final %&gt;% filter(expt == &quot;cdc28&quot;, gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + xlab(&quot;Time (mins)&quot;) This figure represents the same information as our line plot, but now there is row for each gene, and the expression of that gene at a given time point is represented by color (scale given on the right). Missing data is shown as gray boxes. Unfortunately, the default color scale used by ggplot is a very subtle gradient from light to dark blue. This make it hard to distinguish patterns of change. Let’s now see how we can improve that. 7.7.3.1 Better color schemes with RColorBrewer The RColorBrewer packages provides nice color schemes that are useful for creating heat maps. RColorBrewer defines a set of color palettes that have been optimized for color discrimination, many of which are color blind friendly, etc. Install the RColorBrewer package using the command line or the RStudio GUI. Once you’ve installed the RColorBrewer package you can see the available color palettes as so: library(RColorBrewer) # show representations of the palettes par(cex = 0.5) # reduce size of text in the following plot display.brewer.all() We’ll use the Red-to-Blue (“RdBu”) color scheme defined in RColorBrewer, however we’ll reverse the scheme so blues represent low expression and reds represent high expression. We’ll divide the range of color values into 9 discrete bins. # displays the RdBu color scheme divided into a palette of 9 colors display.brewer.pal(9, &quot;RdBu&quot;) # assign the reversed (blue to red) RdBu palette color.scheme &lt;- rev(brewer.pal(9,&quot;RdBu&quot;)) Now let’s regenerate the heat map we created previously with this new color scheme. To do this we specify a gradient color scale using the scale_fill_gradientn() function from ggplot. In addition to specifying the color scale, we also constrain the limits of the scale to insure it’s symmetric about zero. spellman.final %&gt;% filter(expt == &quot;cdc28&quot;, gene %in% genes.of.interest) %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme, limits = c(-2.5, 2.5)) + xlab(&quot;Time (mins)&quot;) 7.7.3.2 Looking for patterns using sorted data and heat maps The real power of heat maps becomes apparent when you you rearrange the rows of the heat map to emphasize patterns of interest. For example, let’s create a heat map in which we sort genes by the time of their maximal expression. This is one way to identify genes that reach their peak expression at similar times, which is one criteria one might use to identify genes acting in concert. For simplicities sake we will restrict our attention to the cdc28 experiment, and only consider the 1000 most variables genes with no more than one missing observation in this experimental condition. cdc28 &lt;- spellman.final %&gt;% filter(expt == &quot;cdc28&quot;) %&gt;% group_by(gene) %&gt;% filter(sum(is.na(expression)) &lt;= 1) %&gt;% ungroup # removes grouping information from data frame top1k.genes &lt;- cdc28 %&gt;% group_by(gene) %&gt;% summarize(expression.var = var(expression, na.rm = TRUE)) %&gt;% arrange(desc(expression.var)) %$% gene[1:1000] top1k.cdc28 &lt;- cdc28 %&gt;% filter(gene %in% top1k.genes) To find the time of maximum expression we’ll employ the function which.max (which.min), which finds the index of the maximum (minimum) element of a vector. For example to find the index of the maximum expression measurement for YAR018C we could do: top1k.cdc28 %&gt;% filter(gene == &quot;YAR018C&quot;) %$% # note the exposition pipe operator! which.max(expression) ## [1] 8 From the code above we find that the index of the observation at which YAR018C is maximal at 8. To get the corresponding time point we can do something like this: top1k.cdc28 %&gt;% filter(gene == &quot;YAR018C&quot;) %$% # again note the exposition pipe operator! time[which.max(expression)] ## [1] 70 Thus YAR018C expression peaks at 70 minutes in the cdc28 experiment. To find the index of maximal expression of all genes we can apply the dplyr::group_by() and dplyr::summarize() functions peak.expression.cdc28 &lt;- top1k.cdc28 %&gt;% group_by(gene) %&gt;% summarise(peak = which.max(expression)) head(peak.expression.cdc28) ## # A tibble: 6 x 2 ## gene peak ## &lt;chr&gt; &lt;int&gt; ## 1 YAL003W 10 ## 2 YAL005C 2 ## 3 YAL022C 17 ## 4 YAL028W 5 ## 5 YAL035C-A 12 ## 6 YAL038W 15 Let’s sort the order of genes by their peak expression: peak.expression.cdc28 %&lt;&gt;% arrange(peak) We can then generate a heatmap where we sort the rows (genes) of the heatmap by their time of peak expression. We introduce a new geom – geom_raster – which is like geom_tile but better suited for large data (hundreds to thousands of rows) The explicit sorting of the data by peak expression is carried out in the call to scale_y_discrete() where the limits (and order) of this axis are set with the limits argument (see scale_y_discrete and discrete_scale in the ggplot2 docs). # we reverse the ordering because geom_raster (and geom_tile) # draw from the bottom row up, whereas we want to depict the # earliest peaking genes at the top of the figure gene.ordering &lt;- rev(peak.expression.cdc28$gene) top1k.cdc28 %&gt;% ggplot(aes(x = time, y = gene)) + geom_raster(aes(fill = expression)) + # scale_fill_gradientn(limits=c(-2.5, 2.5), colors=color.scheme) + scale_y_discrete(limits=gene.ordering) + labs(x = &quot;Time (mins)&quot;, y = &quot;Genes&quot;, title = &quot;1000 most variable genes&quot;, subtitle = &quot;Sorted by time of peak expression&quot;) + # the following line suppresses tick and labels on y-axis theme(axis.text.y = element_blank(), axis.ticks.y = element_blank()) Figure 7.1: A Heatmap showing genes in the cdc28 experiment, sorted by peak expression The brightest red regions in each row of the heat map correspond to the times of peak expression, and the sorting of the rows helps to highlight those gene whose peak expression times are similar. 7.8 Long-to-wide conversion using tidyr::spread Our long data frame consists of four variables – gene, expt, time, and expression. This made it easy to create visualizations and summaries where time and expression were the primaries variables of interest, and gene and experiment type were categories we could condition on. To facilitate analyses that emphasize comparison between genes, we want to create a new data frame in which each gene is itself treated as a variable of interest along with time, and experiment type remains a categorical variable. In this new data frame rather than just four columns in our data frame, we’ll have several thousand columns – one for each gene. To accomplish this reshaping of data, we’ll use the function tidyr::spread(). tidyr::spread() is the inverse of tidyr::gather(). gather() took multiple columns and collapsed them together into a smaller number of new columns. The tidyr documentation calls this “collapsing into key-value pairs”. By contrast, spread() creates new columns by spreading “key-value pairs” (a column representing the “keys” and a column reprsenting the “values”) into multiple columns. Here let’s use spread() to use the gene names (the “key”) and expression measures (the “values”) to create a new data frame where the genes are the primary variables (columns) of the data. spellman.wide &lt;- spellman.final %&gt;% select(-std.name, -description) %&gt;% # drop unneeded columns spread(gene, expression) Now let’s examine the dimensions of this wide version of the data: dim(spellman.wide) ## [1] 73 6180 And here’s a visual view of the first few rows and columns of the wide data: spellman.wide[1:5, 1:8] ## # A tibble: 5 x 8 ## expt time YAL001C YAL002W YAL003W YAL004W YAL005C YAL007C ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alpha 0 -0.15 -0.11 -0.14 -0.02 -0.05 -0.6 ## 2 alpha 7 -0.15 0.1 -0.71 -0.48 -0.53 -0.45 ## 3 alpha 14 -0.21 0.01 0.1 -0.11 -0.47 -0.13 ## 4 alpha 21 0.17 0.06 -0.32 0.12 -0.06 0.35 ## 5 alpha 28 -0.42 0.04 -0.4 -0.03 0.11 -0.01 From this view we infer that the rows of the data set represent the various combination of experimental condition and time points, and the columns represents the 6178 genes in the data set plus the two columns for expt and time. 7.9 Exploring bivariate relationships using “wide” data The “long” version of our data frame proved useful for exploring how gene expression changed over time. By contrast, our “wide” data frame is more convient for exploring how pairs of genes covary together. For example, we can generate bivariate scatter plots depicting the relationship between two genes of interest: two.gene.plot &lt;- spellman.wide %&gt;% filter(!is.na(YAR018C) &amp; !is.na(YAL022C)) %&gt;% # remove NAs ggplot(aes(x = YAR018C, y = YAL022C)) + geom_point() + theme(aspect.ratio = 1) two.gene.plot From the scatter plot we infer that the two genes are “positively correlated” with each other, meaning that high values of one tend to be associated with high values of the other (and the same for low values). We can easily extend this visualization to facet the plot based on the experimental conditions: two.gene.plot + facet_wrap(~expt, nrow = 2, ncol = 2) A statistic we use to measure the degree of association between pairs of continuous variables is called the “correlation coefficient”. Briefly, correlation is a measure of linear association between a pair of variables, and ranges from -1 to 1. A value near zero indicates the variables are uncorrelated (no linear association), while values approaching +1 indicate a strong positive association (the variables tend to get bigger or smaller together) while values near -1 indicate strong negative association (when one variable is larger, the other tends to be small). Let’s calculate the correlation between YAR018C and YAL022C: spellman.wide %&gt;% filter(!is.na(YAR018C) &amp; !is.na(YAL022C)) %&gt;% summarize(cor = cor(YAR018C, YAL022C)) ## # A tibble: 1 x 1 ## cor ## &lt;dbl&gt; ## 1 0.6915 The value of the correlation coefficient for YAR018C and YAL022C, ~0.69, indicates a fairly strong association between the two genes. As we did for our visualization, we can also calculate the correlation coefficients for the two genes under each experimental condition: spellman.wide %&gt;% filter(!is.na(YAR018C) &amp; !is.na(YAL022C)) %&gt;% group_by(expt) %&gt;% summarize(cor = cor(YAR018C, YAL022C)) ## # A tibble: 4 x 2 ## expt cor ## &lt;chr&gt; &lt;dbl&gt; ## 1 alpha 0.6341 ## 2 cdc15 0.5751 ## 3 cdc28 0.8474 ## 4 elu 0.7867 This table suggests that the the strength of correlation between YAR018C and YAL022C may depend on the experimental conditions, with the highest correlations evident in the cdc28 and elu experiments. 7.9.1 Large scale patterns of correlations Now we’ll move from considering the correlation between two specific genes to looking at the correlation between many pairs of genes. As we did in the previous section, we’ll focus specifically onthe 1000 most variable genes in the cdc28 experiment. First we filter our wide data set to only consider the cdc28 experiment and those genes in the top 1000 most variable genes in cdc28: top1k.cdc28.wide &lt;- top1k.cdc28 %&gt;% select(-std.name, -description) %&gt;% spread(gene, expression) With this restricted data set, we can then calculate the correlations between every pair of genes as follows: cdc28.correlations &lt;- top1k.cdc28.wide %&gt;% select(-expt, -time) %&gt;% # drop expt and time cor(use = &quot;pairwise.complete.obs&quot;) The argument use = \"pairwise.complete.obs\" tells the correlation function that for each pair of genes to use only the pariwse where there is a value for both genes (i.e. neither one can be NA). Given \\(n\\) genes, there are \\(n \\times n\\) pairs of correlations, as seen by the dimensions of the correlation matrix. dim(cdc28.correlations) ## [1] 1000 1000 To get the correlations with a gene of interest, we can index with the gene name on the rows of the correlation matrix. For example, to get the correlations between the gene YAR018C and the first 10 genes in the top 1000 set: cdc28.correlations[&quot;YAR018C&quot;,1:10] ## YAL003W YAL005C YAL022C YAL028W YAL035C-A YAL038W ## 0.07100626 -0.53315493 0.84741624 0.33379901 -0.22316755 -0.03984599 ## YAL044C YAL048C YAL060W YAL062W ## 0.32253692 0.12220221 0.49445700 -0.60972118 In the next statement we extract the names of the genes that have correlations with YAR018C greater than 0.6. First we test genes to see if they have a correlation with YAR018C greater than 0.6, which returns a vector of TRUE or FALSE values. This vector of Boolean values is than used to index into the rownames of the correlation matrix, pulling out the gene names where the statement was true. pos.corr.YAR018C &lt;- rownames(cdc28.correlations)[cdc28.correlations[&quot;YAR018C&quot;,] &gt; 0.6] length(pos.corr.YAR018C) ## [1] 65 We then return to our long data to show this set of genes that are strongly positively correlated with YAR018C. top1k.cdc28 %&gt;% filter(gene %in% pos.corr.YAR018C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33) + theme(legend.position = &quot;none&quot;) As is expected, genes with strong positive correlations with YAR018C. show similar temporal patterns with YAR018C. We can similarly filter for genes that have negative correlations with YAR018C. neg.corr.YAR018C &lt;- colnames(cdc28.correlations)[cdc28.correlations[&quot;YAR018C&quot;,] &lt;= -0.6] As before we generate a line plot showing these genes: top1k.cdc28 %&gt;% filter(gene %in% neg.corr.YAR018C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33) + theme(legend.position = &quot;none&quot;) 7.9.2 Adding new columns and combining filtered data frames Now let’s create a new data frame by: 1) filtering on our list of genes that have strong positive and negative correlations with YAR018C; and 2) creating a new variable, “corr.with.YAR018C”, which indicates the sign of the correlation. We’ll use this new variable to group genes when we create the plot. pos.corr.df &lt;- top1k.cdc28 %&gt;% filter(gene %in% pos.corr.YAR018C) %&gt;% mutate(corr.with.YAR018C = &quot;positive&quot;) neg.corr.df &lt;- top1k.cdc28 %&gt;% filter(gene %in% neg.corr.YAR018C) %&gt;% mutate(corr.with.YAR018C = &quot;negative&quot;) combined.pos.neg &lt;- rbind(pos.corr.df, neg.corr.df) Finally, we plot the data, colored according to the correlation with YAR018C: ggplot(combined.pos.neg, aes(x = time, y = expression, group = gene, color = corr.with.YAR018C)) + geom_line(alpha=0.25) + geom_line(aes(x = time, y = expression), data = filter(top1k.cdc28, gene == &quot;YAR018C&quot;), color = &quot;DarkRed&quot;, size = 2,alpha=0.5) + # changes legend title and values for color sclae scale_color_manual(name = &quot;Correlation with YAR018C&quot;, values = c(&quot;blue&quot;, &quot;red&quot;)) + labs(title = &quot;Genes strongly positively and negatively correlated with YAR018C&quot;, subtitle = &quot;YAR018C shown in dark red&quot;, x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;) 7.9.3 A heat mapped sorted by correlations In our previous heat map example figure, we sorted genes according to peak expression. Now let’s generate a heat map for the genes that are strongly correlated (both positive and negative) with YAR018C. We will sort the genes according to the sign of their correlation. # re-factor gene names so positive and negative genes are spatially distinct in plot combined.pos.neg$gene &lt;- factor(combined.pos.neg$gene, levels = c(pos.corr.YAR018C, neg.corr.YAR018C)) combined.pos.neg %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme) + xlab(&quot;Time (mins)&quot;) The breakpoint between the positively and negatively correlated sets of genes is quite obvious in this figure. 7.9.4 A “fancy” figure Recall that we introduced the cowplot library in Chapter 6, as a way to combine different ggplot outputs into subfigures such as you might find in a published paper. Here we’ll make further use cowplot to combine our heat map and line plot visualizations of genes that covary with YAR018C. library(cowplot) cowplot’s draw_plot() function allows us to place plots at arbitrary locations and with arbitrary sizes onto the canvas. The coordinates of the canvas run from 0 to 1, and the point (0, 0) is in the lower left corner of the canvas. We’ll use draw_plot to draw a complex figure with a heatmap on the left, and two smaller line plots on the right. pos.corr.lineplot &lt;- combined.pos.neg %&gt;% filter(gene %in% pos.corr.YAR018C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33, color = &#39;red&#39;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;, title = &quot;Genes Positively correlated\\nwith YAR018C&quot;) neg.corr.lineplot &lt;- combined.pos.neg %&gt;% filter(gene %in% neg.corr.YAR018C) %&gt;% ggplot(aes(x = time, y = expression, group = gene)) + geom_line(alpha = 0.33, color = &#39;blue&#39;) + labs(x = &quot;Time (mins)&quot;, y = &quot;Expression&quot;, title = &quot;Genes negatively correlated\\nwith YAR018C&quot;) heat.map &lt;- ggplot(combined.pos.neg, aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme) + labs(x = &quot;Time (mins)&quot;, y = &quot;Gene&quot;) + theme(legend.position = &quot;none&quot;) The coordinates of the canvas run from 0 to 1, and the point (0, 0) is in the lower left corner of the canvas. We’ll use draw_plot to draw a complex figure with a heatmap on the left, and two smaller line plots on the right. I determined the coordinates below by experimentation to create a visually pleasing layout. fancy.plot &lt;- ggdraw() + draw_plot(heat.map, 0, 0, width = 0.6) + draw_plot(neg.corr.lineplot, 0.6, 0.5, width = 0.4, height = 0.5) + draw_plot(pos.corr.lineplot, 0.6, 0, width = 0.4, height = 0.5) + draw_plot_label(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c(0, 0.6, 0.6), c(1, 1, 0.5), size = 15) fancy.plot "],
["functions-and-control-flow-statements.html", "Chapter 8 Functions and control flow statements 8.1 Writing your own functions 8.2 Control flow statements 8.3 map and related tools", " Chapter 8 Functions and control flow statements 8.1 Writing your own functions So far we’ve been using a variety of built in functions in R. However the real power of a programming language is the ability to write your own functions. Functions are a mechanism for organizing and abstracting a set of related computations. We usually write functions to represent sets of computations that we apply frequently, or to represent some conceptually coherent set of manipulations to data. The general form of an R function is as follows: funcname &lt;- function(arg1, arg2) { # one or more expressions that operate on the fxn arguments # last expression is the object returned # or you can explicitly return an object } To make this concrete, here’s an example where we define a function to calculate the area of a circle: area.of.circle &lt;- function(r){ return(pi * r^2) } Since R returns the value of the last expression in the function, the return call is optional and we could have simply written: area.of.circle &lt;- function(r){ pi * r^2 } Very short and concise functions are often written as a single line. In practice I’d probably write the above function as: area.of.circle &lt;- function(r) {pi * r^2} The area.of.circle function takes one argument, r, and calculates the area of a circle with radius r. Having defined the function we can immediately put it to use: area.of.circle(3) ## [1] 28.27433 radius &lt;- 4 area.of.circle(radius) ## [1] 50.26548 If you type a function name without parentheses R shows you the function’s definition. This works for built-in functions as well (thought sometimes these functions are defined in C code in which case R will tell you that the function is a .Primitive). 8.1.1 Function arguments Function arguments can specify the data that a function operates on or parameters that the function uses. Function arguments can be either required or optional. In the case of optional arguments, a default value is assigned if the argument is not given. Take for example the log function. If you examine the help file for the log function (type ?log now) you’ll see that it takes two arguments, refered to as x and base. The argument x represents the numeric vector you pass to the function and is a required argument (see what happens when you type log() without giving an argument). The argument base is optional. By default the value of base is \\(e = 2.71828\\ldots\\). Therefore by default the log function returns natural logarithms. If you want logarithms to a different base you can change the base argument as in the following examples: log(2) # log of 2, base e ## [1] 0.6931472 log(2,2) # log of 2, base 2 ## [1] 1 log(2, 4) # log of 2, base 4 ## [1] 0.5 Because base 2 and base 10 logarithms are fairly commonly used, there are convenient aliases for calling log with these bases. log2(8) ## [1] 3 log10(100) ## [1] 2 8.1.2 Writing functions with optional arguments To write a function that has an optional argument, you can simply specify the optional argument and its default value in the function definition as so: # a function to substitute missing values in a vector sub.missing &lt;- function(x, sub.value = -99){ x[is.na(x)] &lt;- sub.value return(x) } You can then use this function as so: m &lt;- c(1, 2, NA, 4) sub.missing(m, -999) # explicitly define sub.value ## [1] 1 2 -999 4 sub.missing(m, sub.value = -333) # more explicit syntax ## [1] 1 2 -333 4 sub.missing(m) # use default sub.value ## [1] 1 2 -99 4 m # notice that m wasn&#39;t modified within the function ## [1] 1 2 NA 4 Notice that when we called sub.missing with our vector m, the vector did not get modified in the function body. Rather a new vector, x was created within the function and returned. However, if you did the missing value subsitute outside of a function call, then the vector would be modified: n &lt;- c(1, 2, NA, 4) n[is.na(n)] &lt;- -99 n ## [1] 1 2 -99 4 8.1.3 Putting R functions in Scripts When you define a function at the interactive prompt and then close the interpreter your function definition will be lost. The simple way around this is to define your R functions in a script that you can than access at any time. In RStudio choose File &gt; New File &gt; R Script. This will bring up a blank editor window. Type your function(s) into the editor. Everything in this file will be interpretted as R code, so you should not use the code block notation that is used in Markdown notebooks. Save the source file in your R working directory with a name like myfxns.R. # functions defined in myfxns.R area.of.circle &lt;- function(r) {pi * r^2} area.of.rectangle &lt;- function(l, w) {l * w} area.of.triangle &lt;- function(b, h) {0.5 * b * h } Once your functions are in a script file you can make them accesible by using the source function, which reads the named file as input and evaluates any definitions or statements in the input file (See also the Source button in the R Studio GUI): source(&quot;myfxns.R&quot;) Having sourced the file you can now use your functions like so: radius &lt;- 3 len &lt;- 4 width &lt;- 5 base &lt;- 6 height &lt;- 7 area.of.circle(radius) ## [1] 28.27433 area.of.rectangle(len, width) ## [1] 20 area.of.triangle(base, height) ## [1] 21 Note that if you change the source file, such as correcting a mistake or adding a new function, you need to call the source function again to make those changes available. 8.2 Control flow statements Control flow statements control the order of execution of different pieces of code. They can be used to do things like make sure code is only run when certain conditions are met, to iterate through data structures, to repeat something until a specified event happens, etc. Control flow statements are frequently used when writing functions or carrying out complex data transformation. 8.2.1 if and if-else statements if and if-else blocks allow you to structure the flow of execution so that certain expressions are executed only if particular conditions are met. The general form of an if expression is: if (Boolean expression) { Code to execute if Boolean expression is true } Here’s a simple if expression in which we check whether a number is less than 0.5, and if so assign a values to a variable. x &lt;- runif(1) # runif generates a random number between 0 and 1 face &lt;- NULL # set face to a NULL value if (x &lt; 0.5) { face &lt;- &quot;heads&quot; } face ## NULL The else clause specifies what to do in the event that the if statement is not true. The combined general for of an if-else expression is: if (Boolean expression) { Code to execute if Boolean expression is true } else { Code to execute if Boolean expression is false } Our previous example makes more sense if we include an else clause. x &lt;- runif(1) if (x &lt; 0.5) { face &lt;- &quot;heads&quot; } else { face &lt;- &quot;tails&quot; } face ## [1] &quot;heads&quot; With the addition of the else statement, this simple code block can be thought of as simulating the toss of a coin. 8.2.1.1 if-else in a function Let’s take our “if-else” example above and turn it into a function we’ll call coin.flip. A literal re-interpretation of our previous code in the context of a function is something like this: # coin.flip.literal takes no arguments coin.flip.literal &lt;- function() { x &lt;- runif(1) if (x &lt; 0.5) { face &lt;- &quot;heads&quot; } else { face &lt;- &quot;tails&quot; } face } coin.flip.literal is pretty long for what it does — we created a temporary variable x that is only used once, and we created the variable face to hold the results of our if-else statement, but then immediately returned the result. This is inefficient and decreases readability of our function. A much more compact implementation of this function is as follows: coin.flip &lt;- function() { if (runif(1) &lt; 0.5) { return(&quot;heads&quot;) } else { return(&quot;tails&quot;) } } Note that in our new version of coin.flip we don’t bother to create temporary the variables x and face and we immediately return the results within the if-else statement. 8.2.1.2 Multiple if-else statements When there are more than two possible outcomes of interest, multiple if-else statements can be chained together. Here is an example with three outcomes: x &lt;- sample(-5:5, 1) # sample a random integer between -5 and 5 if (x &lt; 0) { sign.x &lt;- &quot;Negative&quot; } else if (x &gt; 0) { sign.x &lt;- &quot;Positive&quot; } else { sign.x &lt;- &quot;Zero&quot; } sign.x ## [1] &quot;Negative&quot; 8.2.2 for loops A for statement iterates over the elements of a sequence (such as vectors or lists). A common use of for statements is to carry out a calculation on each element of a sequence (but see the discussion of map below) or to make a calculation that involves all the elements of a sequence. The general form of a for loop is: for (elem in sequence) { Do some calculations or Evaluate one or more expressions } As an example, say we wanted to call our coin.flip function multiple times. We could use a for loop to do so as follows: flips &lt;- c() # empty vector to hold outcomes of coin flips for (i in 1:20) { flips &lt;- c(flips, coin.flip()) # flip coin and add to our vector } flips ## [1] &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; ## [9] &quot;heads&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; ## [17] &quot;heads&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; Let’s use a for loop to create a multi.coin.flip function thats accepts an optional argument n that specifies the number of coin flips to carry out: multi.coin.flip &lt;- function(n = 1) { # create an empty character vector of length n # it&#39;s more efficient to create an empty vector of the right # length than to &quot;grow&quot; a vector with each iteration flips &lt;- vector(mode=&quot;character&quot;, length=n) for (i in 1:n) { flips[i] &lt;- coin.flip() } flips } With this new definition, a single call of coin.flip returns a single outcome: multi.coin.flip() ## [1] &quot;heads&quot; And calling multi.coin.flip with a numeric argument returns multiple coin flips: multi.coin.flip(n=10) ## [1] &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;tails&quot; &quot;heads&quot; &quot;heads&quot; &quot;heads&quot; &quot;tails&quot; ## [9] &quot;heads&quot; &quot;heads&quot; 8.2.3 break statement A break statement allows you to exit a loop even if it hasn’t completed. This is useful for ending a control statement when some criteria has been satisfied. break statements are usually nested in if statements. In the following example we use a break statement inside a for loop. In this example, we pick random real numbers between 0 and 1, accumulating them in a vector (random.numbers). The for loop insures that we never pick more than 20 random numbers before the loop ends. However, the break statement allows the loop to end prematurely if the number picked is greater than 0.95. random.numbers &lt;- c() for (i in 1:20) { x &lt;- runif(1) random.numbers &lt;- c(random.numbers, x) if (x &gt; 0.95) { break } } random.numbers ## [1] 0.51867465 0.79064411 0.73236657 0.64387547 0.04589707 0.47608781 ## [7] 0.37579590 0.10782591 0.13373318 0.94555359 0.30829904 0.01843081 ## [13] 0.12574475 0.99739168 8.2.4 repeat loops A repeat loop will loop indefinitely until we explicitly break out of the loop with a break statement. For example, here’s an example of how we can use repeat and break to simulate flipping coins until we get a head: ct &lt;- 0 repeat { flip &lt;- coin.flip() ct &lt;- ct + 1 if (flip == &quot;heads&quot;){ break } } ct ## [1] 2 8.2.5 next statement A next satement allows you to halt the processing of the current iteration of a loop and immediately move to the next item of the loop. This is useful when you want to skip calculations for certain elements of a sequence: sum.not.div3 &lt;- 0 for (i in 1:20) { if (i %% 3 == 0) { # skip summing values that are evenly divisible by three next } sum.not.div3 &lt;- sum.not.div3 + i } sum.not.div3 ## [1] 147 8.2.6 while statements A while statement iterates as long as the condition statement it contains is true. In the following example, the while loop calls coin.flip until “heads” is the result, and keeps track of the number of flips. Note that this represents the same logic as the repeat-break example we saw earlier, but in a a more compact form. first.head &lt;- 1 while(coin.flip() == &quot;tails&quot;){ first.head &lt;- first.head + 1 } first.head ## [1] 3 8.2.7 ifelse The ifelse function is equivalent to a for-loop with a nested if-else statement. ifelse applies the specified test to each element of a vector, and returns different values depending on if the test is true or false. Here’s an example of using ifelse to replace NA elements in a vector with zeros. x &lt;- c(3, 1, 4, 5, 9, NA, 2, 6, 5, 4) newx &lt;- ifelse(is.na(x), 0, x) newx ## [1] 3 1 4 5 9 0 2 6 5 4 The equivalent for-loop could be written as: x &lt;- c(3, 1, 4, 5, 9, NA, 2, 6, 5, 4) newx &lt;- c() # create an empty vector for (elem in x) { if (is.na(elem)) { newx &lt;- c(newx, 0) # append zero to newx } else { newx &lt;- c(newx, elem) # append elem to newx } } newx ## [1] 3 1 4 5 9 0 2 6 5 4 The ifelse function is clearly a more compact and readable way to accomplish this. 8.3 map and related tools Another common situation is applying a function to every element of a list or vector. Again, we could use a for loop, but the map functions often are better alternatives. NOTE: map is a relative newcomer to R and must be loaded with the purrr package (purrr is loaded when we load tidyverse). Although base R has a complicated series of “apply” functions (apply, lapply, sapply, vapply, mapply), map provides similar functionality with a more consistent interface. We won’t use the apply functions in this class, but you may see them in older code. library(tidyverse) 8.3.1 basic map Typically, map takes two arguments – a sequence (a vector, list, or data frame) and a function. It then applies the function to each element of the sequence, returning the results as a list. To illustrate map, let’s consider an example with a list of 2-vectors, where each vector gives the min and max values of some variable of interest for individuals in a sample (e.g. resting heart rate and maximum heart rate during exercise). We can use the map function to quickly generate the difference between the resting and maximum heart rates: heart.rates &lt;- list(bob = c(60, 120), fred = c(79, 150), jim = c(66, 110)) diff.fxn &lt;- function(x) {x[2] - x[1]} map(heart.rates, diff.fxn) ## $bob ## [1] 60 ## ## $fred ## [1] 71 ## ## $jim ## [1] 44 As a second example, here’s how we could use map to get the class of each object in a list: x &lt;- list(c(1,2,3), &quot;a&quot;, &quot;b&quot;, list(lead = &quot;Michael&quot;, keyboard = &quot;Jermaine&quot;)) map(x, class) ## [[1]] ## [1] &quot;numeric&quot; ## ## [[2]] ## [1] &quot;character&quot; ## ## [[3]] ## [1] &quot;character&quot; ## ## [[4]] ## [1] &quot;list&quot; 8.3.2 map_if and map_at map_if is a variant of map that takes a predicate function (a function that evaluates to TRUE or FALSE) to determine which elements of the input sequence are transformed by the map function. All elements of the sequence that do not meet the predicate are left un-transformed. Like map, map_if always returns a list. Here’s an example where we use map_if to apply the stringr::str_to_upper function to those columns of a data frame that are character vectors, and apply abs to obtain the absolute value of a numeric column: a &lt;- rnorm(6) b &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;) c &lt;- c(&quot;u&quot;, &quot;v&quot;, &quot;w&quot;, &quot;x&quot;, &quot;y&quot;, &quot;z&quot;) df &lt;- data_frame(a, b, c) head(df) ## # A tibble: 6 x 3 ## a b c ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 -0.2777 a u ## 2 -0.4215 b v ## 3 0.8979 c w ## 4 1.055 d x ## 5 1.733 e y ## 6 -0.3426 f z df2 &lt;- map_if(df, is.character, str_to_upper) df2 &lt;- map_if(df2, is.numeric, abs) head(df2) ## $a ## [1] 0.2776511 0.4214726 0.8978563 1.0545509 1.7326149 0.3425678 ## ## $b ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; ## ## $c ## [1] &quot;U&quot; &quot;V&quot; &quot;W&quot; &quot;X&quot; &quot;Y&quot; &quot;Z&quot; Note that df2 is a list, not a data frame. We can convert df2 to a data frame df3, using the as_data_frame() function: # Next, create data frame df3 df3 &lt;- as_data_frame(df2) head(df3) ## # A tibble: 6 x 3 ## a b c ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.2777 A U ## 2 0.4215 B V ## 3 0.8979 C W ## 4 1.055 D X ## 5 1.733 E Y ## 6 0.3426 F Z Note that if our goal is to apply functions to the columns of a data frame, it may be easier with dplyr::mutate(): df4 &lt;- df %&gt;% as_tibble() %&gt;% mutate(a = abs(a), b = str_to_upper(b), c = str_to_upper(c)) head(df4) ## # A tibble: 6 x 3 ## a b c ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0.2777 A U ## 2 0.4215 B V ## 3 0.8979 C W ## 4 1.055 D X ## 5 1.733 E Y ## 6 0.3426 F Z 8.3.3 mapping in parallel using map2 The map2 function applies a transformation function to two sequences in parallel. The following example illustrates this: first.names &lt;- c(&quot;John&quot;, &quot;Mary&quot;, &quot;Fred&quot;) last.names &lt;- c(&quot;Smith&quot;, &quot;Hernandez&quot;, &quot;Kidogo&quot;) map2(first.names, last.names, str_c, sep=&quot; &quot;) ## [[1]] ## [1] &quot;John Smith&quot; ## ## [[2]] ## [1] &quot;Mary Hernandez&quot; ## ## [[3]] ## [1] &quot;Fred Kidogo&quot; Note how we can specify arguments to the transformation function as additional arguments to map2 (i.e., the sep argument gets passed to str_c) 8.3.4 map variants that return vectors map, map_if, and map_at always return lists. The purrr library also has a series of map variants that return vectors: map_lgl (for logical vectors) map_chr (for character vectors) map_int (integer vectors) map_dbl (double vectors) # compare the outputs of map and map_chr a &lt;- map(letters[1:6], str_to_upper) str(a) ## List of 6 ## $ : chr &quot;A&quot; ## $ : chr &quot;B&quot; ## $ : chr &quot;C&quot; ## $ : chr &quot;D&quot; ## $ : chr &quot;E&quot; ## $ : chr &quot;F&quot; b &lt;- map_chr(letters[1:6], str_to_upper) str(b) # a vector ## chr [1:6] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; Here’s an example using map_dbl, where we create a data frame with three columns, and compute the median of each column: # Make data frame for analysis df &lt;- tibble(a = rnorm(100), b = rnorm(100),c = rnorm(100)) map_dbl(df, median) # median of each column of df ## a b c ## -0.12040277 -0.05548228 0.03676577 "],
["vector-algebra.html", "Chapter 9 Vector algebra 9.1 Libraries 9.2 Vector Mathematics in R 9.3 Simple statistics in vector form", " Chapter 9 Vector algebra 9.1 Libraries library(tidyverse) library(magrittr) 9.2 Vector Mathematics in R R vectors support basic arithmetic operations that correspond to the same operations on geometric vectors. For example: &gt; x &lt;- 1:15 &gt; y &lt;- 10:24 &gt; x ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 &gt; y ## [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 &gt; x + y # vector addition ## [1] 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39 &gt; x - y # vector subtraction ## [1] -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 -9 &gt; x * 3 # multiplication by a scalar ## [1] 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 R also has an operator for the dot product, denoted %*%. This operator also designates matrix multiplication, which we will discuss in the next chapter. By default this operator returns an object of the R matrix class. If you want a scalar (or the R equivalent of a scalar, i.e. a vector of length 1) you need to use the drop() function. &gt; z &lt;- x %*% x &gt; class(z) # note use of class() function ## [1] &quot;matrix&quot; &gt; z ## [,1] ## [1,] 1240 &gt; drop(z) ## [1] 1240 In lecture we saw that many useful geometric properties of vectors could be expressed in the form of dot products. Let’s start with some two-dimensional vectors where the geometry is easy to visualize: &gt; a &lt;- c(2, 0) # the point (2,0) &gt; b &lt;- c(1, 3) # the point (1,3) To draw our vectors using ggplot, we’ll need to create a data frame with columns representing the x,y coordinates of the end-points of our vectors: df &lt;- data.frame(x.end = c(a[1], b[1]), y.end = c(a[2], b[2]), label = c(&#39;a&#39;, &#39;b&#39;)) ggplot(df) + geom_segment(aes(x=0, y = 0, xend = x.end, yend = y.end, color=label), arrow = arrow()) + labs(x = &quot;x-coordinate&quot;, y = &quot;y-coordinate&quot;) + coord_fixed(ratio = 1) + # insures x and y axis scale are same theme_bw() Let’s see what the dot product can tell us about these vectors. First recall that we can calculate the length of a vector as the square-root of the dot product of the vector with itself (\\(\\vert\\vec{a}\\vert^2 = \\vec{a} \\cdot \\vec{a}\\)) &gt; len.a &lt;- drop(sqrt(a %*% a)) &gt; len.a ## [1] 2 &gt; len.b &lt;- drop(sqrt(b %*% b)) &gt; len.b ## [1] 3.162278 How about the angle between \\(a\\) and \\(b\\)? First we can use the dot product and the previously calculated lengths to calculate the cosine of the angle between the vectors: &gt; cos.ab &lt;- (a %*% b)/(len.a * len.b) &gt; cos.ab ## [,1] ## [1,] 0.3162278 To go from the cosine of the angle to the angle (in radians) we need the arc-cosine function, acos(): &gt; acos(cos.ab) # given angle in radians ## [,1] ## [1,] 1.249046 9.3 Simple statistics in vector form Now let’s turn our attention to seeing how to calculate a variety of simple statistics such as the mean, variance, etc. in terms of vector operations. To illustrate these oeprations we’ll use the I. setosa data from the iris examplar data set. setosa &lt;- filter(iris, Species == &quot;setosa&quot;) 9.3.1 Mean First let’s calculate the mean for the Sepal.Length variable. Referring back to the slides for today’s lecture, we see we can calculate the mean as: \\[ \\bar{x} = \\frac{\\vec{1} \\cdot \\vec{x}}{\\vec{1} \\cdot \\vec{1}} \\] Applying this formula in R: &gt; sepal.length &lt;- setosa$Sepal.Length &gt; ones &lt;- rep(1, length(sepal.length)) # 1-vector of length n &gt; mean.sepal.length &lt;- (ones %*% sepal.length)/(ones %*% ones) &gt; mean.sepal.length %&lt;&gt;% drop # use drop to convert back to scalar &gt; mean.sepal.length ## [1] 5.006 Let’s compare our calculation against the built-in mean function: &gt; mean(sepal.length) ## [1] 5.006 9.3.2 Mean centering Mean centering a vector, means subtracting the mean from each element of that vector: \\[ \\vec{x}_c = \\vec{x} - \\bar{x}\\vec{1} \\] Now let’s create a mean centered vector from sepal.length, which we’ll refer to as the vector of deviates about the mean: &gt; sepal.length.deviates &lt;- sepal.length - mean.sepal.length Note that we didn’t have to explicitly multiply the a one vector by the mean, as R will automatically make the lengths of the sepal.length (a vector of length 150) and mean.sepal.length (a vector of length 1) match by vector recycling. 9.3.3 Variance and standard deviation Using the vector of deviates we can easily calculate the variance and standard deviation of a variable. The variance of a variable, in vector algebraic terms, is: \\[ S_x^2 = \\frac{\\vec{x}_c \\cdot \\vec{x}_c}{n-1} \\] The standard deviation is simply the square root of the variance \\[ S_x = \\sqrt{S_x^2} \\] These calculations for the Sepal.Length variable: &gt; n &lt;- length(sepal.length.deviates) &gt; var.sepal.length &lt;- (sepal.length.deviates %*% sepal.length.deviates)/(n-1) &gt; var.sepal.length ## [,1] ## [1,] 0.124249 &gt; sd.sepal.length &lt;- sqrt(var.sepal.length) &gt; sd.sepal.length ## [,1] ## [1,] 0.3524897 Again, we can compare our calculations to the built-in var() and sd() functions: &gt; var(sepal.length) ## [1] 0.124249 &gt; sd(sepal.length) ## [1] 0.3524897 9.3.4 Covariance and correlation Now let’s consider the common measures of bivariate association, covariance and correlation. Covariance is: \\[ S_{XY} = \\frac{\\vec{x} \\cdot \\vec{y}}{n-1} \\] Correlation is: \\[ r_{XY} = \\frac{\\vec{x} \\cdot \\vec{y}}{|\\vec{x}||\\vec{y}|} = \\frac{S_{XY}}{S_x S_Y} \\] We’ll examine the relationship between sepal length and width: sepal.width &lt;- setosa$Sepal.Width mean.sepal.width &lt;- (ones %*% sepal.width)/(ones %*% ones) sepal.width.deviates &lt;- sepal.width - mean.sepal.width var.sepal.width &lt;- drop((sepal.width.deviates %*% sepal.width.deviates)/(n-1)) sd.sepal.width &lt;- sqrt(var.sepal.width) With the vector of sepal width deviates in hand we can now calculate covariances: &gt; cov.swidth.slength &lt;- (sepal.length.deviates %*% sepal.width.deviates)/(n-1) &gt; cov.swidth.slength ## [,1] ## [1,] 0.09921633 &gt; cov(sepal.length, sepal.width) # and compare to built-in covariance ## [1] 0.09921633 And correlations: &gt; len.sepal.length &lt;- sqrt(sepal.length.deviates %*% sepal.length.deviates) &gt; len.sepal.width &lt;- sqrt(sepal.width.deviates %*% sepal.width.deviates) &gt; &gt; corr.swidth.slength &lt;- + (sepal.length.deviates %*% sepal.width.deviates) / (len.sepal.length * len.sepal.width) &gt; corr.swidth.slength ## [,1] ## [1,] 0.7425467 &gt; cor(sepal.length, sepal.width) # and compare to built-in correlation ## [1] 0.7425467 Alternately, we could have calculated the correlation more simply as follows: &gt; cov.swidth.slength/(sd.sepal.length * sd.sepal.width) ## [,1] ## [1,] 0.7425467 "],
["matrices-in-r.html", "Chapter 10 Matrices in R 10.1 Creating matrices in R 10.2 Matrix arithmetic operations in R 10.3 Descriptive statistics as matrix functions 10.4 Matrix Inverse", " Chapter 10 Matrices in R In R matrices are two-dimensional collections of elements all of which have the same mode or type. This is different than a data frame in which the columns of the frame can hold elements of different type (but all of the same length), or from a list which can hold objects of arbitrary type and length. Matrices are more efficient for carrying out most numerical operations, so if you’re working with a very large data set that is amenable to representation by a matrix you should consider using this data structure. library(tidyverse) 10.1 Creating matrices in R There are a number of different ways to create matrices in R. For creating small matrices at the command line you can use the matrix() function. &gt; x &lt;- matrix(1:5) # creates a column vector &gt; x ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 ## [4,] 4 ## [5,] 5 &gt; X &lt;- matrix(1:12, nrow=4) # creates a matrix &gt; X ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 &gt; dim(X) # give the shape of the matrix ## [1] 4 3 matrix() takes a data vector as input and the shape of the matrix to be created is specified by using the nrow and ncol arguments. If the number of elements in the input data vector is less than nrows \\(\\times\\) ncols the elements will be ‘recycled’ as discussed in previous chapters. Without any shape arguments the matrix() function will create a column vector as shown above. By default the matrix() function fills in the matrix in a column-wise fashion. To fill in the matrix in a row-wise fashion use the argument byrow=T. If you have a pre-existing data set in a list or data frame you can use the as.matrix() function to convert it to a matrix. &gt; iris.mtx &lt;- as.matrix(iris) &gt; head(iris.mtx) # NOTE: the elements were all converted to character ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## [1,] &quot;5.1&quot; &quot;3.5&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; ## [2,] &quot;4.9&quot; &quot;3.0&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; ## [3,] &quot;4.7&quot; &quot;3.2&quot; &quot;1.3&quot; &quot;0.2&quot; &quot;setosa&quot; ## [4,] &quot;4.6&quot; &quot;3.1&quot; &quot;1.5&quot; &quot;0.2&quot; &quot;setosa&quot; ## [5,] &quot;5.0&quot; &quot;3.6&quot; &quot;1.4&quot; &quot;0.2&quot; &quot;setosa&quot; ## [6,] &quot;5.4&quot; &quot;3.9&quot; &quot;1.7&quot; &quot;0.4&quot; &quot;setosa&quot; Since all elements of an R matrix must be of the same type, when we passed the iris data frame to as.matrix(), everything was converted to a character due to the presence of the Species column in the data frame. &gt; # This is probably more along the lines of what you want &gt; iris.mtx &lt;- iris %&gt;% select(-Species) %&gt;% as.matrix &gt; head(iris.mtx) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## [1,] 5.1 3.5 1.4 0.2 ## [2,] 4.9 3.0 1.4 0.2 ## [3,] 4.7 3.2 1.3 0.2 ## [4,] 4.6 3.1 1.5 0.2 ## [5,] 5.0 3.6 1.4 0.2 ## [6,] 5.4 3.9 1.7 0.4 You can use the various indexing operations to get particular rows, columns, or elements. Here are some examples: &gt; X &lt;- matrix(1:12, nrow=4) &gt; X ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 &gt; X[1,] # get the first row ## [1] 1 5 9 &gt; X[,1] # get the first column ## [1] 1 2 3 4 &gt; X[1:2,] # get the first two rows ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 &gt; X[,2:3] # get the second and third columns ## [,1] [,2] ## [1,] 5 9 ## [2,] 6 10 ## [3,] 7 11 ## [4,] 8 12 &gt; Y &lt;- matrix(1:12, byrow=T, nrow=4) &gt; Y ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 &gt; Y[4] # see explanation below ## [1] 10 &gt; Y[5] ## [1] 2 &gt; dim(Y) &lt;- c(2,6) # reshape Y &gt; Y ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 7 2 8 3 9 ## [2,] 4 10 5 11 6 12 &gt; Y[5] ## [1] 2 The example above where we create a matrix Y is meant to show that matrices are stored internally in a column wise fashion (think of the columns stacked one atop the other), regardless of whether we use the byrow=T argument. Therefore using single indices returns the elements with respect to this arrangement. Note also the use of assignment operator in conjuction with the dim() function to reshape the matrix. Despite the reshaping, the internal representation in memory hasn’t changed so Y[5] still gives the same element. You can use the diag() function to get the diagonal of a matrix or to create a diagonal matrix as show below: &gt; Z &lt;- matrix(rnorm(16), ncol=4) &gt; Z ## [,1] [,2] [,3] [,4] ## [1,] 1.24339355 1.09798719 -0.4329977 -0.91014488 ## [2,] 1.46063438 0.09478473 0.8611494 -0.05255224 ## [3,] 0.77395870 2.12077892 0.6208266 -0.73148269 ## [4,] 0.08746849 0.20324673 -0.6107474 -0.22385690 &gt; diag(Z) ## [1] 1.24339355 0.09478473 0.62082659 -0.22385690 &gt; diag(5) # create the 5 x 5 identity matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 0 0 ## [2,] 0 1 0 0 0 ## [3,] 0 0 1 0 0 ## [4,] 0 0 0 1 0 ## [5,] 0 0 0 0 1 &gt; s &lt;- sqrt(10:13) &gt; diag(s) ## [,1] [,2] [,3] [,4] ## [1,] 3.162278 0.000000 0.000000 0.000000 ## [2,] 0.000000 3.316625 0.000000 0.000000 ## [3,] 0.000000 0.000000 3.464102 0.000000 ## [4,] 0.000000 0.000000 0.000000 3.605551 10.2 Matrix arithmetic operations in R The standard mathematical operations of addition and subtraction and scalar multiplication work element-wise for matrices in the same way as they did for vectors. Matrix multiplication uses the operator %*% which you saw last week for the dot product. To get the transpose of a matrix use the function t(). &gt; A &lt;- matrix(1:12, nrow=4) &gt; A ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 &gt; &gt; B &lt;- matrix(rnorm(12), nrow=4) &gt; B ## [,1] [,2] [,3] ## [1,] -0.04072615 0.6294336 -1.3056798 ## [2,] -1.64579784 -0.5126328 -0.4374571 ## [3,] -0.33985613 -2.2758961 0.4015807 ## [4,] -0.17164442 0.9197156 -1.9247242 &gt; &gt; t(A) # transpose ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 &gt; A + B # matrix addition ## [,1] [,2] [,3] ## [1,] 0.9592739 5.629434 7.694320 ## [2,] 0.3542022 5.487367 9.562543 ## [3,] 2.6601439 4.724104 11.401581 ## [4,] 3.8283556 8.919716 10.075276 &gt; A - B # matrix subtraction ## [,1] [,2] [,3] ## [1,] 1.040726 4.370566 10.30568 ## [2,] 3.645798 6.512633 10.43746 ## [3,] 3.339856 9.275896 10.59842 ## [4,] 4.171644 7.080284 13.92472 &gt; 5 * A # multiplication by a scalar ## [,1] [,2] [,3] ## [1,] 5 25 45 ## [2,] 10 30 50 ## [3,] 15 35 55 ## [4,] 20 40 60 When applying matrix multiplication, the dimensions of the matrices involved must be conformable. For example, you can’t do this: A %*% B # do you understand why this generates an error? But this works: &gt; A %*% t(B) ## [,1] [,2] [,3] [,4] ## [1,] -8.644677 -8.146076 -8.10511 -12.89558 ## [2,] -9.361649 -10.741963 -10.31928 -14.07224 ## [3,] -10.078622 -13.337851 -12.53345 -15.24889 ## [4,] -10.795594 -15.933739 -14.74762 -16.42554 10.3 Descriptive statistics as matrix functions Assume you have a data set represented as a \\(n \\times p\\) matrix, \\(X\\), with observations in rows and variables in columns. Below I give formulae for calculating some descriptive statistics as matrix functions. 10.3.1 Mean vector and matrix You can calculate a row vector of means, \\(\\mathbf{m}\\), as: \\[ \\mathbf{m} = \\frac{1}{n} \\mathbf{1}^T X \\] where \\(1\\) is a \\(n \\times 1\\) vector of ones. A \\(n \\times p\\) matrix \\(M\\) where each column is filled with the mean value for that column is: \\[ M = \\mathbf{1}\\mathbf{m} \\] 10.3.2 Deviation matrix To re-express each value as the deviation from the variable means (i.e.~each columns is a mean centered vector) we calculate a deviation matrix: \\[ D = X - M \\] 10.3.3 Covariance matrix The \\(p \\times p\\) covariance matrix can be expressed as a matrix product of the deviation matrix: \\[ S = \\frac{1}{n-1} D^T D \\] 10.3.4 Correlation matrix The correlation matrix, \\(R\\), can be calculated from the covariance matrix by: \\[ R = V S V \\] where \\(V\\) is a \\(p \\times p\\) diagonal matrix where \\(V_{ii} = 1/\\sqrt{S_{ii}}\\). 10.4 Matrix Inverse The function solve() can be used to find matrix inverses in R. A &lt;- matrix(1:4, nrow=2) A ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 Ainv &lt;- solve(A) Ainv ## [,1] [,2] ## [1,] -2 1.5 ## [2,] 1 -0.5 A %*% Ainv # should give identity matrix ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 Ainv %*% A # should also result in identity matrix ## [,1] [,2] ## [1,] 1 0 ## [2,] 0 1 Keep in mind that not all square matrices are invertible: C &lt;- matrix(1:16, nrow=4) C ## [,1] [,2] [,3] [,4] ## [1,] 1 5 9 13 ## [2,] 2 6 10 14 ## [3,] 3 7 11 15 ## [4,] 4 8 12 16 Cinv &lt;- solve(C) ## Error in solve.default(C): Lapack routine dgesv: system is exactly singular: U[3,3] = 0 "],
["fitting-regression-models-in-r.html", "Chapter 11 Fitting Regression Models in R 11.1 Regression terminology 11.2 The optimality criterion for least-squares regression 11.3 Vector geometric perspective 11.4 Solution for the least-squares criterion 11.5 Libraries 11.6 Illustrating linear regression with simulated data 11.7 Specifying Regression Models in R 11.8 Residuals 11.9 Regression as sum-of-squares decomposition 11.10 Variance “explained” by a regression model 11.11 Broom: a library for converting model results into data frames 11.12 Example: Predicting lion age based on nose color", " Chapter 11 Fitting Regression Models in R Statistical models are quantitative statements about how we think variables are related to each other. Linear models are among the simplest statistical models. In a linear model relating two variables \\(X\\) and \\(Y\\), the general form of the model can be stated as “I assume that \\(Y\\) can be expressed as a linear function of \\(X\\)”. The process of model fitting is then the task of finding the coefficients (parameters) of the linear model which best fit the observed data. Linear functions are those whose graphs are straight lines. A linear function of a variable \\(X\\) is usually written as: \\[ \\widehat{Y} = f(X) = a + bX \\] where \\(a\\) and \\(b\\) are constants. In geometric terms \\(b\\) is the slope of the line and \\(a\\) is the value of the function when \\(X\\) is zero (usually the referred to as the “Y-intercept”). The slope tells you have much \\(Y\\) changes per unit change of \\(X\\). There are infinitely many such linear functions of \\(X\\) we could define. Which linear function provides the best fit given our observed values of \\(X\\) and \\(Y\\)? 11.1 Regression terminology Predictors, explanatory, or independent variable – the variables from which we want to make our prediction. Outcomes, dependent, or response variable – the variable we are trying to predict in our regression. 11.2 The optimality criterion for least-squares regression In order to fit a model to data, we have to specify some criterion for judging how well alternate models perform. In linear regression, the optimality criterion can be expressed as “Find the linear function, \\(f(X)\\), that minimizes the following quantity:” \\[ \\sum (y_i - f(x_i))^2 \\] That is, our goal is to find the linear function of \\(X\\) that minimizes the squared deviations in the \\(Y\\) direction. Figure 11.1: A graphical representation of the optimality criterion in bivariate least squares linear regression. 11.3 Vector geometric perspective The figure above illustrates the “variable space” view of linear regression. You’ve also been introduced to the “subject space” representation of regression, in which we can visualize regression as a projection operation. Figure 11.2: A vector representation of bivariate linear regression. The least-squares optimality criterion is to find the vector \\(\\vec{\\widehat{Y}}\\) that minimizes the residual vector \\(\\vec{e}\\). 11.4 Solution for the least-squares criterion With a little calculus and linear algebra one can show that the values of \\(b\\) (slope) and \\(a\\) (intercept) that minimize the sum of squared deviations described above are: \\[\\begin{align} b &amp;= \\frac{s_{xy}}{s^2_x} = r_{xy}\\frac{s_y}{s_x}\\\\ \\\\ a &amp;= \\overline{Y} - b\\overline{X} \\end{align}\\] where \\(r_{xy}\\) is the correlation coefficient between \\(X\\) and \\(Y\\), and \\(s_x\\) and \\(s_y\\) are the standard deviations of \\(X\\) and \\(Y\\) respectively. In vector geometric terms, we estimate the regression coefficient for mean centered vectors as: \\[ b = \\frac{\\vec{x} \\cdot \\vec{y}}{\\vec{x} \\cdot \\vec{x}} \\] 11.5 Libraries library(tidyverse) library(ggExtra) # a new library, provides ggMarginal plot (see below) # install if you don&#39;t already have it 11.6 Illustrating linear regression with simulated data To illustrate how regression works, we’ll use a simulated data set where we specify the relationship between two variables, \\(X\\) and \\(Y\\). Using a simulation is desirable because it allows us to know what the “true” underlying model that relates \\(X\\) and \\(Y\\) is, so we can evaluate how well we do in terms of recovering the model. Let’s generate two vectors representing the variable, \\(X\\) and \\(Y\\), where \\(Y\\) is a function of \\(X\\) plus some independent noise. As specified below, the “true” model is \\(Y = 1.5X + 1.0 + \\epsilon_y\\) where \\(\\epsilon_y\\) is a noise term. # this seeds our random number generator # by setting a seed, we can make random number generation reproducible set.seed(20190227) npts &lt;- 50 X &lt;- seq(1, 5, length.out = npts) + rnorm(npts) a &lt;- 1.0 b &lt;- 1.5 Y &lt;- b*X + a + rnorm(npts, sd = 2) # Y = 1.5X + 1.0 + noise df.xy &lt;- data.frame(X = X, Y = Y) Having generated some simulated data, let’s visualize it. p &lt;- ggplot(df.xy, aes(x = X, y = Y)) + geom_point() ggMarginal(p, type = &quot;histogram&quot;, bins = 11) 11.7 Specifying Regression Models in R As one would expect, R has a built-in function for fitting linear regression models. The function lm() can be used not only to carry out bivariate linear regression but a wide range of linear models, including multiple regression, analysis of variance, analysis of covariance, and others. fit.xy &lt;- lm(Y ~ X, df.xy) The first argument to lm is an R “formula”, the second argument is a data frame. Recall that formulas are R’s way of specifying models, though they find other uses as well (e.g. we saw the formula syntax when we introduced the facet_wrap and facet_grid functions from ggplot, and in the context of ANOVA). The general form of a formula in R is response variable ~ explanatory variables. In the code example above, we have only a single explanatory variable, and thus our response variable is Y and our explanatory variable is X. The lm function returns a list with a number of different components. The ones of most interest to us are fitted.values, coefficients, residuals, and (see the lm documentation for full details.) fit.xy ## ## Call: ## lm(formula = Y ~ X, data = df.xy) ## ## Coefficients: ## (Intercept) X ## 1.819 1.198 Calling summary on a fit model provides more detailed output: summary(fit.xy) ## ## Call: ## lm(formula = Y ~ X, data = df.xy) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.118 -1.138 0.338 1.203 3.260 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.8193 0.6557 2.775 0.00785 ** ## X 1.1976 0.2081 5.755 5.93e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.985 on 48 degrees of freedom ## Multiple R-squared: 0.4083, Adjusted R-squared: 0.3959 ## F-statistic: 33.12 on 1 and 48 DF, p-value: 5.93e-07 As we saw in previous R functions for implementing statistical test, the model object is actually a list-like object with multiple fields: typeof(fit.xy) ## [1] &quot;list&quot; names(fit.xy) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; 11.7.1 Fitted values The component fitted.values gives the predicted values of \\(Y\\) (\\(\\hat{Y}\\) in the equations above) for each observed value of \\(X\\). We can plot these predicted values of \\(Y\\), as shown below. Notice how the predicted values all fall on a line (the regression line itself!) ggplot(df.xy, aes(x = X, y = Y)) + geom_point(alpha=0.7) + # observed data geom_point(aes(x = X, y = fit.xy$fitted.values), # predicted data color=&#39;red&#39;, alpha=0.5) + geom_segment(aes(xend = X, yend = fit.xy$fitted.values), color=&#39;red&#39;, linetype=&#39;dashed&#39;, alpha=0.25) Figure 11.3: Observed (black) and predicted (red) values in a linear regression of Y on X. Dashed lines indicate the residuals from the regression. 11.7.2 Getting the model coefficients The coefficients components gives the value of the model parameters, namely the intercept and slope. &gt; fit.xy$coefficients ## (Intercept) X ## 1.819327 1.197603 As shown above, the estimated slope is 1.20 and the estimated intercept is 1.82. The model estimated by our linear regression is thus \\(\\widehat{Y} = 1.82 + 1.20X\\). Since this is a synthetic example, we know the “true” underlying model, which is \\(Y = 1.5X + 1.0 + \\epsilon_x\\). With the slope and intercept in hand we can draw the regression line as so: ggplot(df.xy, aes(x = X, y = Y)) + geom_point(alpha=0.7) + # observed data geom_abline(slope = fit.xy$coefficients[[2]], intercept = fit.xy$coefficients[[1]], color=&#39;red&#39;, alpha=0.5) Since linear model fitting is a fairly common task, the ggplot library includes a geometric mapping, geom_smooth(), that will fit a linear model (as well as other models, see below) for us and generate the corresponding regression plot. ggplot(df.xy, aes(x = X, y = Y)) + geom_point(alpha = 0.75) + geom_smooth(method=&quot;lm&quot;, color = &#39;red&#39;) By default, geom_smooth draws confidence intervals for the regression model (the shaded gray area around the regression line). Note that confidence intervals for a linear regression model are wider far away from the mean values of \\(X\\) and \\(Y\\). 11.8 Residuals Residuals are the difference between the observed values of \\(Y\\) and the predicted values. You can think of residuals as the proportion of \\(Y\\) unaccounted for by the model. \\[ \\mbox{residuals} = Y - \\hat{Y} \\] The previous figure showed the residuals as dashed lines connected the observed and predicted values. A common way to depict the residuals, is to plot the predictor values versus the corresponding residual value, like so: ggplot(df.xy, aes(x = X)) + geom_point(aes(y = fit.xy$residuals)) + geom_hline(yintercept = 0, color = &#39;red&#39;, linetype = &quot;dashed&quot;) + labs(x = &quot;X&quot;, y = &quot;Residuals&quot;) When the linear regression model is appropriate, residuals should be normally distributed, centered around zero and should show no strong trends or extreme differences in spread (variance) for different values of \\(X\\). 11.9 Regression as sum-of-squares decomposition Regression can be viewed as a decomposition of the sum-of-squared deviations.. \\[ ss(Y) = ss(\\hat{Y}) + ss(\\mbox{residuals}) \\] Let’s check this for our example: &gt; ss.Y &lt;- sum((Y - mean(Y))^2) &gt; ss.Yhat &lt;- sum((fit.xy$fitted.values - mean(Y))^2) &gt; ss.residuals &lt;- sum(fit.xy$residuals^2) &gt; ss.Y ## [1] 319.5811 &gt; ss.Yhat + ss.residuals ## [1] 319.5811 11.10 Variance “explained” by a regression model We can use the sum-of-square decomposition to understand the relative proportion of variance “explained” (accounted for) by the regression model. We call this quantity the “Coefficient of Determination”, designated \\(R^2\\). \\[ R^2 = \\left( 1 - \\frac{SS_{residuals}}{SS_{total}} \\right) \\] For this particular example we can estimate \\(R^2\\) as follows: R2 &lt;- 1.0 - (ss.residuals/ss.Y) R2 ## [1] 0.4082507 In this particular example, we find our linear model accounts for about 63% of the variance in \\(Y\\). Note that the coefficient of determination is also reported when you apply the summary function to a linear model. 11.11 Broom: a library for converting model results into data frames The model fit object we got back when we used the lm function to carry out linear regression, carries lots of useful information it isn’t a particularly “tidy” way to access the data. The R package Broom converts \"statistical analysis objects from R into tidy data frames, so that they can more easily be combined, reshaped and otherwise processed with tools like ‘dplyr’, ‘tidyr’ and ‘ggplot2’. The discussion of Broom below is drawn from the Introduction to Broom If you haven’t already done so, install the broom package before proceeding. library(broom) There are three broom functions that are particularly useful for our purposes. They are: tidy – constructs a data frame that summarizes the model’s statistical findings. augment – add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments. glance – construct a concise one-row summary of the model. 11.11.1 broom::tidy tidy applied to a regression model object returns a table giving the estimated coefficients and other information about the uncertainty of those estimates and corresponding p-values. For now we’re just interested in the estiamtes, the other values will be described in detail when we get to statistical inference. tidy(fit.xy) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.819 0.6557 2.775 0.007853 ## 2 X 1.198 0.2081 5.755 0.0000005930 11.11.2 broom::augment augment creates a data frame that combines the original data with related information from the model fit. df.xy.augmented &lt;- augment(fit.xy, df.xy) head(df.xy.augmented) ## # A tibble: 6 x 9 ## X Y .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.1144 3.049 1.956 0.6343 1.093 0.1021 1.999 0.01921 0.5812 ## 2 1.023 3.630 3.045 0.4722 0.5853 0.05659 2.004 0.002765 0.3036 ## 3 0.3076 0.9912 2.188 0.5985 -1.196 0.09092 1.998 0.01999 -0.6322 ## 4 1.901 3.357 4.095 0.3430 -0.7386 0.02986 2.003 0.002196 -0.3778 ## 5 2.504 1.576 4.818 0.2897 -3.242 0.02130 1.948 0.02966 -1.651 ## 6 1.683 4.492 3.835 0.3708 0.6570 0.03490 2.004 0.002053 0.3369 Now, in addition to the X and Y variables of the original data, we have columns like .fitted (value of Y predicted by the model for the corresponding value of X), .resid (difference between the actual Y and the predicted value), and a variety of other information for evalulating model uncertainty. One thing we can do with this “augmented” data frame is to use it to better visualize and explore the model. For example, if we wanted to generate a figure highlighting the deviations from the model using vertical lines emanating from the regression line, we could do something like this: ggplot(df.xy.augmented, aes(X, Y)) + geom_point() + geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;,se=FALSE) + geom_segment(aes(xend = X, yend = .fitted), linetype=&quot;dashed&quot;) An another example, we can recreate our residual plot using the augmented data frame as so: ggplot(df.xy.augmented, aes(X, .resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, linetype=&#39;dashed&#39;) + labs(y = &quot;Residuals&quot;, title = &quot;Residual plot for synthetic data example.&quot;) 11.11.3 broom::glance glance() provides summary information about the goodness of fit of the model. Most relevant for our current discussion is the column giving the coefficient of determination (r.squared): glance(fit.xy) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.4083 0.3959 1.985 33.12 5.930e-7 2 -104.2 214.4 220.1 ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; 11.12 Example: Predicting lion age based on nose color Having walked through a simulation example, let’s now turn to a real world data set. A study by Whitman et al. (2004) showed that the amount of black coloring on the nose of male lions increases with age, and suggested that this might be used to estimate the age of unknown lions. To establish the relationship between these variables they measured the black coloring on the noses of male lions of known age (represented as a proportion), giving the bivariate relationship (and fitted model) shown below: lions &lt;- read_csv(&quot;https://github.com/bio304-class/bio304-course-notes/raw/master/datasets/ABD-lion-noses.csv&quot;) ggplot(lions, aes(x = proportionBlack, y = ageInYears)) + geom_point() + geom_smooth(method=&quot;lm&quot;, color = &#39;red&#39;) By eye, the linear model looks like a pretty good fit. Let’s take a look at the quantitative values of the regression model, using the various Broom functions to produce nice output. lion.model &lt;- lm(ageInYears ~ proportionBlack, data = lions) tidy(lion.model) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.8790 0.5688 1.545 0.1328 ## 2 proportionBlack 10.65 1.510 7.053 0.00000007677 glance(lion.model) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.6238 0.6113 1.669 49.75 7.677e-8 2 -60.76 127.5 131.9 ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; We then augment our data set with information from the model fit and plot a residual plot: lions.augmented &lt;- augment(lion.model, lions) ggplot(lions.augmented, aes(proportionBlack, .resid)) + geom_point() + geom_hline(yintercept = 0, color=&quot;firebrick&quot;, linetype=&quot;dashed&quot;) From this plot, there may be some indication of greater variance of residuals for larger values of the predictor variable. Let’s check how normal the residuals look using a QQ-plot. Here we construct the QQ-plot using “standardized residuals” which are just z-scores for the residuals. ggplot(lions.augmented, aes(sample = .std.resid)) + geom_qq() + geom_qq_line(color=&quot;firebrick&quot;) Based on the QQ-plot, the residuals seem to diverge somewhat from a normal distirbution, as there’s noticeable curvature in the QQ-plot. When we test for the normality of the residuals using Shapiro-Wilk’s test for normality, we fail to reject the null hypothesis of normality at a significance threshold of \\(\\alpha=0.05\\): shapiro.test(lions.augmented$.resid) ## ## Shapiro-Wilk normality test ## ## data: lions.augmented$.resid ## W = 0.93879, p-value = 0.0692 Even though we failed to reject the null hypothesis of normality for the residuals, but the P-value is very close to significance, suggesting some caution in applying the linear model. "],
["multiple-regression.html", "Chapter 12 Multiple Regression 12.1 Review of bivariate regression 12.2 Multiple regression 12.3 Subject space representation 12.4 Interpretting Multiple Regression 12.5 New Libraries to install 12.6 Libraries 12.7 Examplar data 12.8 Data exploration 12.9 3D Plots 12.10 Fitting the regression model 12.11 Interpretting the regression model 12.12 Exploring the Vector Geometry of the Regression Model 12.13 Exploring the Residuals from the Model Fit 12.14 An alternate model 12.15 Fitting a curvilinear model using lm() 12.16 Exploring the impact of nearly collinear predictors on regression 12.17 LOESS Models", " Chapter 12 Multiple Regression 12.1 Review of bivariate regression Recall the model for bivariate least-squares regression. When we regress \\(Y\\) and \\(X\\) we’re looking for a linear function, \\(f(X)\\), for which the following sum-of-squared deviations is minimized: \\[ \\sum_{i=1}^n (y_i - f(x_i))^2 \\] The general form a linear function of one variable is a line, \\[ \\widehat{Y} = f(x) = a + bX \\] where \\(b\\) is the slope of the line and \\(a\\) is the intercept. 12.2 Multiple regression The idea behind multiple regression is almost exactly the same as bivariate regression, except now we try and fit a linear model for \\(Y\\) using multiple explanatory variables, \\(X_1, X_2,\\ldots, X_m\\). That is we’re looking for a linear function, \\(f(X_1, X_2,\\ldots,X_m)\\) that minimizes: \\[ \\sum_{i=1}^n(y_i - f(x_1, x_2,\\ldots, x_m))^2 \\] A linear function of more than one variable is written as: \\[ \\widehat{Y} = f(X_1, X_2,\\ldots,X_m) = a + b_1X_1 + b_2X_2 + \\cdots + b_mX_m \\] Where \\(a\\) is the intercept and \\(b_1, b_2,\\ldots,b_m\\) are the regression coefficients. 12.2.1 Variable space interpretation Geometrically the regression coefficients have the same interpretation as in the bivariate case – slopes with respect to the corresponding variable. When there are two predictor variables, the linear regression is geometrically a plane in 3-space, as shown in the figure below. When there are more than two predictor variables, the regression solution is a hyper-plane. Multiple regression, two predictor variables 12.3 Subject space representation The subject space (vector geometric) representation of multiple regression is shown below: Figure 12.1: A vector representation of multiple regression. As was the case for bivariate regression, multiple regression is simply a projection operation. 12.3.1 Coefficient of determination for multiple regression As in bivariate regression, the coefficient of determination (\\(R^2\\)) provides a measure of the proportion of variance in the outcome variable (\\(Y\\)) “explained” by the predictor variables (\\(X_1, X_2, \\ldots\\)). 12.4 Interpretting Multiple Regression Here are some things to keep in mind when interpretting a multple regression: In most cases of regression, causal interpretation of the model is not justified. Standard bivariate and multiple regression assumes that the predictor variables ( (\\(X_1, X_2, \\ldots\\)) are observed without error. That is, uncertainty in the regression model is only associated with the outcome variable, not the predictors. Comparing the size of regression coefficients only makes sense if all the predictor (explanatory) variables have the same scale If the explanatory variables (\\(X_1, X_2,\\ldots,X_m\\)) are highly correlated, then the regression solution can be “unstable” – a small change in the data could lead to a large change in the regression model. 12.5 New Libraries to install We’ll be using several new packages for this class session. Install the following packages via one of the standard install mechanisms: scatterplot3d rgl – NOTE: On OS X, rgl requires you to install a program called XQuartz. XQuartz can be downloaded from the XQuartz Home Page. If you’re on a Mac, install XQuartz before installing rgl. You may have to reboot your computer after installing XQuartz. 12.6 Libraries library(tidyverse) library(broom) # for working w/lm output 12.7 Examplar data To illustrate multiple regression in R we’ll use a built in dataset called trees. trees consists of measurements of the girth, height, and volume of 31 black cherry trees (?trees for more info). Let’s assume we’re lumberjacks, but our permit only allows us to harvest a fixed number of trees. We get paid by the total volume of wood we harvest, so we’re interested in predicting a tree’s volume (hard to measure directly) as a function of its girth and height (relatively easy to measure), so we can pick the best trees to harvest. We’ll therefore calculate a multiple regression of volume on height and width. 12.8 Data exploration We’ll start with some summary tables and diagnostic plots to familiarize ourselves with the data: names(trees) ## [1] &quot;Girth&quot; &quot;Height&quot; &quot;Volume&quot; dim(trees) ## [1] 31 3 summary(trees) ## Girth Height Volume ## Min. : 8.30 Min. :63 Min. :10.20 ## 1st Qu.:11.05 1st Qu.:72 1st Qu.:19.40 ## Median :12.90 Median :76 Median :24.20 ## Mean :13.25 Mean :76 Mean :30.17 ## 3rd Qu.:15.25 3rd Qu.:80 3rd Qu.:37.30 ## Max. :20.60 Max. :87 Max. :77.00 We’ll use the GGally::ggpairs() function introduced in problem set 01 to create a scatterplot matrix depicting the pairwise relationships between all the variables library(GGally) ggpairs(trees) As one might expect for morphological measurements related to size, the scatterplot matrix shows that all the variables are positively correlated, and girth and volume have a particularly strong correlation. 12.9 3D Plots ggplot has no built in facilities for 3D scatter plots so we’ll use two new packages, scatterplot3D and rgl, to generate 3D visualizations. 12.9.1 scatterplot3d library(scatterplot3d) # install this package first if needed scatterplot3d(trees, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) The argument pch sets the type of plotting character to use in the plot (for a graphical key of the available plotting characters see this link) and color sets plotting character colors. We can change the angle of the 3D plot using the angle argument: scatterplot3d(trees, pch = 16, color=&quot;steelblue&quot;, angle=75, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) We can add vertical lines to the plot using the type argument and remove the box around the plot: scatterplot3d(trees, pch = 16, color=&quot;steelblue&quot;, angle=75, box = FALSE, type = &quot;h&quot;, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) For more examples of how you can modify plots generated with the scatterplot3d package see this web page). 12.9.2 rgl The package rgl is another package that we can use for 3D visualization. rgl is powerful because it lets us create interactive plots we can rotate and zoom in/out on. You can then create an interactive 3D plot as so: library(rgl) # create 3D scatter, using spheres to draw points plot3d(trees$Girth, trees$Height, trees$Volume, xlab = &quot;Girth&quot;, ylab = &quot;Height&quot;, zlab = &quot;Volume&quot;, type = &quot;s&quot;, size = 1.5, col = &quot;red&quot;) rglwidget() # only need to include this line if using in a markdown document 12.10 Fitting the regression model From the 3D scatter plot it looks like we ought to be able to find a plane through the data that fits the scatter fairly well. Let’s use the lm() function to calculate the multiple regression and summary() to get the details of the model: fit.trees &lt;- lm(Volume ~ Girth + Height, data=trees) summary(fit.trees) ## ## Call: ## lm(formula = Volume ~ Girth + Height, data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4065 -2.6493 -0.2876 2.2003 8.4847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -57.9877 8.6382 -6.713 2.75e-07 *** ## Girth 4.7082 0.2643 17.816 &lt; 2e-16 *** ## Height 0.3393 0.1302 2.607 0.0145 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.882 on 28 degrees of freedom ## Multiple R-squared: 0.948, Adjusted R-squared: 0.9442 ## F-statistic: 255 on 2 and 28 DF, p-value: &lt; 2.2e-16 12.10.1 Visualizing the regression model in scatterplot3d To visualize the multiple regression, let’s use the scatterplot3d package to draw the 3D scatter of plots and the plane that corresponds to the regression model: p &lt;- scatterplot3d(trees, angle=55,type=&#39;h&#39;, pch = 16, color = &quot;steelblue&quot;, main = &#39;Tree Volume as\\na function of Girth and Height&#39;) # add a plane representing the fit of the model p$plane3d(fit.trees, col=&#39;orangered&#39;) If instead of scatterplot3d, we wanted to use rgl to depict the model fit we can use the rgl.planes function as shown below. coefs &lt;- coef(fit.trees) b1 &lt;- coefs[&quot;Girth&quot;] b2 &lt;- coefs[&quot;Height&quot;] c &lt;- -1 a &lt;- coefs[&quot;(Intercept)&quot;] plot3d(trees$Girth, trees$Height, trees$Volume, xlab = &quot;Girth&quot;, ylab = &quot;Height&quot;, zlab = &quot;Volume&quot;, type = &quot;s&quot;, size = 1.5, col = &quot;red&quot;) rgl.planes(b1, b2, c, a, alpha = 0.9, color = &quot;gray&quot;) rglwidget() From the figures it looks like the regression model fits pretty well, as we anticipated from the pairwise relationships. 12.11 Interpretting the regression model The regression equation is: \\(\\hat{y}\\) = + \\(x_1\\) +\\(x_2\\), where \\(y\\) is Volume, and \\(x_1\\) and \\(x_2\\) are Girth and Height respectively. Since they’re on different scales the coefficients for Girth and Height aren’t directly comparable. Both coefficients are significant at the \\(p&lt;0.05\\) level, but note that Girth is the much stronger predictor. In fact the addition of height explains only a minor additional fraction of variation in tree volume, so from the lumberjack’s perspective the additional trouble of measuring height probably isn’t worth it. 12.12 Exploring the Vector Geometry of the Regression Model The object returned by the lm() function hold lots of useful information: names(fit.trees) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; The fitted.values correspond to the predicted values of the outcome variable (\\(\\hat{y}\\)). Alternate we can get this information in useful table form using functions from the broom library: Recall the broom:tidy produces a tabular summary of the coefficients of the model and their associated statistics: broom::tidy(fit.trees) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -57.99 8.638 -6.713 2.750e- 7 ## 2 Girth 4.708 0.2643 17.82 8.223e-17 ## 3 Height 0.3393 0.1302 2.607 1.449e- 2 broom:glance provides information about the fit of the model: broom::glance(fit.trees) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.9480 0.9442 3.882 255.0 1.071e-18 3 -84.45 176.9 ## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; Let’s use our knowledge of vector geometry to further explore the relationship between the predicted Volume and the predictor variables. By definition the vector representing the predicted values lies in the subspace (in this case a plane) defined by Height and Girth, so let’s do some simple calculations to understand their length and angular relationships: # proportional to length of vectors sd(fit.trees$fitted.values) ## [1] 16.00434 sd(trees$Height) ## [1] 6.371813 sd(trees$Girth) ## [1] 3.138139 # cosines of angles btw vectors cor(trees$Height, trees$Girth) ## [1] 0.5192801 cor(trees$Girth, fit.trees$fitted.values) ## [1] 0.9933158 cor(trees$Height, fit.trees$fitted.values) ## [1] 0.6144545 # angles btw vectors in degrees acos(cor(trees$Girth, trees$Height)) * (180/pi) ## [1] 58.71603 acos(cor(trees$Girth, fit.trees$fitted.values)) * (180/pi) ## [1] 6.628322 acos(cor(trees$Height, fit.trees$fitted.values)) * (180/pi) ## [1] 52.08771 12.13 Exploring the Residuals from the Model Fit Now let’s look at the residuals from the regression. The residuals represent the `unexplained’ variance: trees.augmented &lt;- augment(fit.trees, trees) ggplot(trees.augmented, aes(x = Girth, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color=&#39;red&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;Girth&quot;, y = &quot;Residuals&quot;) Ideally the residuals should be evenly scattered around zero, with no trends as we go from high to low values of the dependent variable. As you can see, the residuals are somewhat u-shaped or j-shaped suggesting that there may be a non-linear aspect of the relationship that our model isn’t capturing. 12.14 An alternate model Let’s think about the relationships we’re actually modeling for a few minutes. For the sake of simplicity let’s consider the trunk of a tree to be a cylinder. How do the dimensions of this cylinder relate to its volume? You can look up the formula for the volume of a cylinder, but the key thing you’ll want to note is that volume of the cylinder should be proportional to a characteristic length of the cylinder cubed (\\(V \\propto \\mathrm{L}^3\\)). This suggests that if we want to fit a linear model we should relate Girth and Height to \\(\\sqrt[3]{\\mathrm{Volume}}\\): trees.cuberoot &lt;- mutate(trees, cuberoot.Volume = Volume^0.33) fit.trees.cuberoot &lt;- lm(cuberoot.Volume ~ Girth + Height, data = trees.cuberoot) broom::glance(fit.trees) # summary of fit of original model ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.9480 0.9442 3.882 255.0 1.071e-18 3 -84.45 176.9 ## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; broom::glance(fit.trees.cuberoot) # summary of fit of alternate model ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.9776 0.9761 0.08108 612.4 7.768e-24 3 35.47 -62.94 ## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; Comparing the summary tables, we see indeed that using the cube root of Volume improves the fit of our model some. Let’s examine the residuals of this alternate model. trees.cuberoot &lt;- broom::augment(fit.trees.cuberoot, trees.cuberoot) ggplot(trees.cuberoot, aes(x = cuberoot.Volume, y = .resid)) + geom_point() + geom_hline(yintercept = 0, color=&#39;red&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;Girth&quot;, y = &quot;Residuals&quot;) As we can see the transformation we applied to the data did seem to make our residuals more uniform across the range of observations. 12.15 Fitting a curvilinear model using lm() Above we transformed the volume data in order to fit a straight line relationship between \\(\\sqrt[3]{V}\\) and Girth and Hieght. However, we could just as easily have applied a cubic regression to the original variables (remember this is still linear in the coefficients). Since Height didn’t add much to additional information, we’ll simplify the model to consider only Girth. fit.curvilinear &lt;- lm(Volume ~ I(Girth^3), data=trees) broom::tidy(fit.curvilinear) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 8.043 1.043 7.714 1.663e- 8 ## 2 I(Girth^3) 0.008137 0.0003118 26.10 1.087e-21 broom::glance(fit.curvilinear) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.9592 0.9578 3.379 681.1 1.087e-21 2 -80.70 167.4 ## # … with 3 more variables: BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, df.residual &lt;int&gt; Here’s how we can visualize the corresponding curvilinear regression using ggplot: ggplot(trees, aes(x = Girth, y = Volume)) + geom_point() + geom_smooth(method = &quot;lm&quot;, formula = y ~ I(x^3), se = FALSE) The I() function used above requires a little explanation. Normally, the R formula syntax (see ?formula) treats the carat symbol, ^, as short-hand for factor crossing to the specified degree. For example, the formula (a+b+c)^2 would be interpretted as the model with main effects and all second order interaction terms, i.e. a + b + c + a:b + a:c + b:c where the colons indicate interactions. The I() function `protects’ the object in it’s argument; in this case telling the regression function to treat this as Girth raised to the third power as opposed to trying to construct interaction terms for Girth. 12.16 Exploring the impact of nearly collinear predictors on regression In lecture we discussed the problems that can arise in regression when your predictor variables are nearly collinear. In this section we’ll illustrate some of these issues. Consider again the trees data set. Recall that two of the variables – Girth and Volume – are highly correlated and thus nearly collinear. cor(trees) ## Girth Height Volume ## Girth 1.0000000 0.5192801 0.9671194 ## Height 0.5192801 1.0000000 0.5982497 ## Volume 0.9671194 0.5982497 1.0000000 Let’s explore what happens when we treat Height as the dependent variable, and Girth and Volume as the predictor variables. fit.Height &lt;- lm(Height ~ Girth + Volume, data = trees) broom::glance(fit.Height) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.4123 0.3703 5.056 9.820 5.868e-4 3 -92.65 193.3 199.0 ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; We can, of course, fit the linear model despite the near collinearity, and we find that the model does have some predictive power, with \\(R^2 = 0.41\\), and with Volume being the more significant predictor. Now, let’s created a slightly different version of the trees data set by add some noise to the three variables. Our goal here is to simulate a data set we might have created had we measured a slightly different set of trees during our sampling. We’ll use the jitter() function to add uniform noise to the data set. jitter.Girth &lt;- jitter(trees$Girth, amount= 0.5 * sd(trees$Girth)) jitter.Height &lt;- jitter(trees$Height, amount= 0.5 * sd(trees$Height)) jitter.Volume &lt;- jitter(trees$Volume, amount= 0.5 * sd(trees$Volume)) jitter.trees &lt;- data.frame(Girth = jitter.Girth, Height = jitter.Height, Volume = jitter.Volume) Here we added uniform noise proportional to the one-quarter the standard deviation of each variable. Let’s take a moment to convince ourselves that our new data set, jitter.trees, is not too different from the trees data set from which it was derived. set.seed(20190227) # compare this to broom::tidy(trees) broom::tidy(jitter.trees) ## # A tibble: 3 x 13 ## column n mean sd median trimmed mad min max range skew ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Girth 31 13.21 3.186 12.27 12.97 1.959 7.817 21.08 13.27 0.6754 ## 2 Height 31 76.23 6.785 77.59 76.63 4.560 62.23 87.23 24.99 -0.4336 ## 3 Volume 31 30.34 15.79 28.95 29.10 9.266 4.644 72.70 68.05 0.8033 ## # … with 2 more variables: kurtosis &lt;dbl&gt;, se &lt;dbl&gt; # correlations among jittered variables are # similar to those of the original variables cor(jitter.trees) ## Girth Height Volume ## Girth 1.0000000 0.5826720 0.9073956 ## Height 0.5826720 1.0000000 0.4454657 ## Volume 0.9073956 0.4454657 1.0000000 ## jittered variables are highly correlatd with original variables cor(trees$Height, jitter.trees$Height) ## [1] 0.9625816 cor(trees$Girth, jitter.trees$Girth) ## [1] 0.9526394 cor(trees$Volume, jitter.trees$Volume) ## [1] 0.9403697 Now that we’ve convinced ourselves that our jittered data set is a decent approximation to our original data set, let’s re-calculate the linear regression, and compare the coefficients of the jittered model to the original model: fit.Height.jitter &lt;- lm(Height ~ Girth + Volume, data = jitter.trees) broom::tidy(fit.Height) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 83.30 9.087 9.167 6.333e-10 ## 2 Girth -1.862 1.157 -1.609 1.188e- 1 ## 3 Volume 0.5756 0.2208 2.607 1.449e- 2 broom::tidy(fit.Height.jitter) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 53.95 6.174 8.738 1.731e-9 ## 2 Girth 2.152 0.7548 2.851 8.100e-3 ## 3 Volume -0.2025 0.1523 -1.330 1.943e-1 We see that the coefficients of the linear model have changed substantially between the original data and the jittered data. Our model is unstable to relatively modest changes to the data! Let’s draw some plots to illustrate how different the models fit to the original and jittered data are: # draw 3d scatter plots with small points so as not to obscure regression planes p &lt;- scatterplot3d(x=trees$Girth, y=trees$Volume, z=trees$Height, angle=15, type=&#39;p&#39;, pch=&#39;.&#39;) # original model p$plane3d(fit.Height, col=&#39;orangered&#39;) # jittered model p$plane3d(fit.Height.jitter, col=&#39;blue&#39;) Let’s do the same comparison for the multiple regression of Volume on Height and Girth. In this case the predictor variables are nearly collinear. fit.Volume &lt;- lm(Volume ~ Girth + Height, data = trees) fit.Volume.jitter &lt;- lm(Volume ~ Girth + Height, data = jitter.trees) coefficients(fit.Volume) ## (Intercept) Girth Height ## -57.9876589 4.7081605 0.3392512 coefficients(fit.Volume.jitter) ## (Intercept) Girth Height ## -11.5034008 4.8613703 -0.2933129 For this model, we see that the coefficients have changed only a small amount. The underlying data, jitter.trees, is the same in both cases, but now our model is stable because the predictor variables are only modestly correlated with each other. Let’s generate another plot to illustrate the similarity of the models fit to the original and jittered data when Girth and Height are used to predict Volume. p &lt;- scatterplot3d(x=trees$Girth, y=trees$Height, z=trees$Volume, angle=55, type=&#39;p&#39;, pch=&#39;.&#39;) p$plane3d(fit.Volume, col=&#39;orangered&#39;) p$plane3d(fit.Volume.jitter, col=&#39;blue&#39;) Finally, let’s do some vector calculations to quantify how the angular deviation between the fit data and the predictor variables changes between the original and jittered data set for the two different multiple regressions: # write a quickie fxn to express angle between vectors in degrees vec.angle &lt;- function(x,y) { acos(cor(x,y)) * (180/pi)} # vector angles for fit of Height ~ Girth + Volume (orig) vec.angle(fit.Height$fitted.values, trees$Girth) ## [1] 36.02644 vec.angle(fit.Height$fitted.values, trees$Volume) ## [1] 21.29297 # vector angles for fit of Height ~ Girth + Volume (jittered) vec.angle(fit.Height.jitter$fitted.values, jitter.trees$Girth) ## [1] 18.77548 vec.angle(fit.Height.jitter$fitted.values, jitter.trees$Volume) ## [1] 43.62758 Now the same comparison for the non-collinear model Volume ~ Girth + Height. # vector angles for fit of Volume ~ Girth + Height (orig) vec.angle(fit.Volume$fitted.values, trees$Girth) ## [1] 6.628322 vec.angle(fit.Volume$fitted.values, trees$Height) ## [1] 52.08771 # vector angles for fit of Volume ~ Girth + Height (jittered) vec.angle(fit.Volume.jitter$fitted.values, jitter.trees$Girth) ## [1] 6.440693 vec.angle(fit.Volume.jitter$fitted.values, jitter.trees$Height) ## [1] 60.80199 As these calculation illustrate, the change in the regression plane in the jittered date is much smaller when the dependent variable are not nearly colinear. 12.17 LOESS Models LOESS (aka LOWESS; ‘Locally weighted scatterplot smoothing’) is a modeling technique that fits a curve (or surface) to a set of data using a large number of local linear regressions. Local weighted regressions are fit at numerous regions across the data range, using a weighting function that drops off as you move away from the center of the fitting region (hence the \"local aspect). LOESS combines the simplicity of least squares fitting with the flexibility of non-linear techniques and doesn’t require the user to specify a functional form ahead of time in order to fit the model. It does however require relatively dense sampling in order to produce robust fits. Formally, at each point \\(x_i\\) we estimate the regression coefficients \\(\\hat{\\beta}_j(x)\\) as the values that minimize: \\[ \\sum_{k=1}^n w_k(x_i)(y_k - \\beta_0 - \\beta_1 x_k - \\ldots - \\beta_d x_k^2)^2 \\] where \\(d\\) is the degree of the polynomial (usually 1 or 2) and \\(w_k\\) is a weight function. The most common choice of weighting function is called the “tri-cube” function as is defined as: \\[\\begin{align*} w_k(x_i) &amp;= (1-|x_i|^3)^3, \\mbox{for}\\ |x_i| \\lt 1 \\\\ &amp;= 0, \\mbox{for}\\ |x_i| \\geq 1 \\end{align*}\\] where \\(|x_i|\\) is the normalized distance (as determined by the span parameter of the LOESS model) of the observation \\(x_i\\) from the focal observation \\(x_k\\). The primary parameter that a user must decide on when using LOESS is the size of the neighborhood function to apply (i.e. over what distance should the weight function drop to zero). This is referred to as the “span” in the R documentation, or as the parameter \\(\\alpha\\) in many of the papers that discuss LOESS. The appropriate span can be determined by experimentation or, more rigorously by cross-validation. We’ll illustrate fitting a Loess model using data on Barack Obama’s approval ratings over the period from 2008 to 2001 (obama-polls.txt). polls &lt;- read_delim(&#39;https://github.com/Bio723-class/example-datasets/raw/master/obama-polls-2008-2011.txt&#39;, delim=&quot;\\t&quot;, trim_ws=TRUE) # note that we needed to use &quot;trim_ws&quot; above because there were # some lurking spaces in the fields of that tab delimited data file head(polls) ## # A tibble: 6 x 6 ## Pollster Dates `N/Pop` Approve Disapprove Undecided ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Rasmussen 9/17-19/11 1500 LV 46 52 - ## 2 Rasmussen 9/14-16/11 1500 LV 45 55 - ## 3 Gallup 9/13-15/11 1500 A 39 52 - ## 4 CBS/Times 9/10-15/11 1452 A 43 50 7 ## 5 Marist/McClatchy 9/13-14/11 825 RV 39 52 9 ## 6 Rasmussen 9/11-13/11 1500 LV 45 54 - Notice that the Dates column is not very tidy. Each “date” is actually a range of dates of the form Month/DayStart-DayEnd/Year (e.g. “9/1/09” is September 01, 2009). Even nastier, some dates are in the form Month/Day/Year (only a single day) or MonthStart/DayStart-MonthEnd/DayEnd/Year (e.g. “2/26-3/1/11” is February 26,2011 to March 01, 2011) . Whoever formatted the data in this fashion must really hate tidy data! To deal with this nightmare we’re going to use the tidyr::extract() function to employ regular expressions (regex) to parse this complicated data field into it’s constituent parts. For more details on regular expression see the R Regular Expession Cheat Sheet and R for Data Science. polls &lt;- polls %&gt;% # first separate left most and right most fields as month and year respectively tidyr::extract(&quot;Dates&quot;, c(&quot;month&quot;, &quot;day.range&quot;, &quot;year&quot;), regex=&quot;(\\\\d+)/(.+)/(\\\\d+$)&quot;, convert = TRUE) %&gt;% # now deal with the complicated middle field. For simplicities sake we&#39;re just # going to focus on extracting the start day tidyr::extract(&quot;day.range&quot;, c(&quot;day.start&quot;, &quot;day.other&quot;), regex = &quot;(\\\\d+)(.+)&quot;, convert = TRUE) %&gt;% # finally convert YY to 20YY mutate(year = 2000 + year) head(polls) ## # A tibble: 6 x 9 ## Pollster month day.start day.other year `N/Pop` Approve Disapprove ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rasmuss… 9 17 -19 2011 1500 LV 46 52 ## 2 Rasmuss… 9 14 -16 2011 1500 LV 45 55 ## 3 Gallup 9 13 -15 2011 1500 A 39 52 ## 4 CBS/Tim… 9 10 -15 2011 1452 A 43 50 ## 5 Marist/… 9 13 -14 2011 825 RV 39 52 ## 6 Rasmuss… 9 11 -13 2011 1500 LV 45 54 ## # … with 1 more variable: Undecided &lt;chr&gt; For the next steps we’ll need the lubridate library (install if needed): library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date polls &lt;- polls %&gt;% mutate(date = make_date(year = year, month=month, day = day.start)) head(polls) ## # A tibble: 6 x 10 ## Pollster month day.start day.other year `N/Pop` Approve Disapprove ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Rasmuss… 9 17 -19 2011 1500 LV 46 52 ## 2 Rasmuss… 9 14 -16 2011 1500 LV 45 55 ## 3 Gallup 9 13 -15 2011 1500 A 39 52 ## 4 CBS/Tim… 9 10 -15 2011 1452 A 43 50 ## 5 Marist/… 9 13 -14 2011 825 RV 39 52 ## 6 Rasmuss… 9 11 -13 2011 1500 LV 45 54 ## # … with 2 more variables: Undecided &lt;chr&gt;, date &lt;date&gt; polls.plot &lt;- polls %&gt;% ggplot(aes(x = date, y = Approve)) + geom_point(alpha=0.5, pch=1) + labs(x = &quot;Date&quot;, y = &quot;Approval Rating&quot;, title = &quot;Barack Obama&#39;s Approval Ratings, 2008-2011&quot;) polls.plot We can fit the LOESS as so, and get back the predicted values using the predict() function: loess.approval &lt;- loess(Approve ~ as.numeric(date), data = polls) loess.predicted.values &lt;- predict(loess.approval) head(loess.predicted.values) ## [1] 44.55653 44.59349 44.60572 44.64216 44.60572 44.63006 Usually we’ll want to visualize the LOESS regression, which we can conveniently do with ggplot::geom_smooth without having to explicitly calculate the LOESS: polls.plot + geom_smooth(color=&#39;red&#39;, method=&quot;loess&quot;, se=FALSE) Here’s the same data fit with a smaller span (the paramater that controls the “local neighborhood” size in LOESS): polls.plot + geom_smooth(color=&#39;red&#39;, method=&quot;loess&quot;, se=FALSE, span=0.1) The high density of the polling justifies the smaller span, and the additional deviations apparent when the LOESS is fit with the smaller span likely reflect real world changes in approval, induced by a variety of political and other news events. For example, we can zoom in on 2011: polls.plot + geom_smooth(color=&#39;red&#39;, method=&quot;loess&quot;, se=FALSE, span=0.1) + coord_cartesian(xlim=c(ymd(20110101), ymd(20110901)), ylim=c(35,65)) + scale_x_date(date_breaks=&quot;1 month&quot;, date_label=&quot;%B&quot;) + labs(title=&quot;Barack Obama&#39;s Approval Ratings, Jan - Sep 2011&quot;) Increased approval ratings in January coincide with the approval of a tax deal and a speech to the nation following the shooting of congresswoman Gabbie Giffords in Tuscson, AZ (https://www.cnbc.com/id/41139968). The spike apparent in early May coincides with the death of Osama Bin Laden. You might take a look at major policitcal events in otehr years to see if you can identify drivers behind other approval rating shifts. "],
["logistic-regression.html", "Chapter 13 Logistic regression 13.1 A web app to explore the logistic regression equation 13.2 Titanic data set 13.3 Subsetting the data 13.4 Visualizing survival as a function of age 13.5 Fitting the logistic regression model 13.6 Visualizing the logistic regression 13.7 Impact of sex and passenger class on the models 13.8 Fitting multiple models based on groupings use dplyr::do", " Chapter 13 Logistic regression Logistic regression is used when the dependent variable is discrete (often binary). The explanatory variables may be either continuous or discrete. Examples: Whether a gene is turned off (=0) or on (=1) as a function of levels of various proteins Whether an individual is healthy (=0) or diseased (=1) as a function of various risk factors. Whether an individual died (=0) or survived (=1) some selective event as a function of behavior, morphology, etc. We model the binary response variable, \\(Y\\), as a function of the predictor variables, \\(X_1\\), \\(X_2\\), etc as : \\[ P(Y = 1|X_1,\\ldots,X_p) = f(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p) \\] So we’re modeling the probability of the state of Y as a function of a linear combination of the predictor variables. For logistic regression, \\(f\\) is the logistic function: \\[ f(z) = \\frac{e^z}{1+e^z} = \\frac{1}{1 + e^{-z}} \\] Therefore, the bivariate logistic regression is given by: \\[ P(Y = 1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}} \\] Note that \\(\\beta_0\\) here is akin to the intercept in our standard linear regression. 13.1 A web app to explore the logistic regression equation To help you develop an intuition for the logistic regression equation, I’ve developed a small web app, that allows you to explore how the shape of the regression curve responds to changes in the regression coefficients \\(\\beta_0\\) and \\(\\beta_1\\). Open the app in another browser window and play with the sliders that control the coeffients \\(B_0\\) and \\(B_1\\). In the assignment associated with today’s class you’ll be asked to answer some specific questions based on this app. 13.2 Titanic data set titanic.csv contains information about passengers on the Titanic. Variables in this data set include information such as sex, age, passenger class (1st, 2nd, 3rd), and whether or not they survived the sinking of the ship (0 = died, 1 = survived). library(tidyverse) library(broom) library(cowplot) library(ggthemes) titanic &lt;- read_csv(&quot;http://bit.ly/bio304-titanic-data&quot;) names(titanic) ## [1] &quot;pclass&quot; &quot;survived&quot; &quot;name&quot; &quot;sex&quot; &quot;age&quot; ## [6] &quot;sibsp&quot; &quot;parch&quot; &quot;ticket&quot; &quot;fare&quot; &quot;cabin&quot; ## [11] &quot;embarked&quot; &quot;boat&quot; &quot;body&quot; &quot;home.dest&quot; 13.3 Subsetting the data We’ve all heard the phrase, “Women and children first”, so we might expect that the probability that a passenger survived the sinking of the Titanic is related to their sex and/or age. Let’s create separate data subsets for male and female passengers. male &lt;- filter(titanic, sex == &quot;male&quot;) female &lt;- filter(titanic, sex == &quot;female&quot;) 13.4 Visualizing survival as a function of age Let’s create visualizations of survival as a function of age for the male and female passengers. fcolor = &quot;lightcoral&quot; mcolor = &quot;lightsteelblue&quot; female.plot &lt;- ggplot(female, aes(x = age, y = survived)) + geom_jitter(width = 0, height = 0.05, color = fcolor) + labs(title = &quot;Female Passengers&quot;) male.plot &lt;- ggplot(male, aes(x = age, y = survived)) + geom_jitter(width = 0, height = 0.05, color = mcolor) + labs(title = &quot;Male Passengers&quot;) plot_grid(female.plot, male.plot) The jittered points with Y-axis value around one are passengers who survived, the point jittered around zero are those who died. 13.5 Fitting the logistic regression model The function glm (generalized linear model) can be used to fit the logistic regression model (as well as other models). Setting the argument family = binomial gives us logistic regression. Note that when fitting the model the dependent variable needs to be numeric, so if the data is provided as Boolean (logical) TRUE/FALSE values, they should be converted to integers using as.numeric(). First we fit the regression for the famale passengers. fit.female &lt;- glm(survived ~ age, family = binomial, female) tidy(fit.female) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.4934 0.2542 1.941 0.05226 ## 2 age 0.02252 0.008535 2.638 0.008342 The column “estimate” gives the coefficients of the model. The “intercept”\" estimate corresponds to \\(B_0\\) in the logistic regression equation, the “age” estimate corresponds to the coefficient \\(B_1\\) in the equation. Now we repeat the same step for the male passengers. fit.male &lt;- glm(survived ~ age, family = binomial, male) tidy(fit.male) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -0.6608 0.2248 -2.939 0.003290 ## 2 age -0.02376 0.007276 -3.266 0.001092 Notice that the female coefficients are both positive, while the male coefficients are negative. We’ll visualize what this means in terms of the model below. 13.6 Visualizing the logistic regression To visualize the logistic regression fit, we first use the predict function to generate the model predictions about probability of survival as a function of age. ages &lt;- seq(0, 75, 1) # predict survival for ages 0 to 75 predicted.female &lt;- predict(fit.female, newdata = data.frame(age = ages), type = &quot;response&quot;) predicted.male &lt;- predict(fit.male, newdata = data.frame(age = ages), type = &quot;response&quot;) Having generated the predicted probabilities of survival we can then add these prediction lines to our previous plot using geom_line. female.logistic.plot &lt;- female.plot + geom_line(data = data.frame(age = ages, survived = predicted.female), color = fcolor, size = 1) male.logistic.plot &lt;- male.plot + geom_line(data = data.frame(age = ages, survived = predicted.male), color = mcolor, size = 1) plot_grid(female.logistic.plot, male.logistic.plot) We see that for the female passengers, the logistic regression predicts that the probability of survival increases with passenger age. In contrast, the model fit to the male passengers suggests that the probability of survival decreases with passenger age. For the male passengers, the data is consistent with “children first”; for female passengers this model doesn’t seem to hold. However, there are other factors to consider as we’ll see below. 13.6.1 Quick and easy visualization Here’s an alternative “quick and easy” way to generate the plot above using the awesome power of ggplot. The downside of this approach is we don’t generate the detailed information on the model, which is something you’d certainly want to have in any real analysis. ggplot(titanic, aes(x=age, y=survived, color=sex)) + geom_jitter(width = 0, height = 0.05) + geom_smooth(method=&quot;glm&quot;, method.args = list(family=&quot;binomial&quot;)) + labs(x = &quot;Age&quot;, y = &quot;P(Survival)&quot;) + facet_wrap(~ sex) + scale_color_manual(values = c(fcolor, mcolor)) 13.7 Impact of sex and passenger class on the models In our previous analysis we considered the relationship between survival and age, conditioned (facted) on passenger sex. In a complex data set like this one, it is often useful to condition on multiple variables simultaneously. Lets extend our visualization to look at the regression faceted on both class and sex, using facet_grid: ggplot(titanic, aes(x=age, y=survived, color=sex)) + geom_jitter(width = 0, height = 0.05) + geom_smooth(method=&quot;glm&quot;, method.args = list(family=&quot;binomial&quot;)) + labs(x = &quot;Age&quot;, y = &quot;P(Survival)&quot;) + facet_grid(pclass ~ sex) + scale_color_manual(values = c(fcolor, mcolor)) + theme_few() Having conditioned on both sex and ticket class, our figure now reveals a much more complex relationship between age and survival. Almost all first class female passengers survived, regardless of age. For second calss female passengers, the logistic regression suggests a very modest decrease in survival with increasing age. The negative relationship between age and survival is stronger still for third class females. Male passengers on the other hand show a negative relationship between sex and survival, regardless of class, but the models suggest that there are still class specific differences in this relationship. 13.8 Fitting multiple models based on groupings use dplyr::do In the figure above we used ggplot and facet_grid to visualize logistic regression of survival on age, conditioned on both sex and class. What if we wanted to calculate the terms of the logistic regressions for each combination of these two categorical variables? There are three passenger classes and two sexes, meaning we’d have to create six data subsets and fit the model six times if we used the same approach we used previously. Luckily, dplyr provides a powerful function called do() that allows us to carry out arbitrary computations on grouped data. There are two ways to use do(). The first way is to give the expressions you evaluate in do() a name, in which case do() will store the results in a column. The second way to use do() is for the expression to return a data frame. In this first example, the model fits are stored in the fits column. When using do() you can refer to the groupings using a period (.): grouped.models &lt;- titanic %&gt;% group_by(sex, pclass) %&gt;% do(fits = glm(survived ~ age, family = binomial, data = .)) grouped.models ## # A tibble: 6 x 3 ## sex pclass fits ## &lt;chr&gt; &lt;dbl&gt; &lt;list&gt; ## 1 female 1 &lt;S3: glm&gt; ## 2 female 2 &lt;S3: glm&gt; ## 3 female 3 &lt;S3: glm&gt; ## 4 male 1 &lt;S3: glm&gt; ## 5 male 2 &lt;S3: glm&gt; ## 6 male 3 &lt;S3: glm&gt; Notice that the “fits” column doesn’t explicitly print out the details of the model. The object returned by glm() can’t be simply represented as text string (it’s a list), so we seea place holder string that tells us that there is data here represented a glm object. However, we can access the the columns with the fits just like any other variable: # get the summary of the second logistic regression (Female, 2nd Class) tidy(grouped.models$fits[[2]]) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.491 0.9037 3.863 0.0001119 ## 2 age -0.04502 0.02548 -1.767 0.07730 Now we illustrate the second approach to using do(). When no name is provided, do() expects its expression to return a dataframe. Here we use the broom::tidy() function to get the key results of each fit model into a data frame: titanic %&gt;% group_by(sex, pclass) %&gt;% do(tidy(glm(survived ~ age, family = binomial, data = .))) ## # A tibble: 12 x 7 ## sex pclass term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 1 (Intercept) 2.896 1.238 2.339 0.01932 ## 2 female 1 age 0.009599 0.03263 0.2942 0.7686 ## 3 female 2 (Intercept) 3.491 0.9037 3.863 0.0001119 ## 4 female 2 age -0.04502 0.02548 -1.767 0.07730 ## 5 female 3 (Intercept) 0.2887 0.3412 0.8461 0.3975 ## 6 female 3 age -0.01783 0.01361 -1.310 0.1901 ## 7 male 1 (Intercept) 0.8803 0.5275 1.669 0.09514 ## 8 male 1 age -0.03743 0.01276 -2.934 0.003351 ## 9 male 2 (Intercept) 1.042 0.5990 1.740 0.08181 ## 10 male 2 age -0.1122 0.02492 -4.502 0.000006724 ## 11 male 3 (Intercept) -0.7613 0.3418 -2.228 0.02591 ## 12 male 3 age -0.03392 0.01339 -2.534 0.01129 Using this approach we get a nice data frame showing the logistic regression coefficients, and associated statistics (standard error, P-values, etc) for the regression of survival on age, for each combination of sex and class. "],
["principal-components-analysis.html", "Chapter 14 Principal Components Analysis 14.1 Libraries 14.2 Matrices as linear transformations 14.3 Eigenanalysis in R 14.4 Principal Components Analysis in R 14.5 Drawing Figures to Represent PCA", " Chapter 14 Principal Components Analysis 14.1 Libraries library(tidyverse) library(broom) library(GGally) library(cowplot) 14.2 Matrices as linear transformations In lecture we introduced the notion that pre-multiplying a vector, \\(x\\), by a matrix \\(\\mathbf{A}\\) represents a linear transformation of \\(x\\). Let’s explore that visually for 2D vectors. For illustration let’s generate a 2D vector representing the coordinates of points on the sine function. sin.xy &lt;- data_frame(x = seq(0,2*pi, length.out = 50) - pi, y = sin(x)) ggplot(sin.xy, aes(x = x, y= y)) + geom_point() Let’s start with some of the transformations we discussed in lecture. First we look at reflection about the x-axis: # this matrix represents reflection about the x-axis A &lt;- matrix(c(1, 0, 0, -1), byrow=TRUE, nrow=2) # apply this transformation to each of the points in our vector `sin.xy` Ax &lt;- A %*% t(sin.xy) # creae a data frame from this transformation Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) # plot the transformed points ggplot(Ax.df, aes(x = x, y = y)) + geom_point() If instead we apply a matrix that represents a shear in the x-axis we get: # shear parallel to the x-axis A &lt;- matrix(c(1, 1.5, 0, 1), byrow=TRUE, nrow=2) Ax &lt;- A %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) ggplot(Ax.df, aes(x = x, y= y)) + geom_point() And finally rotation: # rotation by pi/2 radians (90 degrees) A &lt;- matrix(c(cos(pi/2), -sin(pi/2), sin(pi/2), cos(pi/2)), byrow=TRUE, nrow=2) Ax &lt;- A %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) ggplot(Ax.df, aes(x = x, y= y)) + geom_point() 14.3 Eigenanalysis in R As we discussed in lecture, the eigenvectors of a square matrix, \\(\\mathbf{A}\\), point in the directions that are unchanged by the transformation specified by \\(\\mathbf{A}\\). Let’s start with yet another transformation matrix, \\(\\mathbf{A}\\), the effects of which are illustrated below: A &lt;- matrix(c(1.0, 1.5, 0, 2.0), byrow=TRUE, nrow=2) Ax &lt;- A %*% t(sin.xy) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) ggplot(mapping=aes(x = x, y = y)) + geom_point(data = sin.xy, shape=16, alpha=0.5) + geom_point(data = Ax.df, shape=17, alpha=0.5, color=&#39;red&#39;) This transformation is a combination of stretching (dilation) in the Y-axis, and shear parallel to the x-axis. The eigen() function computes the eigenvalues and eigenvectors of a square matrix: eigen.A &lt;- eigen(A) eigen.A ## eigen() decomposition ## $values ## [1] 2 1 ## ## $vectors ## [,1] [,2] ## [1,] 0.8320503 1 ## [2,] 0.5547002 0 The following relationships relate \\(\\mathbf{A}\\) to it’s eigenvectors and eigenvalues: \\[\\mathbf{V}^{-1}\\mathbf{A}\\mathbf{V} = \\mathbf{D} \\] \\[\\mathbf{A} = \\mathbf{V}\\mathbf{D}\\mathbf{V}^{-1}\\] where \\(\\mathbf{V}\\) is a matrix where the columns represent the eigenvectors, and \\(\\mathbf{D}\\) is a diagonal matrix of eigenvalues. Let’s confirm that those relationships hold for our example: V &lt;- eigen.A$vectors D &lt;- diag(eigen.A$values) # diagonal matrix of eigenvalues Vinv &lt;- solve(V) V %*% D %*% Vinv # reconstruct our original matrix (see lecture slides) ## [,1] [,2] ## [1,] 1 1.5 ## [2,] 0 2.0 all.equal(Vinv %*% A %*% V, D) # test &#39;near equality&#39; ## [1] TRUE V[,1] %*% V[,2] # note that the eigenvectors are NOT orthogonal. Why? ## [,1] ## [1,] 0.8320503 Since we’re dealing with a 2D transformation matrix, we can easily calculate the slopes of the eigenvectors: eigenvec.A.slope1 = eigen.A$vectors[2,1]/eigen.A$vectors[1,1] eigenvec.A.slope2 = eigen.A$vectors[2,2]/eigen.A$vectors[1,2] Now we plot the eigenvectors to show the directions that are unaffected by the transformation represented by the matrix \\(\\mathbf{A}\\): # Note that the slopes can be infinite (inf), indicating a vertical eigenvector (parallel to y-axis) # this function is to draw the eigenvectors correctly regardless of the slope geom_ab_or_vline &lt;- function(slope, intercept, ...) { if (is.infinite(slope)) { return(geom_vline(yintercept = intercept, ...)) } else { return(geom_abline(slope=slope, intercept=intercept, ...)) } } ggplot(mapping=aes(x = x, y = y)) + geom_point(data = sin.xy, alpha=0.5) + geom_point(data = Ax.df, color=&#39;red&#39;, alpha=0.5) + geom_ab_or_vline(slope = eigenvec.A.slope1,intercept=0, color=&#39;red&#39;,linetype=&#39;dashed&#39;) + geom_ab_or_vline(slope = eigenvec.A.slope2,intercept=0, color=&#39;red&#39;,linetype=&#39;dashed&#39;) + coord_fixed() Referring back to the eigenvectors, we see that it’s the second eigenvector that is represented by the horizontal line: eigen.A$vectors ## [,1] [,2] ## [1,] 0.8320503 1 ## [2,] 0.5547002 0 The implies that lines pointing in the horizontal are unchanged in the direction by the transformation represented by A. To drive home this point, here’s another configuration of points and the corresponding points under the same transformation: horiz.x &lt;- seq(-1,1,by=0.1) horiz.y &lt;- rep(0, length(horiz.x)) horiz.x2 &lt;- seq(-0.5,0.5,by=0.1) horiz.y2 &lt;- rep(0.5, length(horiz.x2)) horiz.x3 &lt;- seq(-0.5,0.5,by=0.1) horiz.y3 &lt;- rep(-0.5, length(horiz.x3)) vert.y &lt;- seq(-1,1,by=0.1) vert.x &lt;- rep(0, length(vert.y)) x &lt;- c(horiz.x, horiz.x2, horiz.x3, vert.x) y &lt;- c(horiz.y, horiz.y2, horiz.y3, vert.y) cross.df &lt;- data_frame(x = x, y = y) Ax &lt;- A %*% t(cross.df) Ax.df &lt;- as.data.frame(t(Ax)) %&gt;% rename(x = V1, y = V2) plot.A &lt;- ggplot(cross.df, mapping = aes(x=x,y=y)) + geom_point() + lims(x=c(-1.5,1.5), y=c(-2,2)) + coord_fixed() + labs(x = &quot;X&quot;, y = &quot;Y&quot;, title=&quot;Points prior to transformation&quot;) plot.B &lt;- ggplot(Ax.df, mapping = aes(x=x,y=y)) + geom_point(color=&#39;red&#39;) + lims(x=c(-1.5,1.5), y=c(-2,2)) + coord_fixed() + labs(x = &quot;X&quot;, y = &quot;Y&quot;, title=&quot;Points after transformation&quot;) plot_grid(plot.A, plot.B) 14.4 Principal Components Analysis in R There are two functions in R for carrying out PCA - princomp() and prcomp(). The princomp() function uses the eigen() function to carry out the analysis on the covariance matrix or correlation matrix, while carries out an equivalent analysis, starting from a data matrix, using a technique called singular value decomposition (SVD). The SVD routine has greater numerical accuracy, so the prcomp() function should generally be preferred. The princomp() function is useful when you don’t have access to the original data, but you do have a covariance or correlation matrix (a frequent situation when re-analyzing data from the literature). We’ll concentrate on using the prcomp() function. 14.4.1 Bioenv dataset To demonstrate PCA we’ll use a dataset called `bioenv.txt’ (see class wiki), obtained from a book called “Biplots in Practice” (M. Greenacre, 2010). Here is Greenacre’s description of the dataset: The context is in marine biology and the data consist of two sets of variables observed at the same locations on the sea-bed: the first is a set of biological variables, the counts of five groups of species, and the second is a set of four environmental variables. The data set, called “bioenv”, is shown in Exhibit 2.1. The species groups are abbreviated as “a” to “e”. The environmental variables are “pollution”, a composite index of pollution combining measurements of heavy metal concentrations and hydrocarbons; depth, the depth in metres of the sea-bed where the sample was taken; “temperature”, the temperature of the water at the sampling point; and “sediment”, a classification of the substrate of the sample into one of three sediment categories. We’ll start by reading the bioenv.txt data set from the Github repository: bioenv &lt;- read_tsv(&#39;https://github.com/Bio723-class/example-datasets/raw/master/bioenv.txt&#39;) ## Parsed with column specification: ## cols( ## X1 = col_character(), ## a = col_double(), ## b = col_double(), ## c = col_double(), ## d = col_double(), ## e = col_double(), ## Pollution = col_double(), ## Depth = col_double(), ## Temperature = col_double(), ## Sediment = col_character() ## ) names(bioenv) ## [1] &quot;X1&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ## [6] &quot;e&quot; &quot;Pollution&quot; &quot;Depth&quot; &quot;Temperature&quot; &quot;Sediment&quot; Notice that the first column got assigned the generic name X1. This is because there is a missing column header in the bioenv.txt file. This first column corresponds to the sampling sites. Before we move on let’s give this column a more meaningful name: bioenv &lt;- bioenv %&gt;% rename(Site = X1) names(bioenv) ## [1] &quot;Site&quot; &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ## [6] &quot;e&quot; &quot;Pollution&quot; &quot;Depth&quot; &quot;Temperature&quot; &quot;Sediment&quot; The columns labeled a to e contain the counts of the five species at each site, while the remaining columns give additional information about the physical properties of each sampling site. For our purposes today we’ll confine our attention to the abundance data. abundance &lt;- bioenv %&gt;% select(Site, a, b, c, d, e) head(abundance) ## # A tibble: 6 x 6 ## Site a b c d e ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 s1 0 2 9 14 2 ## 2 s2 26 4 13 11 0 ## 3 s3 0 10 9 8 0 ## 4 s4 0 0 15 3 0 ## 5 s5 13 5 3 10 7 ## 6 s6 31 21 13 16 5 The data is currently in a “wide” format. For the purposes of plotting it will be more convenient to generate a “long” version of the data using functions from the tidyr library (see the Data Wrangling chapter). long.abundance &lt;- abundance %&gt;% tidyr::gather(Species, Count, -Site) head(long.abundance) ## # A tibble: 6 x 3 ## Site Species Count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 s1 a 0 ## 2 s2 a 26 ## 3 s3 a 0 ## 4 s4 a 0 ## 5 s5 a 13 ## 6 s6 a 31 ggplot(long.abundance, aes(x = Species, y = Count)) + geom_boxplot() + labs(x = &quot;Species&quot;, y = &quot;Count&quot;, title=&quot;Distribution of\\nSpecies Counts per Site&quot;) From the boxplot it looks like the counts for species `e’ are smaller on average, and less variable. The mean and variance functions confirm that. long.abundance %&gt;% group_by(Species) %&gt;% summarize(mean(Count), var(Count)) ## # A tibble: 5 x 3 ## Species `mean(Count)` `var(Count)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 a 13.47 157.6 ## 2 b 8.733 83.44 ## 3 c 8.4 73.63 ## 4 d 10.9 44.44 ## 5 e 2.967 15.69 A correlation matrix suggests weak to moderate associations between the variables: abundance.only &lt;- abundance %&gt;% select(-Site) # drop the Site column cor(abundance.only) ## a b c d e ## a 1.0000000 0.67339954 -0.23992888 0.358192050 0.273522301 ## b 0.6733995 1.00000000 -0.08041947 0.501834036 0.036914702 ## c -0.2399289 -0.08041947 1.00000000 0.081504483 -0.343540453 ## d 0.3581921 0.50183404 0.08150448 1.000000000 -0.004048517 ## e 0.2735223 0.03691470 -0.34354045 -0.004048517 1.000000000 However a scatterplot matrix generated by the GGally::ggapirs() function suggests that many of the relationships have a strong non-linear element. ggpairs(abundance.only) 14.4.2 PCA of the Bioenv dataset Linearity is not a requirement for PCA, as it’s simply a rigid rotation of the original data. So we’ll continue with our analysis after taking a moment to read the help on the prcomp() function that is used to carry-out PCA in R. abundance.pca &lt;- prcomp(abundance.only, center=TRUE, retx=TRUE) # center=TRUE mean centers the data # retx=TRUE returns the PC scores # if you want to do PCA on the correlation matrix set scale.=TRUE # -- notice the period after scale! summary(abundance.pca) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 ## Standard deviation 14.8653 8.8149 6.2193 5.03477 3.48231 ## Proportion of Variance 0.5895 0.2073 0.1032 0.06763 0.03235 ## Cumulative Proportion 0.5895 0.7968 0.9000 0.96765 1.00000 We see that approximately 59% of the variance in the data is capture by the first PC, and approximately 90% by the first three PCs. Let’s compare the values return by PCA to what we would get if we carried out eigenanalysis of the covariance matrix that corresponds to our data. First the list object return by prcomp(): abundance.pca ## Standard deviations (1, .., p=5): ## [1] 14.865306 8.814912 6.219250 5.034774 3.482308 ## ## Rotation (n x k) = (5 x 5): ## PC1 PC2 PC3 PC4 PC5 ## a 0.81064462 0.07052882 -0.53108427 0.18442140 -0.14771336 ## b 0.51264394 -0.27799671 0.47711910 -0.63418946 0.17342177 ## c -0.16235135 -0.88665551 -0.40897655 -0.01149647 0.14173943 ## d 0.22207108 -0.31665237 0.56250980 0.72941223 -0.04422938 ## e 0.06616623 0.17696554 -0.08141111 0.17781482 0.96231977 And now the corresponding values returned by eigenanaysis of the covariance matrix generated from the abundance data: eig.abundance &lt;- eigen(cov(abundance.only)) eig.abundance$vectors # compare to rotation matrix of PCA ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.81064462 -0.07052882 0.53108427 0.18442140 -0.14771336 ## [2,] 0.51264394 0.27799671 -0.47711910 -0.63418946 0.17342177 ## [3,] -0.16235135 0.88665551 0.40897655 -0.01149647 0.14173943 ## [4,] 0.22207108 0.31665237 -0.56250980 0.72941223 -0.04422938 ## [5,] 0.06616623 -0.17696554 0.08141111 0.17781482 0.96231977 sqrt(eig.abundance$values) # compare to sdev of PCA ## [1] 14.865306 8.814912 6.219250 5.034774 3.482308 Notice that the rotation object returned by the prcomp() represents the scaled eigenvectors (scaled to have length 1). The standard deviations of the PCA are the square roots of the eigenvalues of the covariance matrix. 14.4.3 Calculating Factor Loadings Let’s calculate the “factor loadings” associated with the PCs: V &lt;- abundance.pca$rotation # eigenvectors L &lt;- diag(abundance.pca$sdev) # diag mtx w/sqrts of eigenvalues on diag. abundance.loadings &lt;- V %*% L abundance.loadings ## [,1] [,2] [,3] [,4] [,5] ## a 12.0504801 0.6217053 -3.3029460 0.92852016 -0.5143835 ## b 7.6206090 -2.4505164 2.9673232 -3.19300085 0.6039081 ## c -2.4134024 -7.8157898 -2.5435276 -0.05788214 0.4935804 ## d 3.3011545 -2.7912626 3.4983893 3.67242602 -0.1540203 ## e 0.9835813 1.5599356 -0.5063161 0.89525751 3.3510942 The magnitude of the factor loadings is what you want to focus on. For example, species a and b contribute most to the first PC, while species c has the largest influence on PC2. You can think of the factor loadings, as defined above, as the components (i.e lengths of the projected vectors) of the original variables with respect to the PC basis vectors. Since vector length is proportional to the standard deviation of the variables they represent, you can think of the loadings as giving the standard deviation of the original variables with respect the PC axes. This implies that the loadings squared sum to the total variance in the original data, as illustrated below. sum(abundance.loadings**2) ## [1] 374.8345 abundance.only %&gt;% purrr::map_dbl(var) %&gt;% sum ## [1] 374.8345 14.5 Drawing Figures to Represent PCA 14.5.1 PC Score Plots The simplest PCA figure is to depict the PC scores, i.e. the projection of the observations into the space defined by the PC axes. Let’s make a figure with three subplots, depicting PC1 vs PC2, PC1 vs PC3, and PC2 vs. PC3. pca.scores.df &lt;- as.data.frame(abundance.pca$x) coord.system &lt;- coord_fixed(ratio=1, xlim=c(-30,30),ylim=c(-30,30)) pc1v2 &lt;- pca.scores.df %&gt;% ggplot(aes(x = PC1, y= PC2)) + geom_point() + coord.system pc1v3 &lt;- pca.scores.df %&gt;% ggplot(aes(x = PC1, y= PC3)) + geom_point() + coord.system pc2v3 &lt;- pca.scores.df %&gt;% ggplot(aes(x = PC2, y= PC3)) + geom_point() + coord.system cowplot::plot_grid(pc1v2, pc1v3, pc2v3, align=&quot;hv&quot;) When plotting PC scores, it is very important to keep the aspect ratio with a value of 1, so that the distance between points in the plot is an accurate representation of the distance in the PC space. Note too that I used the xlim and ylim arguments to keep the axis limits the same in all plots; comparable scaling of axes is important when comparing plots. Also note the use of the align=\"hv\" argument to plot_grid() to keep my plots the same size when combining them into a single figure. 14.5.2 Simultaneous Depiction of Observations and Variables in the PC Space Let’s return to our simple PC score plot. As we discussed above, the loadings are components of the original variables in the space of the PCs. This implies we can depict those loadings in the same PC basis that we use to depict the scores. First let’s create a data frame with the loadings from the first two PCs as well as a column representation the variable names: loadings.1and2 &lt;- data.frame(abundance.loadings[,1:2]) %&gt;% rename(PC1.loading = X1, PC2.loading = X2) %&gt;% mutate(variable = row.names(abundance.loadings)) loadings.1and2 ## # A tibble: 5 x 3 ## PC1.loading PC2.loading variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 12.05 0.6217 a ## 2 7.621 -2.451 b ## 3 -2.413 -7.816 c ## 4 3.301 -2.791 d ## 5 0.9836 1.560 e With this data frame in hand, we can now draw a set of vector to represent the original variables projected into the subspace defined by PCs 1 and 2: pc1v2.biplot &lt;- pc1v2 + geom_segment(data=loadings.1and2, aes(x = 0, y = 0, xend = PC1.loading, yend = PC2.loading), color=&#39;red&#39;, arrow = arrow(angle=15, length=unit(0.1,&quot;inches&quot;))) + geom_text(data=loadings.1and2, aes(x = PC1.loading, y = PC2.loading, label=variable), color=&#39;red&#39;, nudge_x = 1, nudge_y = 1) pc1v2.biplot The figure above is called a “biplot”\", as it simultaneously depicts both the observations and variables in the same space. From this biplot we can immediately see that variable a is highly correlated with PC1, but only weakly associated with PC2. Conversely, variable c is strongly correlated with PC2 but only weakly so with PC1. We can also approximate the correlations among the variables themselves – for example b and d are fairly strongly correlated, but weakly correlated with c. Keep in mind however that with respect to the relationships among the variables, this visualization is a 2D projection of a 5D space so the geometry is approximate. The biplot is a generally useful tool for multivariate analyses and there are a number of different ways to define biplots. We’ll study biplots more formally in a few weeks after we’ve covered singular value decomposition. "],
["singular-value-decomposition.html", "Chapter 15 Singular Value Decomposition 15.1 Libraries 15.2 SVD in R 15.3 Writing our own PCA function using SVD 15.4 Writing a biplot function 15.5 Data compression and noise filtering using SVD 15.6 Image compression using SVD in R", " Chapter 15 Singular Value Decomposition 15.1 Libraries library(tidyverse) library(stringr) library(broom) library(GGally) library(cowplot) 15.2 SVD in R \\[\\newcommand{\\Mtx}[1]{\\mathbf{#1}}\\] If \\(\\Mtx{x}\\) is an \\(n \\times p\\) matrix, and the singular value decomposition of \\(\\Mtx{A}\\) is given by \\(\\Mtx{A} = \\Mtx{U} \\Mtx{S} \\Mtx{V}^T\\), the columns of the matrix \\(\\Mtx{V}^T\\) are the eigenvectors of the square matrix \\(\\Mtx{A}^T \\Mtx{A}\\) (sometimes refered to as the minor product of ). The singular values of are equal to the square roots of the eigenvalues of \\(\\Mtx{A}^T \\Mtx{A}\\). The svd() function computes the singular value decomposition of an arbitrary rectangular matrix. Below I demonstrate the use of the svd() function and confirm the relationships described above: A &lt;- matrix(c(2,1,2,3),nrow=2) A ## [,1] [,2] ## [1,] 2 2 ## [2,] 1 3 a.svd &lt;- svd(A) # R uses the notation A = u d v&#39; rather than A = u s v&#39; a.svd$u ## [,1] [,2] ## [1,] -0.6618026 -0.7496782 ## [2,] -0.7496782 0.6618026 a.svd$d ## [1] 4.1306486 0.9683709 a.svd$v ## [,1] [,2] ## [1,] -0.5019268 -0.8649101 ## [2,] -0.8649101 0.5019268 We should be able to reconstruct \\(\\Mtx{A}\\) from the SVD: all.equal(A, a.svd$u %*% diag(a.svd$d) %*% t(a.svd$v)) ## [1] TRUE We can also demonstrate the relationship between the SVD of \\(\\Mtx{A}\\) and the eigendecomposition of \\(\\Mtx{A}^T\\Mtx{A}\\): AtA &lt;- t(A) %*% A eigen.AtA &lt;- eigen(AtA) all.equal(a.svd$d, sqrt(eigen.AtA$values)) ## [1] TRUE As we discussed in lecture, the eigenvectors of square matrix, \\(\\Mtx{A}\\), point in the directions that are unchanged by the transformation specified by \\(\\Mtx{A}\\). 15.3 Writing our own PCA function using SVD In lecture we discussed the relationship between SVD and PCA. Let’s walk through some code that carries out PCA via SVD, and then we’ll implement our own PCA function. iris.numeric &lt;- select(iris, select=-Species) iris.ctrd &lt;- scale(iris.numeric, center=TRUE, scale=FALSE) iris.svd &lt;- svd(iris.ctrd) U &lt;- iris.svd$u S &lt;- diag(iris.svd$d) V &lt;- iris.svd$v pc.scores &lt;- U %*% S colnames(pc.scores) &lt;- str_c(&quot;PC&quot;, 1:ncol(pc.scores)) pc.scores.df &lt;- as.data.frame(pc.scores) pc.scores.df &lt;- pc.scores.df %&gt;% mutate(Species = iris$Species) ggplot(pc.scores.df, aes(PC1, PC2, color=Species)) + geom_point() + coord_fixed(ratio=1) n &lt;- nrow(iris.ctrd) pc.sdev &lt;- sqrt((S**2/(n-1))) pc.sdev ## [,1] [,2] [,3] [,4] ## [1,] 2.056269 0.0000000 0.0000000 0.0000000 ## [2,] 0.000000 0.4926162 0.0000000 0.0000000 ## [3,] 0.000000 0.0000000 0.2796596 0.0000000 ## [4,] 0.000000 0.0000000 0.0000000 0.1543862 V ## [,1] [,2] [,3] [,4] ## [1,] 0.36138659 -0.65658877 0.58202985 0.3154872 ## [2,] -0.08452251 -0.73016143 -0.59791083 -0.3197231 ## [3,] 0.85667061 0.17337266 -0.07623608 -0.4798390 ## [4,] 0.35828920 0.07548102 -0.54583143 0.7536574 For comparison, here’s what the builtin prcomp() function gives us: iris.pca &lt;- prcomp(iris.ctrd) iris.pca$sdev ## [1] 2.0562689 0.4926162 0.2796596 0.1543862 iris.pca$rotation ## PC1 PC2 PC3 PC4 ## Sepal.Length 0.36138659 -0.65658877 0.58202985 0.3154872 ## Sepal.Width -0.08452251 -0.73016143 -0.59791083 -0.3197231 ## Petal.Length 0.85667061 0.17337266 -0.07623608 -0.4798390 ## Petal.Width 0.35828920 0.07548102 -0.54583143 0.7536574 Now that we have a sense of the key calculations, let’s turn this into a function. # a user defined version of principal components analysis PCA &lt;- function(X, center=TRUE, scale=FALSE){ x &lt;- scale(X, center=center, scale=scale) n &lt;- nrow(x) p &lt;- ncol(x) x.svd &lt;- svd(x) U &lt;- x.svd$u S &lt;- diag(x.svd$d) V &lt;- x.svd$v # check for zero eigenvalues tolerance = .Machine$double.eps^0.5 has.zero.singval &lt;- any(x.svd$d &lt;= tolerance) if(has.zero.singval) print(&quot;WARNING: Zero singular values detected&quot;) pc.scores &lt;- U %*% S colnames(pc.scores) &lt;- str_c(&quot;PC&quot;, 1:ncol(pc.scores)) pc.sdev &lt;- diag(sqrt((S**2/(n-1)))) return(list(vectors = V, scores=pc.scores, sdev = pc.sdev)) } Note I also included some code to warn the user when the covariance matrix is singular. Use the help to read about variables defined in .Machine. Let’s put our function through its paces: iris.pca &lt;- PCA(iris.numeric) ggplot(as.data.frame(iris.pca$scores), aes(x = PC1, y = PC2)) + geom_point() + coord_fixed(ratio=1) # flip the rows and columns of i.sub and do PCA # Our function should detect singular values equal to zero. Why? singular.pca &lt;- PCA(t(iris.numeric)) ## [1] &quot;WARNING: Zero singular values detected&quot; tree.pca &lt;- PCA(trees) tree.pca$sdev ## [1] 17.1834214 4.9820035 0.7485858 # compare to prcomp prcomp(trees)$sdev ## [1] 17.1834214 4.9820035 0.7485858 To bring things full circle, let’s make sure that the covariance matrix we reconstruct from our PCA analysis is equal to the covariance matrix calculated directly from the data set: n &lt;- nrow(iris.numeric) V &lt;- iris.pca$vectors S &lt;- diag( sqrt(iris.pca$sdev**2 * (n-1)) ) # turn sdev&#39;s back into singular values reconstructed.cov &lt;- (1/(n-1)) * V %*% S %*% S %*% t(V) # see pg. 11 of slides all.equal(reconstructed.cov, cov(iris.numeric), check.attributes=FALSE) ## [1] TRUE Great! It seems like things are working as expected. 15.4 Writing a biplot function We can use the same basic framework above to create a function to compute a biplot for our PCA. The function below includes an argument emphasize.variables, If emphasize.variables is TRUE, the function returns the biplot corresponding to \\(\\alpha=0\\), the column-metric preserving biplot. If this arugment is FALSE it returns calculations correspoding to \\(\\alpha=1\\), the row-metric preserving biplot. # a user defined version of principal components analysis PCA.biplot &lt;- function(X, center=TRUE, scale=FALSE, emphasize.variables=TRUE){ x &lt;- scale(X, center=center, scale=scale) n &lt;- nrow(x) p &lt;- ncol(x) x.svd &lt;- svd(x) U &lt;- (x.svd$u)[,1:2] S &lt;- diag(x.svd$d[1:2]) V &lt;- x.svd$v[,1:2] if (emphasize.variables) { biplot.scores &lt;- sqrt(n) * U biplot.vectors &lt;- (1/sqrt(n)) * V %*% S } else { biplot.scores &lt;- U %*% S biplot.vectors &lt;- V } colnames(biplot.scores) &lt;- str_c(&quot;PC&quot;, 1:ncol(biplot.scores)) colnames(biplot.vectors) &lt;- str_c(&quot;PC&quot;, 1:ncol(biplot.vectors)) rownames(biplot.vectors) &lt;- colnames(X) list(scores=as.data.frame(biplot.scores), vectors=as.data.frame(biplot.vectors) %&gt;% mutate(Variable = colnames(X))) } Let’s first create a biplot that emphasize the relationships between the variables. iris.biplot.var &lt;- PCA.biplot(iris.numeric, emphasize.variables = TRUE) small.arrow = arrow(angle = 15, length = unit(0.15, &quot;inches&quot;)) p1 &lt;- ggplot() + geom_point(data=iris.biplot.var$scores %&gt;% mutate(Species = iris$Species), mapping=aes(x=PC1, y=PC2, color=Species)) + geom_segment(data=iris.biplot.var$vectors, mapping=aes(x=0, y=0, xend=PC1, yend=PC2), arrow=small.arrow, color=&#39;red&#39;) + geom_text(data=iris.biplot.var$vectors, mapping=aes(x=PC1 * 1.1, y=PC2*1.1, label=Variable), color=&#39;red&#39;) + coord_fixed() p1 The biplot we just created emphasizes information about the variables in our data, and their relationship to each other and the principal components. Notice that the angule beween Petal Width and Petal Length is very small – this indicates they are highly correlated, while Sepal Width is nearly uncorrelated with the previous two variables. Petal Width and Petal Length also approximately parallel to PC1, indicating that they have large loadings with the first PC. The Petal Length vector is the longest indicating that it has the largest standard deviation. This biplot however distorts the relationships among the objects (observations) – the scatter of scores in this biplot is not an optimal representation of the true distances between the observations in the original space. Now we create a biplot that emphasizes the relationships between the objects. iris.biplot.obj &lt;- PCA.biplot(iris.numeric, emphasize.variables = FALSE) p2 &lt;- ggplot() + geom_point(data=iris.biplot.obj$scores %&gt;% mutate(Species = iris$Species), mapping=aes(x=PC1, y=PC2, color=Species)) + geom_segment(data=iris.biplot.obj$vectors, mapping=aes(x=0, y=0, xend=PC1, yend=PC2), arrow=small.arrow, color=&#39;red&#39;) + geom_text(data=iris.biplot.obj$vectors, mapping=aes(x=PC1 * 1.1, y=PC2*1.1, label=Variable), color=&#39;red&#39;) + coord_fixed() p2 In this view, the points representing the weighted scores are a better representation of the relationships among the observations, but the geometry of the vectors representing the associations among the variables is no longer optimal. 15.5 Data compression and noise filtering using SVD Two common uses for singular value decomposition are for data compression and noise filtering. We’ll illustrate these with two examples involving matrices which represent image data. This example is drawn from an article by David Austin, found on a tutorial about SVD at the American Mathematical Society Website. 15.5.1 Example image The file zeros.dat from the course wiki. This is a \\(25 \\times 15\\) binary matrix that represents pixel values in a simple binary (black-and-white) image. zero &lt;- read_tsv(&#39;https://github.com/pmagwene/Bio723/raw/master/datasets/zero.dat&#39;, col_names=FALSE) head(zero) ## # A tibble: 6 x 15 ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 1 1 1 1 1 1 1 ## 2 1 1 1 1 1 1 1 1 1 1 1 1 ## 3 1 1 1 1 1 1 1 1 1 1 1 1 ## 4 1 1 1 1 1 1 1 1 1 1 1 1 ## 5 1 1 1 1 1 1 1 1 1 1 1 1 ## 6 1 1 0 0 0 0 0 0 0 0 0 0 ## # … with 3 more variables: X13 &lt;dbl&gt;, X14 &lt;dbl&gt;, X15 &lt;dbl&gt; # we&#39;ll use the image() function to visualize the matrix x &lt;- 1:15 y &lt;- 1:25 image(x, y, t(zero), xlim=c(1,15), ylim=c(1,25), col=c(&#39;black&#39;,&#39;white&#39;),asp=1) This data matrix can be thought of as being composed of just three types of vectors, each illustrated in the following figure: The three vector types in the “zero” matrix If SVD is working like expected it should capture that feature of our input matrix, and we should be able to represent the entire image using just three singular values and their associated left- and right-singular vectors. zero.svd &lt;- svd(zero) round(zero.svd$d,2) ## [1] 14.72 5.22 3.31 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## [12] 0.00 0.00 0.00 0.00 D &lt;- diag(zero.svd$d[1:3]) U &lt;- zero.svd$u[,1:3] V &lt;- zero.svd$v[,1:3] new.zero &lt;- U %*% D %*% t(V) all.equal(new.zero, as.matrix(zero), check.attributes=FALSE) ## [1] TRUE # and let&#39;s double check using the image() function image(x, y, t(new.zero), asp=1,xlim=c(1,15),ylim=c(1,25), col=c(&#39;black&#39;,&#39;white&#39;)) Our original matrix required \\(25 \\times 15\\) (\\(= 375\\)) storage elements. Using the SVD we can represent the same data using only \\(15 \\times 3 + 25 \\times 3 + 3 = 123\\) units of storage (corresponding to the truncated U, V, and D in the example above). Thus our SVD allows us to represent the same data with at less than \\(1/3\\) the size of the original matrix. In this case, because all the singular values after the 3rd were zero this is a lossless data compression procedure. The file noisy-zero.dat is the same ‘zero’ image, but now sprinkled with Gaussian noise draw from a normal distribution \\(N(0,0.1)\\). As in the data compression case we can use SVD to approximate the input matrix with a lower-dimensional approximation. Here the SVD is “lossy”\" as our approximation throws away information. In this case we hope to choose the approximating dimension such that the information we lose corresponds to the noise which is “polluting”\" our data. noisy.zero &lt;- read_tsv(&#39;https://raw.githubusercontent.com/pmagwene/Bio723/master/datasets/noisy-zero.dat&#39;,col_names = FALSE) x &lt;- 1:15 y &lt;- 1:25 # create a gray-scale representation of the matrix image(x, y, t(noisy.zero), asp=1, xlim=c(1,15), ylim=c(1,25), col=gray(seq(0,1,0.05))) Now we carry out SVD of the matrix representing this noisy image: noisy.svd &lt;- svd(noisy.zero) # as before the first three singular values dominate round(noisy.svd$d,2) ## [1] 13.63 4.87 3.07 0.40 0.36 0.31 0.27 0.26 0.21 0.19 0.13 ## [12] 0.11 0.09 0.06 0.04 nD &lt;- diag(noisy.svd$d[1:3]) nU &lt;- noisy.svd$u[,1:3] nV &lt;- noisy.svd$v[,1:3] approx.noisy.zero &lt;- nU %*% nD %*% t(nV) Finally we plot the approximation of this image based on the first three singular vectors/values: # now plot the original and approximating matrix side-by-side par(mfrow=c(1,2)) image(x, y, t(noisy.zero), asp=1,xlim=c(1,15),ylim=c(1,25),col=gray(seq(0,1,0.05))) image(x, y, t(approx.noisy.zero), asp=1,xlim=c(1,15),ylim=c(1,25),col=gray(seq(0,1,0.05))) As you can see from the images you created the approximation based on the approximation based on the SVD manages to capture the major features of the matrix and filters out much of (but not all) the noise. 15.6 Image compression using SVD in R We’ll illustrate another application of SVD – approximating a high dimensional data set using a lower dimensional approximation. We’ll apply this concept to image compression. R doesn’t have native support for common image files like JPEG and PNG. However, there are packages we can install that will allow us to read in such files and treat them as matrices. We’ll use a package called “imager” to work with images. Install the “imager” library via the standard installation mechanism Once imager is installed, download this picture of my dog Chester to your local filesystem and load it as follows: library(imager) # change the path to match the location you&#39;ve downloaded the iamge chester &lt;- load.image(&quot;~/Downloads/chesterbw.jpg&quot;) We can plot images loaded by imager plot(chester) Let’s explore the object returned by imager: dim(chester) ## [1] 605 556 1 1 typeof(chester) ## [1] &quot;double&quot; class(chester) ## [1] &quot;cimg&quot; &quot;imager_array&quot; &quot;numeric&quot; imager stores the image information in a multidimensional array that has a class of “cimg” (type ?cimg). To manipulate this information using SVD we’ll need to turn it into a conventional matrix first. chester.matrix &lt;- as.matrix(chester) Now we’ll use SVD to create a low-dimensional approximation of this image. img.svd &lt;- svd(chester) U &lt;- img.svd$u S &lt;- diag(img.svd$d) Vt &lt;- t(img.svd$v) U15 &lt;- U[,1:15] # first 15 left singular vectors S15 &lt;- S[1:15,1:15] # first 15 singular values Vt15 &lt;- Vt[1:15,] # first 15 right singular values, NOTE: we&#39;re getting rows rather than columns here approx15 &lt;- U15 %*% S15 %*% Vt15 plot(as.cimg(approx15)) # to plot the matrix as an image we cast it back to a cimg Above we created a rank 15 approximation to the rank 556 original image matrix. This approximation is crude (as judged by the visual quality of the approximating image) but it does represent a very large savings in space. Our original image required the storage of \\(605 \\times 556 = 336380\\) integer values. Our approximation requires the storage of only \\(15 \\times 556 + 15 \\times 605 + 15 = 17430\\) integers. This is a saving of roughly 95%. Of course, as with any lossy compression algorithm, you need to decide what is the appropriate tradeoff between compression and data loss for your given application. Finally, let’s look at the “error term” associated with our approximation, i.e. what we did not capture in the 15 singular vectors. img.diff &lt;- chester.matrix - approx15 nr &lt;- nrow(img.diff) nc &lt;- ncol(img.diff) plot(as.cimg(img.diff)) "],
["canonical-variates-analysis.html", "Chapter 16 Canonical Variates Analysis 16.1 Libraries 16.2 Discriminant Analysis in R 16.3 Estimating confidence regions for group means in CVA 16.4 Calculating the Within and Between Group Covariance Matrices", " Chapter 16 Canonical Variates Analysis Canonical Variates Analysis (CVA) is also referred to in the literature as “Linear Discrimination Analysis” (LDA). Confusingly, there is also a technique usualled called Canonical Correlation Analysis that is sometimes referred to as “Canonical Variates Analysis” in the literature. Canonical variate analysis is used for analyzing group structure in multivariate data. Canonical variate axes are directions in multivariate space that maximally separate (discriminate) the pre-defined groups of interest specified in the data. Unlike PCA, canonical variate axes are not, in general, orthogonal in the space of the original variables. 16.1 Libraries library(tidyverse) library(cowplot) library(broom) library(magrittr) 16.2 Discriminant Analysis in R The function lda(), found in the R library MASS, carries out linear discriminant analysis (i.e. canonical variates analysis). library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select lda.iris &lt;-lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, iris, prior=c(1,1,1)/3) lda.iris ## Call: ## lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, ## data = iris, prior = c(1, 1, 1)/3) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 5.006 3.428 1.462 0.246 ## versicolor 5.936 2.770 4.260 1.326 ## virginica 6.588 2.974 5.552 2.026 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 0.8293776 0.02410215 ## Sepal.Width 1.5344731 2.16452123 ## Petal.Length -2.2012117 -0.93192121 ## Petal.Width -2.8104603 2.83918785 ## ## Proportion of trace: ## LD1 LD2 ## 0.9912 0.0088 When using lda() we specify a formula, with the grouping variable on the left and the quantitative variables on which you want to bases the discriminant axes, on the left. The prior argument given in the lda() function call isn’t strictly necessary because by default the lda() function will assign equal probabilities among the groups. However I included this argument call to illustrate how to change the prior if you wanted. The output give some simple summary statistics for the group means for each of the variables and then gives the coefficients of the canonical variates. The `Proportion of trace’ output above tells us that 99.12% of the between-group variance is captured along the first discriminant axis. 16.2.1 Shorthand Formulae in R You’ve encountered the use of model formulae in R throughout the course. Relevant to our current example is a shorthand way for specifying multiple variables in a formula. In the example above we called the |lda()| function with a formula of the form: Species ~ Sepal.Length + Sepal.Width + .... Writing the names of all those variables is tedious and error prone and would be unmanageable if we were analyzing a data set with tens or hundreds of variables. Luckily we can use the shorthand name . to specify all other variables in the data frame except the variable on the left. For example, we can rewrite the lda() call above as: iris.lda &lt;- lda(Species ~ ., data = iris) 16.2.2 Working with the output of lda() The object returned by lda() is of class “lda” with a number of components (see ?lda for details): class(iris.lda) ## [1] &quot;lda&quot; names(iris.lda) ## [1] &quot;prior&quot; &quot;counts&quot; &quot;means&quot; &quot;scaling&quot; &quot;lev&quot; &quot;svd&quot; &quot;N&quot; ## [8] &quot;call&quot; &quot;terms&quot; &quot;xlevels&quot; The scaling component gives the coefficients of the CVA that we can use to calculate the “scores” of the observations in the space of the canonical variates. iris.lda$scaling ## LD1 LD2 ## Sepal.Length 0.8293776 0.02410215 ## Sepal.Width 1.5344731 2.16452123 ## Petal.Length -2.2012117 -0.93192121 ## Petal.Width -2.8104603 2.83918785 The columns LD1 and LD2 give the coffiecients, \\(\\bf{a}\\), that we can use in the formula \\(\\bf{y}_\\text{discrim} = \\bf{Xa}\\) iris.sub &lt;- iris %&gt;% dplyr::select(-Species) %&gt;% # drop Species column as.matrix # cast to matrix for calculations # calculate CV scores CVA.scores &lt;- iris.sub %*% iris.lda$scaling # create data frame with scores iris.CV &lt;- data.frame(CVA.scores) iris.CV$Species &lt;- iris$Species Having calculated the CVA scores we can now generate a plot: iris.cva.plot &lt;- ggplot(iris.CV, aes(x = LD1, y = LD2)) + geom_point(aes(color=Species, shape=Species), alpha=0.5) + labs(x = &quot;CV1&quot;, y = &quot;CV2&quot;) + coord_fixed(ratio=1) # keep the unit scaling of the plot fixed at 1 iris.cva.plot Since most of the between group variation is captured by CV1, a density plot is an alternative in this case: ggplot(iris.CV, aes(x = LD1)) + geom_density(aes(color=Species)) + labs(x = &quot;CV1&quot;) The density plot of CV1 makes it clear how well the first canonical variate does in separating the three groups. 16.3 Estimating confidence regions for group means in CVA I stated in lecture that for the canonical variate diagram we can estimate the \\(100(1-\\alpha)\\) confidence region for a group mean as a circle centered at the mean having a radius \\((\\chi^{2}_{\\alpha,r}/n_i)^{1/2}\\) where \\(r\\) is the number of canonical variate dimensions considered. Using similar reasoning the \\(100(1-\\alpha)\\) “tolerance regions” for the whole population is given by a hypersphere centered at the mean with radius \\((\\chi^{2}_{\\alpha,r})^{1/2}\\). These tolerance regions are the regions in the CVA space where we expect approximately \\(100(1-\\alpha)\\) percent of samples belong to a given group to be found. To calculate these confidence regions you could look up the appropriate value of the the \\(\\chi^2\\) distribution in a book of statistical tables, or we can use the |qchisq()| function which gives the inverse cumulative probability distribution for the \\(\\chi^2\\) function: chi2 = qchisq(0.05,2, lower.tail=FALSE) chi2 ## [1] 5.991465 CIregions.mean.and.pop &lt;- iris.CV %&gt;% group_by(Species) %&gt;% summarize(CV1.mean = mean(LD1), CV2.mean = mean(LD2), mean.radii = sqrt(chi2/n()), popn.radii = sqrt(chi2)) CIregions.mean.and.pop ## # A tibble: 3 x 5 ## Species CV1.mean CV2.mean mean.radii popn.radii ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.502 6.877 0.3462 2.448 ## 2 versicolor -3.930 5.934 0.3462 2.448 ## 3 virginica -7.888 7.174 0.3462 2.448 16.3.1 Drawing the CVA confidence regions Surprisingly, ggplot2 has no built-in functions for drawing circles, despite having geom_rect() and geom_polygon() functions. Instead we turn to a package called ggforce which provides a convenient geom_circle() function as well as a number of other useful extensions of ggplot. Install “ggforce” through the normal package installation mechanism and then load it. library(ggforce) We can then use ggforce::geom_circle() to draw confidence regions for the mean and population in our 2D CVA plot: iris.cva.plot2 &lt;- iris.cva.plot + geom_circle(data = CIregions.mean.and.pop, mapping = aes(x0 = CV1.mean, y0 = CV2.mean, r = mean.radii), inherit.aes = FALSE) + geom_circle(data = CIregions.mean.and.pop, mapping = aes(x0 = CV1.mean, y0 = CV2.mean, r = popn.radii), linetype = &quot;dashed&quot;, inherit.aes = FALSE) iris.cva.plot2 Let’s put the finishing touch on our plots by adding some color coded rug plots to the first CV axis. iris.cva.plot2 + geom_rug(aes(color=Species), sides = &quot;b&quot;) 16.4 Calculating the Within and Between Group Covariance Matrices The lda() function conveniently carries out the key steps of a canonical variates analysis for you. However, what if we wanted some of the intermediate matrices relevant to the analysis such as the within- and between group covariances matrices? The code below shows you how to calculate these: nobs &lt;- nrow(iris) ngroups &lt;- nlevels(iris$Species) # calculate deviations around grand mean tot.deviates &lt;- iris %&gt;% dplyr::select(-Species) %&gt;% # review the course notes on dplyr to remind # yourself about how the mutate_all() and funs() fxns work mutate_all(funs(. - mean(.))) %&gt;% as.matrix # Total SSQ and covariance matrix ssq.tot &lt;- t(tot.deviates) %*% tot.deviates cov.tot &lt;- ssq.tot/nobs # calculate deviations around group means win.deviates &lt;- iris %&gt;% group_by(Species) %&gt;% mutate_all(funs(. - mean(.))) %&gt;% ungroup %&gt;% dplyr::select(-Species) %&gt;% as.matrix ## `mutate_all()` ignored the following grouping variables: ## Column `Species` ## Use `mutate_at(df, vars(-group_cols()), myoperation)` to silence the message. # Within group SSQ and covariance ssq.win &lt;- t(win.deviates) %*% win.deviates cov.win &lt;- ssq.win/(nobs - ngroups) # Between group deviates btw.deviates &lt;- iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) %&gt;% dplyr::select(-Species) %&gt;% mutate_all(funs(. - mean(.))) %&gt;% as.matrix # Between group SSQ and covariance ssq.btw &lt;- ngroups * t(btw.deviates) %*% btw.deviates cov.btw &lt;- ssq.btw/(ngroups-1) 16.4.1 Recapitulating the CVA analysis of lda() If we wanted to recapitulate the calculations that the lda() function carries out, we can do so based on the within- and between-group covariance matrices we estimated in the previous code block: # Cacluate the eigenvectors of W^{-1}B WinvB = solve(cov.win) %*% cov.btw eigin.WinvB = eigen(WinvB) cva.vecs &lt;- Re(eigin.WinvB$vectors)[,1:ngroups-1] cva.vals &lt;- Re(eigin.WinvB$values)[1:ngroups-1] unscaled.scores &lt;- win.deviates %*% cva.vecs # figure out scaling so group covariance matrix is spherical scaling &lt;- diag(1/sqrt((t(unscaled.scores) %*% unscaled.scores)/(nobs-ngroups))) # compare to &quot;scaling&quot; component object returned by lda() scaled.cva.vecs &lt;- cva.vecs %*% diag(scaling) cva.scores &lt;- iris.sub %*% scaled.cva.vecs colnames(cva.scores) &lt;- c(&quot;CV1&quot;,&quot;CV2&quot;) cva.scores &lt;- as.data.frame(cva.scores) cva.scores$Species &lt;- iris$Species Let’s plot the set of CVA scores that we calculated “by hand” to visually confirm our analysis produced similar results to the lda() function: ggplot(cva.scores, aes(x = CV1, y = CV2)) + geom_point(aes(color=Species, shape=Species)) + coord_fixed(ratio=1) Note that the CVA ordination above is “flipped” left-right relative to our earlier CVA figures. Canonical variates, like principal components, are identical with respect to reflection. "],
["clustering-in-r.html", "Chapter 17 Clustering in R 17.1 Libraries 17.2 Data set 17.3 Hierarchical Clustering in R 17.4 Manipulating hierarchical clusterings with dendextend 17.5 Plotting dendrograms in dendextend 17.6 Cutting dendrograms 17.7 Looking at clusters 17.8 Generating a heat map from a cluster 17.9 Working with sub-trees 17.10 Setting dendrogram parameters in dendextend 17.11 Combining heatmaps and dendrograms 17.12 K-means/K-medoids Clustering in R 17.13 Combining clusters and correlation matrix heatmaps 17.14 Depicting the data within clusters", " Chapter 17 Clustering in R 17.1 Libraries library(tidyverse) library(RColorBrewer) 17.2 Data set To illustrate clustering method, we’ll use a subset of the Spellman et al. gene expression data set we introduced in the Data Wrangling chapter. Recall that, Spellman and colleagues tried to identify all the genes in the yeast genome (&gt;6000 genes) that exhibited oscillatory behaviors suggestive of cell cycle regulation. To do so, they measured gene expression over time, using six different types of cell cycle synchronization experiments. Rather than working with the whole genome, we’re going to focus on clustering ~700 genes that Spellman and colleagues identified as oscillatory. This data set can be loaded from this link. spellman &lt;- read_csv(&quot;https://github.com/Bio723-class/example-datasets/raw/master/spellman-wide.csv&quot;) dim(spellman) ## [1] 73 726 This data is provided in a tidy “wide” format with genes in columns. The first two columns give the experiment name and the corresponding time point at which the sample was collected. Let’s confirm that the case by looking at the first few rows and columns of the data: spellman[1:5,1:8] ## # A tibble: 5 x 8 ## expt time YAL022C YAL040C YAL053W YAL067C YAR003W YAR007C ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 alpha 0 -0.36 1.04 0.21 0.01 -0.3 -0.48 ## 2 alpha 7 -0.42 0.19 -0.2 0.07 -0.45 -0.42 ## 3 alpha 14 0.290 0.47 0.2 0.17 0.75 0.87 ## 4 alpha 21 -0.14 -1.03 0.15 NA 0.37 0.92 ## 5 alpha 28 -0.19 -0.63 0.38 -0.1 0.27 0.67 17.3 Hierarchical Clustering in R Hierarchical clustering in R can be carried out using the hclust() function. The method argument to hclust determines the group distance function used (single linkage, complete linkage, average, etc.). The input to hclust() is a dissimilarity matrix. The function dist() provides some of the basic dissimilarity measures (e.g. Euclidean, Manhattan, Canberra; see method argument of dist) but you can convert an arbitrary square matrix to a distance object by applying the as.dist function to the matrix. We’re primarily interested in clustering the variables of our data set – genes – in order to discover what sets of gene are expressed in similar patterns (motivated by the idea that genes that are expressed in a similar manner are likely regulated by the same sets of transcription factors). So we need an appropriate similarity/dissimilarity measure for variables. The correlation coefficient is a suitable measure of linear association between variables. Correlations range from 1 for perfectly correlated variables to -1 for anti-correlated variables. Uncorrelated variables have values near zero. Correlation is a measure of similarity so we’ll turn it to into a measure of dissimilarity before passing it to the as.dist function. spellman.cor &lt;- dplyr::select(spellman, -time, -expt) %&gt;% cor(use=&quot;pairwise.complete.obs&quot;) spellman.dist &lt;- as.dist(1 - spellman.cor) The use argument to the cor function specifies that when there are missing values, the function should use all the available observations that have data necessary to calculate a correlation for any given pair of genes. We then turn the correlation into a distance measure by subtracting it from 1 (so perfectly positively correlated variables have distance 0) and passing it to the as.dist function. We’re now ready to put the hclust function to use. We first generate the hierarchical clustering, use the “complete linkage” method (see lecture slides): spellman.tree &lt;- hclust(spellman.dist, method=&quot;complete&quot;) Having generated the tree object, we can plot it using the multipurpose plot() function (Note that plot() is part of the base R graphics package, and hence unrelated to ggplot): plot(spellman.tree) Ugh - that’s an ugly plot! One major problem is that the text labels at the bottom of the tree are too large, and they’re overlapping each other. We can tweak that a little by changing the text size with the cex parameter. plot(spellman.tree, cex=0.2) That’s a little better, but we’re going to need some additional tools to wrangle this dendrogram into a more usable state. 17.4 Manipulating hierarchical clusterings with dendextend To work with and manipulate hierarchical clusterings and to create nicer dendrograms we’re going to use a package called dendextend. Install dendextend via one of the standard package installation mechanisms before proceeding. library(dendextend) First we’ll create a dendrogram object from our clustering tree, and use some dendextend features to examine a few of the properties of the dendrogram. spellman.dend &lt;- as.dendrogram(spellman.tree) # create dendrogram object dendextend includes a number of functions for examing the tree. For example, to examine the number of “leaves” (= # of genes we clustered) or nodes (= # of leaves + number of internal joins) in the tree we can do the following: nleaves(spellman.dend) # number of leaves in tree ## [1] 724 nnodes(spellman.dend) # number of nodes (=leaves + joins) in tree ## [1] 1447 17.5 Plotting dendrograms in dendextend The plot function for dendextend dendrogram objects (see ?plot.dendrogram) has a number of additional parameters that allows us to tweak the plot. For example, for large dendrograms it often makes sense to remove the leaf labels entirely as they will often be too small to read. This can be accomplished using the leaflab argument: plot(spellman.dend, leaflab = &quot;none&quot;) 17.6 Cutting dendrograms When looking at the figure we just generated it looks like there may be four major clusters. We’ll use the cutree function to cut the tree into four pieces and examine the implied clustering (note that the cutree function can also be used to cut the tree at a specified height). clusters &lt;- cutree(spellman.dend, k=4) table(clusters) ## clusters ## 1 2 3 4 ## 189 144 227 164 When we cut the tree we got four clusters, whose size is given by the table command above. If you examine the cutree documentation (reminder: use ?cutree from the command line) you will see that it returns a vector of integers, giving the corresponding cluster to which each variable (gene) is assigned. The code below shows the cluster assignments for the first six genes. clusters[1:6] ## YAL022C YAL040C YAL053W YAL067C YAR003W YAR007C ## 1 2 3 4 3 3 Next let’s create a nicer plot that highlights each of the clusters. The function color_branches() does essentially the same thing as cutree but it returns information that the plot function can use to appropriately color branches of the tree according to cluster membership. plot(color_branches(spellman.dend, k=4),leaflab=&quot;none&quot;) Now we’re getting somewhere! However, our clusters are still pretty big. Let’s check out the clusterings we get when we cut with eight clusters rather than four. plot(color_branches(spellman.dend, k=8),leaflab=&quot;none&quot;) And here are the number of genes in each cluster: clusters &lt;- cutree(spellman.dend, k=8, order_clusters_as_data = FALSE) table(clusters) ## clusters ## 1 2 3 4 5 6 7 8 ## 164 169 58 81 108 74 16 54 17.7 Looking at clusters To further explore the clusters, let’s create a data frame that holds the information about genes and their cluster assignments: clusters.df &lt;- data.frame(gene = names(clusters), cluster = clusters) Having created this data frame, it’s straightforward to lookup the cluster to which a gene belongs: clusters.df[&quot;YAL022C&quot;,] ## # A tibble: 1 x 2 ## gene cluster ## &lt;fct&gt; &lt;int&gt; ## 1 YAL022C 5 or to get all the names of genes in a given cluster: cluster3.genes &lt;- filter(clusters.df, cluster == 3)$gene cat(as.character(cluster3.genes[1:10]), quote=FALSE,sep=&quot;\\n&quot;); ## YEL068C ## YOR242C ## YKL172W ## YBR087W ## YJR043C ## YLR154C ## YGR276C ## YGR042W ## YPL232W ## YOR355W ## FALSE Note the use of the cat function to print out a list of the gene names for cluster 7, with the names separated by returns (\"\\n\"). This is useful if you want to cut and paste the gene names into a document, or an online analysis tool such as various Gene Ontology (GO) browsers (we’ll talk about these in a later class session). 17.8 Generating a heat map from a cluster Let’s generate a heat map showing the expression of all the genes in the alpha factor experiment for the first cluster that we found above. To generate this visualization it will be convenient to work with the data in a tidy long format, so we use dplyr::gather to restructure the data first: spellman.long &lt;- gather(spellman, gene, expression, -time, -expt) head(spellman.long) ## # A tibble: 6 x 4 ## expt time gene expression ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 alpha 0 YAL022C -0.36 ## 2 alpha 7 YAL022C -0.42 ## 3 alpha 14 YAL022C 0.290 ## 4 alpha 21 YAL022C -0.14 ## 5 alpha 28 YAL022C -0.19 ## 6 alpha 35 YAL022C -0.52 Having restructured the data we can now generate our desired heat map: color.scheme &lt;- rev(brewer.pal(8,&quot;RdBu&quot;)) # generate the color scheme to use spellman.long %&gt;% filter(gene %in% cluster3.genes &amp; expt == &quot;alpha&quot;) %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme, limits = c(-2,2)) + theme(axis.text.y = element_text(size = 6)) # set size of y axis labels 17.9 Working with sub-trees The cutree() function illustrated above gives us the groupings implied by cutting the tree at a certain height. However, it does not explicitly return objects representing the sub-trees themselves. If you want to do computations or generate figures of the sub-trees, you’ll need to use the cut() function. # note that I determined the height to cut at by looking at the colored dendrogram # plot above for 8 clusters sub.trees &lt;- cut(spellman.dend, h = 1.48) The cut function returns multiple sub-trees designated upper and lower. The upper tree is the tree “above” the cut, while the multiple “lower” trees represent the disconnected sub-trees “below” the cut. For purposes of clustering you usually are most interested in the sub-trees (clusters) below the cut. sub.trees$lower ## [[1]] ## &#39;dendrogram&#39; with 2 branches and 164 members total, at height 1.469983 ## ## [[2]] ## &#39;dendrogram&#39; with 2 branches and 169 members total, at height 1.178098 ## ## [[3]] ## &#39;dendrogram&#39; with 2 branches and 58 members total, at height 1.32836 ## ## [[4]] ## &#39;dendrogram&#39; with 2 branches and 81 members total, at height 1.231597 ## ## [[5]] ## &#39;dendrogram&#39; with 2 branches and 108 members total, at height 1.366951 ## ## [[6]] ## &#39;dendrogram&#39; with 2 branches and 74 members total, at height 1.463623 ## ## [[7]] ## &#39;dendrogram&#39; with 2 branches and 16 members total, at height 1.041689 ## ## [[8]] ## &#39;dendrogram&#39; with 2 branches and 54 members total, at height 1.188419 We can retrieve any particular sub-tree by indexing into the list: cluster3.tree &lt;- sub.trees$lower[[3]] cluster3.tree ## &#39;dendrogram&#39; with 2 branches and 58 members total, at height 1.32836 nleaves(cluster3.tree) ## [1] 58 17.10 Setting dendrogram parameters in dendextend dendextend has a generic function – set() – for changing the parameters associated with dendrograms. The basic form of the function is set(object, what, value), where object is the dendrogram you’re working with, what is a character string indicating the parameter you want to change, and value is the setting you wishing to assign to that parameter. A full list of dendrogram parameters that can be changed is provided in the dendextend documentation. We’ll use the set function to make a nice diagram of our cluster 3 sub-tree: cluster3.tree %&gt;% set(&quot;labels_cex&quot;, 0.45) %&gt;% set(&quot;labels_col&quot;, &quot;red&quot;) %&gt;% plot(horiz = TRUE) # plot horizontally 17.11 Combining heatmaps and dendrograms A common visualization used in transcriptome studies is to combine dendrograms and heatmaps. To do this with a minimum of fuss we’ll use a package called “gplots” which includes a heatmap function that will also plot dendrograms next. Install “gplots” via the standard package installation mechanism. library(gplots) The gplots function we will use is called heatmap.2. This function requires as input our data in the form of a matrix. If you provide no other information heatmap.2 will carry out clustering for you, clustering both the rows and columns of the data matrix. However, here we want to draw a dendrogram (representing similarity among variables) we’ve already calculated, and to create a heat map just for the alpha factor data, so we need to do some pre-calculations and tweak the heatmap.2 arguments: # subset out the alpha factor data alpha.factor &lt;- filter(spellman, expt == &quot;alpha&quot;) # create matrix after dropping time and expt columns alpha.mtx &lt;- as.matrix(dplyr::select(alpha.factor, -time, -expt)) # drop time, expt columns # set row names to corresponding time points for nice plotting row.names(alpha.mtx) &lt;- alpha.factor$time # transpose the matrix so genes are drawn in rows transposed.alpha.mtx &lt;- t(alpha.mtx) Having defined the data we want to plot in the heatmap we can now use heatmap.2 as follows # this is a large figure, so if working in RMarkdown document I suggest specifying # the code block header as so to make the figure large # {r, fig.width = 8, fig.height = 8} heatmap.2(transposed.alpha.mtx, Rowv = cluster3.tree, # use the dendrogram previously calculated Colv = NULL, # don&#39;t mess with my columns! (i.e. keep current ordering ) dendrogram = &quot;row&quot;, # only draw row dendrograms breaks = seq(-2, 2, length.out = 9), # OPTIONAL: set break points for colors col = color.scheme, # use previously defined colors trace = &quot;none&quot;, density.info = &quot;none&quot;, # remove distracting elements of plot xlab = &quot;Time (mins)&quot;) 17.12 K-means/K-medoids Clustering in R The kmeans() function calculates standard k-means clusters in R. However, we’re actually going to use a related function that calculates “k-medoids” clustering. K-medoids clustering differs from k-means in that the objective function is to minimize the sum of dissimilarities from the cluster centers (“medoids”) rather then the sum of squared distances. K-medoids clustering tends to be more robust to outliers than K-means. Another advantage for our purposes is that the k-medoids algorithm, unlike the standard implementation of k-means, can accept a distance or dissimilarity matrix as input. K-medoids clustering is implemented in the function pam() (Partitioning Around Medoids), which is found in a package called cluster that is included with the standard R installation. library(cluster) spellman.kmedoids &lt;- pam(spellman.dist, 8) # create k-medoids clustering with 8 clusters kclusters &lt;- spellman.kmedoids$cluster table(kclusters) ## kclusters ## 1 2 3 4 5 6 7 8 ## 121 104 70 95 85 94 65 90 K-medoids (or K-means) will, in general, find different clusterings than our hierarchical clustering illustrated previously. For comparison with our earlier hierarchical clustering results, lets plot the k-medoids inferred clusters back onto our earlier dendrogram. # reorder genes so they match the dendrogram kclusters.reordered &lt;- kclusters[order.dendrogram(spellman.dend)] # get branch colors so we&#39;re using the same palette dend.colors &lt;- unique(get_leaves_branches_attr(color_branches(spellman.dend, k=8), attr=&quot;col&quot;)) # color the branches according to their k-means cluster assignment plot(branches_attr_by_clusters(spellman.dend, kclusters.reordered , dend.colors),leaflab=&quot;none&quot;) Comparing the inferred k-medoids clustering to our previous complete linkage clustering we see some clusters that are similar between the two, but there are also significant differences. 17.12.1 Heat map from k-medoids cluster In the same manner we generated a heat map for one of the hierarchical clustering sub-trees, we can generate a similar heat map for a k-medoids cluster. Let’s examine cluster 4: kcluster4.genes &lt;- names(kclusters[kclusters == 4]) spellman.long %&gt;% filter(gene %in% kcluster4.genes &amp; expt == &quot;alpha&quot;) %&gt;% ggplot(aes(x = time, y = gene)) + geom_tile(aes(fill = expression)) + scale_fill_gradientn(colors=color.scheme, limits=c(-2,2)) + labs(title = &quot;K-medoids Clustering of Spellman et al. Data\\nCluster 4&quot;) + theme(axis.text.y = element_text(size = 6)) # set size of y axis labels 17.13 Combining clusters and correlation matrix heatmaps How do we know if we have a sensible clustering? For example, above we illustrated how to use complete linkage clustering and the k-medoids algorithm to produce two different clusterings of the same data. We produce a result with 8 clusters for each, but there were significant differences in terms of the membership of genes in each cluster. Which one of these methods produced a “better” result for the data in hand? There are many criteria – biological, mathematical, computational – one might apply to answer such a question, but for now let’s consider the idea that a good clustering is one that produces “natural groups” in data. In the lecture slides , I proferred the following “common sense” definition of natural groups: Natural Groups Groups of objects where the similarity between objects is higher within groups than between groups. How might we evaluate a proposed clustering with respect to this definition of natural groups? One way to do so is the examine our matrix of similarity according to the implied clusters to see if the clusters are consistent with high within group similarity and low between group similarity. In the clustering applications we’ve looked at so far, our measure of similarity has been based on correlations. We’ll first look at the “raw” correlation matrix, unsorted with respect to the implied clusterings, and then we’ll take a look at the correlation matrix sorted by hierarchical clustering and then k-medoids clustering. We can use the heatmap.2() function to visualize the correlation matrix. color.scheme &lt;- rev(brewer.pal(10,&quot;RdBu&quot;)) # generate the color scheme to use heatmap.2(spellman.cor, Rowv = NULL, # don&#39;t cluster rows Colv = NULL, # don&#39;t cluster columns dendrogram = &quot;none&quot;, # don&#39;t draw dendrograms trace = &quot;none&quot;, density.info = &quot;none&quot;, col = color.scheme, keysize = 1, labRow = FALSE, labCol = FALSE, xlab = &quot;genes&quot;, ylab = &quot;genes&quot;) A correlation matrix is a square symmetric matrix. The dark red line down the diagonal represents correlations of genes with themselves (i.e. perfectly correlated). Off diagonal elements range from blue (negative correlations) to gray (near zero correlations) to red (positive correlations). 17.13.1 Hierarchical clustering, visualized on correlation matrix Now let’s re-generate the heatmap of the correlation matrix with the genes sorted by our hierarchical clustering: heatmap.2(spellman.cor, Rowv = ladderize(spellman.dend), Colv = ladderize(spellman.dend), dendrogram = &quot;both&quot;, revC = TRUE, # rev column order of dendrogram so conforms to natural representation trace = &quot;none&quot;, density.info = &quot;none&quot;, col = color.scheme, key = FALSE, labRow = FALSE, labCol = FALSE) This reordered correlation matrix (and accompanying dendrograms) are useful for a number of purposes. First, the block structure along the diagonal indicates the correlation structure within the implied clusters. Uniformity in sign and magnitude of the on-diagongal blocks is an indicator of “within cluster” similarity. The off-diagonal blocks indicate the relationship between clusters. For example, the four “major” clusterings implied by the dendrogram are apparent in the block structure of the correlation matrix, but we also see that the two clusters at the bottom of the figure show some patches of weak or even negative correlations, suggesting that those may not be natural clusters. Also apparent are relationships between clusters – for example we see that genes in the first cluster (upper left of diagram) share strong positive correlations with many genes in the right half of the second cluster. The same relationship, though to a lesser extant, is apparent between clusters one and four. 17.13.2 K-medoids clustering visualized on the correlation matrix Let’s do a similar sorting of the correlation matrix based on k-medoids clustering. Since there is no dendrogram to pass to the heatmap.2 function, we’ll sort the correlation matrix ourselves by indexing on the output of the order function applied to the cluster assignments (read the order function help for more info). # reorder correlation matrix by ordering given by clustering kmedoids.cor &lt;- spellman.cor[order(kclusters), order(kclusters)] Having reordered the correlation matrix by the k-clusters, we can again use the heatmap.2 function to visualize the results. heatmap.2(kmedoids.cor, Rowv = NULL, Colv = NULL, dendrogram = &quot;none&quot;, trace = &quot;none&quot;, density.info = &quot;none&quot;, col = color.scheme, key = FALSE, labRow = FALSE, labCol = FALSE) The diagram has the same interpretation as the previous figure. However, here we notice that the clusters seem to be more internally consistent, as evidence by the greater within cluster uniformity of correlations. Positive and negative relationships between clusters are also readily apparent in this figure. 17.14 Depicting the data within clusters As a final step we’ll generate a figure depicting the time series behavior of genes in different clusters. Overlain over the individual gene expression time series, we’ll overlay the average time series for each cluster. I illustrate this with clusters generated by hierarchical clustering. This is easily adapted to clusterings generated by other methods. Create a data frame holding cluster membership for each gene. clusters.df &lt;- data.frame(gene = names(kclusters), cluster = as.factor(kclusters)) For the purposes of easy plotting in ggplot, we want to combine the information about cluster assignment to the original data. However the cluster assignments and corresponding names aren’t necessarily in the same order as our original data frame. We’ll use a “left join” to combine the two data frames, matching on the “gene” column: # do a left_join, combining the information in spellman.long # with clusters.df (matched on gene). This effectively adds the # cluster information as a new column to spellman.long data frame # keeping the appropriate matches by gene name alpha.long &lt;- spellman.long %&gt;% filter(expt == &quot;alpha&quot;) %&gt;% left_join(clusters.df, by = c(&quot;gene&quot;)) Next we calculate cluster means: # calculate the mean at each time point within each cluster cluster.means &lt;- alpha.long %&gt;% group_by(cluster, time) %&gt;% summarize(mean.exp = mean(expression, na.rm = TRUE)) And finally we plot the time series data for each cluster, with the clusters means overlain: # draw a figure showing time varying gene expression # in each cluster, overlain with the each clusters # mean time series alpha.long %&gt;% ggplot(aes(time, expression, group=gene)) + geom_line(alpha=0.25) + geom_line(aes(time, mean.exp, group=NULL,color=cluster), data = cluster.means, size=1.1) + ylim(-2.5, 2.5) + facet_wrap(~cluster, ncol=4) To my eye, oscillatory behavior is fairly apparent in most of the clusters except cluster 7. "],
["simulation-i-sampling-distributions.html", "Chapter 18 Simulation I: Sampling distributions 18.1 Sampling distrubtions 18.2 Simulating sampling from an underlying population 18.3 Seeding the pseudo-random number generator 18.4 Libraries 18.5 Random sampling from the simulated population 18.6 Simulated sampling distribution of the mean 18.7 Standard Error of the Mean 18.8 Sampling Distribution of the Standard Deviation 18.9 What happens to the sampling distribution of the mean and standard deviation when our sample size is small?", " Chapter 18 Simulation I: Sampling distributions In our final two class sessions we will explore how we can use R to carry out statistical simulations. Such simulations can be useful for a variety of purposes, such as understanding classical results in statistical inference, deriving novel results for unusual statistics or underlying distributions, or to help design experiments. More generally, simulation is a useful tool for honing our intuition or revealing key properties about complex systems. In this chapter we will focus on how we can use simulation to derive well known and not so well known results regarding sampling distributions for a variety of statistics. 18.1 Sampling distrubtions Usually when we collect biological data, it’s because we’re trying to learn about some underlying “population” of interest. Population here could refer to an actual population (e.g. all males over 20 in the United States; brushtail possums in the state of Victoria, Australia), an abstract population (e.g. corn plants grown from Monsanto “round up ready” seed; yeast cells with genotypes identical to the reference strain S288c), outcomes of a stochastic process we can observe and measure (e.g. meiotic recombination in flies; rainfall per square meter in the Brazilian amazon), etc. It is often impractical or impossible to measure all objects/individuals/instances of a population of interest, so we take a sample (ideally a random sample) from the population and make measurements on the variables of interest in that sample. We do so with the hope that the various statistics we calculate on the variables of interest in that sample will be useful estimates of those same statistics in the underlying population. However, we must always keep in mind that the statistics we calculate from our sample will almost never exactly match those of the underlying population. That is when we collect a sample, and measure a statistic (e.g. mean or standard deviation or skew or…) on variable \\(X\\) in the sample, there is a degree of uncertainty about how well our estimate matches the true value of that statistic in the underlying population. We know that if we took another random sample from the same population, and re-calculated the statistic of interest, we’re likely to obtain an estimate that differs from that in our first sample. We can imagine repeating this procedure again and again a large number of times, recording our estimate each time which would yield a new distribution, which we’ll call a sampling distribution. A sampling distribution is the probability distribution of a given statistic for samples of a given size. Sampling distributions are a linchpin at the heart of statistical inference, how we quantify the uncertainty associated with statistics and use that information to test hypotheses and evaluate models. Traditionally sampling distributions were derived analytically. In this class session we’ll see how to approximate sampling distributions for any statistic using computer simulation. 18.2 Simulating sampling from an underlying population To illustrate the concept of sampling distributions, we’ll simulate drawing samples for an underlying population that we’re trying to estimate statistics about. This will allow us to compare the various statistics we calculate and their sampling distributions to their “true” values. Let’s simulate a population consisting of 25,000 individuals with a single trait of interest – height (measured in centimeters). We will simulate this data set based on information about the distribution of the heights of adult males in the US from a study carried out from 2011-2014 by the US Department of Health and Human Services1. # male mean height and sd in centimeters from USDHHS report true.mean &lt;- 175.7 true.sd &lt;- 15.19 18.3 Seeding the pseudo-random number generator When carrying out simulations, we employ random number generators (e.g. to choose random samples). Most computers can not generate true random numbers – instead they use algorithms that approximate the generation of random numbers (pseudo-random number generators). One important difference between a true random number generator and a pseudo-random number generator is that we can regenerate a series of pseudo-random numbers if we know the “seed” value that initialized the algorithm. We can specifically set this seed value, so that we can guarantee that two different people evaluating this notebook get the same results, even though we’re using (pseudo)random numbers in our simulation. # make our simulation repeatable by seeding RNG set.seed(20190409) 18.4 Libraries library(tidyverse) library(magrittr) library(stringr) 18.4.1 Properties of the underlying population Heights in human populations are approximately normally distributed, so we’ll assume that the distribution of our simulated variable is also normally distributed. Let’s take a moment to visualize the probability distribution of of a normal distribution with a mean and standard deviation as given above. pop.distn &lt;- data_frame(height = seq(100, 250, 0.5), density = dnorm(height,mean = true.mean, sd = true.sd)) ggplot(pop.distn) + geom_line(aes(height, density)) + # vertical line at mean geom_vline(xintercept = true.mean, color=&quot;red&quot;, linetype=&quot;dashed&quot;) + # vertical line at mean + 1SD geom_vline(xintercept = true.mean + true.sd, color = &quot;blue&quot;, linetype=&quot;dashed&quot;) + # vertical line at mean - 1SD geom_vline(xintercept = true.mean - true.sd, color = &quot;blue&quot;, linetype=&quot;dashed&quot;) + labs(x = &quot;Height (cm)&quot;, y = &quot;Density&quot;, title = &quot;Distribution of Heights in the Population of Interest&quot;, subtitle = &quot;Red and blue lines indicate the mean \\nand ±1 standard deviation respectively.&quot;) 18.4.2 Other R functions related to the normal distribution As shown above the dnorm() function calculates the probability density at given values of a variable x, given the specified mean and standard deviation. pnorm() gives the cumulative density function (also known as the distribution function) for the normal distribution, as shown below: cdf &lt;- data_frame(height = seq(100, 250, 0.5), cum.prob = pnorm(height, true.mean, true.sd)) ggplot(cdf) + geom_line(aes(height, cum.prob)) + labs(x = &quot;Height&quot;, y = &quot;Cumulative probability&quot;) qnorm() is the quantile function for the normal distribution. The input is the probabilities of interest (single value or vector), and the mean and standard deviation of the distribution. The output is the corresponding value of the variable corresponding to the given percentiles. For example, to estimate the lower 30th percentile of heights in adult males in the US we can use qnorm() as follows: qnorm(0.3, true.mean, true.sd) ## [1] 167.7344 Using qnorm we find that in a population where height ~ \\(N(175.7, 15.19)\\), 167.7cm is the approximate cutoff for the lower 30th percentile. We can illustrate that as shown below: library(glue) # note the use of the glue library (part of tidyverse) # to enable string literals as used in the title below perc.30 &lt;- qnorm(0.3, true.mean, true.sd) label.offset &lt;- 18 # determined by trial and error to make a # nice looking figure heights.less.perc.30 &lt;- seq(100, perc.30, by=0.5) density.less.perc.30 &lt;- dnorm(heights.less.perc.30, true.mean, true.sd) ggplot(pop.distn) + geom_line(aes(x = height, y = density)) + geom_vline(xintercept = perc.30, linetype=&#39;dashed&#39;) + geom_area(aes(x = heights.less.perc.30, y = density.less.perc.30), fill = &quot;gray&quot;, data = data_frame(x = heights.less.perc.30)) + annotate(&quot;text&quot;, x = perc.30 - label.offset, y = 0.025, label = &quot;30th percentile&quot;, color = &#39;red&#39;) + labs(title = glue(&quot;Probability distribution as calculated by dnorm()\\nand the 30th percentile as calculated by qnorm()\\nfor a normal distribution ~N({true.mean},{true.sd}).&quot;)) 18.5 Random sampling from the simulated population Let’s simulate the process of taking a single sample of 30 individuals from our population, using the rnorm() function which takes samples if size n from a normal distribution with the given mean and standard deviation: sample.a &lt;- data_frame(height = rnorm(n = 30, mean = true.mean, sd = true.sd)) Now we’ll create a histogram of the height variable in our sample. For reference we’ll also plot the probability for the true population (but remember, in the typical case you don’t know what the true population looks like) sample.a %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = ..density..), fill = &#39;steelblue&#39;, alpha=0.75, bins=10) + geom_line(data=pop.distn, aes(x = height, y = density), alpha=0.25,size=1.5) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;, color=&quot;red&quot;) + geom_vline(xintercept = mean(sample.a$height), linetype = &quot;solid&quot;) + labs(x = &quot;Height (cm)&quot;, y = &quot;Density&quot;, title = &quot;Distribution of heights in the underlying population (line)\\nand a single sample of size 30 (blue)&quot;) The dashed vertical line represent the true mean of the population, the solid line represents the sample mean. Comparing the two distributions we see that while our sample of 30 observations is relatively small,its location (center) and spread that are roughly similar to those of the underlying population. Let’s create a table giving the estimates of the mean and standard deviation in our sample: sample.a %&gt;% summarize(sample.mean = mean(height), sample.sd = sd(height)) ## # A tibble: 1 x 2 ## sample.mean sample.sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 175.5 12.41 Based on our sample, we estimate that the mean height of males in our population of interest is 175.5215685cm with a standard deviation of 12.4101548cm. 18.5.1 Another random sample Let’s step back and think about our experiment. We took a random sample of 30 individuals from the population. The very nature of a “random sample” means we could just as well have gotten a different collection of individuals in our sample. Let’s take a second random sample of 25 individuals and see what the data looks like this time: sample.b &lt;- data_frame(height = rnorm(30, mean = true.mean, sd = true.sd)) sample.b %&gt;% ggplot(aes(x = height)) + geom_histogram(aes(y = ..density..), fill = &#39;steelblue&#39;, alpha=0.75, bins=10) + geom_line(data=pop.distn, aes(x = height, y = density), alpha=0.25,size=1.5) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;, color=&quot;red&quot;) + geom_vline(xintercept = mean(sample.a$height), linetype = &quot;solid&quot;) + labs(x = &quot;Height (cm)&quot;, y = &quot;Density&quot;, title = &quot;Distribution of heights in the underlying population (line)\\nand a single sample of size 30 (blue)&quot;) sample.b %&gt;% summarize(sample.mean = mean(height), sample.sd = sd(height)) ## # A tibble: 1 x 2 ## sample.mean sample.sd ## &lt;dbl&gt; &lt;dbl&gt; ## 1 175.7 17.18 This time we estimated the mean height to be 175.651398 cm and the standard deviation to be 17.1780732 cm. 18.5.2 Simulating the generation of many random samples When we estimate population parameters, like the mean and standard deviation, based on a sample, our estimates will differ from the true population values by some amount. For any given sample we can’t know how close our estimates of statistics like the mean and standard deviation are to the true population values, but we we can study the behavior of such estimates across many simulated samples and learn something about how well our estimates do on average, as well the spread of these estimates. 18.5.3 A function to estimate statistics of interest in a random sample First we’re going to write a function called rnorm.stats that carries out the following steps: Take a random sample of size n from a normal distribution with a given mean (mu) and standard deviation (sigma) Calculates the mean and standard deviation of the random sample Return a table giving the sample size, sample mean, and sample standard deviation, represented as a data frame Take a moment to make sure you understand how this function works. rnorm.stats &lt;- function(n, mu, sigma) { the.sample &lt;- rnorm(n, mu, sigma) data_frame(sample.size = n, sample.mean = mean(the.sample), sample.sd = sd(the.sample)) } Let’s test rsample.stats() for a sample of size 30, drawn from a popultion with a mean and standard deviation corresponding to our height exmaple: rnorm.stats(30, true.mean, true.sd) ## # A tibble: 1 x 3 ## sample.size sample.mean sample.sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 176.4 17.28 18.5.4 Generating statistics for many random samples Now we’ll see how to combine rnorm.stats with two additional functions to repeatedly run the rsample.stats function: df.samples.of.30 &lt;- rerun(2500, rnorm.stats(30, true.mean, true.sd)) %&gt;% bind_rows() The function rerun is defined in the purrr library (automatically loaded with tidyverse). purrr:rerun() re-runs an expression(s) multiple times. The first argument to rerun() is the number of times you want to re-run, and the following arguments are the expressions to be re-run. Thus the second line of the code block above re-runs the rnorm.stats function 2500 times, generating sample statistics for samples of size 30 each time it’s run. rerun() returns a list whose length is the specified number of runs. The third line includes a call the dplyr::bind_rows(). This simply takes the list that rerun returns and collapses the list into a single data frame. df.samples.of.30 is thus a data frame in which each row gives the sample size, sample mean, and sample standard deviation for a random sample of 30 individuals drawn from our underlying population with a normally distributed variable. head(df.samples.of.30) ## # A tibble: 6 x 3 ## sample.size sample.mean sample.sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 174.5 12.39 ## 2 30 178.3 15.43 ## 3 30 177.9 13.32 ## 4 30 179.0 14.44 ## 5 30 174.9 12.77 ## 6 30 176.6 16.48 18.6 Simulated sampling distribution of the mean Let’s review what we just did: We generated 2500 samples of size 30 sampled from a population for which the variable of interest has a normal distribution For each of the samples we calculated the mean and standard deviation in that sample We combined each of those estimates of the mean and standard deviation into a data frame The 2500 estimates of the mean we generated represents a new distribution – what we will call a sampling distribution of the mean for samples of size 30. Let’s plot this sampling distribution: ggplot(df.samples.of.30, aes(x = sample.mean, y = ..density..)) + geom_histogram(bins=25, fill = &#39;firebrick&#39;, alpha=0.5) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;, color=&quot;red&quot;) + labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, title = &quot;Distribution of mean heights for 2500 samples of size 30&quot;) 18.6.1 Differences between sampling distributions and sample/population distributions Note that this is not a sample distribution of the variable of interest (“heights”), but rather the distribution of means of the variable of interest (“mean heights”) you would get if you took many random samples (in one sample you’d estimate the mean height as 180cm, in another you’d estimate it as 172 cm, etc). To emphasize this point, let’s compare the simulated sampling distribution of the mean (red histogram) to the population distribution of the variable (grey line) and the distributions of heights in a single sample (blue histogram): ggplot(df.samples.of.30, aes(x = sample.mean, y = ..density..)) + geom_histogram(bins=50, fill = &#39;firebrick&#39;, alpha=0.5) + geom_histogram(data=sample.a, aes(x = height, y = ..density..), bins=11, fill=&#39;steelblue&#39;, alpha=0.25) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;, color=&#39;red&#39;) + geom_line(data=pop.distn, aes(x = height, y = density), alpha=0.25,size=1.5) + xlim(125,225) + labs(x = &quot;height or mean(height) in cm&quot;, y = &quot;Density&quot;, title = &quot;Distribution of mean heights for 2500 samples\\nof size 30 (red) compared to the distribution of a single sample (blue)\\nand the population distribution of heights (gray line)&quot;) 18.6.2 Using sampling distributions to understand the behavior of statistics of interest The particular sampling distribution of the mean, as simulated above, is a probability distribution that we can use to estimate the probability that a sample mean falls within a given interval, assuming our sample is a random sample of size 30 drawn from our underlying population. From our visualization, we see that the distribution of sample mean heights is approximately centered around the true mean height. Most of the sample estimates of mean height are within 5 cm of the true population mean height, but a small number of estimates of the sample mean as off by nearly 10cm. Let’s make this more precise by calculating the mean and standard deviation of the sampling distribution of means: df.samples.of.30 %&gt;% summarize(mean.of.means = mean(sample.mean), sd.of.means = sd(sample.mean)) ## # A tibble: 1 x 2 ## mean.of.means sd.of.means ## &lt;dbl&gt; &lt;dbl&gt; ## 1 175.8 2.738 IMPORTANT! – the values above are estimates of the mean and standard deviation of the sampling distribution of means for samples of size 30, they are not estimates of the mean or standard deviation of the variable of interest (though they are related to these statistics as we’ll see below). 18.6.3 Sampling distributions for different sample sizes In the example above we simulated the sampling distribution of the mean for samples of size 30. How would the sampling distribution change if we increased the sample size? In the next code block we generate sampling distributions of the mean (and standard deviation) for samples of size 50, 100, 250, and 500. df.samples.of.50 &lt;- rerun(2500, rnorm.stats(50, true.mean, true.sd)) %&gt;% bind_rows() df.samples.of.100 &lt;- rerun(2500, rnorm.stats(100, true.mean, true.sd)) %&gt;% bind_rows() df.samples.of.250 &lt;- rerun(2500, rnorm.stats(250, true.mean, true.sd)) %&gt;% bind_rows() df.samples.of.500 &lt;- rerun(2500, rnorm.stats(500, true.mean, true.sd)) %&gt;% bind_rows() To make plotting and comparison easier we will combine each of the individual data frames, representing the different sampling distributions for samples of a given size, into a single data frame. df.combined &lt;- bind_rows(df.samples.of.30, df.samples.of.50, df.samples.of.100, df.samples.of.250, df.samples.of.500) %&gt;% # create a factor version of sample size to facilitate plotting mutate(sample.sz = as.factor(sample.size)) We then plot each of the individual sampling distributions, faceting on sample size. ggplot(df.combined, aes(x = sample.mean, y = ..density.., fill = sample.sz)) + geom_histogram(bins=25, alpha=0.5) + geom_vline(xintercept = true.mean, linetype = &quot;dashed&quot;) + facet_wrap(~ sample.sz, nrow = 1) + scale_fill_brewer(palette=&quot;Set1&quot;) + # change color palette labs(x = &quot;Sample means&quot;, y = &quot;Density&quot;, title = &quot;Distribution of mean heights for samples of varying size&quot;) 18.6.4 Discussion of trends for sampling distributions of different sample sizes The key trend we see when comparing the sampling distributions of the mean for samples of different size is that as the sample size gets larger, the spread of the sampling distribution of the mean becomes narrower around the true mean. This means that as sample size increases, the uncertainty associated with our estimates of the mean decreases. Let’s create a table, grouped by sample size, to help quantify this pattern: sampling.distn.mean.table &lt;- df.combined %&gt;% group_by(sample.size) %&gt;% summarize(mean.of.means = mean(sample.mean), sd.of.means = sd(sample.mean)) sampling.distn.mean.table ## # A tibble: 5 x 3 ## sample.size mean.of.means sd.of.means ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 175.8 2.738 ## 2 50 175.7 2.198 ## 3 100 175.7 1.488 ## 4 250 175.7 0.9698 ## 5 500 175.7 0.6714 18.7 Standard Error of the Mean We see from the graph and table above that our estimates of the mean cluster more tightly about the true mean as our sample size increases. This is obvious when we compare the standard deviation of our mean estimates as a function of sample size. The standard deviation of the sampling distribution of a statistic of interest is called the Standard Error of that statistic. Here, through simulation, we are approximating the Standard Error of the Mean. A well known mathematical results that your already familiar with is that that the expected Standard Error of the Mean can be related to sample size and the standard deviation of the underlying population distribution as follows \\[ \\mbox{Standard Error of Mean} = \\frac{\\sigma}{\\sqrt{n}} \\] where \\(\\sigma\\) is the population standard deviation (i.e. the “true” standard deviation), and \\(n\\) is the sample size. This result for the standard error of the mean is true regardless of the form of the underlying population distribution. Let’s compare that theoretical expectation to our simulated results: se.mean.theory &lt;- sapply(seq(10,500,10), function(n){ true.sd/sqrt(n) }) df.se.mean.theory &lt;- data_frame(sample.size = seq(10,500,10), std.error = se.mean.theory) ggplot(sampling.distn.mean.table, aes(x = sample.size, y = sd.of.means)) + # plot standard errors of mean based on our simulations geom_point() + # plot standard errors of the mean based on theory geom_line(aes(x = sample.size, y = std.error), data = df.se.mean.theory, color=&quot;red&quot;) + labs(x = &quot;Sample size&quot;, y = &quot;Std Error of Mean&quot;, title = &quot;A comparison of theoretical (red line)\\nand simulated (points) estimates of the \\nstandard error of the mean for samples of different size&quot;) We see that as sample sizes increase, the standard error of the mean decreases. This means that as our samples get larger, our uncertainty in our sample estimate of the mean (our best guess for the population mean) gets smaller. 18.8 Sampling Distribution of the Standard Deviation Above we explored how the sampling distribution of the mean changes with sample size. We can similarly explore the sampling distribution of any other statistic, such as the standard deviation, or the median, or the the range, etc. Recall that when we drew random samples we calculated the standard deviation of each of those samples in addition to the mean. We can look at the location and spread of the estimates of the standard deviation: sampling.distn.sd.table &lt;- df.combined %&gt;% group_by(sample.size) %&gt;% summarize(mean.of.sds = mean(sample.sd), sd.of.sds = sd(sample.sd)) sampling.distn.sd.table ## # A tibble: 5 x 3 ## sample.size mean.of.sds sd.of.sds ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 15.08 2.022 ## 2 50 15.11 1.512 ## 3 100 15.17 1.082 ## 4 250 15.16 0.6776 ## 5 500 15.19 0.4758 As we did for the sampling distribution of the mean, we can visualize the sampling distribution of the standard deviation as shown below: ggplot(df.combined, aes(x = sample.sd, y = ..density.., fill = sample.sz)) + geom_histogram(bins=25, alpha=0.5) + geom_vline(xintercept = true.sd, linetype = &quot;dashed&quot;) + facet_wrap(~ sample.sz, nrow = 1) + scale_fill_brewer(palette=&quot;Set1&quot;) + labs(x = &quot;Sample standard deviations&quot;, y = &quot;Density&quot;, title = &quot;Sampling distribution of standard deviation of height for samples of varying size&quot;) The key trend we saw when examining the sampling distribution of the mean is also apparent for standard deviation – bigger samples lead to tighter sampling distributions and hence less uncertainty in the sample estimates of the standard deviation. 18.8.1 Standard error of standard deviations For normally distributed data the expected Standard Error of the Standard Deviation (i.e. the standard deviation of standard deviations!) is approximately: \\[ \\mbox{Standard Error of Standard Deviation} \\approx \\frac{\\sigma}{\\sqrt{2(n-1)}} \\] where \\(\\sigma\\) is the population standard deviation, and \\(n\\) is the sample size. As before, let’s visually compare the theoretical expectation to our simulated estimates. se.sd.theory &lt;- sapply(seq(10, 500, 5), function(n){ true.sd/sqrt(2*(n-1))}) df.se.sd.theory &lt;- data_frame(sample.size = seq(10,500,5), std.error = se.sd.theory) ggplot(sampling.distn.sd.table, aes(x = sample.size, y = sd.of.sds)) + # plot standard errors of mean based on our simulations geom_point() + # plot standard errors of the mean based on theory geom_line(aes(x = sample.size, y = std.error), data = df.se.sd.theory, color=&quot;red&quot;) + labs(x = &quot;Sample size&quot;, y = &quot;Std Error of Standard Deviation&quot;, title = &quot;A comparison of theoretical (red line) and\\nsimulated (points) estimates of\\nthe standard error of the standard deviation\\n for samples of different size&quot;) 18.9 What happens to the sampling distribution of the mean and standard deviation when our sample size is small? We would hope that, regardless of sample size, the sampling distributions of both the mean and standard deviation should be centered around the true population value, \\(\\mu\\) and \\(\\sigma\\) respectively. That seemed to be the case for the modest to large sample sizes we’ve looked at so far (30 to 500 observations). Does this also hold for small samples? Let’s use simulation to explore how well this is expectation is met for small samples. As we’ve done before, we simulate the sampling distribution of the mean and standard deviation for samples of varying size. Because we’re dealing with small samples we’ll use larger simulations (5000 samples) so we get better estiamtes of their behavior (in general, the noisier the process you’re simulating the more simulations you should do) # A new version of rnorm.stats that includes the sample estimate of the # standard error of the mean rnorm.stats.2 &lt;- function(n, mu, sigma) { the.sample &lt;- rnorm(n, mu, sigma) data_frame(sample.size = n, sample.mean = mean(the.sample), sample.sd = sd(the.sample), sample.estimate.SE = sample.sd/sqrt(sample.size)) } # sample sizes we&#39;ll conside ssizes &lt;- c(2, 3, 4, 5, 7, 10, 20, 30, 50) # number of samples to draw *for each sample size* nsamples &lt;- 5000 # create a data frame with empty columns df.combined.small &lt;- data_frame(sample.size = double(), sample.mean = double(), sample.sd = double(), sample.estimate.SE = double()) for (i in ssizes) { df.samples.of.size.i &lt;- rerun(nsamples, rnorm.stats.2(i, true.mean, true.sd)) %&gt;% bind_rows() df.combined.small &lt;- bind_rows(df.combined.small, df.samples.of.size.i) } df.combined.small %&lt;&gt;% mutate(sample.sz = as.factor(sample.size)) 18.9.1 For small samples, sample standard deviations systematically underestimate the population standard deviation Let’s examine how the well centered the sampling distributions of the mean and standard deviation are around their true values, as a function of sample size. First a table summarizing this information: by.sample.size &lt;- df.combined.small %&gt;% group_by(sample.size) %&gt;% summarize(mean.of.means = mean(sample.mean), mean.of.sds = mean(sample.sd)) by.sample.size ## # A tibble: 9 x 3 ## sample.size mean.of.means mean.of.sds ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 175.8 12.18 ## 2 3 175.8 13.53 ## 3 4 175.7 13.91 ## 4 5 175.7 14.33 ## 5 7 175.7 14.52 ## 6 10 175.7 14.70 ## 7 20 175.6 15.04 ## 8 30 175.7 15.06 ## 9 50 175.7 15.11 We see that the sampling distributions of means are well centered around the true mean for both small and medium, and there is no systematic bias one way or the other. By contrast the sampling distribution of standard deviations tends to underestimate the true standard deviation when the samples are small (less than 30 observations). We can visualize this bias as shown here: ggplot(by.sample.size, aes(x = sample.size, y = mean.of.sds)) + geom_point(color = &#39;red&#39;) + geom_line(color = &#39;red&#39;) + geom_hline(yintercept = true.sd, color = &#39;black&#39;, linetype=&#39;dashed&#39;) + labs(x = &quot;Sample Size&quot;, y = &quot;Mean of Sampling Distn of Std Dev&quot;) The source of this bias is clear if we look at the sampling distribution of the standard deviation for samples of size 3, 5, and 30. filtered.df &lt;- df.combined.small %&gt;% filter(sample.size %in% c(3, 5, 30)) ggplot(filtered.df, aes(x = sample.sd, y = ..density.., fill = sample.sz)) + geom_histogram(bins=50, alpha=0.65) + facet_wrap(~sample.size, nrow = 1) + geom_vline(xintercept = true.sd, linetype = &#39;dashed&#39;) + labs(x = &quot;Std Deviations&quot;, y = &quot;Density&quot;, title = &quot;Sampling distributions of the standard deviation\\nas a function of sample size&quot;) There’s very clear indication that the the sampling distribution of standard deviations is not centered around the true value for \\(n=3\\) and for \\(n=5\\), however with samples of size 30 the sampling distribution of the standard deviation appears fairly well centered around the true value of the underlying population. 18.9.2 Underestimates of the standard deviation for small \\(n\\) lead to understimates of the SE of the mean When sample sizes are small, sample estimates of the standard deviation, \\(s_x\\), tend to underestimate the true standard deviation, \\(\\sigma\\). Given this, it follows that sample estimates of the standard error of the mean, \\(SE_{\\overline{x}} = \\frac{s_x}{\\sqrt{n}}\\), must tend to understimate the true standard error of the mean, \\(SE_\\mu = \\frac{\\sigma}{\\sqrt{n}}\\). 18.9.3 The t-distribution is the appropriate distribution for describing the sampling distribution of the mean when \\(n\\) is small The problem associated with estimating the standard error of the mean for small sample sizes was recognized in the early 20th century by William Gosset, an employee at the Guinness Brewing Company. He published a paper, under the pseudonym “Student”, giving the appropriate distribution for describing the standard error of the mean as a function of the sample size \\(n\\). Gosset’s distribution is known as the “t-distribution” or “Student’s t-distribution”. The t-distribution is specified by a single parameter, called degrees of freedom (\\(df\\)) where \\({df} = n - 1\\). As \\(df\\) increases, the t-distribution becomes more and more like the normal such that when \\(n \\geq 30\\) it’s nearly indistinguishable from the standard normal distribution. In the figures below we compare the t-distribution and the standard normal distribution for sample sizes \\(n = {3, 5, 30}\\). x &lt;- seq(-6, 6, length.out = 200) n &lt;- c(3, 5, 30) # sample sizes distns.df &lt;- data_frame(sample.size = double(), z.or.t = double(), norm.density = double(), t.density = double()) for (i in n) { norm.density &lt;- dnorm(x, mean = 0, sd = 1) t.density &lt;- dt(x, df = i - 1) df.temp &lt;- data_frame(sample.size = i, z.or.t = x, norm.density = norm.density, t.density = t.density) distns.df &lt;- bind_rows(distns.df, df.temp) } distns.df %&lt;&gt;% mutate(df = as.factor(sample.size - 1)) ggplot(distns.df, aes(x = z.or.t, y = t.density, color = df)) + geom_line() + geom_line(aes(y = norm.density), color=&#39;black&#39;, linetype=&quot;dotted&quot;) + labs(x = &quot;z or t value&quot;, y = &quot;Probablity density&quot;, title = &quot;Standard normal distribution (black dotted line)\\nversus t-distributions for different degrees of freedom&quot;) US Dept. of Health and Human Services; et al. (August 2016). “Anthropometric Reference Data for Children and Adults: United States, 2011–2014” (PDF). National Health Statistics Reports. 11. https://www.cdc.gov/nchs/data/series/sr_03/sr03_039.pdf↩ "],
["simulating-confidence-intervals.html", "Chapter 19 Simulating confidence intervals 19.1 Confidence intervals 19.2 Generic formulation for confidence intervals 19.3 Example: Confidence intervals for the mean 19.4 A problem arises! 19.5 Confidence intervals under sample estimates of standard errors 19.6 t-distribution for confidence intervals of the mean", " Chapter 19 Simulating confidence intervals Recall the concept of the sampling distribution of a statistic – this is simply the probability distribution of the statistic of interest you would observe if you took a large number of random samples of a given size from a population of interest and calculated that statistic for each of the samples. You learned that the standard deviation of the sampling distribution of a statistic has a special name – the standard error of that statistic. The standard error of a statistic provides a way to quantify the uncertainty of a statistic across random samples. Here we show how to use information about the standard error of a statistic to calculate plausible ranges for a statistic of interest that take into account the uncertainty of our estimates. We call such plausible ranges Confidence Intervals. 19.1 Confidence intervals We know that given a random sample from a population of interest, the value of a statistic of interest is unlikely to be exactly equally to the true population value of that statistic. However, our simulations have taught us a number of things: As sample size increases, the sample estimate of the given statistic is more likely to be close to the true value of that statistic As sample size increases, the standard error of the statistic decreases We will define an “X% percent confidence interval for a statistic of interest”, as an interval (upper and lower bound) that when calculated from a random sample, would include the true population value of the statistic of interest, X% of the time. This quote from the NIST page on confidence intervals, which I’ve adapted to refer to any statistic, helps to make this concrete regarding confidence intervals: As a technical note, a 95% confidence interval does not mean that there is a 95% probability that the interval contains the true [statistic]. The interval computed from a given sample either contains the true [statistic] or it does not. Instead, the level of confidence is associated with the method of calculating the interval … That is, for a 95% confidence interval, if many samples are collected and the confidence interval computed, in the long run about 95% of these intervals would contain the true [statistic]. The idea behind a 95% confidence interval is illustrated in the following figure: Figure 19.1: Point estimates and confidence intervals for a theoretical statistic of interest. 19.2 Generic formulation for confidence intervals We define the \\((100\\times\\beta)\\)% confidence interval for the statistic \\(\\phi\\) as the interval: \\[ CI_\\beta = \\phi_{n} \\pm (z \\times {SE}_{\\phi,n}) \\] Where: \\(\\phi_{n}\\) is the statistic of interest in a random sample of size \\(n\\) \\({SE}_{\\phi,n}\\) is the standard error of the statistic \\(\\phi\\) (via simulation or analytical solution) And the value of \\(z\\) is chosen so that: across many different random samples of size \\(n\\), the true value of the \\(\\phi\\) in the population of interest would fall within the interval approximately \\((100\\times\\beta)\\)% of the time So rather than estimating a single value of \\(\\phi\\) from our data, we will use our observed data plus knowledge about the sampling distribution of \\(\\phi\\) to estimate a range of plausible values for \\(\\phi\\). The size of this interval will be chosen so that if we considered many possible random samples, the true population value of \\(\\phi\\) would be bracketed by the interval in \\((100\\times\\beta)\\)% of the samples. 19.3 Example: Confidence intervals for the mean To make the idea of a confidence interval more concrete, let’s consider confidence intervals for the mean of a normally distributed variable. Recall that if a variable \\(X\\) is normally distributed in a population of interest, \\(X \\sim N(\\mu, \\sigma)\\), then the sampling distribution of the mean of \\(X\\) is also normally distributed with mean \\(\\mu\\), and standard error \\({SE}_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\): \\[ \\overline{X} \\sim N \\left( \\mu, \\frac{\\sigma}{\\sqrt{n}}\\ \\right) \\] In our simulation we will explore how varying the value of \\(z\\) in the formula \\(CI_\\beta = \\phi_{n} \\pm (z \\times {SE}_{\\phi,n})\\) changes the percentage of times that the confidence interval brackets the true population mean. 19.3.1 Simulation of means In our simulation we’re going to generate a large number of samples, and for each sample we will calculate the sample estimate of the mean, and then quantify how much each sample mean differs from the true mean in terms of units of the population standard error of the mean. We’ll then use this information to calibrate the wide of our confidence intervals. For the sake of simplicity we’ll simulate sampling from the “Standard Normal Distribution” – a normal distribution with mean \\(\\mu=0\\), and standard deviation \\(\\sigma=1\\). First we load our standard libraries: library(tidyverse) library(magrittr) library(cowplot) set.seed(20190416) # initialize RNG Then we write our basic framework for our simulations: rnorm.stats &lt;- function(n, mu, sigma) { s &lt;- rnorm(n, mu, sigma) df &lt;- data_frame(sample.size = n, sample.mean = mean(s), sample.sd = sd(s), pop.SE = sigma/sqrt(n)) } And then use this to simulate samples of size 50. true.mean &lt;- 0 true.sd &lt;- 11 n &lt;- 50 samples.50 &lt;- rerun(10000, rnorm.stats(n, true.mean, true.sd)) %&gt;% bind_rows() 19.3.2 Distance between sample means and true means We append a new column to our samples.50 data frame, which is the result of calculating the distance of each sample mean from the true mean, expressed in terms of units of the population standard error of the mean: samples.50 &lt;- samples.50 %&gt;% mutate(z.pop = (sample.mean - true.mean)/pop.SE) Since the sampling distribution of the mean of a normally distributed variable (\\(N(\\mu,\\sigma)\\)) is itself normally distributed (\\(N(\\mu, SE_{\\overline{X}})\\)), then the distribution of \\(z = \\frac{\\overline{X} - \\mu}{SE}\\) is \\(N(0,1)\\). This is illustrated in the figure below where we compare our simulated z-scores to the theoretical expectation: SE &lt;- 1 ggplot(samples.50) + geom_histogram(aes(x = z.pop, y=..density..), bins=50) + stat_function(fun = function(x){dnorm(x, mean=0, sd=SE)}, color=&quot;red&quot;) For a given value of \\(z\\) we can ask what fraction of our simulated means fall within \\(\\pm z\\) standard errors of the true mean. samples.50 %&gt;% summarize(frac.win.1SE = sum(abs(z.pop) &lt;= 1)/n(), frac.win.2SE = sum(abs(z.pop) &lt;= 2)/n()) ## # A tibble: 1 x 2 ## frac.win.1SE frac.win.2SE ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.681 0.957 We see that roughly 68% of our sample means are within 1 SE of the true mean; ~95% are within 2 SEs. If we wanted to get exact multiples of the SE corresponding to different percentiles of the distribution of z-scores, based on the theoretical result (z scores ~ \\(N(0,1)\\)), we can use the qnorm() function: frac.of.interest &lt;- c(0.68, 0.90, 0.95, 0.99) # we use 1 - frac to get left most critical value # we divide by two here two account for area under left and right tails left.critical.value &lt;- qnorm((1 - frac.of.interest)/2, mean = 0, sd=1) data_frame(Percentile = frac.of.interest * 100, Critical.value = abs(left.critical.value)) ## # A tibble: 4 x 2 ## Percentile Critical.value ## &lt;dbl&gt; &lt;dbl&gt; ## 1 68 0.994 ## 2 90 1.64 ## 3 95 1.96 ## 4 99 2.58 19.3.3 Calculating a CI If we knew the standard error of the mean for variable of interest, in order to a confidence interval we could simply look up the corresponding critical value for our percentile of interst in a table like the one above and calculate our CI as: \\[ \\overline{X} \\pm \\text{critical value} \\times SE_{\\overline{X}} \\] For example, we see that the critical value for 95% CIs is ~1.96. 19.4 A problem arises! If you’re a critical reader you should have noticed that calculating confidence intervals using the above formula presumes we know the standard error of the mean for the variable of interest. If we knew the standard deviation, \\(\\sigma\\), of our variable, we could calculate this as \\(SE_{\\overline{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) but in general we do not know \\(\\sigma\\) either. Instead we must estimate the standard error of the mean using our sample standard deviation: \\[ \\widehat{SE}_{\\overline{x}} = \\frac{s_x}{\\sqrt{n}} \\] This introduces another level of uncertainty and also a complication. The complication is due to the fact that for small samples, sample estimates of the standard deviation tend to be biased (smaller) relative to the true population standard deviation (see previous chapter). 19.5 Confidence intervals under sample estimates of standard errors Let’s extend our simulations to gain insight into how using sample estimates of standard errors affect our calculations of confidence interval. First we’ll rewrite our rnorm.stats function to include columns for both the true values of the mean, standard deviation, and standard error of the mean as well as the sample estimates of those statistics. rnorm.stats &lt;- function(n, mu, sigma) { s &lt;- rnorm(n, mu, sigma) df &lt;- data_frame(sample.size = n, true.mean = mu, true.sd = sigma, true.se = sigma/sqrt(n), sample.mean = mean(s), sample.sd = sd(s), estimated.se = sample.sd/sqrt(n)) df } 19.5.1 The spread of sample estimates of the mean in units of true and estimates SEs Now we run simulations for samples of size 5 and 50, and calculate z-scores for the sample means relative to the true mean in terms of units of standard error (both true and estiamted). samples.5 &lt;- rerun(10000, rnorm.stats(5, true.mean, true.sd)) %&gt;% bind_rows() samples.5 &lt;- samples.5 %&gt;% mutate(z.true = (sample.mean - true.mean)/true.se, z.est = (sample.mean - true.mean)/estimated.se) samples.50 &lt;- rerun(10000, rnorm.stats(50, true.mean, true.sd)) %&gt;% bind_rows() samples.50 &lt;- samples.50 %&gt;% mutate(z.true = (sample.mean - true.mean)/true.se, z.est = (sample.mean - true.mean)/estimated.se) First we’ll look at the distribution of z-scores for samples of size 50: samples.50 %&gt;% ggplot() + geom_density(aes(x=z.true), alpha=0.5, color=&#39;blue&#39;, fill=&#39;blue&#39;) + geom_density(aes(x=z.est), alpha=0.25, color=&#39;red&#39;, fill=&#39;red&#39;) + labs(title=&quot;Distribution of z.true(blue) and z.est(red)\\nfor 10,000 samples of size 50&quot;) We see that the distributions are similar whether we used the true SE of the mean, or the estimated SEs. This reflects the fact that the sample estimate of the SE of the mean is a good estimator when sample sizes are relatively large. Now we do the same for samples of size 5: samples.5 %&gt;% ggplot() + geom_density(aes(x=z.true), alpha=0.5, color=&#39;blue&#39;, fill=&#39;blue&#39;) + geom_density(aes(x=z.est), alpha=0.25, color=&#39;red&#39;, fill=&#39;red&#39;) + labs(title=&quot;Distribution of z.true(blue) and z.est(red)\\nfor 10,000 samples of size 5&quot;) + xlim(-10,10) Unlike in the previous case, for samples of sie 5 we find that the distribution of z-scores is significantly wider based on samples estimates of the SE of the mean, rather than the true value of the SE of the mean. This reflects the fact that our sample estimates of the SE of the mean systematically underestimate the true value when sample sizes are small. 19.5.2 Quantifying deviations of sample estimates of the mean To quantify the patterns that is apparent in the figures we generated previously, we can ask, “What fraction of sample means are less than on, two, and three standard errors away from the true mean?” We will ask this question using z-scores based on both the true SE of the mean as well as the estimated SEs of the mean. 19.5.2.1 Samples of size 5 First for samples of size 5, based on the true SEs: samples.5 %&gt;% summarize(less.1SE = sum(abs(z.true) &lt; 1)/n(), less.2SE = sum(abs(z.true) &lt; 2)/n(), less.3SE = sum(abs(z.true) &lt; 3)/n()) ## # A tibble: 1 x 3 ## less.1SE less.2SE less.3SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.683 0.956 0.998 Now for the same samples of size 5, based on the estimated SEs: samples.5 %&gt;% summarize(less.1SE = sum(abs(z.est) &lt; 1)/n(), less.2SE = sum(abs(z.est) &lt; 2)/n(), less.3SE = sum(abs(z.est) &lt; 3)/n()) ## # A tibble: 1 x 3 ## less.1SE less.2SE less.3SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.625 0.887 0.962 Interpretation: For samples of size 5, if we use sample estimates of the standard error of the mean as our “ruler”, we must use an interval of about 3 SE units to capture 95% of the distribution of sample means. Contrast this with the situation in which we knew the true SE; in that case we only had to consider a spread of approximately +/- 2 SE units to capture 95% of the sampling distribution of the mean. 19.5.2.2 Samples of size 50 Now we ask the same questions for samples of size 50: samples.50 %&gt;% summarize(less.1SE = sum(abs(z.true) &lt; 1)/n(), less.2SE = sum(abs(z.true) &lt; 2)/n(), less.3SE = sum(abs(z.true) &lt; 3)/n()) ## # A tibble: 1 x 3 ## less.1SE less.2SE less.3SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.681 0.960 0.997 samples.50 %&gt;% summarize(less.1SE = sum(abs(z.est) &lt; 1)/n(), less.2SE = sum(abs(z.est) &lt; 2)/n(), less.3SE = sum(abs(z.est) &lt; 3)/n()) ## # A tibble: 1 x 3 ## less.1SE less.2SE less.3SE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.676 0.954 0.996 Interpretation: For samples of size 50, we get similar estimats of the spread of the sampling distribution of the mean regardless of whether we use the true SE or the estiamted SEs. 19.6 t-distribution for confidence intervals of the mean The “Student’s t-distribution” is the appropriate distribution to use when we want to estimate the sampling distribution of the mean of a normally distributed variable when samples sizes are small (very frequently) and/or the standard deviation of the population we’re sampling from is unknown (almost always the case). You can think of values of \\(t\\) as multiples of the estimated standard error. The t-distribution is specified by a single parameter, called degrees of freedom (\\(df\\)) where \\({df} = n - 1\\). As \\(df\\) increases, the t-distribution becomes more and more like the “standard normal distribution” (\\(N(0,1)\\). You can use the function dt() to calculate the t-distribution for appropriate degrees of freedom. For example, here we compare the t-distributions with 4 and 49 degrees of freedom, corresponding to samples of size 5 and 50: npts &lt;- 500 df.t5 &lt;- data_frame(sample.size = rep(5, npts), t = seq(-10, 10, length.out = npts), density = dt(t, df=4)) df.t50 &lt;- data_frame(sample.size = rep(50, npts), t = seq(-10, 10, length.out = npts), density = dt(t, df=49)) df.t &lt;- bind_rows(df.t5, df.t50) df.t %&gt;% ggplot(aes(x = t, y = density, color=as.factor(sample.size))) + geom_line() 19.6.1 t-distribution vs standard normal distribution For large degrees of freedom, the t-distribution and the standard normal distribution are very similar. For example, let’s compare these disetributions when df=49: df.norm &lt;- data_frame(t = seq(-10, 10, length.out = npts), density = dnorm(t)) t.vs.normal.df49 &lt;- df.t50 %&gt;% ggplot(aes(x = t, y = density)) + geom_line(color=&#39;red&#39;) + geom_line(data=df.norm, color = &#39;blue&#39;) + labs(title = &quot;Standard normal distribution (blue) vs t(df=49) distn.&quot;) + theme(plot.title = element_text(size = 10)) t.vs.normal.df49 However, when the degrees of freedom is small, the t-distribution and the standard normal distribution differ appreciably. b) Draw a figure comparing the t-distribution for samples of size 5 to the standard normal distribution. Draw another figure comparing the t-distribution for samples of size 50 to the standard normal distribution. Use cowplot to display these two figure as &quot;A&quot; and &quot;B&quot; subfigures. [3 pts] t.vs.normal.df4&lt;- df.t5 %&gt;% ggplot(aes(x = t, y = density)) + geom_line(color=&#39;red&#39;) + geom_line(data=df.norm, color = &#39;blue&#39;) + labs(title = &quot;Standard normal distribution (blue) vs t(df=4) distn.&quot;) + theme(plot.title = element_text(size = 10)) t.vs.normal.df4 19.6.2 Formula for confidence intervals of the mean based on sample estimates of the SE and the t-distribution To calculate the lower and upper bounds of the 95% confidence intervals for the mean based on the t-distribution, for a sample of size \\(n\\), we can use the following formulas: \\[ \\begin{align} \\text{lower bound:}\\ \\overline{x} + t_{0.025}(df = n-1) \\times \\widehat{SE}_\\overline{x} \\\\ \\text{upper bound:}\\ \\overline{x} + t_{0.975}(df = n-1) \\times \\widehat{SE}_\\overline{x} \\end{align} \\] where \\(t_{0.025}(df = n-1)\\) and \\(t_{0.975}(df = n-1)\\) are the 2.5-percentile and 97.5-percentile of the t-distribution with \\(n-1\\) degrees of freedom. "]
]
